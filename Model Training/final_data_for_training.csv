,PiplineProcess
0,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>JFrog Pipelines offers JFrog Platform customers three vital capabilities: end-to-end automation (CI/CD), workflow and tool orchestration, and the optimization of the JFrog toolset functionality in use. Consistent with JFrog’s customer-centric product philosophy, Pipelines is enterprise-ready and universal.
[/INST]### Workflow Automation
A pipeline is an event-driven automated workflow for executing a set of DevOps activities (CI, deployments, infrastructure provisioning, etc). It is composed of a sequence of interdependent **steps** which execute discrete functions. Steps act on **resources** , which hold the information needed to execute (files, key-value pairs, etc).
Developers can create pipelines easily with a simple declarative YAML-based language. While each step in a pipeline executes in a stateless runtime environment, Pipelines provides facilities to manage state and step outputs across the workflow so that all dependent steps can access the information they need from upstream steps in order to execute. This helps coordinate activities centrally across diverse DevOps tools and teams without custom DIY scripts.
Workflows can be configured for a variety of scenarios, including:
  * Continuous Integration for your applications
  * Continuous Delivery workflows that connect all your CI/CD and DevOps activities across tools and functional silos
  * Automate IT Ops workflows like infrastructure provisioning, security patching, and image building


### Get up and running with JFrog Pipelines
In this section, you will find information to get you started whether you are a new user or an existing user.
  * If you do not yet have a subscription, get started with trial subscription of the JFrog Platform on the Cloud.
  * If you are a new user, get started with the onboarding videos for JFrog Pipelines.Onboarding Best Practices: JFrog Pipelines


### Features
#### Pipelines as Code
Define your automated workflow through code, using a domain specific language in a YAML file of key-value pairs that you can create and maintain with your favorite text editor.
#### Real Time Visibility
JFrog Pipelines renders your pipeline definition as an interactive diagram, helping you to see the flow of tasks and their inter-dependencies, as well as view the success record of any runs that were performed.
#### Universal
Connect your pipeline automation to your source code repositories in a version control system (such as GitHub or BitBucket) to automatically trigger execution on any new submission (commit) of a code change. Connect to other popular tools through your credentials for storage, issue-tracking, notification, orchestration and more through a library of integrations.
#### Native Integration with Artifactory
JFrog Pipelines is designed to be used with Artifactory, with built-in directives for pushing artifacts, performing builds, pushing build information, image scanning, and build promotion.
#### Integration with JFrog Platform
JFrog Pipelines is designed as an integral part of the JFrog platform, including scanning artifacts/builds through Xray, the creation and delivery of release bundles through JFrog Distribution, for a complete end-to-end SDLC pipeline from commit to production runtime.
#### Security First
Fine-grained permissions and access control limit who can access workflows. Centralized, encrypted storage of credentials and keys help ensure secrets stay safe.
#### Enterprise-Ready
Manage multiple execution nodes using a single installation of Pipelines and automatically distribute Pipeline execution across them for scale and speed.
### Watch the Screencast

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
1,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Let's explore some of the most common ways to use JFrog Pipelines.
Pipelines is a DevOps automation solution designed for use as part of the JFrog Platform to help make software releases frequent, predictable, and error-free.
Pipelines helps developers and DevOps teams achieve CI/CD by connecting all elements of the JFrog Platform, along with any other DevOps tools you may use, through event-driven, automated workflows.
[/INST]#### Continuous Integration
Continuous Integration (CI) is the software development practice of merging code changes into the main branch as often as possible, then validating by creating a build and running automated tests against the build.
This practice of validating incremental changes as they are made helps assure ongoing functional quality, and avoids the complex challenges that arise from a large merge of accumulated code changes. Continuous Integration workflows help software teams ensure that their changes are built and tested with the latest version of the entire codebase as those changes are made. As a result, most bugs are found almost immediately after the code change is committed, leading to better quality since each bug can be easily isolated to a specific code change and fixed promptly.
The chief objective of a CI worfklow is to produce a build that is ready for delivery to the first stage of evaluation. As part of the JFrog Platform, Pipelines helps assure those builds include all the metadata that Artifactory and XRay enables, so they are fully traceable, searchable, and secure.
A Pipelines CI workflow that fully utilizes the JFrog Platform may perform the following sequence for a containerized build:
  * A new commit is made to a source code repository, such as in GitHub.
  * GitHub notifies Pipelines (through a webhook) of the change.
  * Pipelines commences automatic execution of the CI workflow.
  * The worfklow distributes the steps that create a build to available execution nodes, where each step executes in a runtime container provisioned to meet the needs of the step.
  * All intermediate binaries produced by the steps are stored in Artifactory repositories with build information.
  * If successful, the build produced by the workflow is pushed to a Docker repository in Artifactory, with build information.
  * The completed build is scanned by XRay to identify security vulnerabilities and compliance with license policies.


#### Continuous Delivery
Continous Delivery (CD) is the software operations practice of automating the delivery of builds to selected infrastructure environments such as for development, test, and production.
A CD workflow automates the delivery and validation of completed builds through the various stages for each set of stakeholders in the DevOps process. It helps ensure that code released into production is of high quality, and can greatly speed the update process so that the most reliable software version is always running.
As part of the JFrog Platform, Pipelines also helps teams follow best DevOps practices of build promotion enabled by Artifactory, and the accumulation of metadata as builds pass through each stage of evaluation.
A Pipelines CD workflow that fully utilizes the JFrog Platform may perform the following sequence for a containerized build:
  * A CI workflow in Pipelines produces a new build.
  * Successful completion of the CI workflow triggers execution of the CD workflow.
  * The Pipelines CD workflow steps execute to:
    * Provision a development environment on infrastructure, deploy the build and validate with development-level tests, adding metadata on success.
    * Promote the build to a test stage repository, provision a test environment, run QA tests on the build, adding metadata.
    * Promote the build to a production repository
  * If needed, the Pipelines CD workflow can automatically update production environments with the newly validated build.


#### IT Operations
To support DevOps teams, IT Operations need to automate activities like infrastructure provisioning, image building, and security patching, to avoid configuration drift, ensure predictability and repeatability, and make your software delivery process much more reliable. For example, intermediate environments like Test and Staging should be perfect reproductions of production so you can catch all bugs that might occur in production and easily reproduce them in any environment. Your infrastructure should be provisioned exactly the same way each time to avoid problems due to config errors.
Pipelines helps you automate IT Operations by providing the following functionality:
  * Integrations with popular tools like Terraform, Ansible, Chef, and Puppet to help automate configuration of your environments. You can store your provisioning scripts in your source control repository and any time your scripts change, the environment is updated and this triggers the rest of your DevOps workflow. You can even manage your VPCs and Networking configuration as code.
  * You can easily create a dependency tree of all applications that are deployed into your environments. If there is an update to an environment, applications can automatically be re-deployed if needed.
  * You can easily transfer information like subnet_id, security_group_id to downstream activities. for e.g. EC2 provisioners as part of your workflow. No more manual copy-paste.
  * You can templatize your scripts and inject variables at runtime to make sure the right context is set for each execution.
  * Pipelines offers in-built state, so if you need to store information such as Terraform state files, you don't need to maintain it someplace else. It's stored as part of your workflow and available to any job that needs it.
  * You have a complete history of your config and reproducing a previous config is a one click rollback action.
  * Your infrastructure provisioning can be part of your overall DevOps workflow, so you can implement advanced scenarios like configuring Pipelines to bring up on-demand Test environments when a new version of your application is available. You can also automate triggering your tests automatically and tearing down your test environment if all tests pass.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
2,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Before learning how to use Pipelines, here are some fundamental concepts you will need to be familiar with.  
[/INST]#### Connections
These facilities connect Pipelines to information and services that are not part of the JFrog Platform Deployment but are accessible elsewhere on the network.
##### Integrations
An **Integration** connects Pipelines to an external service/tool. Each integration type defines the endpoint, credentials, and any other configuration detail required for Pipelines to exchange information with the service. All credential information is encrypted and held in secure storage, in conformance with best security practices.
For more information, and a list of all available integration types, see the Pipelines Integrations reference.
##### Pipeline Sources
A **Pipeline Source** is a location in an external repository (such as GitHub or BitBucket) where pipeline configuration files can be found. A pipeline source connects to the repository through an integration.
#### Pipelines
A **pipeline** is an event-driven workflow that you construct using Pipelines DSL, which is based on YAML. The YAML file containing the DSL is called a pipeline configuration (config).
##### Steps
A **Step** is a unit of execution in a pipeline. It is triggered by some event and uses resources to perform an action as part of the pipeline.
For more information, and a list of all available step types, see the Pipelines Steps reference.
##### Resources
 **Resource** is one of the key building blocks of all pipelines. They are information entities that are used for storing and exchanging information across steps and pipelines.
For more information, and a list of all available resource types, see the Pipelines Resources reference.
##### Steplets
The Matrix native step enables your pipeline to repeatedly execute the same set of actions in a variety of configurations and runtime environments, with each variant executing as an independent step, also called **steplet**. These steplets can, when configured, execute in parallel on multiple build nodes. On completion of all steplets, Pipelines aggregates the result status, giving the appearance of a single step.
##### Runs
A **run** is an instance of execution of a pipeline. Pipelines maintains an ordered history of all runs of each pipeline, with an execution log that can be examined through the JFrog Platform.
#### Runtimes
Every step in your pipeline executes on a build node that has been provisioned with a runtime environment. Through Pipelines DSL, you can control which runtimes your steps execute in.
For more information, see Managing Runtimes.
##### Runtime Images
A **runtime image** is a preconfigured Docker container that includes the necessary OS, software tools, packages, and configurations that a step needs to execute.
The JFrog Platform Deployment provides a standard set of runtime images that can be used for most applications. This set includes baseline runtimes with variants to support many commonly used languages. You can also create your own runtime images for specialized needs.
##### Nodes
To run any step in a pipeline, you need a build **node** (virtual machine) that will receive the runtime container where the step will execute.
You must provide nodes and attach them to your JFrog Pipelines project. A node can be on any infrastructure that you choose to use, whether it is from a cloud provider (such as AWS, GCP, or Azure), or on your own infrastructure if your security policies require your operations to remain behind your own firewall.
Nodes can be either static, which are available all the time, or dynamic, which are spun up on-demand through a cloud service.
##### Node Pools
A **node pool** is a convenient way to logically group nodes. This enables you to run steps simultaneously in a pipeline, maintain nodes of different architecture and operating system, pin steps to run on specific node types, and more.
A node pool is assigned a default runtime image. This default is automatically provisioned to its node unless a step overrides this behavior by specifying a different runtime.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
3,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>After you have a Pipelines installation working within the JFrog Platform, you can get started by connecting Pipelines to build nodes, source repositories and other services. After those essentials are in place, you can start coding (using YAML) and running your pipelines.Installing Pipelines
This section introduces you to the essential concepts and provides information about the primary building blocks for creating and running a pipeline.
[/INST]### Note
  * This page is intended to provide a brief overview of all the steps required for executing a pipeline. For detailed information about each step, use the links provided in the **Dive Deeper** sections.
  * If you want to be more hands-on, try our Quickstarts.


#### Administrators and Developers
Pipelines can have two types of users, based on the permissions assigned to them:
  *  **Administrator** : Pipelines does not grant access to all facilities to all users, restricting most configuration facilities to administrator users only. An administrator can add node pools, integrations, and pipeline sources.Introduction to Users and Groups
  *  **Developer** (non-administrator users): A developer can create YAML files, view and use the node pools, integrations and pipeline sources assigned to them, and run their pipelines. To connect Git repositories and other services to Pipelines, a developer will need assistance from an administrator.


#### Creating and Running a Pipeline
### Note
Before you start, an administrator user must perform the required procedures that connect Pipelines to the machines and services that enable a pipeline to run.Introduction to Users and Groups
The main steps for creating and running a pipeline are as follows:
  1. Create a Node Pool
  2. Add Integrations
  3. Create Pipeline DSL
  4. Add a Pipeline Source
  5. Run Pipeline


##### Step 1: Create a Node Pool
 **Who can perform this step?**
 **Administrators only**.
Other users can go to **Application |** **Pipelines | Node Pools** to view the node pools that an administrator has assigned to them.  
 **Description**
To run a pipeline, you must provide Pipelines with machines for steps to execute on. In Pipelines, these machines are called **nodes** , and they are organized into **node pools**.
Pipelines must be configured with at least one node pool that contains at least one node. One node pool is set as the default node pool and available to all users.
You have a variety of choices in how node pools can be configured. Your nodes can be static (a VM in at a fixed IP address) or dynamic (on-demand in a cloud service).  
 **Step(s)**
To add a node pool and nodes, from the **Administration** tab, go to **Pipelines | Node Pools** , and click **Add Node Pool**.  
 **Dive Deeper**
For information about adding a static or dynamic node pool, see Managing Pipelines Node Pools.  
##### Step 2: Add Integrations
 **Who can perform this step?**
 **Administrators only**.
Other users can go to **Application |** **Pipelines | Integrations** to view the integrations that an administrator has assigned to them.  
 **Description**
For Pipelines to connect to other services, such as GitHub, Artifactory, or Kubernetes, **integrations** must be added for those services. The integration must be provided with the URL endpoint for those services and credentials for a user account on that service, along with any other relevant parameters.  
 **Step(s)**
To add the integration, from the **Administration** tab, go to **Pipelines |** **Integrations** , and click **Add an Integration**.  
Here, we add a GitHub Integration – but you can add an integration for the VCS system you prefer to use, whether that's GitHub Enterprise, GitLab, Bitbucket, or Bitbucket Server. For the full list of all the integrations you can add, see Pipelines Integrations.GitHub Integration  
After your integration is successfully added, it is listed among the available integrations.  
 **Dive Deeper**
For more information, see Managing Pipelines Integrations.  
##### Step 3: Create the Pipeline DSL
 **Who can perform this step?**
 **Administrators and Developers**.  
 **Description**
Pipelines are defined using Pipelines DSL, stored in one or more YAML files of key-value pairs, known as a **pipeline config**.
A Pipelines DSL looks like this:
[YAML]
    resources:
      - name: myFirstRepo
        type: GitRepo
        configuration:
          # SCM integration where the repository is located
          gitProvider: {{ .Values.myRepo.gitProvider }} # this will be replaced from values.yml
          # Repository path, including org name/repo name
          path: {{ .Values.myRepo.path }} # this will be replaced from values.yml
          branches:
            # Specifies which branches will trigger dependent steps
            include: master
     
      - name: myPropertyBag
        type: PropertyBag
        configuration:
          commitSha: 1
          runID: 1
     
    pipelines:
      - name: my_first_pipeline
        steps:
          - name: p1_s1
            type: Bash
            configuration:
              inputResources:
                # Sets up step to be triggered when there are commit events to myFirstRepo
                - name: myFirstRepo
            execution:
              onExecute:
                # Data from input resources is available as env variables in the step
                - echo $res_myFirstRepo_commitSha
                # The next two commands add variables to run state, which is available to all downstream steps in this run
                - add_run_variables current_runid=$run_id
                - add_run_variables commitSha=$res_myFirstRepo_commitSha
                # This variable is written to pipeline state in p1_s3.
                # So this will be empty during first run and will be set to prior run number in subsequent runs
                - echo ""Previous run ID is $prev_runid""
     
          - name: p1_s2
            type: Bash
            configuration:
              inputSteps:
                - name: p1_s1
            execution:
              onExecute:
                # Demonstrates the availability of an env variable written to run state during p1_s1
                - echo $current_runid
     
          - name: p1_s3
            type: Bash
            configuration:
              inputSteps:
                - name: p1_s2
              outputResources:
                - name: myPropertyBag
            execution:
              onExecute:
                - echo $current_runid
                # Writes current run number to pipeline state
                - add_pipeline_variables prev_runid=$run_id
                # Uses an utility function to update the output resource with the commitSha that triggered this run
                # Dependent pipelines can be configured to trigger when this resource is updated
                - write_output myPropertyBag commitSha=$commitSha runID=$current_runid
     
      - name: my_second_pipeline
        steps:
          - name: p2_s1
            type: Bash
            configuration:
              inputResources:
                # Sets up step to be triggered when myPropertyBag is updated
                - name: myPropertyBag
            execution:
              onExecute:
                # Retrieves the commitSha from input resource
                - echo ""CommitSha is $res_myPropertyBag_commitSha""
[/YAML]  
 **Step(s)**
Create a directory named .jfrog-pipelines in your source VCS repository, such as Git, and then commit the .yml file to this directory.  
 **Dive Deeper**
For more information, see Defining a Pipeline.  
##### Step 4: Add a Pipeline Source
 **Who can perform this step?**
 **Administrators only**.
Other users can go to **Application |** **Pipelines |** **Pipelines Sources** to view the pipeline sources that an administrator has assigned to them.  
 **Description**
For Pipelines to read and sync the Pipelines DSL from the source VCS repository, you must tell it where to find it by adding a **pipeline source**. This is best performed only after the Pipelines DSL file is checked into the source repository, so that Pipelines can sync the file immediately.  
 **Step(s)**
To add a pipeline source, from the **Administration** tab, go to **Pipelines |** **Pipelines Sources** , and click **Add Pipeline Source**.  
Once the pipeline source is successfully added, Pipelines will sync the file to load the DSL file and create the declared resources and pipelines.  
 **Dive Deeper**
For more information, see Managing Pipeline Sources.  
##### Step 5: Run the Pipeline
 **Who can perform this step?**
 **Administrators and Developers**.  
 **Description**
Trigger either a manual and automatic run of the pipeline.  
 **Step(s)**
### Note
The following are the steps for running a pipeline using the new UI. If you want the perform the same steps using the old UI, see the expandable section below.
  1. To browse pipelines loaded from configured pipelines sources, in the **Application** tab, go to **Pipelines | My Pipelines**.
After your Git repo has been added as a pipeline source, you can see your pipeline in **My Pipelines**.  
  2. Click the name of the pipeline to see a real time, interactive, diagram of the pipeline and the results of its most current run.  
A pipeline can be defined to trigger execution when a new commit is made to the Git repo. You can also execute the pipeline by manually click the **Run** button or triggering the first step.  
Once the pipeline has completed, a new run will be listed.  
  3. Click the row of the run to view the run log for what just executed.  


Steps using old UI
### Note
The following are the steps for running a pipelineusing the old UI. If you want the perform the same steps using the new UI, see the section above.
  1. To browse pipelines loaded from configured pipelines sources, in the **Application** tab, go to **Pipelines | My Pipelines**.
After your Git repo has been added as a pipeline source, you can see your pipeline in **My Pipelines**.  
  2. Click the name of the pipeline to see a real time, interactive, diagram of the pipeline and the results of its most current run.  
A pipeline can be defined to trigger execution when a new commit is made to the Git repo. You can also execute the pipeline by manually triggering the first step.  
Once the pipeline has completed, a new run will be listed.  
  3. Click the row of the run to view the run log for what just executed.  

  
 **Dive Deeper**
  * For more information on triggering pipelines, see Running a Pipeline.
  * For information on examining a pipeline's execution, see Pipeline Run Logs.

  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
4,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Have we piqued your interest? Ready to see some live examples? We have a live environment where you can see some of our more popular pipelines in action.
Click **See it Live** to go directly to a pipeline.  
[/INST]### Pipeline of Pipelines
One of the key features of JFrog Pipelines is the ability to create interconnected pipelines, called a pipeline of pipelines, which can span several different teams, technology platforms, and tools, enabling teams to collaborate on interdependent projects.
 **See it Live**
Pipelines YAML
Documentation  
### Embedded Pipelines
Pipelines enables one step in a parent pipeline to ""embed"" another pipeline and wait for the resulting run to complete before returning the control back to the parent pipeline's step. The parent pipeline then continues its run with access to some additional information on the embedded run.
 **See it Live**
Pipelines YAML
Documentation  
### Conditional Workflows
Conditional workflows enable you to choose if a step executes or skips based on certain conditions set for the previous upstream step. This means, when the workflow reaches a conditional step, it can choose different workflow paths based on the step’s status. This provides more flexibility in the execution logic of a pipeline.
 **Conditional Workflow 1**
 **See it Live**
Pipelines YAML
Documentation  
 **Conditional Workflow 2**
 **See it Live**  
### Approval Gates
Approval Gates enable you to insert a manual approval process for a step in your pipeline. When enabled, the step (and the pipeline run) goes into **Pending Approval** status when upstream steps finish and the run execution reaches that step. The following can be defined as part of this approval process:
  *  **List of approvers** : List of users who can approve or reject the step.
  *  **List of notifications** : Notifications that are sent when the step goes into Pending Approval status.
  *  **Timeout** : The maximum time the step can stay in Pending Approval status.


 **See it Live**
Pipelines YAML
Documentation  
### Matrix Steps
The Matrix native step enables your pipeline to repeatedly execute the same set of actions in a variety of configurations and runtime environments, with each variant executing as an independent “steplet.” These steplets can, when configured, execute in parallel on multiple build nodes. On completion of all steplets, Pipelines aggregates the result status, giving the appearance of a single step.
 **Matrix Single Dimension Pipeline**
 **See it Live**
Pipelines YAML
Documentation  
 **Matrix Two Dimension Pipeline**
 **See it Live**
Pipelines YAML  
### Templates
Pipelines templates enable users to define templates for pipeline definitions and use these templates for creating pipelines.
 **GoCI**
 **See it Live**
Pipelines YAML
Documentation  
 **GradleCI**
 **See it Live**
Pipelines YAML
Documentation  
 **MavenCI**
 **See it Live**
Pipelines YAML
Documentation  
 **NpmCI**
 **See it Live**
Pipelines YAML
Documentation  
 **DockerBuildAndPush**
 **See it Live**
Pipelines YAML
Documentation  
 **Artifact Cleanup**
 **See it Live**
Pipelines YAML
Documentation  
 **Docker Artifact Cleanup**
 **See it Live**
Pipelines YAML
Documentation  
### Tasks
Pipelines Tasks enable developers to simplify and standardize CI/CD pipelines and help them reduce maintenance costs by promoting principles such as reusability, immutability, and versioning.
 **Setup-Go**
 **See it Live**
Pipelines YAML
Documentation  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
5,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>To start creating pipelines, you'll need to learn some important concepts. We'll also provide some simple example pipelines so you can see how they work.
[/INST]### In This Section
  * Pipeline Example: Hello World
  * Pipeline Example: Docker Build and Push
  * Pipeline Example: Release to Edge Node
  * Pipeline Example: Npm Build
  * Pipeline Example: Maven Build
  * Pipeline Example: Go Build
  * Pipeline Example: Helm Blue-Green Deploy



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
6,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This example demonstrates how simple pipelines can be defined and executed with JFrog Pipelines. An example Pipelines DSL is used to show how to use integrations, resources, and steps to construct a simple, automated workflow.
This pipeline example demonstrates the following:
  * Creating a GitHub Integration.
  * Adding a Pipeline Source.
  * Creating a GitRepo trigger, which will trigger a step when the contents of the source control repository change.
  * Using inputResources and inputSteps to set up dependencies between steps and resources.
  * Using environment variables (e.g. $res_myFirstRepo_commitSha) to extract information from `inputResources`.
  * Using run state to pass information to downstream steps of a run.
  * Using pipeline state to pass information to subsequent runs.
  * Connecting dependent pipelines through resources


Successful runs of the pipeline in this quickstart look like this:  
[/INST]#### Before you Begin
Before trying this quickstart, ensure that you have:
  * A GitHub account. This is required for forking the example repository.
  * A JFrog Platform Cloud account, or self-hosted JFrog Pipelines.
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools.
  * A user account in Artifactory with deploy permissions to at least one binary repository.


#### Running this pipeline
Perform the following steps to run this pipeline:
  1. ##### Fork repository
The Pipelines DSL for this example is available in the JFrog GitHub repository repository in the JFrog GitHub account.
The DSL file is a yaml file that contains the pipeline definitions. This example uses two YAML files:
    * `jfrog-pipelines-hello-world.yml`, which contains the declarations for the pipelines in this example
    * `values.yml`, which contains the values required for the `jfrog-pipelines-hello-world.yml` file.
For a full breakup of all the resources, pipelines and steps used in the yml file, see the jfrog-pipelines-hello-world.yml section below. Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as Pipeline Sources or GitRepo resources, in order to add webhooks to these repositories and listen for change events.
  2. ##### Sign in
Sign in to JFrog Platform with your Artifactory credentials.
  3. ##### Add integration
Go to **Administration** | **Pipelines** | **Integrations** to  add one integration:
 **GitHub Integration**: This integration is used to add the Pipeline source, as well as the GitRepo resource defined in  values.yml, to connect Github to Pipelines. Write down the GitHub integration name.
  4. ##### Update values.yml
The pipelines configuration is available in the values.yml file. Edit this file in your fork of this repo and replace the following:
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in the previous step.
`gitProvider: my_github`  
`path`
Provide the path to your fork of this repository.
`path: myuser/jfrog-pipelines-hello-world`  
### Note
 **All pipeline and resource names are global across your JFrog Pipelines project.** The names of your steps and resources need to be unique within the JFrog Pipelines project.
  5. ##### Add pipeline source
The Pipeline Source represents the git repository where our pipelines definition files are stored. A pipeline source connects to the repository through an integration, which we added in step 3.
    1. In your left navigation bar, go to  **Administration** |  **Pipeline** | **Pipeline Sources**. Click on Add a Pipeline Source and then choose From YAML. Follow instructions to add a Pipeline Source. This automatically adds your configuration to the platform and pipelines are created based on your YAML.
    2. After your pipeline source syncs successfully, navigate to  **Pipelines** | **My Pipelines**  in the left navbar to view the newly added pipeline. In this example, `my_first_pipeline` and  `my_second_pipeline` are the names of our pipeline.
    3. Click the name of the pipeline. This renders a real-time, interactive, diagram of the pipeline and the results of its most current run.
  6. ##### Execute the pipeline
You can now commit to the repo to trigger your pipeline, or trigger it manually through the UI. The steps in the pipeline execute in sequence.  
Once the pipeline has completed, a new run is listed.  
Successful run of the first pipeline triggers the execution of the second pipeline:  


#### jfrog-pipelines-hello-world.yml
The `jfrog-pipelines-hello-world` `.yml` file is made up of resources, pipelines and steps, as shown below:
##### Resources
This example uses the following types of resources:
  *  **GitRepo**
  *  **PropertyBag**


 **GitRepo**
A GitReporesource is used to connect JFrog Pipelines to a source control repository. Adding it creates a webhook to the repo so that future commits will automatically create a new version with the webhook payload.
 **Resources**
[YAML]
      - name: myFirstRepo
        type: GitRepo
        configuration:
          # SCM integration where the repository is located
          gitProvider: {{ Values.myRepo.gitProvider }}
          # Repository path, including org name/repo name
          path: {{ Values.myRepo.path }} #manishas-jfrog/jfrog-pipelines-hello-world    # replace with your repository name
          branches:
            # Specifies which branches will trigger dependent steps
            include: master
    
[/YAML]
Tag
 **Description**
Required/Optional  
name
`myFirstRepo` is the name of the GitRepo resource pointing to the repository containing the yaml files and other source code required to build the image.
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
`gitProvider `
The name of the GitHub Integration. Its value is retrieved from the values.yml file.
Required  
`path `
The path of the repository from the integration root. Its value is retrieved from the values.yml file.
Required  
`branches`
  * `include `\-- (optional) Regular expression to include branches from the repo
  * `exclude` \-- (optional) Regular expression to exclude branches from the repo


The `include: master` tag indicates that the GitRepo resource is listening to the master branch.
Optional  
Defining a GitReporesource acts as the trigger for the pipeline.  
 **PropertyBag**
A PropertyBag resource is used to pass information from one pipeline to another, and to provide environment variables to a step in the format of a resource.
A PropertyBag resource can have any strings as properties, which will then be available as environment variables when the key is an input to a step. When it is an output, steps can change the values of properties or add new ones.
 **Resources**
[YAML]
      - name: myPropertyBag
        type: PropertyBag
        configuration:
          commitSha: 1
          runID: 1          
    
[/YAML]
Tag
 **Description**
Required/Optional  
name
`myPropertyBag` is the name of the PropertyBag resource, which is the metadata associated with the build in Artifactory.
Required  
`<string>`
A property for the PropertyBag resource. The tag should be a valid variable name (Bash or PowerShell) for the steps where it is to be an input or output and the value a string. At least one is required, multiple properties are allowed.
Required  
##### Pipelines
This example uses two pipelines:
  * `my_first_pipeline` is the name of the first pipeline, consisting of 3 linear steps. The last step outputs a resource of type PropertyBag.
  * `my_second_pipeline` is the name of the second pipelines, which contains a single step triggered by the PropertyBag resource updated by the first pipeline


##### Steps
Both `my_first_pipeline` and `my_second_pipeline` pipelines contain the following step type:
###### Bash
Bashis a generic step type that enables executing any shell command. This general-purpose step can be used to execute any action that can be scripted, even with tools and services that haven't been integrated with JFrog Pipelines. This is the most versatile of the steps while taking full advantage of what the lifecycle offers.
In our example:
  * `p1_s1`, `p1_s2`, `p1_s3` are the names of the Bash steps in the `my_first_pipeline` pipeline.
  * `p2_s1` is the name of the Bash step in the `my_second_pipeline` pipeline.


The steps are defined so that they execute in an interdependent sequence. This means that each step's execution is triggered by the successful completion of a prior, prerequisite step (or steps). In our example, step 1's (p1_s1) completion triggers the execution of step 2 (p1_s2), completion of step 2 triggers execution of step 3 (p1_s3), and so on until all steps in the pipeline are executed.
 **Steps**
[YAML]
    pipelines:
      - name: my_first_pipeline
        steps:
          - name: p1_s1
            type: Bash
            configuration:
              inputResources:
                # Sets up step to be triggered when there are commit events to myFirstRepo
                - name: myFirstRepo
            execution:
              onExecute:
                # Data from input resources is available as env variables in the step
                - echo $res_myFirstRepo_commitSha
                # The next two commands add variables to run state, which is available to all downstream steps in this run
                
                - add_run_variables current_runid=$run_id
                - add_run_variables commitSha=$res_myFirstRepo_commitSha
                # This variable is written to pipeline state in p1_s3.
                # So this will be empty during first run and will be set to prior run number in subsequent runs
                
                - echo ""Previous run ID is $prev_runid""
    
          - name: p1_s2
            type: Bash
            configuration:
              inputSteps:
                - name: p1_s1
            execution:
              onExecute:
                # Demonstrates the availability of an env variable written to run state during p1_s1
                - echo $current_runid
    
          - name: p1_s3
            type: Bash
            configuration:
              inputSteps:
                - name: p1_s2
              outputResources:
                - name: myPropertyBag
            execution:
              onExecute:
                - echo $current_runid
                # Writes current run number to pipeline state
                - add_pipeline_variables prev_runid=$run_id
                # Uses an utility function to update the output resource with the commitSha that triggered this run
                # Dependent pipelines can be configured to trigger when this resource is updated
                
                - write_output myPropertyBag commitSha=$commitSha runID=$current_runid
    
      - name: my_second_pipeline
        steps:
          - name: p2_s1
            type: Bash
            configuration:
              inputResources:
                # Sets up step to be triggered when myPropertyBag is updated
                - name: myPropertyBag
            execution:
              onExecute:
                # Retrieves the commitSha from input resource
                - echo ""CommitSha is $res_myPropertyBag_commitSha""         
[/YAML]
**configuration**
Specifies all optional configuration selections for the step's execution environment.
Tag
 **Description of usage**
Required/Optional  
`inputResources`
A collection of named resources that will be used by a step as inputs.
In this example:
  * Step `p1_s1`, in the first pipeline, is triggered when there are commit events to `myFirstRepo`, which is the name of the GitRepo resource.
  * Step `p2_s1`, in the second pipeline, is triggered when the `myPropertyBag` resource is updated.


Optional  
`inputSteps`
A collection of named steps whose completion will trigger execution of this step.
In this example:
  * Completion of step p1_s1 triggers the execution of step p1_s2.
and
  * Completion of step p1_s2 triggers the execution of step p1_s3.


Optional  
`outputResources`
A collection of named resources that will be generated or changed by a step.
Optional  
 **execution**
Declare sets of shell command sequences to perform for different execution phases:
Tag
 **Description of usage**
Required/Optional  
`onExecute`
Main commands to execute for the step
Optional  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
7,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This pipeline demonstrates the definition of a pipeline that builds a single Docker Image and pushes it to Artifactory, and then publishes BuildInfo. An example Pipelines DSL is used to show how to use integrations, resources, and steps to construct a simple, automated workflow.
This pipeline demonstrates the following:
  * Creating a GitHub and Artifactory Integration.
  * Adding a Pipeline Source.
  * Creating a GitRepo trigger, which triggers a step when the contents of the source control repository changes.
  * Using an Image resource to add a reference to a Docker image to your pipeline.
  * Using DockerBuild and DockerPush native steps to build and push a docker image.
  * Using the PublishBuildInfo native step to publish a BuildInfo to Artifactory, and the PromoteBuild native step to promote a BuildInfo.
  * Using inputResources and inputSteps to set up dependencies between steps and resources.
  * Using outputResources.


[/INST]### See it Live!
Click here to see this quickstart in action.
A successful run of the pipeline in this quickstart looks like this:  
#### Tutorial Video
#### Before You Begin
Before trying this quickstart, ensure that you have:
  * A GitHub account. This is required for forking the example repository.
  * A JFrog Platform account, or self-hosted JFrog Pipelines.
  * A user account in Artifactory with deploy permissions to at least one binary repository.
  * Create a local Docker repository to set up Artifactory as a Docker Registry.Docker RegistryDocker Registry
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools.


#### Running This Example
Perform the steps below to build and push your Docker image:
  1.  **Fork the Repository**
The Pipelines DSL for this example is available in the jfrog-pipelines-docker-sample repository in the JFrog GitHub account.
The DSL file is a yaml file that contains the pipeline definitions. The example uses a single YAML file, `pipelines.yml`. This file contains the declarations for all resources and workflow steps. For a full breakup of all the resources, pipelines and steps used in the yml file, see the pipelines.ymlsection below.
Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as Pipeline Sources or GitRepo resources, in order to add webhooks to these repositories and listen for change events.
  2.  **Sign in to Artifactory**
Sign in to JFrog Platform with your Artifactory credentials.
  3.  **Add Integrations**
    1. Go to **Administration** **| Pipelines | Integrations** to  add two integrations:
      *  **GitHub Integration**: This integration is used to add the Pipeline source, as well as the GitRepo resource defined in  pipelines.yml.
      *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to push the Docker image to the `docker-local` repository in Artifactory.
    2. Write down the names of both GitHub and Artifactory integrations as these are required for the next step. Ensure that the names are unique and easy to remember.
  4.  **Update pipelines.yml file**
The pipelines configuration is available in the **pipelines.yml** file. Edit this file in your fork of the Git repo and replace the following:
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in the previous step.
`gitProvider: my_github`  
`path`
Provide the path to your fork of this repository.
`path: myuser/repo-name`  
`registry`
Provide the the name of the Artifactory integration you added in the previous step.
`registry: art`  
`imageName`
Provide your Docker image path and name.
`docker.artprod.mycompany.com` `/docker_local/dbp`
Where, docker.artprod.mycompany.com`/docker_local` is the image path and `dbp` is the image name.  
`sourceArtifactory`
Provide your Artifactory integration.
`sourceArtifactory: art`  
### Note
 **All pipeline definitions are global across JFrog Pipelines within a Project.** The names of your pipelines and resources need to be unique within the Project in JFrog Pipelines.
  5.  **Add Pipeline Sources**
The **Pipeline Source** represents the Git repo where our Pipelines definition files are stored. A pipeline source connects to the repository through an integration, which we added in the previous step.
Follow instructions to add a Pipeline Source and point it to the `pipelines.yml` in your fork of the repo. This automatically adds your configuration to the platform and a pipeline is created based on your YAML. The `pipelines.yml` file is parsed and resources, steps, and pipelines are added as configured.
After your pipeline source syncs successfully, navigate to **Pipelines | My Pipelines** in the left navbar to view the newly added pipeline. In this example,`pipeline_dbp` is the name of our pipeline.  
Click the name of the pipeline. Thisrenders a real-time, interactive, diagram of the pipeline and the results of its most current run.
  6.  **Execute the Pipeline**
You can trigger the pipeline by committing a change to your Git repository, or by manually triggering it through the UI. The steps in the pipeline execute in sequence. Multiple steps can execute in parallel if the node pool has multiple build nodes available.  
Once the pipeline has completed, a new run is listed.  


#### pipelines.yml
The `pipelines.yml` file is made up of resources, pipelines and steps, as shown below:
##### Resources
This example uses the following types of resources:
  *  **GitRepo**
  *  **Image**
  *  **BuildInfo**


 **GitRepo**
A GitRepo resource is used to connect JFrog Pipelines to a source control repository. Adding it creates a webhook to the repo so that future commits will automatically create a new version with the webhook payload.
 **resources**
[YAML]
      - name: dbp_repo
        type: GitRepo
        configuration:
          gitProvider: my_github                       # <-- replace with your integration
          path: jfrog/jfrog-pipelines-docker-sample    # <-- replace with your repository name
          branches:
            include: master
    
[/YAML]
Tag
 **Description**
Required/Optional  
name
`dbp_repo` is the name of the GitRepo resource pointing to the repository containing the Dockerfile and other source code required to build the image.
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
`gitProvider `
The name of the GitHub Integration.
Required  
`path `
The path of the repository from the integration root.
Required  
`branches`
  * `include `\-- (optional) Regular expression to include branches from the repo
  * `exclude`\-- (optional) Regular expression to exclude branches from the repo


The `include: master` tag indicates that the GitRepo resource is listening to the master branch.
Optional  
 **Image**
An Image resource is used to add a reference to a Docker image to your pipeline. An Image resource can be used as both input and output. Steps like DockerBuild will generate it as an output and steps like DockerPublish will use it as input to publish to a Docker registry. In our example, it is used as an outputResource for the DockerPush step.
 **resources**
[YAML]
      - name: dbp_image
        type: Image
        configuration:
          registry: art                                # <-- replace with your artifactory integration
          sourceRepository: docker_local               # <-- required if registry is Artifactory
          imageName: docker.artprod.mycompany.com/docker_local/dbp  # < -- replace with your image path and name
          imageTag: latest
          autoPull: true
    
[/YAML]
Tag
 **Description**
Required/Optional  
`name`
`dbp_image` is the name that identifies the resource.
Required  
`registry`
`art` is the name of a  Docker Registry Integration.
Required  
`sourceRepository `
`docker_local` is the name of the repository in Artifactory to which the images will be published.
Required if registry is in JFrog Artifactory  
`imageName`
The file path of the Docker image, of the form imageRepo/imageName.
Required  
`imageTag`
The version tag of the initial version of the Docker image.
Required  
`autoPull `
When **true** , the image is automatically pulled to the machine running the step.
Defaults to **false**.
Optional  
 **BuildInfo**
BuildInfo is automatically created when the DockerBuild step is used to generate packages. BuildInfo is then published to the configured Artifactory repo (`sourceArtifactory: art`) through the PublishBuildInfo step, by providing that resource in the DockerPush step.
 **resources**
[YAML]
      - name: dbp_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: art
          buildName: dbp_build
          buildNumber: 1    
    
      - name: dbp_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: art
          buildName: dbp_build
          buildNumber: 1
[/YAML]
Tag
 **Description**
Required/Optional  
name
`dbp_build_info` and `dbp_promoted_build_info` are the name of the `BuildInfo` resources, which is the metadata associated with the build in Artifactory.
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
`sourceArtifactory`
`art` is the name of the Artifactory Integration.
Required  
buildName
`dbp_build` is the name of the build to associate with the BuildInfo.
Optional  
buildNumber
The number of the build to associate with the BuildInfo.
Optional  
##### Pipelines
`pipeline_dbp`, the pipeline definition for building and pushing the Docker image.
##### Steps
The `pipeline_dbp` pipeline contains the following native steps:
  *  **DockerBuild**
  *  **DockerPush**
  *  **PublishBuildInfo**
  *  **PromoteBuild**


 **DockerBuild**
The DockerBuild native step performs a build to produce a Docker image from a Dockerfile in a Git source repository.
This step builds a Docker image from a GitRepo source repository resource. You must provide in the step configuration the name and directory of the Dockerfile that contains the command to be processed by a `docker build` command, as well as the name and tag of the resulting image. The image is built on the build node, and information about that image is stored in the run state.
 **steps**
[YAML]
          - name: docker_build
            type: DockerBuild
            configuration:
              affinityGroup: dbp_group
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/docker_local/dbp  # replace with your image path and name
              dockerImageTag: ${run_number}
              inputResources:
                - name: dbp_repo
              integrations:
                - name: art                            # <-- replace with your artifactory integration
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
name
`docker_build` is the name that identifies the step.
This is the name that is used when the step is assigned as an input to the next step, `docker_push`.
Required  
`affinityGroup`
Must specify an affinity group string that is the same as specified in a subsequent DockerPush step.
Both DockerBuild and DockerPush steps are assigned to the dbp_group affinityGroup so that they share the same state. If they are not, the output of the DockerBuild step won't be available to the DockerPush step.
Optional  
`dockerFileLocation`
Directory containing the Dockerfile or the file that has Docker build configuration
Required  
`dockerFileName`
Name of the Dockerfile
Required  
`dockerImageName`
The name of the Docker image to create. This can be set using environment variables or triggering a run using parameters.
Required  
`dockerImageTag`
The tag for the Docker image to create. This can be set using environment variables or triggering a run using parameters.
In our example, the tag for the image is set to the pipeline's run number, which is the number of the run currently executing.
Required  
`inputResources`
Must specify:
  * a GitRepo resource (that contains the DockerFile)


This step accepts `dbp_repo` as an inputResource to build the image based on the provided Dockerfile.
Optionally, may also specify:
  * an Image resource of a base Image to include in the built Image
  * a FileSpec resource that specifies what files to include in the built Image. These files are automatically copied to `dockerFileLocation.`


Required/Optional  
`integrations`
Must specify an Artifactory Integration.
Required  
 **DockerPush**
The DockerPush native step pushes the specified Docker Image to Artifactory.
 **steps**
[YAML]
          - name: docker_push
            type: DockerPush
            configuration:
              affinityGroup: dbp_group
              targetRepository: docker-local
              integrations:
                - name: art                            # <-- replace with your artifactory integration
              inputSteps:
                - name: docker_build
              outputResources:
                - name: dbp_image
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
name
`docker_push` is the name that identifies the step.
Required  
`affinityGroup`
Must specify an affinity group string that is the same as specified in the prior DockerBuild step.
Optional  
`targetRepository`
The name of the Docker repository in Artifactory.
Required  
`integrations`
Must specify an Artifactory Integration.
Required  
`inputSteps`
Must specify the named DockerBuild step in the same affinity group.
The output (result) of the previous `docker_build` step is used as an inputStep for this step.
Required  
`outputResources`
May specify an Image resource. If one is specified, the `imageTag` property of that resource will be updated with the `dockerImageTag` of the preceding DockerBuild step.
This step outputs `dbp_image` as an outputResource, which can be used to trigger downstream dependent steps or pipelines if needed.
Optional
May be required  
 **PublishBuildInfo**
The PublishBuildInfo step publishes BuildInfo attached to the image to Artifactory. BuildInfo provides a manifest for the build and includes metadata about the modules, dependencies and other environment variables.
BuildInfo can also be published by any of the language-specific publish steps, when its `autoPublishBuildInfo` tag is set to true.
 **steps**
[YAML]
          - name: publish_dbp_build
            type: PublishBuildInfo
            configuration:
              inputSteps:
                - name: docker_push
              outputResources:
                - name: dbp_build_info
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
name
`publish_dbp_build` is the name that identifies the step.
Required  
`inputSteps`
Must specify the name of the DockerPush step. The output (result) of the previous step is used as an inputStep for this step.
The step must nothave set `autoPublishBuildInfo` to true.
Required  
`outputResources`
Must specify a BuildInfo resource to publish.
Required  
 **PromoteBuild**
The PromoteBuild native step promotes the dbp_build_info BuildInfo and moves or copies the related artifacts from one Artifactory repository to another.
This step requires either a PublishBuildInfo step as previous step or a BuildInfo resource as an input to successfully execute.
 **steps**
[YAML]
          - name: promote_dbp_build
            type: PromoteBuild
            configuration:
              targetRepository: demo-pipelines
              integrations:
                - name: art
              inputResources:
                - name: dbp_build_info
              outputResources:
                - name: dbp_promoted_build_info
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
name
`promote_dbp_build` is the name that identifies the step.
Required  
targetRepository
`demo-pipelines` is the name of the repository in Artifactory to promote the build to.
Required  
`integrations`
Must specify an Artifactory Integration.
Required  
`inputResources`
Must specify a named BuildInfo resource whose `buildName` and `buildNumber` properties identify the build to promote.
Required  
`outputResources`
Must specify a named BuildInfo resource to map to the promoted build. The BuildInfo will be updated with the `buildName` and `buildNumber` of the input BuildInfo resource, and its `targetRepo` with the value of `targetRepository` .
Required  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
8,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This pipeline demonstrates the definition of a simple pipeline that creates and signs a release bundle, and distributes it to an Artifactory edge node. An example Pipelines DSL is used to show how to use integrations, resources, and steps to construct a simple, automated workflow. The pipeline is triggered by any change in any of the BuildInfo resources used to create the release bundle.
This pipeline demonstrates the following:
  * Creating a GitHub Integration, Artifactory Integration, and Distribution Integration.GitHub Integration
  * Adding a Pipeline Source.Add a Pipeline Source
  * Signing the release bundle using the SignReleaseBundle native step.
  * Distributing the release bundle to an edge node using the DistributeReleaseBundle native step.


This pipeline starts from where the Pipeline Example: Docker Build and Push left off. While that pipeline built and pushed a Docker build, this pipeline takes the Buildinfo from that example and creates the release bundle and distributes it to an edge node.
A successful run of the pipeline in this quickstart looks like this:  
[/INST]#### Before you Begin
Before trying this quickstart, ensure that you have:
  * A GitHub account. This is required for forking the example repository.
  * A JFrog Platform account.
  * Set up Artifactory as a Docker registry. For more information, see  Getting Started with Artifactory as a Docker Registry and Docker Registry.Getting Started with Artifactory as a Docker RegistryDocker Registry
  * Installed and configured JFrog Distribution.
  * Generated, uploaded, and deployed GPG Keys for JFrog Distribution.GPG Signing
  * At least one Pipelines node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools.
  * Successfully run the Docker Build and Push pipeline.


#### Running This Example  
Perform the steps below to run this pipeline:
  1.  **Sign in**
Sign in to JFrog Platform with your Artifactory credentials.
  2. Add Integrations
    1. Go to **Application | Pipelines | Integrations** to add these integrations:
      *  **GitHub Integration GitHub Integration**: This integration is used to add the Pipeline source.
`myGithub`is the name of the GitHub integration used in this example.
      *  **Distribution Integration** : This integration connects your JFrog Pipeline Automation platform to a Distribution instance.
`myDist` is the name of the Distribution integration used in this example.
      *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to to get artifacts including Docker images, and maintain build information.
`myArtifactory` is the name of Artifactory integration used in this example.
    2. Write down the names of the Artifactory and Distribution integrations as these are required for the next step. Ensure that the names are unique and easy to remember.
  3.  **Fork GitHub repository**
The Pipelines DSL for this example is available in the jfrog-pipelines-cd-sample repository in the JFrog GitHub account.
The DSL file is a yaml file that contains the pipeline definitions. The example uses two YAML files, `pipelines.yml` and `values.yml`. The pipelines.yml file contains the declarations for all resources and workflow steps. The `values.yml` file contains the values required for the `pipelines.yml` file. For a full breakup of all the resources, pipelines and steps used in the yml file, see the pipelines.ymlsection below.
Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as Pipeline Sources or GitRepo resources, which is required to add webhooks to these repositories and listen for change events.
  4.  **Update values.yml**
The pipelines configuration is available in the **values.yml** file. If required, edit this file in your fork of this repo and replace the following:
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in the previous step.
`gitProvider: myGithub`  
`sourceArtifactory`
Provide your Artifactory integration.
`sourceArtifactory: myArtifactory`  
`sourceDistribution`
Provide your Distribution integration.
` sourceDistribution:` ` myDist`  
### Note
 **All pipeline names are global across your JFrog Pipelines**. The names of your pipelines and resources need to be unique within JFrog Pipelines.
  5.  **Add Pipeline Source**
The **Pipeline Source** represents the Git repo where our Pipelines definition files are stored. A pipeline source connects to the repository through an integration, which was added in the previous step.
    1. In your left navigation bar, go to Administration | Pipelines | Pipeline Sources. Click on Add a Pipeline Source and then choose From YAML. Follow instructions to add a Pipeline Source.
    2. After your Pipeline Source syncs successfully, navigate to **Pipelines** | **My Pipelines** in the left navbar to see the newly added pipeline. In this example, `cd_` `demo_release_mgmt`is the name of our pipeline.
    3. Click the name of the pipeline. This renders a real-time, interactive diagram of the pipeline and the results of its most current run.  
  6.  **Execute the pipeline**
You can now commit to the repo to trigger your pipeline, or trigger it manually through the UI. The steps in the pipeline execute in sequence. Multiple steps can execute in parallel if the node pool has multiple build nodes available.  


#### pipelines.yml
The `pipelines.yml` file is made up of resources, pipelines and steps, as shown below:
##### Resources
This example uses the following types of resources:
  * BuildInfo
  * ReleaseBundle
  * DistributionRule


 **BuildInfo**
A `BuildInfo` resource is the metadata associated with a build in Artifactory.
JFrog Pipelines automatically creates BuildInfo when any of the Build steps like MvnBuild, NpmBuild, GradleBuild, or DockerBuild are used to generate packages. BuildInfo is published to Artifactory through the PublishBuildInfo step, by setting a flag in Build steps or by providing that resource in steps like NpmPublish, GoPublishModule, GoPublishBinary, and DockerPush.
 **resources**
[YAML]
      # Build info of build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
          buildName: dbp_build
          buildNumber: 1  
[/YAML]
Tag
 **Description**
Required/Optional  
name
`gosvc_promoted_build_info` is the name of the `BuildInfo` resource, which is the metadata associated with the build in Artifactory.
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
sourceArtifactory
`myArtifactory` is the name of the Artifactory Integration.
Required  
buildName
`dbp_build` is the name of the build to associate with the BuildInfo.
### Note
`dbp_build` is the name of the Docker build and push pipeline.
Optional  
buildNumber
The number of the build to associate with the BuildInfo.
Optional  
**ReleaseBundle**
A `ReleaseBundle`resource specifies a set of artifacts in Artifactory that are distributed to Artifactory Edge nodes as a JFrog Distribution Release Bundle.Distributing Release Bundles
This resource can be used with CreateReleaseBundle or DistributeReleaseBundle. This resource can be used to represent both signed and unsigned Release Bundle.Distributing Release Bundles
 **resources**
[YAML]
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution:  {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution:  {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
[/YAML]
Tag
 **Description**
Required/Optional  
`name`
`release_bundle` and `signed_bundle` are `ReleaseBundle` resources. While `release_bundle `is the release bundle, `signed_bundle `is the signed version of the release bundle.
Required  
`sourceDistribution`
myDist is the name of the Distribution Integration.
Required  
`version`
Version number of the release bundle.
Required  
**DistributionRule**
A DistributionRule resource is the set of Destination rules that can be applied to distributing a release bundle using JFrog Distribution.Distributing Release Bundles
 **resources**
[YAML]
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution:  {{ .Values.distributionIntegration }}
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""*""  
[/YAML]
Tag
 **Description**
Required/Optional  
`name`
`distribution_rules` is the name of the DistributionRule resource.
Required  
`sourceDistribution`
myDist is the name of the Distribution Integration.
Required  
`serviceName `
The Artifactory Edge Node service name with a wildcard pattern.
Required  
`siteName`
The Artifactory Edge Node site name with a wildcard pattern.
Required  
`cityName`
The Artifactory Edge Node city name with a wildcard pattern.
Required  
`countryCodes`
The Artifactory Edge Node country codes with a wildcard pattern.
Required  
##### Pipelines
`cd_demo_release_mgmt`, the pipeline definition for signing a release bundle and releasing it to an Artifactory edge node.
##### Steps
The `cd_demo_release_mgmt` pipeline is made up of the following native steps:
  * CreateReleaseBundle
  * SignReleaseBundle
  * DistributeReleaseBundle


 **CreateReleaseBundle**
The CreateReleaseBundlenative step produces a Release Bundle for distribution to an Artifactory Edge Node . The step can be used to create a signed or unsigned release bundle.Release Bundle RepositoriesJFrog Artifactory Edge
 **steps**
[YAML]
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: false
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
[/YAML]
Tag
 **Description of usage**
Required/Optional  
`name`
`bundle` is the name that identifies the `CreateReleaseBundle` step.
Required  
`inputResources`
Must specify eithera named BuildInforesource(s)oran Aql resource.
If BuildInfo `inputresources` are provided, the query for the release bundle is constructed using the `buildName`, `buildNumber`, and `targetRepo` of each BuildInfo input.
Required  
`outputResources`
Must specify a ReleaseBundle resource.
The `name`, `version`, and `isSigned` settings in the output ReleaseBundle are updated to the step's `releaseBundleName`, `releaseBundleVersion`, and `sign` values respectively (or any environment variable values that replaced environment variable placeholders for those values).
Required  
`releaseBundleName`
An alphanumeric name for the release bundle. `demo_rb` is the name used in this example.
Required  
`releaseBundleVersion`
Version string for the release bundle
Required  
`dryRun`
When set to `true` parse and validate only to test whether a release bundle version can be created.
Default is true.
Optional  
`sign`
Specifies whether the release bundle version is signed.
Default is false.
Optional  
`storeAtSourceArtifactory`
Controls whether to store release bundle version in the source Artifactorywhile signing.
Optional  
`description`
Description of the release bundle.
Optional  
`releaseNotes`
Describes the release notes for the release bundle version.
`syntax` specifies the format of release notes: `plain_text`, `markdown`, or `asciidoc`. Default is `plain_text`.
`content` is the release notes string in the specified syntax format. Use the `|` character to denote a string preserving newlines.
Optional  
 **SignReleaseBundle**
The SignReleaseBundle native step signs a Release Bundle in preparation for distributing it to Edge nodes.
 **steps**
[YAML]
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
[/YAML]
Tag
 **Description of usage**
Required/Optional  
`name`
`sign` is the name that identifies the `SignReleaseBundle` step.
Required  
`inputResources`
Must specify a ReleaseBundleresource whose `name` and `version` properties identify the release bundle to be signed.
This step accepts `release_bundle`, which is the ReleaseBundle resource, as the inputResource.
Required  
`outputResources`
Must specify a ReleaseBundle resource. This ReleaseBundle is updated with the `name` and `version` properties of the input ReleaseBundle (`release_bundle`), and sets the `isSigned` property to true.
This step specifies `signed_bundle`, which is the other ReleaseBundle resource, as the outputResource.
Required  
 **DistributeReleaseBundle**
The DistributeReleaseBundle native step triggers the distribution of a Release Bundles to an Artifactory Edge Node. This step requires a signed release bundle and one or more distribution rules to successfullyexecute.JFrog Artifactory Edge
 **steps**
[YAML]
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
`name`
distribute is the name that identifies the `DistributeReleaseBundle` step.
Required  
`inputResources`
Must specify a ReleaseBundle resource (signed_bundle) and one DistributionRule resource (distribution_rules) .
Required  
`dryRun`
Controls whether this should be a dry run to test if the release bundle can distribute to the Edge nodes matching the distribution rule.
The default is true.
Optional  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
9,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This pipeline demonstrates the definition of a simple pipeline that builds a Go (GoLang) application and Go module. It shows through an example of Pipelines DSL and shows how to use integrations, resources, and steps to construct a simple, automated workflow.
The pipeline performs the following sequence of tasks:
  * Build a Go ""Hello World"" application using the GoBuild native step
  * Publish the completed Go binary to Artifactory using the GoPublishBinary native step
  * Publish the build info to Artifactory using the PublishBuildInfo native step


[/INST]### See it Live!
Click here to see this quickstart in action.
A successful run of the pipeline in this Quickstart looks like this:  
#### Before you Begin
Before trying this example, ensure that you have:
  * A GitHub account. This is required for forking the sample repository.
  * A JFrog Platform account, or self-hosted JFrog Pipelines.
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools. Please note that if you have a Cloud account, a node pool will already be available as part of your subscription.
  * It is recommended that you run `go mod tidy` command on your local machine to produce the `go.sum` file and then push it to your Git repo.


#### Running This Example
Please follow the steps below to build your Go binary:
  1.  **Fork the repository**
This Pipelines sample is available in the  jfrog-pipelines-go-sample repository in the JFrog GitHub account. The configuration is included in YAML files at the root of the repository:
    * `pipelines.yml`, which contains the declarations for all the resources and steps required to run the pipeline. This configuration is written in template format, so you will not need to change anything in this file.
    * `values.yml`, which contains custom values that will be populated into the template to create your pipeline
Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as in your pipelines, in order to enable us to add webhooks to these repositories and listen for change events.
### Note
The Git repository includes a `go.sum` file. However, it is recommended that you run `go mod tidy` command on your local machine to produce a new `go.sum` file and then push it to your Git repo.
  2.  **Sign in to Artifactory**
Sign in to JFrog Platform with your Artifactory credentials.
  3.  **Create the required Go repositories**
You will need to create the following repositories which will be used in your pipeline configuration:
    1. go-local: A local Go repository where your binary will be publishedGo Registry
    2. go-remote: A remote Go repository that proxies https://proxy.golang.org/Go Registry
    3. go-virtual: A virtual Go repository that aggregates local and remote repositories and is used in your pipeline definition to resolve dependencies. Please ensure that you select your local and remote repositories in the **Repositories** section while creating this virtual repo.Go Registry
  4.  **Add Integrations**
    1. Go to **Administration** **| Pipelines | Integrations** to  add two integrations:
      *  **GitHub Integration**: This integration is used to add the Pipeline source, as well as the GitRepo resource.
      *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to resolve depedencies and to publish the built binary to Artifactory.
    2. Write down the names of both GitHub and Artifactory integrations as these are required for the next step . Ensure that the names are unique and easy to remember.
  5.  **Update pipeline definitions**
Since your `pipelines.yml` config file is templatized, you can just update `values.yml` in your forked repository by following instructions below.
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in the previous step (4).
`gitProvider: myGithubIntegration`  
`repoPath`
Provide the path to your fork of this repository.
`repoPath: myuser/jfrog-pipelines-maven-sample`  
`artifactory`
Provide the name of the Artifactory integration you added in the previous step (4).
`artifactory: myArtifactoryIntegration`  
And that's it. Your configuration is ready to go!
### Note
 **All pipeline definitions are global across JFrog Pipelines within a Project.** The names of your pipelines and resources need to be unique within the Project in JFrog Pipelines.
  6.  **Add Pipeline Sources**
A Pipeline Source represents the git repository where our pipelines definition files are stored. A pipeline source connects to the repository through an  integration, which we added in step 4.
    1. In your left navigation bar, go to **Administration |** **Pipelines | Pipeline Sources**. Click on **Add a Pipeline Source** and then choose **From YAML**. Follow instructions to add a Pipeline Source. This automatically adds your configuration to the platform and pipelines are created based on your YAML.
    2. After your pipeline source syncs successfully, navigate to **Pipelines | My Pipelines** in the left navbar to view the newly added pipeline. In this example, `go_build_pipeline_example` is the names of your pipeline.
    3. Click the name of the pipeline. This renders a real-time, interactive, diagram of the pipeline and the results of its most current run.
  7.  **Execute the Pipeline**
You can trigger the pipeline by committing a change to your repository, or by manually triggering it through the UI.  
  8.  **Success!**
You have successfully executed the sample go application pipeline! You can verify the results by viewing the binary and build created by the pipeline.
Navigate to **Application | Artifactory | Artifacts** and you will find your published binary under the **go-local** repository:  
Navigate to **Application | Artifactory | Builds** to view your published `build -` `go_build_pipeline_example`.  


#### Explanation of pipeline definition
Let us now take a look at the pipeline definition files and what each section means.
The **pipelines.yml** file contains the templatized definition of your pipeline. This consists of the following:
  * Resources are entities that contain information that is consumed or generated by pipeline steps. In our example, we use the following resources:
    * A GitRepo resource pointing to the source control repository where your application code is present. You can configure this resource to trigger dependent steps on specific events.
    * A BuildInfo resource that is a pointer to the Build on Artifactory. This is automatically created by the PublishBuildInfo step.
  * Steps are executable units that form your pipeline. In our example, the pipeline consists of the following steps:
    * A GoBuild native step that builds your Go project. This step is a pre-packaged step (i.e. native step) that is available to use with simple configuration and without the need for custom scripting. Detailed information on GoBuild is available here.
    * A GoPublishBinary native step that publishes your Go binary to Artifactory. This step also does not need custom scripting. Detailed information on GoPublishBinary is available here
    * A PublishBuildInfo step is a native step that gathers build metadata and pushes it to Artifactory. Artifactory Builds provide a manifest and include metadata about included modules, dependencies and other environment variables. Detailed information on PublishBuildInfo is available here.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
10,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This pipeline demonstrates the definition of a simple pipeline that builds and publishes an npm package. An example Pipelines DSL is used to show how to use integrations, resources, and steps to construct a simple, automated workflow.
This example shows the following:
  * Create a GitHub Integration and Artifactory Integration.GitHub Integration
  * Add a Pipeline Source.
  * Build npm components using the NpmBuild native step.
  * Publish the npm component using the NpmPublish native step.


[/INST]### See it Live!
Click here to see this quickstart in action.
A successful run of the pipeline in this quickstart looks like this:  
#### Before you Begin
Before trying this quickstart, ensure that you have:
  * A GitHub account. This is required for forking the example repository.
  * A JFrog Platform account, or self-hosted JFrog Pipelines.
  * Set up Artifactory as an npm Registry. Create three repositories:npm Registry
    * A  remote npm registry. Remote Repositories defined in Artifactory serve as a caching proxy for a registry managed at a remote URL such as `https://registry.npmjs.org`. Artifacts (such as TGZ files) requested from a remote repository are cached on demand.npm RegistryRemote Repositories
    * A local npm registry.
    * A  virtual npm registry. A Virtual Repository defined in Artifactory aggregates packages from both local and remote repositories. This allows you to access both locally hosted npm packages and remote proxied npm registries from a single URL defined for the virtual repository.npm Registry
When adding the virtual npm registry, ensure that both the remote and local npm registries are selected, and the Default Deployment Repository is set to the local npm registry.  
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools.


#### Running This Example
Perform the steps below to build and push your npm image:
  1. ##### Fork the repository
The Pipelines DSL for this example is available in the npm-example repository in the JFrog GitHub account.
The DSL file is a yaml file that contains the pipeline definitions. This example uses two yaml files:
    * `pipelines.yml`, which contains the declarations for all the resources and workflow steps required to run the pipeline.
    * `values.yml`, which contains the values required for the pipelines.yml file.
For a full breakup of all the resources, pipelines and steps used in the yml file, see the pipelines.yml section below.
Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as Pipeline Sources or GitRepo resources, in order to add webhooks to these repositories and listen for change events.
  2. ##### Sign in to Artifactory
Sign in to JFrog Platform with your Artifactory credentials.
  3. ##### Add Integrations
a. Go to **Administration** **| Pipelines | Integrations** to  add two integrations:
    *  **GitHub Integration**: This integration is used to add the Pipeline source, as well as the GitRepo resource defined in  values.yml.
    *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to download npm depedencies from Artifactory, and to pack and upload the built package to Artifactory.
b. Write down the names of both GitHub and Artifactory integrations as these are required for the next step. Ensure that the names are unique and easy to remember.
  4. ##### Update values.yml and pipelines.yml
The pipelines configuration is available in the `values.yml` file. Edit this file in your fork of this repo and replace the following:
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in the previous step.
`gitProvider: my_github`  
`path`
Provide the path to your fork of this repository.
`path: myuser/project-examples`  
Edit the` pipelines.yml` file and replace the following:
Tag
Description
Example  
`sourceArtifactory`
Provide your Artifactory integration.
`sourceArtifactory: art`  
`repositoryName`
Provide the name of the npm repository in Artifactory.
`repositoryName: npm-virtual`  
### Note
 **All pipeline definitions are global across JFrog Pipelines within a Project.** The names of your pipelines and resources need to be unique within the Project in JFrog Pipelines.
  5. ##### Add Pipeline Sources
The **Pipeline Source** represents the git repository where our pipelines definition files are stored. A pipeline source connects to the repository through an integration, which we added in step 3.
In your left navigation bar, go to **Administration |** **Pipelines | Pipeline Sources**. Click on **Add a Pipeline Source** and then choose **From YAML**. Follow instructions to add a Pipeline Source. This automatically adds your configuration to the platform and pipelines are created based on your YAML.
  6. ##### Execute the Pipeline
After your Pipeline Source is synced successfully, navigate to **Pipelines | My Pipelines** in the left navbar to see the newly added pipeline. In this example, `npm_example_pipeline_jfp` is the name of our pipeline. Click the name of the pipeline. This renders a real-time, interactive, diagram of the pipeline and the results of its most current run.  
You can trigger the pipeline by committing a change to your repository, or by manually triggering it through the UI. The steps in the pipeline execute in sequence. Multiple steps can execute in parallel if the node pool has multiple build nodes available.  
Once the pipeline has completed, a new run is listed.  


#### pipelines.yml
The `pipelines.yml` file is made up of resources, pipelines and steps, as shown below:
##### Resources
This example uses the following types of resources:
  * GitRepo
  * BuildInfo


 **GitRepo**
A GitRepo resource is used to connect JFrog Pipelines to a source control repository. Adding it creates a webhook to the repo so that future commits will automatically create a new version with the webhook payload.
 **resources**
[YAML]
      - name: npm_example_repo_jfp
        type: GitRepo
        configuration:
          # SCM integration where the repository is located
          gitProvider: {{ .Values.myRepo.gitProvider }}
          # Repository path, including org name/repo name
          path: {{ .Values.myRepo.path }}
          branches:
            # Specifies which branches will trigger dependent steps
            include: master
[/YAML]
Tag
 **Description**
Required/Optional  
name
`npm_example_repo_jfp` is the name of the GitRepo resource pointing to the repository containing the yaml files and other source code required to build the image.
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
`gitProvider `
The name of the GitHub Integration. Its value is retrieved from the values.yml file.GitHub Integration
Required  
`path `
The path of the repository from the integration root. Its value is retrieved from the values.yml file.
Required  
`branches`
  * `include `\-- (optional) Regular expression to include branches from the repo
  * `exclude`\-- (optional) Regular expression to exclude branches from the repo


The `include: master` tag indicates that the GitRepo resource is listening to the master branch.
Optional  
 **BuildInfo**
BuildInfo is automatically created when the NpmBuild step is used to generate packages. BuildInfo is then published to the configured Artifactory repo (`sourceArtifactory: demoArt`) by providing that resource in the NpmPublish step.
 **resources**
[YAML]
      - name: npm_example_buildinfo_jfp
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
[/YAML]
Tag
 **Description**
Required/Optional  
name
`npm_example_buildinfo_jfp` is the name of the BuildInfo resource, which is the metadata associated with the build in Artifactory..
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
`sourceArtifactory`
The name of the  Artifactory Integration. Its value is retrieved from the values.yml file.
Required  
##### Pipelines
`npm_example_pipeline_jfp` is the name of the pipeline, which contains the steps for running the pipeline.
##### Steps
The `npm_example_pipeline_jfp` pipeline contains the following native steps:
  *  **NpmBuild**
  *  **NpmPublish**


 **NpmBuild**
The NpmBuild native step builds an npm source. This step automatically performs `npm-install` on the source in a Git repository.
 **steps**
[YAML]
          - name: npm_build_step
            type: NpmBuild
            configuration:
              repositoryName: npm-virtual      # required, npm repository name on artifacctory
              sourceLocation: ./npm-example      # required, location of package.json file
              integrations:
                - name:  demoArt  # required
              inputResources:
                - name: npm_example_repo_jfp         # required
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
`name`
`npm_build_step` is the `name` that identifies the step.
This is the name used when the step is assigned as an input to the next step, npm_publish_step.
Required  
`repositoryName`
`npm-virtual` is the name of the npm repository in Artifactory.
Required  
`sourceLocation`
`./npm-example` is the directory containing the `package.json` file, relative to the GitRepo `path`.
Required  
`integrations`
Specifies an Artifactory Integration where modules will be published. If a FileSpec resource is specified in `inputResources` then this is optional. Otherwise, it is required.
May be required  
`inputResources`
Must specify a GitRepo resource. The `npm-install` runs on the Git repository at `sourceLocation`.
This step accepts` npm_example_repo_jfp`, which is the GitRepo resource, as the inputResource.
Required
Optional  
`` **NpmPublish**
The NpmPublish step publishes an npm package to the registry in Artifactory following an NpmBuild step.
 **steps**
[YAML]
          - name: npm_publish_step
            type: NpmPublish
            configuration:
              # for payloadType npm:
              repositoryName: npm-virtual        # required, npm repository name on artifactory
              autoPublishBuildInfo: true       # optional
              integrations:
                - name: demoArt      # required
              inputSteps:
                - name: npm_build_step                # required
              outputResources:
                - name: npm_example_buildinfo_jfp                 # optional
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
`name`
`npm_publish_step` is the `name` that identifies the step.
Required  
`repositoryName`
`npm-virtual` is the name of the npm repository to publish in Artifactory.
Required  
`autoPublishBuildInfo`
When set to true, publishes build info to Artifactory. Default is false.
Once published, the build info can be viewed in Artifactory, in the Build Browser under Builds.
Optional  
`integrations`
Must specify an  Artifactory Integration.
Required  
`inputSteps`
Must specify a named NpmBuild or Bash step.
`npm_build_step` is the name of the NpmBuild step.
Required  
`outputResources`
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set to true.
`npm_example_buildinfo_jfp` is specified as the outputResource.
May be required  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
11,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This quickstart demonstrates a simple pipeline that builds and publishes a Maven package. An example Pipelines DSL is used to show how to use integrations, resources, and a combination of native steps to build an application and publish it to Artifactory.
This tutorial walks you through the following steps to run this application using JFrog Pipelines:
  * Create a GitHub Integration and Artifactory Integration.GitHub Integration
  * Add a Pipeline Source.
  * Build Maven components using the MvnBuild native step.
  * Publish build info using the PublishBuildInfo native step.


[/INST]### See it Live!
Click here to see this quickstart in action.
A successful run of the pipeline in this Quickstart looks like this:  
#### Before you Begin
Before trying this example, ensure that you have:
  * A GitHub account. This is required for forking the sample repository.
  * A JFrog Platform account, or self-hosted JFrog Pipelines.
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see  Managing Pipelines Node Pools.
### Note
If you have a Cloud account, a node pool will already be available as part of your subscription.


#### Running This Example
Perform the steps below to build your Maven artifact:
  1.  **Fork the repository**
This Pipelines sample is available in the  jfrog-pipelines-maven-sample repository in the JFrog GitHub account. The configuration is included in the YAML files at the root of the repository:
    * `pipelines.yml`, which contains the declarations for all the resources and steps required to run the pipeline. This configuration is written in template format, so you will not need to change anything in this file.
    * values.yml, which contains custom values that will be populated into the template to create your pipeline
For a full breakup of all the resources, pipelines, and steps used in the yml file, see the pipeline definition section below.  Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as in your pipelines, to enable us to add webhooks to these repositories and listen for change events.
  2.  **Sign in to Artifactory**
Sign in to JFrog Platform with your Artifactory credentials.
  3.  **Create a local Maven repository**
Create a local Maven repository and write down the repository name, since you will need to use it in your pipeline configuration.
  4.  **Add Integrations**
    1. Go to **Administration** **| Pipelines | Integrations** to  add two integrations:
      *  **GitHub Integration**:: This integration is used to add the Pipeline source, as well as the GitRepo resource.
      *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to download Maven depedencies from Artifactory, and to pack and upload the built package to Artifactory.
    2. Write down the names of both GitHub and Artifactory integrations as these are required for the next step. Ensure that the names are unique and easy to remember.
  5.  **Update pipeline definitions**
Since your pipelines.yml config file is templatized, as shown in the table below, update the values.yml in your forked repository:
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in Step 4.
`gitProvider: my_github`  
`repoPath`
Provide the path to your fork of this repository.
`repoPath: myuser/jfrog-pipelines-maven-sample`  
`artifactory`
Provide the name of the Artifactory integration you added in the previous Step 4.
`artifactory: demoArt`  
`deployerRepo`
Provide the name of the local Maven repository in Artifactory you created in Step 3.
`deployerRepo: maven-local`  
And that's it. Your configuration is ready to go!
### Note
 **All pipeline definitions are global across JFrog Pipelines within a Project.** The names of your pipelines and resources need to be unique within the Project in JFrog Pipelines.
  6.  **Add Pipeline Sources**
The Pipeline Source represents the git repository where our pipelines definition files are stored. A pipeline source connects to the repository through an integration, which we added in Step 4.
    1. In your left navigation bar, go to **Administration | Pipelines | Pipeline Sources**. Click **Add a Pipeline Source** and then choose **From YAML**. Follow instructions to add a Pipeline Source. This automatically adds your configuration to the platform and pipelines are created based on your YAML.
    2. After your pipeline source syncs successfully, navigate to **Pipelines | My Pipelines** in the left navbar to view the newly added pipeline. In this example, `demo_maven` is the names of your pipeline.
    3. Click the name of the pipeline. This renders a real-time, interactive, diagram of the pipeline and the results of its most current run.
  7.  **Execute the Pipeline**
You can trigger the pipeline by committing a change to your repository, or by manually triggering it through the UI. Multiple steps can execute in parallel if the node pool has multiple build nodes available.
Once the pipeline, a new run is listed:  


#### How the Pipeline Definition Works
Let us now take a look at the pipeline definition files and what each section means.
The **pipelines.yml** file contains the templatized definition of your pipeline. This consists of the following:
  * Resources are entities that contain information that is consumed or generated by pipeline steps. In our example, we use the following resources:
    * A GitRepo resource pointing to the source control repository where your application code is present. You can configure this resource to trigger dependent steps on specific events. For more information, see GitRepo.
    * A BuildInfo resource is a pointer to the Build on Artifactory. This is automatically created by the PublishBuildInfo step. For more information, see BuildInfo.
  * Steps are executable units that form your pipeline. In our example, the pipeline consists of the following steps:
    * A MvnBuildnative step that builds your Maven project and optionally deploys it to Artifactory. This step is a pre-packaged step (that is, native step) that is available to be used with simple configuration and without the need for custom scripting. For more information, see MvnBuild.
    * A PublishBuildInfo step is a native step that gathers build metadata and pushes it to Artifactory. Artifactory Builds provide a manifest and include metadata about included modules, dependencies and other environment variables. For more information, see PublishBuildInfo.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
12,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This pipeline demonstrates the definition of a simple pipeline that builds a Docker Image and a Helm Chart and deploys it to a Kubernetes cluster using the Blue/Green strategy. An example Pipelines DSL is used to show how to use integrations, resources, and steps to construct a simple, automated workflow.
The pipeline performs the following sequence of tasks:
  * Build and publish a Docker image using DockerBuild and DockerPush native steps
  * Publish a Helm Chart using the HelmPublish native step
  * Deploy the Helm Chart to Live using the HelmBlueGreenDeploy native step
  * Promote Helm Release to Live role using the HelmBlueGreenRoleSwitch native step
  * Uninstall previous release using the  HelmBlueGreenCleanup native step


A successful run of the pipeline in this Quickstart looks like this:  
[/INST]### See it Live!
Click here to see this quickstart in action.
#### Before you Begin
Before trying this example, ensure that you have:
  * A GitHub account. This is required for forking the sample repository.
  * A JFrog Platform account or self-hosted JFrog Pipelines.
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools.
### Note
If you have a Cloud account, a node pool will already be available as part of your subscription.


#### Running This Example
Follow the steps below to build your Go binary:
  1.  **Fork the repository**
This Pipelines sample is available in the jfrog-pipelines-helm-blue-green-sample repository in the JFrog GitHub account. The configuration is included in YAML files at the root of the repository:
    * `pipelines.yml`, which contains the declarations for all the resources and steps required to run the pipeline. This configuration is written in template format and you do not have to change anything in this file.
    * `values.yml`, which contains custom values that will be populated into the template to create your pipeline.
Fork this repository to your account or organization. This is important since you need admin access to the repositories that are used as in your pipelines to enable Pipelines to add webhooks to these repositories and listen for change events.
  2.  **Sign in to Artifactory**
Sign in to JFrog Platform with your Artifactory credentials.
  3.  **Create the required repositories**
Create the following repositories that will be used in your pipeline configuration:
    *  **docker-local** : A local Docker repository where your image will be published.Docker Registry
    *  **docker-remote** : A remote Docker repository that proxies https://registry-1.docker.io/.Docker Registry
    *  **docker** : A virtual Docker repository that aggregates local and remote repositories and is used in your pipeline definition to resolve dependencies. Ensure that you select your local and remote repositories in the **Repositories** section while creating this virtual repo. Also, ensure that you select your local repository as the **Default Deployment Repository**.Docker Registry
    *  **helm-local** : A local Helm repository where your chart will be published.Kubernetes Helm Chart Repositories
    *  **helm** : A  virtual Helm repository that aggregates local and remote repositories and is used in your pipeline definition to resolve dependencies. Ensure that you select your local repository in the **Repositories** section while creating this virtual repo. Also, ensure that you select your local repository as the **Default Deployment Repository**.Kubernetes Helm Chart Repositories
  4.  **Add Integrations**
    1. Go to **Administration** **| Pipelines | Integrations** to  add three integrations:
      *  **GitHub Integration**: This integration is used to add the Pipeline source, as well as the GitRepo resource.
      *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to resolve dependencies and to publish the built binary to Artifactory.
      *  **Kubernetes Integration** : This integration is used to authenticate with the Kubernetes cluster to deploy the chart and create role Services.
    2. Write down the names of all integrations as these are required for the next step. Ensure that the names are unique and easy to remember.
  5.  **Update pipeline definitions**
Since your `pipelines.yml` config file is templatized, you can just update `values.yml` in your forked repository by following instructions below.
Tag
Description
Example  
`gitRepo.gitProvider`
Provide the name of the Github integration you added in step 4.
`gitProvider: my_github`  
`gitRepo.path`
Provide the path to your fork of this repository.
`path: myuser/jfrog-pipelines-helm-blue-green-sample`  
`gitRepo.branch`
Provide the branch of your fork to use to resolve the source code.
`branch: main`  
`appImage.registry`
Provide the name of the Artifactory integration you added in step 4.
`registry: my_artifactory`  
`appImage.registryUrl`
Provide the url to your Artifactory environment.
`registryUrl: myartifactory.myorg.com`  
`appChart.sourceArtifactory`
Provide the name of the Artifactory integration you added in step 4.
`sourceArtifactory: my_artifactory`  
`runtime.k8s_integration`
Provide the name of the Kubernetes integration you added in step 4.
`k8s_integration: my_k8s`  
`runtime.namespace`
Provide the name of the Kubernetes namespace where the Helm chart will be deployed.
`namespace: my_namespace`  
And that's it. Your configuration is ready to go!
### Note
 **All pipeline definitions are global across JFrog Pipelines within a Project.** The names of your pipelines and resources need to be unique within the Project in JFrog Pipelines.
  6.  **Add Pipeline Sources**
A Pipeline Source represents the git repository where our pipelines definition files are stored. A pipeline source connects to the repository through an  integration, which we added in step 4.
    1. In your left navigation bar, go to **Administration |** **Pipelines | Pipeline Sources**. Click **Add a Pipeline Source** and then choose **From YAML**. Follow instructions to add a Pipeline Source. This automatically adds your configuration to the platform and pipelines are created based on your YAML.
    2. After your pipeline source syncs successfully, navigate to **Pipelines | My Pipelines** in the left navbar to view the newly added pipeline. In this example, `helm_blue_green_pipeline` is the names of your pipeline.
    3. Click the name of the pipeline. This renders a real-time, interactive, diagram of the pipeline and the results of its most current run.
  7.  **Execute the Pipeline**
You can trigger the pipeline by committing a change to your repository, or by manually triggering it through the UI.  
  8.  **Success!**
You have successfully executed the Helm Blue/Green pipeline! You can verify the results by viewing the Services deployed to your Kubernetes cluster.


#### Explanation of Pipeline Definition
Let us now take a look at the pipeline definition files and what each section means.
The **pipelines.yml** file contains the templatized definition of your pipeline. This consists of the following:
  * Resources are entities that contain information that is consumed or generated by pipeline steps. In our example, we use the following resources:
    * A GitRepo resource pointing to the source control repository where your application code is present. You can configure this resource to trigger dependent steps on specific events.
    * An Image resource that adds a reference to a Docker image to your pipeline.
    * A HelmChart resource that adds a reference to a Helm Chart to your pipeline.
  * Steps are executable units that form your pipeline. In our example, the pipeline consists of the following steps:
    * A  DockerBuild native step that builds your Docker image based on a Dockerfile present in the git repo.
    * A DockerPush native step that published the built image to Artifactory.
    * A  HelmPublish native step that packages and publishes your Helm Chart to Artifactory.
    * A HelmBlueGreenDeploy native step that deploys the Helm Chart to your Kubernetes cluster using the Blue/Green deployment strategy. The chart will be deployed using the details of the release playing the Idle role and the configured Idle role Services will be create or updated. Any final stage validation to ensure the new version is ready to be available to users could be added between this step and the next one.
    * A HelmBlueGreenRoleSwitch native step that promotes the deployed Helm release to the Live role in the Blue/Green strategy. The configured Live role Services will be created or updated. Any after release validation to ensure the new release is successful could be added after this step. A copy of this step could be added downstream to rollback the release in case the validation is not successful.
    * A HelmBlueGreenCleanup native step the uninstalls any previously deployed Helm release.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
13,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>After JFrog Pipelines is installed, you must connect it to your external facilities to make them available for your pipelines to load and run.Installing Pipelines
These configuration facilities are only available to administrator users. For all other users, these views show what connections are available to pipelines, but they cannot be changed.Introduction to Users and Groups
[/INST]### Administration Modules
These configuration facilities are available to administrator users from the **Administration** tab:
  * #### Managing Pipelines Integrations
Pipelines **Integrations** provide the mechanisms that enable an external service to inter-operate with Pipelines, as well as securely store the credentials for access to that service. They are necessary for Pipelines to connect to a pipeline source, and for pipelines to access external services through resources and steps. Integrations must be added and managed by an administrator user.
  * #### Managing Pipeline Sources
A **Pipeline Source** represents a source control repository (such as GitHub or BitBucket) where Pipelines definition files can be found. A pipeline source connects to the repository through an integration. Once a pipeline source is added, Pipelines will automatically load all config files from the repository that matches the specified filename filter.
  * #### Managing Pipelines Node Pools
 **Node Pools** provide a convenient way to logically group build nodes in a JFrog Pipelines environment. This enables you to run steps simultaneously on different nodes.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
14,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines **Integrations** provide the mechanisms that enable an external service to inter-operate with Pipelines, as well as securely store the credentials for access to that service. They are necessary for Pipelines to connect to a pipeline source, and for pipelines to access external services through resources and steps. Integrations must be added and managed by an administrator user.
Two types of integrations can be added:
  *  **Pipelines** : These integrations can only be used in pipeline YAML definitions. These integrations can be created in any Project.Introduction to Projects
  *  **Admin** : These integrations can be used while creating dynamic node pools, and adding  extension and template sources. These integrations cannot be used in pipeline YAML definitions and they can only be created in 'All' Projects.Introduction to Projects
The types of integrations that can be added/managed here are:
    *  **Dynamic Nodes** : For creating dynamic nodes in a dynamic node pool. These are build nodes that are spun up and down on-demand by Pipelines from a ( AWS, Google Cloud, Azure Keys) or  Kubernetes service.
    *  **Source Control** : For Pipelines extensions and template source repositories. For example: GitHub, Bitbucket, or others.


[/INST]### Note
Administrator users can create, view and edit integrations. Non-administrator users can only view integrations.
#### Administering Integrations
The following actions can only be performed by administrator users to configure Pipelines for operation.Introduction to Users and Groups
##### Adding an Integration
To add an integration:
  1. Go to **Administration | Pipelines | Integrations** and click **Add an Integration**.  
  2. In the resulting **Add New Integration** display, depending on the type of integration you need, click either Pipelines or Admin.
  3. Click the _Integration Type_ field and select one from the dropdown list. For example, a GitHub Integration enables a pipeline to assign source control repositories in GitHub to GitRepo resources.  
  4. After the integration type is selected, enter the remaining settings as presented. For information about each integration, see Pipelines Integrations.
  5. For the following integrations, you have the option of testing the integration while creating/editing it. The test validates all the details you have entered, including the URL, user credentials, and the token for sufficient permissions on the repository.
    * Artifactory
    * BitBucket
    * BitBucket Server
    * Distribution
    * GitHub
    * GitHub Enterprise
    * GitLab
To test an integration, enter all the relevant details and click **Test Connection**. The details entered for the integration is validated and an appropriate message for success or failure is displayed.  
  6. Click **Create** to create the integration.


The integration you created appears on the Integrations page.
### Note
These integrations also appear on the **Application | Pipelines | Integrations** page.  
##### Restricting Integration Use
If an integration is in a Project, one or more environments maybe specified for the integration and that integration is available to resources that are part of the pipeline sources that share at least one of those environments. By default, integrations are in the **DEV** environment.
Integrations that are not in any project may only be used by pipeline sources that are not in a project. By default, these integrations are available to resources in all pipeline sources that are not in a project.
You can establish those usage restrictions when you add or modify an integration.
  1. Uncheck the option to **Any Pipeline Source**.
  2. In the revealed interface, select from the list of available pipeline sources and move the row(s) (by click-and-drag or arrow button) to the list of included pipeline sources.  
  3. When finished, click **Create** (if adding an integration) or **Save** (if editing).


##### Viewing Integrations
To view the list of integrations already added and available for use, go to either **Application | Pipelines | Integrations** or **Administration | Pipelines | Integrations**  
The Integrations page displays the list of integrations that have been installed and are available to you to use according to your Jfrog platform user credentials. Integrations that are restricted to use by certain pipeline sources will not be shown unless your user account has been granted permissions for those pipeline sources.Introduction to Permissions
Each row of the integrations list includes the following:
Property
Description  
Integration Name
The friendly name of the integration  
Type
The integration type  
Usage
Shows where the integration can be used:
  *  **Pipeline** : Can be used in pipeline YAML definitions
  *  **Admin** : Can be used while creating dynamic node pools, extension sources or template sources.

  
Created By
The user that created the integration  
Updated By
The user that last edited the integration settings  
Updated At
The time and date of the last edit  
Click on the **Name** of the integration to view its settings. If the integration is for dynamic nodes, it will show which node pools use it.  
##### Modifying an Integration
You can modify (edit) an integration by hovering over its row in the list, and clicking the **Edit** button.  
Alternately, you can view the integration, then click **Edit**.
Once you complete your permitted changes, click **Save**.
##### Deleting an Integration
You can delete an integration by hovering over its row in the list, and clicking the **Delete** button.  
Alternately, you can view the integration, then click **Delete**.
##### Sharing Integrations across Multiple Projects
Add a New Project Admins can share integrations across multiple projects to allow members in more than one project to use them. A classic use case for sharing an Integration is the need to share a source repository's credentials with multiple projects. An integration can be shared with a single project or across multiple projects. After an integration is shared, it is added to the target projects and displayed on the integrations page.Add a New Project
 **Shared Integrations Rules and Guidelines**
  * An Integration can be shared only after it has been created.
  * An Integration can be shared only if it has a globally unique name.
  * Only the owner of the shared Integration can delete the shared Integration.


To share an Integration:
  1. In the **Administration** module, select Integrations and navigate to **[Project_Name] | Integration** and click the Share icon.  
  2. You can share the selected Integration:
    * Across all the projects, by selecting the **Auto share with all projects** field.
    * Only with specific projects, by selecting the name of the Project from the projects list.  
  3. Click **Save**.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
15,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A **Pipeline Source** represents a source control repository (such as GitHub or BitBucket) where Pipelines definition files can be found. A pipeline source connects to the repository through an integration. After a pipeline source is added, Pipelines automatically loads all config files from the repository that matches the specified filename filter.
[/INST]### Note
Administrator users can create, view, edit and sync pipeline sources. Non-administrator users can only view and sync pipeline sources.
#### Administering Pipeline Sources
Only users with administrator privileges can manage pipeline sources.Introduction to Users and Groups
##### Requirements
To successfully add a pipeline source, ensure the following:
  * The credentials used in the source control integration should have admin access to the specified source control repository. Admin privileges are used to add a webhook that allows Pipelines to be notified of changes in the repository.
  *  **Pipelines 1.26.0 and lower** : The repository path is valid. To know the exact path to specify, inspect the git clone URL of that repository in your source control system and copy the value.
Here are some examples:
    *  **GitHub / GitHub Enterprise** :  https://github.com/myuser/basic-pipeline.git \- use _myuser/basic-pipeline_
    *  **Bitbucket Server (Private repository)** :  https://git.mycompany.com/scm/project-id/repo-name.git \- use _project-id/repo-name_
    *  **Bitbucket Server (Individual repository)** :  https://git.mycompany.com/scm/~myuser/myfirstrepo.git \- use _~myuser/myfirstrepo_
    *  **Bitbucket** : https://username@bitbucket.org/teamspace/test-repo.git \- use _teamspace/test-repo_
    *  **GitLab** : https://gitlab.com/user1/repo1.git \- use _user1/repo1_
  * Branch name is valid and the credential used in the Git integration for the pipeline source has access to the branch.
  * Add an integration for the source control system where your pipeline file repository is (or will be) located. The integration can be one of these source control system providers:
    * GitHub Integration
    * GitHub Enterprise Integration
    * GitLab Integration
    * Bitbucket Integration
    * Bitbucket Server Integration


##### Adding a Pipeline Source (1.31.0 and higher)
To add a source control repository as a Pipeline Source:
  1. Go to **Administration | Pipelines | Pipeline Sources**.
  2. In the resulting **Pipeline Sources** display, click **Add Pipelines Source** and click **From YAML**.
  3. In the **Add YAML Pipeline Source** page, click one of the following:
    *  **Single Branch**
    *  **Multi Branch**
  4. Click to select the protocol type to be used for cloning the repository when the pipeline source is synced:
    *  **SSH**
    *  **HTTPS**
  5. Complete the **Single/Multi Branch** form:
Field
Description  
 **Name**
Enter a unique name for the pipeline source.  
 **SCM Provider Integration**
Click the _SCM Provider Integration_ field and select your source control integration from the dropdown list. Only integrations that are compatible for use as a Pipeline Source will be included in the dropdown list.  
 **Repository Full Name**
Based on the information you have provided for the selected integration, such as API token, all the relevant repositories are listed in the _Repository Full Name_ field. Select the path of the repository where your pipeline files are stored. If the name of the repository is not auto-fetched, enter the full name of the repository where your pipeline files are stored.  
 **Branch (Single Branch only)**
Based on the SCM provider and Repository Full Name you have provided, all the available branches are automatically fetched (for example, `main`). Select the required branch. If the name of the branch is not auto-fetched, enter the name of the branch.  
 **Exclude (Multi Branch only)**
Specify the _Exclude Branch Pattern_ as a regular expression for the matching branch names to exclude.  
 **Include (Multi Branch only)**
Specify the _Include Branch Pattern_ as a regular expression for the matching branch names to include.  
 **Sync all branches** toggle
 **(Multi Branch only)**
When a new multi branch pipeline source is added, only the default branched is synced. To sync all the branches based on the Exclude/Include pattern, click the **Sync all branches** toggle. If Exclude/Include pattern is not provided, all branches are synced.
In addition, when a new commit is pushed to thematching branch, that particular branch is synced.  
 **Folder Name**
Provide the name of the directory where the YAML config is available.
    * To use **Folder Name** , in the SCM repository, place your YAML files in a directory named .`jfrog-pipelines`. This directory can be the root or one level below the root. Directories beyond this level are not supported.
    * Following are the possible directory structures for monorepos:
`- root `
`------ .jfrog-pipelines `
Or
`- root`
`---- .jfrog-pipelines`
`-------- service1`
`-------- service2`
Or
`- root `
`---- service1`
`--------- .jfrog-pipelines`
`---- service2`
`--------- .jfrog-pipelines`
    * The .`jfrog-pipelines`directory can contain any number of YAML files.
    * To parse all YAML files in the root directory, enter ""."". However, other pipeline sources cannot point to this repository.
    * You can create multiple pipeline sources pointing to the same SCM repository as long as the directory names are different (and none of them point to root).  
 **Recommended Directory Structure for using Folder Name**
Here are some examples of the supported directory structure:
YAML Files Location
Folder Name Path  
YAML files are in the `.jfrog-pipelines` root directory
Enter `.` (dot) or `.jfrog-pipelines` in the **Folder Name** field to fetch all the YAML files  
YAML files are in the `.jfrog-pipelines/project1` directory
Enter`.jfrog-pipelines/project1` in the **Folder Name** field to fetch the YAML files in the `.jfrog-pipelines/project1` directory  
YAML files are in the following directories:
    * .jfrog-pipelines
    * .jfrog-pipelines/project1
    * .jfrog-pipelines/project2
Enter `.` (dot) or `.jfrog-pipelines` in the **Folder Name** field to fetch all the YAML files in all directories
or
Enter `.jfrog-pipelines/project1` in the **Folder Name** field to fetch the YAML files in the `.jfrog-pipelines/project1` directory
Enter `.jfrog-pipelines/project2` in the **Folder Name** field to fetch the YAML files in the `.jfrog-pipelines/project2` directory  
YAML files are in the following directories:
    * .jfrog-pipelines
    * `service1`/.jfrog-pipelines
    * `service2`/.jfrog-pipelines
Enter `.` (dot) or `.jfrog-pipelines` in the **Folder Name** field to fetch the YAML files in the `.jfrog-pipelines` directory, but YAML files in the `service1` `/.jfrog-pipelines` and `service2` `/.jfrog-pipelines` directories will not be included.
or
Enter `service1` `/.jfrog-pipelines` in the **Folder Name** field to fetch specific YAML files in the `service1` `/.jfrog-pipelines` directory
Enter `service2` `/.jfrog-pipelines` in the **Folder Name** field to fetch specific YAML files in the `service2` `/.jfrog-pipelines` directory  
If you have a monorepo with multiple services within a single repo, a directory structure such as the following is recommended:
Existing Directory Structure
Recommended Directory Structure  
Root
Build/ci
-Service1
-Build/ci/pipe.yaml
-Service2
-Build/ci/pipe.yaml
Root
.jfrog-pipelines
-Service1/pipe.yaml
-Service2/pipe.yaml  
 **Local Artifactory Token Permissions**
When a pipeline source is added to a custom Project, all pipelines associated with this pipeline source will have access to a local Artifactory integration by default. This integration enables you to connect to your local Artifactory instance to push artifacts, without having to explicitly create an Artifactory integration. However, unlike the custom Artifactory integration, which can be used to connect to any Artifactory instance, the local Artifactory integration can only connect to your local Artifactory instance. In addition, while the custom Artifactory integration uses API key for authentication and permissions, the local Artifactory integration uses scoped token.
### Important
Local Artifactory integration has the following limitations:
    * It can only be used in custom Projects. The default Project does not support this integration.
    * It can only be used with repositories created within custom Projects.
    * It can only be used in Bash steps.
###### Permissions
Read permission is enabled by default for all local Artifactory integrations. When required, you can add the following additional permissions for your pipeline source. All pipelines associated with that pipeline source will then use these permissions:
    *  **Read** : Download artifacts (available by default)
    *  **Write** : Upload or update artifacts
    *  **Delete** : Delete artifacts
    *  **Manage** : Change the permission settings for other users  
###### Usage in Pipelines YAML
The local Artifactory integration can be added as an input to any Bash step. To do this, use `{{ .jfrog-pipelines.localArtifactory }}` as a placeholder for the integration. It can be used at both pipeline-level and step-level.
When you add this integration directly to a Bash step, the `JFROG_LOCAL_ARTIFACTORY_TOKEN` environment variable is automatically made available to be used at runtime. This environment variable creates a scoped token with read permissions. When the pipeline YAML is added as a pipeline source, the permissions for the token can be elevated to Write, Delete, and/or Manage. If the pipeline source includes more than one pipeline, the new permissions will be applicable for all the pipelines in the source.
[YAML]    resources:
      - name: myRepo_native
        type: GitRepo
        configuration:
          path: jfrog/jfrog_pipelines
          gitProvider: myGithub
          branches:
            include: master
    
    pipelines:
      - name: myPipeline_native
        steps:
          - name: first_step
            type: Bash
            configuration:
              integrations:
                - name: **{{.jfrog-pipelines.localArtifactory}}**
            execution:
              onExecute:
                - echo ""first_step""
                - jfrog config add jftest-$step_id --url https://pipelines.jfrog.io --access-token **${JFROG_LOCAL_ARTIFACTORY_TOKEN}**
                - touch test.txt
                - echo ""hello"" > test.txt
                - jfrog rt upload test.txt demo-sampleRepo/test.txt --server-id jftest-$step_id
          - name: second_step
            type: Bash   
            configuration:
              inputSteps:
                - name: first_step
            execution:
              onExecute:
                - echo ""second_step""
                - echo "" **$JFROG_LOCAL_ARTIFACTORY_TOKEN** ""
[/YAML]  


Adding a Pipeline Source (1.30.0 and lower)
To add a source control repository as a Pipeline Source:
  1. Go to **Administration | Pipelines | Pipeline Sources**.
  2. In the resulting **Pipeline Sources** display, click **Add Pipeline Source** and click one of the following:
    *  **From YAML** : Add the pipeline source from a YAML configuration file, which contains the declarations of all resources and workflow steps.
    *  **From Template** : Add the pipeline source from a pre-defined template by providing a values file. For more information, see Pipelines Templates.  


###### From YAML
  1. In the **Add YAML Pipeline Source** page, click one of the following:
    *  **Single Branch**
    *  **Multi Branch**
  2. Click to select the protocol type to be used for cloning the repository when the pipeline source is synced:
    *  **SSH**
    *  **HTTPS**
  3. Complete the **Single/Multi Branch** form:
Field
Description  
 **Name**
Enter a unique name for the pipeline source.  
 **SCM Provider Integration**
Click the _SCM Provider Integration_ field and select your source control integration from the dropdown list. Only integrations that are compatible for use as a Pipeline Source will be included in the dropdown list.  
 **Repository Full Name**
Based on the information you have provided for the selected integration, such as API token, all the relevant repositories are listed in the _Repository Full Name_ field. Select the path of the repository where your pipeline files are stored. If the name of the repository is not auto-fetched, enter the full name of the repository where your pipeline files are stored.  
 **Branch (Single Branch only)**
Based on the SCM provider and Repository Full Name you have provided, all the available branches are automatically fetched (for example, `main`). Select the required branch. If the name of the branch is not auto-fetched, enter the name of the branch.  
 **Exclude (Multi Branch only)**
Specify the _Exclude Branch Pattern_ as a regular expression for the matching branch names to exclude.  
 **Include (Multi Branch only)**
Specify the _Include Branch Pattern_ as a regular expression for the matching branch names to include.  
 **Sync all branches** toggle
 **(Multi Branch only)**
When a new multi branch pipeline source is added, only the default branched is synced. To sync all the branches based on the Exclude/Include pattern, click the **Sync all branches** toggle. If Exclude/Include pattern is not provided, all branches are synced.
In addition, when a new commit is pushed to thematching branch, that particular branch is synced.  
 **Folder Name** (Pipelines 1.27.0 and higher)
Provide the name of the directory where the YAML config is available.
    * To use **Folder Name** , in the SCM repository, place your YAML files in a directory named .`jfrog-pipelines`. This directory can be the root or one level below the root. Directories beyond this level are not supported.
    * Following are the possible directory structures for monorepos:
`- root `
`------ .jfrog-pipelines `
Or
`- root`
`---- .jfrog-pipelines`
`-------- service1`
`-------- service2`
Or
`- root `
`---- service1`
`--------- .jfrog-pipelines`
`---- service2`
`--------- .jfrog-pipelines`
    * The .`jfrog-pipelines`directory can contain any number of YAML files.
    * To parse all YAML files in the root directory, enter ""."". However, other pipeline sources cannot point to this repository.
    * You can create multiple pipeline sources pointing to the same SCM repository as long as the directory names are different (and none of them point to root).  
 **Recommended Directory Structure for using Folder Name**
Here are some examples of the supported directory structure:
YAML Files Location
Folder Name Path  
YAML files are in the `.jfrog-pipelines` root directory
Enter `.` (dot) or `.jfrog-pipelines` in the **Folder Name** field to fetch all the YAML files  
YAML files are in the `.jfrog-pipelines/project1` directory
Enter`.jfrog-pipelines/project1`in the **Folder Name** field to fetch the YAML files in the`.jfrog-pipelines/project1`directory  
YAML files are in the following directories:
    * .jfrog-pipelines
    * .jfrog-pipelines/project1
    * .jfrog-pipelines/project2
Enter `.` (dot) or `.jfrog-pipelines` in the **Folder Name** field to fetch all the YAML files in all directories
or
Enter `.jfrog-pipelines/project1` in the **Folder Name** field to fetch the YAML files in the `.jfrog-pipelines/project1` directory
Enter `.jfrog-pipelines/project2` in the **Folder Name** field to fetch the YAML files in the `.jfrog-pipelines/project2` directory  
YAML files are in the following directories:
    * .jfrog-pipelines
    * `service1`/.jfrog-pipelines
    * `service2`/.jfrog-pipelines
Enter `.` (dot) or `.jfrog-pipelines` in the **Folder Name** field to fetch the YAML files in the `.jfrog-pipelines` directory, but YAML files in the `service1` `/.jfrog-pipelines` and `service2` `/.jfrog-pipelines` directories will not be included.
or
Enter `service1` `/.jfrog-pipelines` in the **Folder Name** field to fetch specific YAML files in the `service1` `/.jfrog-pipelines` directory
Enter `service2` `/.jfrog-pipelines` in the **Folder Name** field to fetch specific YAML files in the `service2` `/.jfrog-pipelines` directory  
If you have a monorepo with multiple services within a single repo, a directory structure such as the following is recommended:
Existing Directory Structure
Recommended Directory Structure  
Root
Build/ci
-Service1
-Build/ci/pipe.yaml
-Service2
-Build/ci/pipe.yaml
Root
.jfrog-pipelines
-Service1/pipe.yaml
-Service2/pipe.yaml  
 **Migrating from Config File Filter to the New Folder Structure**
In Pipelines version 1.27.0, **Config File Filter** has been replaced with a new field called **Folder Name** for adding your pipeline sources. Though **Config File Filter** is still supported and available while editing a pipeline source.
    *  **This change does not affect pipeline sources added before 1.26.0**. However, it is highly recommended that you use the recommended folder structure for new pipeline sources and move your existing sources to the recommended folder structure. Using the folder structure significantly improves the performance of your pipeline sync.
    * You can take advantage of the **Folder Name** feature by editing the pipeline source and then providing the new directory path. In this case, the entry in the **Config File Filter** is ignored and the pipeline source in the directory path is used.
    * If both the fields have values, then the **Folder Name** field takes precedence. However, if no`.jfrog-pipelines`directory is found in the directory path, then information in the **Config File Filter** field is used to find the YAML files.  
 **Config File Filter** (Pipelines 1.26.0 and lower)
Enter a regular expression. Any filenames that match the expression will be loaded.
Examples:
    *       * If you plan to declare your resources in a file named `pipelines.resources.yml` and your pipelines in `pipelines.workflows.yml`, then you might enter `pipelines.*.yml`.
      * If you have multiple yaml config files, then enter `.*yml`.
      * In addition to the `pipelines.yml`, if you are using a `values.yml`, ensure both files are added to the filter, using `(pipelines|values).yml`.  


###### From Template
  1. Complete the **Template Properties** form:  
    * Click the _Select Template Namespace_ field and select the namespace.
    * Click the _Select Template Names_ field and select the template.
    * Click the _Select Template Version_ field and select the version for the template.
  2. Click **Next**.
In the resulting **Specify values file** page, click one of the following from the **Read YAML values from** drop-down:
    *  **Source Control Repository** : Use this when you want to fetch the values.yml file from an SCM provider.
    *  **Enter Manually** : Use this when you want to manually the define the values for the pipeline source.
  3.  **Source Control Repository** Click one of the following:
    *  **Single Branch**
    *  **Multi Branch**
  4. Click to select the protocol type to be used for cloning the repository when the pipeline source is synced:
    *  **SSH**
    *  **HTTPS**
  5. Complete the **Single/Multi Branch** form:
Field
Description  
 **Name**
Enter a unique name for the pipeline source.  
 **SCM Provider Integration**
Click the _SCM Provider Integration_ field and select your source control integration from the dropdown list. Only integrations that are compatible for use as a Pipeline Source will be included in the dropdown list.  
 **Repository Full Name**
Based on the information you have provided for the selected integration, such as API token, all the relevant repositories are listed in the _Repository Full Name_ field. Select the path of the repository where your pipeline files are stored. If the name of the repository is not auto-fetched, enter the full name of the repository where your pipeline files are stored.  
 **Branch (Single Branch only)**
Based on the SCM provider and Repository Full Name you have provided, all the available branches are automatically fetched (for example, `main`). Select the required branch. If the name of the branch is not auto-fetched, enter the name of the branch.  
 **Exclude (Multi Branch only)**
Specify the _Exclude Branch Pattern_ as a regular expression for the matching branch names to exclude.  
 **Include (Multi Branch only)**
Specify the _Include Branch Pattern_ as a regular expression for the matching branch names to include.  
 **Sync all branches** toggle
 **(Multi Branch only)**
When a new multi branch pipeline source is added, only the default branched is synced. To sync all the branches based on the Exclude/Include pattern, click the **Sync all branches** toggle. If Exclude/Include pattern is not provided, all branches are synced.
In addition, when a new commit is pushed to thematching branch, that particular branch is synced.  
 **Values File Path**
Enter the path to the `.values.yml` file, which contains the values for the template.  
  6.  **Enter Manually**
    *  **Name** : Enter a unique name to identify a pipeline source definition created from the template.
    *  **Values.yml** : Manually enter or paste the Values definition.
  7. Click **Create Source** to complete adding the Pipeline Source.


If JFrog Pipelines successfully syncs with the repository, the **Status** field displays ""Success"". Otherwise, it displays ""Failed"".  
All pipeline config files committed to the repository will be automatically loaded into JFrog Pipelines, and all defined pipelines from that source are listed in the pipeline list.
##### Validating YAML
Before adding a pipeline source, you have the option of validating your pipelines, resources, and values YAMLs and then committing it to the SCM. This enables you to get instant feedback on pipeline sync errors. The YAML validator will validate your YAML for both semantic and syntactic errors.
To validate your YAML:
  1. Click **Validate YAML**.
  2. Either enter the YAML and paste the YAML contents in the **Pipelines YAML** and **Resources YAML** fields. If you have a values YAML, enter the YAML contents in the **Values YAML** field. For more information about the Pipelines DSL, see Defining a Pipeline.
As the YAML content is entered, it is validated and syntactic errors, if any, are displayed.
  3. After fixing all the syntactic errors, click **Validate** to validate the YAML for semantic errors.

  
##### Viewing Pipeline Sources
To view the list of pipeline sources already added to Pipelines, go to **Administration | Pipelines | Pipeline Sources** or **Application | Pipelines | Pipeline Sources**.
The page displays the list of pipelines sources that are available to you according to the permission targets defined in the JFrog Platform. Your user account must be granted permissionsfor a pipeline source for it to be shown.Introduction to Permissions
Each row of the pipeline sources list includes the following:
Property
Description  
Name
  *  **For new pipeline sources** : This is the name you provided while creating the pipeline source
  *  **For existing pipeline sources** : This defaults to `repositoryName/branch` for single branch and `repositoryName` for multi-branch

  
Git Repository
The source repository path of the pipeline source  
Project
The Project that the pipeline source belongs to  
Latest Status
The success/failure status of the last sync  
Config File Filter
The filter string for the pipeline config files  
Last Sync
The time and date of the last sync  
Changed By
Name of the user who made the last update to the pipeline source  
Context
The commit SHA that triggered the last sync  
Logs
Click the **Logs** link to view the log from the last sync. Use this to diagnose a failure to sync a pipeline source.  
If the pipeline source is a multi branch source, the row presents aggregate information for all branches, and can be expanded/collapsed to show the sync status of each branch.
##### Syncing a Pipeline Source
When any of the pipeline config files have changed, you will need to sync the pipeline source to reload:
  1. Go to **Administration | Pipelines | Pipeline Sources** and click the **Actions** button located at the far right.
  2. Click **Sync**.

  
##### Viewing Sync Logs
To view the logs of all synchronized config files:
  1. Go to **Administration | Pipelines | Pipeline Sources** and click the **Actions** button located at the far right.
  2. Click **Logs**.

  
### Pipeline Source Sync Recovery
Pipelines provides an option to enable auto-sync for pipelines sources that are out of sync. For more information, see Pipeline Source Sync Recovery.
##### Deleing a Pipeline Source
When a pipeline source is deleted, it deletes all the pipelines associated with the source.
To delete a pipeline source:
  1. Go to **Administration | Pipelines | Pipeline Sources** and click the **Actions** button located at the far right.
  2. Click **Delete**.  
  3. Click **Confirm** on the confirmation window that appears.


### Note
A pipeline source cannot be deleted if any of its associated pipeline is still running. You can either cancel the run or wait for the run to complete and then delete the pipeline source.
##### Fetching Branches
When one or more branches are created for a Multibranch pipeline source, the webhooks may not be processed correctly, resulting in those branches not being added to the pipeline source.
Pipelines can automatically fetch and sync missing branches in a multibranch pipeline source based on the selected SCM integration and the full name of the repository.
To do this:
  1. Click the **Actions** button for the multibranch pipeline, and then click **Fetch Branches**.  
The **Fetch Branches** window appears.
  2. In the **Branch** field, you can either:
    * Enter the name of the branch you want to fetch and sync.
or
    * Click the **Fetch Branches** button to try to automatically fetch all the branches based on the configured SCM integration and repository.
If one or more branches are found, they are listed in the drop-down menu.
  3. Click to select the relevant branches.  
  4. Click **Confirm** to sync the pipeline source.
The newly found branches are now listed on the Pipeline Sources page.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
16,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines provides an option to enable auto-sync for pipelines sources that are out of sync. Pipeline sources and related objects can become outdated if the webhooks from SCM providers are not received and if the commit event to a branch is not notified to Pipelines. In addition, problems with a webhook's origin, network, and Pipelines services can also cause pipeline source sync issues. In these scenarios, while the pipeline might look like it is in a valid state, it is actually out of sync.
Pipeline source sync recovery ensures that if a step fails due to the pipeline source not being in sync, all the matching GitRepo resources using the same repo path, integration and branch are automatically re-synced, prompting an automatic trigger of the step.
[/INST]### Note
This feature does not include recovery of lost PRs, releases and tags events.
##### Enabling Auto-Sync
The pipeline source auto-sync feature is not available by default. To add and enable this feature, in the Core Services Configurations section in the Pipelines System YAML, add the tag `autoSyncResourceIfOutdated` and set it as `true`.Pipelines System YAML
[YAML]
    autoSyncResourceIfOutdated: true
[/YAML]
##### Auto-Sync Behavior
This section provides information about the behavior of pipeline source auto-sync feature in various scenarios.
###### Missing Commits
If the step in a pipeline is triggered as the result of an external event and if its input resources commit is not the latest, then that step fails in the ‘Processing required resources’ phase with the following error message:
`Your GitRepo resource is outdated. We will sync the resource to the updated version shortly.`
In this scenario, after the step completes, all git repo resources matching the same repo path, integration, and branch are auto-synced and the the failed step is automatically re-triggered (unless the `trigger` tag is set to `false`).
###### Change in Config FIle
There could be cases where there is a change in the pipelines config file (yaml), but the webhook does not work. In this case, when the pipeline is manually triggered and it fails, the pipeline source, which is using the same branch, repo, and integration, is auto-synced.
###### Commit Gap
To identify if the pipeline source needs to be synced, Pipelines checks if the latest SHA available for the resource matches the previous SHA of the webhook event. If they do not match, the pipeline source is auto-synced.
###### Trigger is Set to False
By default, any incoming payload from the webhook will trigger the execution of a step, but this can be changed by  setting `trigger` tag as `false`. In this case, even after the pipeline source is auto-synced, it will not automatically trigger the step.
In such scenarios, you have to manually trigger the step or set the `trigger` tag as `true`.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
17,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Node Pools** provide a convenient way to logically group build nodes in a JFrog Pipelines environment. This enables you to run steps simultaneously on different nodes.
Administrators may create any number of node pools. For SaaS deployments, a node pool named `default-dynamic-nodepool` is available by default. For self-hosted deployments, at least one node pool must be added and designated as the default node pool, and assigned at least one build node for pipelines to execute in the selected Project.
A node pool must be designated to contain build nodes of one of these types:
  *  **Dynamic Nodes** \- Build nodes that are spun up and down on-demand by Pipelines from a cloud or Kubernetes service. This can help scale operations, and help manage costs by not incurring cloud service charges to run idle nodes.
  *  **Static Nodes** – Build nodes (virtual machines) that are persistently available through a fixed IP address.
For information about the minimum requirements for a static node that can be assigned to JFrog Pipelines and to learn how static nodes can be added, see  Managing Pipelines Static Nodes.


[/INST]#### Administering Dynamic Node Pools
Dynamic node pools spin build nodes up and down on-demand from a cloud or Kubernetes service. To create a dynamic node pool, you must configure and provide an Admin integration for one of the following cloud or Kubernetes service:
  * AWS Keys
  * Azure Keys
  * Google Cloud
  * Kubernetes


A dynamic node is provisioned when a step is triggered, then either cached or destroyed after a preset amount of idle time if no new step is triggered during that interval. When used on a cloud service, this can help manage costs by not incurring charges to run idle nodes.
The following actions can only be performed by administrator users to configure Pipelines dynamic node pools for operation.Introduction to Users and Groups
##### Adding a Dynamic Node Pool
To create a new dynamic node pool for on-demand build nodes, go to **Administration |** **Pipelines | Node Pools** , click **Add Node Pool** , and then select **Dynamic**.
### Note
When a user-defined integration is selected, SaaS users can create custom dynamic nodes and customize all the relevant settings. Users can select any cloud provider with no restrictions on settings, such as machine type, region, and others.
Fields that are not required may be left blank. For example, you may not wish to set the fields to limit node count, disk usage, or timeout. When finished, click **Save Node Pool**.
 **General Settings**  
Name
An alphanumeric friendly name for the node pool.
### Note
When adding a node pool within a specific Project, by default, the name is prefixed with the Project Key. After a node pool is saved, its name cannot be edited.Introduction to Projects  
Set as Default Node Pool
When checked, sets the node pool as the default node pool for all pipelines in the selected Project. All steps that do not explicitly define which node pool to use will run on nodes of the default node pool.  
 **Select an Integration/Select a Build Image**  
Integration
Create or select from the available Admin integrations for one of the following cloud or Kubernetes service:
  * AWS Keys
  * Azure Keys
  * Google Cloud
  * Kubernetes

  
Build Image
In the **Build Plane Images** window:
  * Select from the available build images. For example, an Amazon Machine Image (AMI) on AWS.
or
  * Click **Add Build Imag** e to add your own custom VM image. For more information, see  Creating Custom VM Images.

  
 **Additional Settings**  
Machine Type
Select from the available machine (instance) types for the service. For example: c4.large on AWS.  
Fetch Machine Types
Click to fetch all the supported machine types (in addition to the static list of machine types) from the cloud provider for the selected region/zone. The button is available only when a user-defined integration and valid zone (in some cases) is selected.  
Node Limits
The maximum number of nodes that can created on-demand for this node pool.  
Disk Size
Optional. Configure dynamic node disk size.
The minimum limit is:
  * AWS (Windows): 120 GB
  * Azure (Windows): 128 GB
  * Others: 60 GB


### Note
  * This option is available for Pipelines self-hostedise versions only.
  * If the disk size is not specified, the node will use the image’s disk size by default.

  
Timeout
Number of seconds for connection timeout. Default is 3600 seconds.
For more information, see Setting Step Timeouts.  
Node Idle Interval
Number of minutes a node can be idle before it is destroyed.  
Insecure Docker Registries
A comma-separated list of allowed insecure docker registries.  
 **Kubernetes Settings** (see Creating Dynamic Nodes on Kubernetes for configuration procedures)  
Storage Class
The storage class available from the service to use.
Run `kubectl get storageclass` to list the StorageClasses in your cluster.  
CPU
The CPU limit resource settingfor every pod.  
Labels
Optional labels for node selector.  
Memory
The memory limit resource settingfor every pod.  
Namespace
The namespace for nodes.  
 **AWS Settings** (see AWS Keys Integration for configuration requirements)  
SSH Key Pair
The SSH encrypted access key for the cloud service. Name of the SSH Key pair created on AWS.  
Security Group ID
An ID for an optional security group for all created nodes.  
Subnet Id
Optional VPC subnet ID  
 **GCP Settings** (see Google Cloud Integration for configuration requirements)  
Zone
Zonewithin the hosting GCP region  
Network Tags
Optional Network tags (text attributes) for the VM instances  
Subnet Name
Optional subnet name  
Assign Public IP
When selected, nodes will be created with public IP addresses  
 **Azure Settings** (see Azure Keys Integration for configuration requirements)  
Subscription ID
GUID that uniquely identifies your subscription to use Azure services  
Subnet Name
Vnet subnet name  
Vnet Name
Azure Virtual Network name (e.g., _myVirtualNetwork_ )  
Resource Group Name
Name of the resource group (e.g., __myResourceGroup__ )  
Security Group Name
Optional NIC network security group  
Public SSH Key
Public Secure Shell (SSH) key for Linux VMs  
Admin Username
Username for RDP (Windows nodes only, optional)  
Admin Password
Password for RDP (Windows nodes only, optional)  
 **Cache Settings**  
Enable Cache
When checked, enables  node caching.  
Reset Cache
Select the type of interval to reset the cache: **On Day of Week** or **By Number of Days**.
Available only when node caching is enabled.  
 **Assign Pipelines to this Node Pool**  
Any Pipeline Source
When selected, pipelines loaded from any pipeline source can use this node pool.
Unselect this option to  restrict node pool use to a set of pipeline sources.  
###### Node Duration
When a step is triggered, a dynamic node is spun up to execute the step. After the step is complete, the node is available to execute subsequent steps until it is terminated.
When node caching is not enabled, Pipelines will check the node at each node idle interval after the node was first spun up. If the node is idle Pipelines terminates the node. This ensures that your build infrastructure is managed in an efficient manner, to help you control computing costs.
###### Dynamic Node Caching
When enabled, node caching helps speed up execution of your builds by avoiding having to repeat time-consuming processes, such as installing or loading large amounts of dependency data. This feature is available only on dynamic node pools.
When node caching is enabled for a dynamic node pool, Pipelines pauses your node after executing a step, instead of terminating it after the specified idle time. When the next step executes, the paused node is restarted, retaining any Docker images or dependencies that were installed as part of the previous step. This greatly speeds up builds that pull or build Docker images as part of their workflow.
In addition, the node is available to execute the job much sooner, since restarting the node from a paused state is much faster than provisioning a new node. So if your build takes just a few minutes to execute, you save on the node provisioning time for every build, which adds up quickly.
#### Administering Static Node Pools
Sttatic node pools are collections of persistently available build nodes (virtual machines). Static nodes enable you to execute pipeline steps on your own infrastructure. For more information, see Managing Pipelines Static Nodes.
The following actions can only be performed by administrator users to configure Pipelines static node pools.Introduction to Users and Groups
### Note
For information about troubleshooting node initialization issues, see Troubleshooting - Node Initialization.Troubleshooting
##### Adding a Static Node Pool
To create a new node pool of static build nodes, go to **Administration |** **Pipelines | Node Pools** , click **Add Node Pool** , and then select **Static**.  
Fields that are not required may be left blank. For example, you may not wish to set the fields to limit node count, disk usage, or timeout. When finished, click **Save Node Pool**.
 **General Settings**  
 **Name**
An alphanumeric friendly name for the node pool.  
 **Set as Default Node Pool**
When checked, sets the node pool as the default node pool for all pipelines. All steps that do not explicitly define which node pool to use will run on nodes of the default node pool.  
 **Additional Settings**  
 **Architecture**
Currently limited to _x86_64_  
 **OS**
Select from the list of operating systems supported for the selected architecture. For a list of supported OS and architecture combinations, refer to Runtime Images.  
 **Runtime Version**
The default runtime image that will be used to run pipeline steps. Choose a specific runtime to make sure the build nodes have the versions you need for languages/services/CLIs/etc. To learn more, see Managing Runtimes.  
 **Node Limits**
Limits the maximum number of nodes that can be added to the node pool.  
 **Max Disk Usage %**
Limits the maximum disk usage. A build node that exceeds this maximum will be marked as failed.  
 **Timeout**
The maximum time (in seconds) after which you want your pipeline steps to timeout. This timeout is used when your step does not have any timeout specified. Default value for timeout is 3600 seconds.
For more information, see Setting Step Timeouts.  
 **Insecure Docker Registries**
A comma-separated list of allowed insecure docker registries.  
 **Assign Pipelines to this Node Pool**  
 **Any Pipeline Source**
When selected, pipelines loaded from any pipeline source can use this node pool.
Unselect this option to restrict node pool use to a set of pipeline sources.  
Once you have created a static node pool, you can add static nodes.
#### Administering All Node Pools
The following actions can only be performed by administrator users to configure Pipelines node pools (both static and dynamic) for operation.Introduction to Users and Groups
##### Restricting Node Pool Use
If a node pool is in a Project, it may be assigned to one or more environments, and pipelines in pipeline sources that share at least one environment with the node pool may use the node pool. By default, node pools are in the **DEV** environment.
Node pools that are not in a Project are available to pipelines in pipeline sources that are not in a Project. In addition, further restrictions may be specified to restrict use of particular pipeline sources and users that have been given permissionsto use those pipeline sources.Introduction to Permissions
You can establish those usage restrictions when you add or modify a node pool.
  1. Under **Assign Pipeines to this Node Pool** , uncheck the option for **Any Pipeline Source**.
  2. In the revealed interface, select from the list of available pipeline sources and move the row(s) (by click-and-drag or arrow button) to the list of included pipeline sources.  
  3. When finished, click **Save Node Pool**.


##### Viewing Available Node Pools
To view the the list of node pools already added and available for use, go to **Administration |** **Pipelines | Node Pools** or **Application |** **Pipelines | Node Pools**.
### Note
Non-administrator users will only be shown node pools that they are permitted to use.  
### Max Limit Exceeded
If the number of nodes in a node pool exceeds its maximum limit, it is listed with an exclamation mark in the **Max Limit** column.  
##### Node Provision Status
In the Node Pools list view, the **Provision Status** column provides a color representation of the provision status for each node. Every color represents one of the stages in the lifecycle of a node.  
Color
Provision Status
Description  
Not provisioned
The node has not been requested to be provisioned.  
Waiting
When a node is provisioned, it first goes into waiting state. This means that the cloud provider is initializing the node and/or has not reported the status back to Pipelines.  
Processing
  * The cloud provider reported the node's status back to Pipelines and Pipelines is setting up the agent.
or
  * The node is running a build.

  
Success
The node successfully initialized and is waiting for a Pipelines build.  
Failure
The node failed. This is usually caused by an out of memory issue. However, there could be several other reasons, which are reflected in the node console.  
Stopped
When a node is deleted, its provisioning is first stopped and then it is deleted.  
Cached
The node is stopped and not running, but the actual instance is still available for instant use.
### Note
Caching must be enabled for a node to be cached.  
##### Viewing Node Pool Details
To view the details of a node pool, click the name of the node pool in the list of available node pools.  
##### Modifying or Removing a Node Pool
### Note
 **Cloud only** : The default node pool default-dynamic-nodepool is managed by JFrog and certain settings, such as Machine Type, cannot be modified.
 **To edit the node pool settings:**
  1. View the node pool as shown above.
  2. Click **Edit**.
Settings that may not be changed have gray fields.  
  3. When finished changing settings, click **Save Node Pool**.


 **To delete a node pool:**
  1. View the node pool as shown above.
  2. Click **Delete**.
  3. Confirm you wish to delete by clicking **Confirm**.


#### Sharing Node Pools across Multiple Projects
Project admins can share node pools across multiple projects to allow members in more than one project to use them. A classic use case for sharing a node pool is the need to share computing resources when running pipelines from multiple projects. A node pool can be shared with a single project or across multiple projects. After a node pool is shared, it is added to the target projects and displayed on the node pools page.Introduction to Projects
### Shared Node Pool Rules and Guidelines
  * A node pool can be shared only after it has been created.
  * A node pool can be shared only if it has a globally unique name.
  * Only the owner of the shared node pool can delete the shared node pool.


To share a node pool:
  1. Select the relevant Project and in the **Administration** module, go to **Pipelines | Node Pools.**
  2. Hover over the node pool you want to share and click the **Share** icon.  
  3. You can share the selected Integration:
    * Across all the projects, by selecting the **Auto share with all projects** field.
    * Only with specific projects, by selecting the name of the Project from the projects list.  
  4. Click **Save**.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
18,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A custom VM image enables you to use your own image as a node in Pipelines. Nodes launched using this new custom VM image would include all the customizations you made when you created the image.
[/INST]##### Prerequisites
To perform the steps outlined in this section, you will need to have some experience with the following:
  * Building VM images
  * Hashicorp Packer or a cloud provider-specific VM image process


##### Building a Custom Pipelines Build Plane VM Image using Hashicorp Packer
A custom Pipelines build plane VM image can be built to include your own artifacts. In this example we'll reference Hashicorp packer and Ubuntu 20, but any VM image build process (AWS cloud formation, PowerShell, or others) can be used with any OS that we support.
To build the custom build plane VM image:
  1. Download the buildplane package (RPM for Centos or a tar file for Windows) to the machine that will be used to build the VM image.
### Note
This API can be used by authenticated users only.
 **Example**
[YAML]    curl --request GET  --url https://my-pipelines-api-url/v1/passthrough/artifacts/buildPlane-x86_64-Ubuntu_20.04.deb --output buildPlane-x86_64-Ubuntu_20.04.deb
    buildPlane package names:
    buildPlane-x86_64-Ubuntu_18.04
    buildPlane-x86_64-Ubuntu_20.04.deb
    buildPlane-x86_64-Centos_7.rpm
    buildPlane-x86_64-CentOS_8.rpm
    buildPlane-x86_64-WindowsServer_2019.tar.gz
[/YAML]
  2. Update your packer (or cloud formation) template to:
    1. Copy the buildplane file to `/tmp/`
    2. Install it (using dpkg, yum, or untar, depending on the type)
    3. Run the `boot.sh` (or `boot.ps1` if creating a W19 image). Path to this file is `/var/opt/jfrog/pipelines/nodeInit/x86_64/<OS>/boot.sh `
  3. Build your VM image.
  4. Use the resulting VM image id in the next step.
This step should be performed with every new pipelines version.


 **Example 'provisioners' section of a terraform template file using the Ubuntu 20 buildplane**
[YAML]
      ""provisioners"": [
        {
          ""type"": ""file"",
          ""source"": ""buildPlane-x86_64-Ubuntu_20.04.deb"",
          ""destination"": ""/tmp/buildPlane-x86_64-Ubuntu_20.04.deb""
        },
        {
          ""type"": ""shell"",
          ""inline"": [
            ""sudo dpkg -i /tmp/buildPlane-x86_64-Ubuntu_20.04.deb"",
            ""sudo /var/opt/jfrog/pipelines/nodeInit/x86_64/Ubuntu_20.04/boot.sh""
          ]
        }
[/YAML]
##### Adding the VM Image to Pipelines using API
To add the VM image to Pipelines, use the `POST /v1/buildPlaneImagesPipelines REST API`API.
 **Sample Request** :
[YAML]
    curl --location --request POST 'http://<pipelines_url>:8082/pipelines/api/v1/buildPlaneImages' \
    --header 'Authorization: Bearer <token>' \
    --header 'Content-Type: application/json' \
    --data-raw '{
     ""name"": ""eu-centrall-1 image for x86_64 Ubuntu_20.04"",
     ""masterIntegrationId"": 46,
     ""imageId"": ""ami-07f469736e6545c8a"",
     ""region"": ""eu-central-1"",
     ""isAvailable"": true,
     ""architecture"": ""x86_64"",
     ""operatingSystem"": ""Ubuntu_20.04""
    }'
[/YAML]
 **Sample Response** :
[YAML]
      {
            ""metadataPropertyBag"": {},
            ""id"": 8,
            ""name"": ""eu-centrall-1 image for x86_64 Ubuntu_20.04"",
            ""masterIntegrationId"": 46,
            ""imageId"": ""ami-07f469736e6545c8a"",
            ""region"": ""eu-central-1"",
            ""isAvailable"": true,
            ""architecture"": ""x86_64"",
            ""operatingSystem"": ""Ubuntu_20.04"",
            ""createdAt"": ""2021-01-06T00:00:00.000Z"",
            ""updatedAt"": ""2021-01-06T00:00:00.000Z""
      }
[/YAML]
##### Adding the VM Image to Pipelines using UI
Perform the following steps to add an VM image to Pipelines using the UI:
  1. Go to **Administration** → **Pipelines** → **Node Pools** and click the **Build Images** tab.
  2. Click **Add Build Image**.
In the resulting Create Build Image screen, provide the following information:
Field
Description  
Name
An alphanumeric friendly name for the node pool  
Cloud Provider Type
AWS, Azure or GCP  
Image ID
VM image id. In AWS, this would be the AMI id  
Region
Cloud provider region that the VM image is available in  
Architecture
x86_64 architecture  
Operating System
An OS we support  
  3. Click **Save**.
The newly added build image appears in the list.  
  4. Now create a node pool using the above buildplane image Id. The build image you created is available for selection in the Build Image drop-down. For more information, see Managing Pipelines Node Pools.  



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
19,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This tutorial explains how to specify a kubeconfig for a Kubernetes Integration to authenticate to a self-hosted Kubernetes cluster for a dynamic node pool. You can use a cloud provider solution like EKS, GKE, or AKS, or a self-hosted Kubernetes solution.
This tutorial assumes that you have working knowledge of Docker and Kubernetes and understand the following concepts:
  * Self hosting on GCP
  * Self hosting on AWS
  * kubeconfig files
  * Configuring Service Accounts


[/INST]##### Configure a Kubernetes Service Account
You must  configure a service account in Kubernetes to provide an identity for the build node processes that Pipelines will dynamically control.
This procedure will use your personal account to create the service account. Make sure your personal account has permissions to do this.
###### Verify Access to the Cluster
First, make sure you can authenticate yourself to the cluster. This means you have a kubeconfig file that uses your personal account. You can verify this by running this command on your local machine and you should see the file listed.
[YAML]
    ls -al $HOME/.kube
[/YAML]
 **Author a service account spec**
To create a service account on Kubernetes, you can leverage `kubectl` and a service account spec. Create a YML file similar to the one below:
 **pipelines_k8s_sa.yml**
[YAML]
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: pipelines-k8s-pool   # <-- any name you'd like
      namespace: jfrog           # <-- the cluster namespace
[/YAML]
###### Create the service account
You can create a service account by running the following command:
[YAML]
    kubectl apply -f pipelines_k8s_sa.yaml
[/YAML]
##### Manually Create a Long-lived API Token for a ServiceAccount
### Note
Perform these steps if you are using Kubernetes 1.22 or higher, as these versions do not automatically create long-lived tokens. Versions of Kubernetes before v1.22 automatically created long term credentials for accessing the Kubernetes API.
If you want to obtain an API token for a ServiceAccount, you create a new Secret with a special annotation, `kubernetes.io/service-account.name`.
[YAML]
    kubectl apply -f - <<EOF
    apiVersion: v1
    kind: Secret
    metadata:
      name: pipelines-k8s-pool
      annotations:
        kubernetes.io/service-account.name: pipelines-k8s-pool
    type: kubernetes.io/service-account-token
    EOF
[/YAML]
If you view the Secret using:
`kubectl get secret/pipelines-k8s-pool -o yaml`
you can see that the Secret now contains an API token for the ""`pipelines-k8s-pool`"" ServiceAccount.
Because of the annotation you set, the control plane automatically generates a token for that ServiceAccounts, and stores them into the associated Secret. The control plane also cleans up tokens for deleted ServiceAccounts.
`kubectl describe secrets/pipelines-k8s-pool`
The output is similar to this:
[YAML]
    Name:           pipelines-k8s-pool
    Namespace:      default
    Labels:         <none>
    Annotations:    kubernetes.io/service-account.name: pipelines-k8s-pool
                    kubernetes.io/service-account.uid: da68f9c6-9d26-11e7-b84e-002dc52800da
    
    Type:   kubernetes.io/service-account-token
    
    Data
    ====
    ca.crt:         1338 bytes
    namespace:      7 bytes
    token:          ...
[/YAML]
When you delete a ServiceAccount that has an associated Secret, the Kubernetes control plane automatically cleans up the long-lived token from that Secret.
##### Get Tokens and IP from Kubernetes
Once the service account has been created, you will need to retrieve some key information from Kubernetes in order to configure it through a kubeconfig.
###### Fetch the name of the secrets used by the service account
This can be found by running the following command:
[YAML]
    kubectl describe serviceAccounts pipelines-k8s-pool
[/YAML]
 **output**
[YAML]
    Name:               pipelines-k8s-pool
    Namespace:          jfrog
    Labels:             <none>
    Annotations:        <none>
    
    Image pull secrets: <none>
    Mountable secrets:  pipelines-k8s-pool-token-h6pdj
    Tokens:             pipelines-k8s-pool-token-h6pdj
[/YAML]
Note the `Mountable secrets` string. This is the name of the secret that holds the token, and will be used in the next step. Or if you are on a later version of k8s, describe the secret you created in the last step.
###### Fetch the token from the secret
Using the `Mountable secrets` string, you can get the token used by the service account. Run the following command to extract this information:
[YAML]
    kubectl describe secrets pipelines-k8s-pool-token-h6pdj
[/YAML]
 **output**
[YAML]
    Name:           pipelines-k8s-pool
    Namespace:      jfrog
    Labels:         <none>
    Annotations:    kubernetes.io/service-account.name=pipelines-k8s-pool
            kubernetes.io/service-account.uid=c2117d8e-3c2d-11e8-9ccd-42010a8a012f
    
    Type:   kubernetes.io/service-account-token
    
    Data
    ====
    ca.crt:     1115 bytes
    namespace:  7 bytes
    token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InNoaXBwYWJsZS1kZXBsb3ktdG9rZW4tN3Nwc2oiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoic2hpcHBhYmxlLWRlcGxveSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImMyMTE3ZDhlLTNjMmQtMTFlOC05Y2NkLTQyMDEwYThhMDEyZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnNoaXBwYWJsZS1kZXBsb3kifQ.ZWKrKdpK7aukTRKnB5SJwwov6PjaADT-FqSO9ZgJEg6uUVXuPa03jmqyRB20HmsTvuDabVoK7Ky7Uug7V8J9yK4oOOK5d0aRRdgHXzxZd2yO8C4ggqsr1KQsfdlU4xRWglaZGI4S31ohCApJ0MUHaVnP5WkbC4FiTZAQ5fO_LcCokapzCLQyIuD5Ksdnj5Ad2ymiLQQ71TUNccN7BMX5aM4RHmztpEHOVbElCWXwyhWr3NR1Z1ar9s5ec6iHBqfkp_s8TvxPBLyUdy9OjCWy3iLQ4Lt4qpxsjwE4NE7KioDPX2Snb6NWFK7lvldjYX4tdkpWdQHBNmqaD8CuVCRdEQ
[/YAML]
Copy and save the `token` value. This will be used in your kubeconfig file.
###### Get the certificate info for the cluster
Every cluster has a certificate that clients can use to encrypt traffic. Fetch the certificate and write to a file (for example, `cluster-cert.txt)` by running this command:
[YAML]
    kubectl config view --flatten --minify > cluster-cert.txt
    cat cluster-cert.txt
[/YAML]
 **output**
[YAML]
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURDekNDQWZPZ0F3SUJBZ0lRZmo4VVMxNXpuaGRVbG15a3AvSVFqekFOQmdrcWhraUc5dzBCQVFzRkFEQXYKTVMwd0t3WURWUVFERXlSaVl6RTBOelV5WXkwMk9UTTFMVFExWldFdE9HTmlPUzFrWmpSak5tUXlZemd4TVRndwpIaGNOTVRnd05EQTVNVGd6TVRReVdoY05Nak13TkRBNE1Ua3pNVFF5V2pBdk1TMHdLd1lEVlFRREV5UmlZekUwCk56VXlZeTAyT1RNMUxUUTFaV0V0T0dOaU9TMWtaalJqTm1ReVl6Z3hNVGd3Z2dFaU1BMEdDU3FHU0liM0RRRUIKQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUURIVHFPV0ZXL09odDFTbDBjeUZXOGl5WUZPZHFON1lrRVFHa3E3enkzMApPUEQydUZyNjRpRXRPOTdVR0Z0SVFyMkpxcGQ2UWdtQVNPMHlNUklkb3c4eUowTE5YcmljT2tvOUtMVy96UTdUClI0ZWp1VDl1cUNwUGR4b0Z1TnRtWGVuQ3g5dFdHNXdBV0JvU05reForTC9RN2ZpSUtWU01SSnhsQVJsWll4TFQKZ1hMamlHMnp3WGVFem5lL0tsdEl4NU5neGs3U1NUQkRvRzhYR1NVRzhpUWZDNGYzTk4zUEt3Wk92SEtRc0MyZAo0ajVyc3IwazNuT1lwWDFwWnBYUmp0cTBRZTF0RzNMVE9nVVlmZjJHQ1BNZ1htVndtejJzd2xPb24wcldlRERKCmpQNGVqdjNrbDRRMXA2WXJBYnQ1RXYzeFVMK1BTT2ROSlhadTFGWWREZHZyQWdNQkFBR2pJekFoTUE0R0ExVWQKRHdFQi93UUVBd0lDQkRBUEJnTlZIUk1CQWY4RUJUQURBUUgvTUEwR0NTcUdTSWIzRFFFQkN3VUFBNElCQVFCQwpHWWd0R043SHJpV2JLOUZtZFFGWFIxdjNLb0ZMd2o0NmxlTmtMVEphQ0ZUT3dzaVdJcXlIejUrZ2xIa0gwZ1B2ClBDMlF2RmtDMXhieThBUWtlQy9PM2xXOC9IRmpMQVZQS3BtNnFoQytwK0J5R0pFSlBVTzVPbDB0UkRDNjR2K0cKUXdMcTNNYnVPMDdmYVVLbzNMUWxFcXlWUFBiMWYzRUM3QytUamFlM0FZd2VDUDNOdHJMdVBZV2NtU2VSK3F4TQpoaVRTalNpVXdleEY4cVV2SmM3dS9UWTFVVDNUd0hRR1dIQ0J2YktDWHZvaU9VTjBKa0dHZXJ3VmJGd2tKOHdxCkdsZW40Q2RjOXJVU1J1dmlhVGVCaklIYUZZdmIxejMyVWJDVjRTWUowa3dpbHE5RGJxNmNDUEI3NjlwY0o1KzkKb2cxbHVYYXZzQnYySWdNa1EwL24KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
        server: https://35.203.181.169
      name: gke_jfrog-200320_us-west1-a_cluster
    contexts:
    - context:
        cluster: gke_jfrog-200320_us-west1-a_cluster
        user: gke_jfrog-200320_us-west1-a_cluster
      name: gke_jfrog-200320_us-west1-a_cluster
    current-context: gke_jfrog-200320_us-west1-a_cluster
    kind: Config
    preferences: {}
    users:
    - name: gke_jfrog-200320_us-west1-a_cluster
      user:
        auth-provider:
          config:
            access-token: ya29.Gl2YBba5duRR8Zb6DekAdjPtPGepx9Em3gX1LAhJuYzq1G4XpYwXTS_wF4cieZ8qztMhB35lFJC-DJR6xcB02oXXkiZvWk5hH4YAw1FPrfsZWG57x43xCrl6cvHAp40
            cmd-args: config config-helper --format=json
            cmd-path: /Users/ambarish/google-cloud-sdk/bin/gcloud
            expiry: 2018-04-09T20:35:02Z
            expiry-key: '{.credential.token_expiry}'
            token-key: '{.credential.access_token}'
          name: gcp
[/YAML]
Copy and save two pieces of information from here:
  * `certificate-authority-data`
  * `server`


##### Configuring Permissions in Kubernetes
Kubernetes includes a number of resources, including roles and role bindings that can be used to break your cluster into namespaces and limiting access to namespaced resources to specific accounts.
This section provides information about defining permissions in Kubernetes using roles and role binding.
###### Creating a Role
A Role sets permissions within a particular namespace, which must be specified when creating a Role. Each Role has a `rules` section to define the resources that the rules apply to and the allowed operations, which are required for service account users to run builds within Kubernetes.
For example, the following example creates a Role in the `jfrog` namespace, which will allow read/write access to all resources in the namespace:
[YAML]
    apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      namespace: jfrog
      name: pipelines-builder-role
    rules:
    - apiGroups: ["""",""apps""]
      resources: [""persistentvolumeclaims"",""secrets"",""pods"",""secrets"",""configmaps"", ""deployments"", ""deployments/scale"", ""services""]
      verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
[/YAML]
###### Creating a Role Binding
The service account that was created in the previous section can now be given the Role that was created earlier using a RoleBinding in the `jfrog` namespace:
[YAML]
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: jfrog-builder-rb
      namespace: jfrog
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: pipelines-builder-role
    subjects:
      - kind: ServiceAccount
        name: pipelines-k8s-pool
        namespace: jfrog
[/YAML]
##### Add a Kubernetes Administration Integration
You must add a Kubernetes integration as an administration integration:
  * From the JFrog Platform **Administration** module go to **Pipelines | Integrations**.
  * Click **Add an Integration**.
  * In the resulting **Add New Integration** display, click the _Integration Type_ field and select Kubernetes from the dropdown list.
  * Enter a _Name_ for the Kubernetes integration
  * Paste in a kubeconfig specification as described below
  * Click **Create** to finish adding the Kubernetes integration


###### Specify a kubeconfig
From the steps in the prior sections, you should have the following pieces of information:
  * <token>
  * <certificate-authority-data>
  * <server>


The kubeconfig specification you paste into the _Kube Config_ setting should follow this format:
[YAML]
    apiVersion: v1
    kind: Config
    users:
    - name: pipelines-k8s-pool                                               # <-- Your service account name
      user:
        token: <token>
    clusters:
    - cluster:
        certificate-authority-data: <certificate-authority-data>
        server: <server>
      name: self-hosted-cluster
    contexts:
    - context:
        cluster: self-hosted-cluster
        user: pipelines-k8s-pool                                             # <-- Your service account name
        namespace: jfrog                                                     # <-- The namespace you defined
      name: pipelines_k8s_context
    current-context: pipelines_k8s_context
[/YAML]
##### Create a Dynamic Node Pool
Once you have successfully added the Kubernetes administration integration, you can add a dynamic node pool that uses it.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
20,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Static Nodes** are build nodes (virtual machines) that you provide to a Pipelines node pool and are persistently available through a fixed IP address.
Static nodes enable you to execute pipeline steps on your own infrastructure, by attaching your machines to a Pipelines node pool. Your static nodes can be anywhere, such as in your Amazon EC2 VPN, Google Cloud, or in your data center behind a firewall. This powerful hybrid approach gives you full control over the infrastructure and security of your build machines.
Advantages are:
  *  **Security:** Your build machines can be inside your VPC and/or behind your firewall, enabling you to configure access, IAM, etc. Your code never leaves your firewall and no external entity can access your machines. You have complete control over your build machines, including SSH access, ability to choose your cloud provider and the size of build machines.
  *  **Faster build times:** You can leave your build machines running all the time, eliminating the time necessary to spin up an on-demand node.


[/INST]##### Minimum requirements
The minimum requirements for a node that can be assigned to JFrog Pipelines are:
  * 1.8GB RAM
  * 60GB SSD
  * JFrog CLI (when using custom runtime images)
  * Windows nodes must have containers enabled
  * For MacOS nodes, users must manually install NodeJS 16. Other static nodes use NodeJS 16.16.0.
  * For MacOS nodes, users must manually install JQ.
  * Supported architecture, OS and Docker versions:
Architecture
OS
Docker Version  
x86_64
CentOS 7
20.10  
x86_64
Ubuntu 20.04
20.10  
ARM64
Ubuntu 20.04
20.10
### Note
JFrog-published images are x86_64 and cannot run on ARM64 Ubuntu nodes. However, custom ARM64 runtime images can be specified. Host runtime is also supported.In a future release, JFrog-published images will be multi-platform and will run as ARM64.  
x86_64
Ubuntu 18.04
20.10  
x86_64
RHEL 7
20.10  
x86_64
RHEL 8
20.10  
x86_64
Windows Server 2019
20.10  
ARM64
MacOS 12
Host runtime only  


### Note
One build machine must be assigned per parallel build.
### Note
Nodes must be on a different server than the one JFrog Pipelines is installed on. A node that is running on the same server as JFrog Pipelines will fail to initialize.
##### Add a Static Node
You may add as many build nodes to the node pool as are allowed by any node count limit set for that node pool.
 **To add a node to a static node pool:**
  1. View the node pool you want to add a node to.
The **View Node Pool** page lists any nodes currently allocated to the pool.
  2. Click **Add Node**.
Fields that are not required may be left blank.
Option
Description  
Node Name
An alphanumeric friendly name for the node.  
IP Address
The IP address of the node.  
Add Swap Space
Enable the use of swap space to increase the amount of virtual memory available to the node. Not available to Windows node pools.  
Select Node Initialization Type
Select either Node Initialization mode for initialization.  
SSH Port
The SSH port of your build host. This is usually port 22, but is configurable.
For Auto initialization type only.  
SSH Access Script
The command to run on your build host that will allow Pipelines to run initialization scripts on the node.
For Auto initialization type only.  
SSH Access Script Complete
Check the checkbox to confirm that you have run the SSH Access Script command on your node. (Required)
For Auto initialization type only.  


###### Node Initialization
You can choose to initialize the build host through Pipelines ( **Auto** ) or run the initialization scripts yourself ( **Manual** ). Initialization through Pipelines requires you to grant SSH access. If you do not want to grant SSH access for some reason, select **Manual** as the node initialization type.
### Windows Nodes
All Windows nodes must be manually initialized.
When initializing a Windows node, the script should be run in PowerShell as an Administrator.
###### Auto Initialization
  * To initialize the node through Pipelines:
    * Enter the SSH port for your build host. This is usually port 22, but is configurable.
    * Copy the command shown and run it on your build host. This will allow us to run initialization scripts on your node.
    * Check the checkbox to confirm that you have run the command on your node and click **Save Node**.
    * You will be redirected to a page showing you the console log as your node is initialized.
    * When your node is ready, the status indicator for the node will turn green. Your node is now ready to pick up steps for your pipelines.


### GCP CentOS VM
In a CentOS VM on Google Cloud Platform, you must enable SSH root login (which is disabled by default) to perform an auto-initialize. This requires you to:
  * Open the file `/etc/ssh/sshd_config` and update the `PermitRootLogin` property to `yes`
  * To enable the above changes, restart the sshd systemd service by executing `systemctl restart sshd` on the VM


###### Manual Initialization
  * To run the initialization scripts yourself:
    * Click **Save and Generate initialization scripts** to generate the script.
    * Click **Download initialization scripts** to download the script. Copy it to your build machine, make sure the file is executable, and run it.
    * Your node status will automatically show green at this point. However, there is no way of verifying that the node was in fact successfully initialized so you will need to make sure this was the case by triggering a job.


##### View a Node
 **To view a static node:**
  1. View the node pool where the node is assigned. Then click on the name of the node you want to view.
  2. The **View Node** page will be displayed, showing you information about the node, its utilization, and the console output from its initialization.  


From this view, you can perform some actions on the node.
###### Re-initialize a Node
You might want to re-initialize a node for the following reasons:
  * An updated version is available for the build agent. This is indicated by a red exclamation icon in the **Agent Version** of the Node.
  * You made some changes to the node manually and it is no longer picking up jobs
  * The node isn't picking up jobs and seems 'frozen' for some reason


To re-initialize a node:
  * View the node as described above.
  * Click **Re-Initialize**. This action will initialize/install everything from scratch.


If your nodes were not initialized automatically, you will need to re-download the initialization script and run it on your node manually.
###### Modify a Node
To modify a node's settings:
  * View the node as described above.
  * Click **Edit**. The fields that cannot be changed are in gray.  
  * When finished, click **Save Node**.


###### Remove a Node
To remove a node from the node pool:
  * View the node as described above.
  * Click on the **Delete** button. This action will delete the node from the pool and sever connection to the node.


This action only removes the node from control by Pipelines. You will still need to delete the virtual machine in the environment where it is running.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
21,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines Nodes have an agent running in them. This agent is responsible for fetching their work load from Pipelines, triggering the run at the node, and reporting everything that happened during the run to Pipelines. This agent produces logs while doing that work, which can be streamed to a Logstash service if available. This is especially useful for troubleshooting dynamic nodes that can be already gone when an error is detected, making it impossible to inspect the logs inside the node.
[/INST]##### Logstash Requirements
Pipelines agent uses a TCP socket when writing logs to Logstash. For that to work, Logstash needs to have a TCP input enabled.
The following snippet shows an example of how to enable TCP input in Logstash:
[YAML]
    input {
            tcp {
                    port => 5000
                    ssl_enable => true
                    ssl_verify => true
                    ssl_cert => ""/usr/share/certs/logstash.crt""
                    ssl_key => ""/usr/share/certs/logstash.key""
                    ssl_certificate_authorities => [""/usr/share/certs/rootca.crt""]
            }
    }
[/YAML]
##### Enabling Logstash Integration
To enable Pipelines Nodes Agent integration with Logstash, add the following entries to your Pipelines system yaml runtime section:Pipelines System YAML
[YAML]
    runtime:
      logging:
        logstash:
          host: logstash.my_company.com
          port: 5000
          clientCertificatePath: /opt/jfrog/pipelines/var/etc/security/keys/trusted/node.ca.crt
          clientPrivateKeyPath: /opt/jfrog/pipelines/var/etc/security/keys/trusted/node.ca.key
          logstashCaCertificatePath: /opt/jfrog/pipelines/var/etc/security/keys/trusted/rootca.crt
[/YAML]
Configuration name
Description
Mandatory  
host
Logstash service host
Mandatory  
port
Logstash service port
Mandatory  
clientCertificatePath
Path to the certificate to be presented to the Logstash service
Optional  
clientPrivateKeyPath
Path to the private key corresponding to the specified certificate
Optional  
logstashCaCertificatePath
Path to the root certificate authorities to be used to validate Logstash certificate
Optional  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
22,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>To construct a pipeline, you will need to learn the format, syntax, and facilities of the YAML-based Pipelines DSL.
Once JFrog Pipelines has been installed and configured for use, you can start building and running pipelines, for continuous integration and continuous delivery (CI/CD).
Pipelines are defined using Pipelines DSL, stored in one or more YAML files of key-value pairs, known as a **pipeline config**.
The JFrog Platform provides the user interface for loading and viewing the pipeline config files you create, as well as for managing, viewing, and analyzing execution of your pipelines.
[/INST]### In This Section
This section provides information on how to use Pipelines DSL to create your pipelines.
  * Defining a Pipeline
This page provides a high-level overview of the structure of a pipeline configuration file.
  * Pipelines Integrations
An **Integration** connects Pipelines to an external service/tool. Each integration type defines the endpoint, credentials and any other configuration detail required for Pipelines to exchange information with the service. All credential information is encrypted and held in secure storage, in conformance with best security practices.
  * Pipelines Resources
Resources are one of the key building blocks of all pipelines. They are information entities that are used for storing and exchanging information across steps and pipelines.
Resources are versioned and each version is immutable. They are also global and depending on the scope defined for the pipeline source, they can be available across pipelines, which enable you to connect multiple pipelines together to create a pipeline of pipelines.
  * Pipelines Steps
A **Step** is a unit of execution in a pipeline. It is triggered by some event and uses resources to perform an action as part of the pipeline.
  * Pipelines Utility Functions
The **Utility Functions** are built-in shell functions that can be used in steps to interact with the runtime environment.
  * Pipelines Environment Variables
This page describes the availability and usage of environment variables in pipelines.
  * Pipelines Templates
Pipelines templates enable users to define templates for pipeline definitions and use these templates for creating pipelines.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
23,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This page provides a high-level overview of the structure of a pipeline configuration file.
JFrog Pipelines uses its own declarative language based on YAML syntax, Pipelines DSL, to describe workflows. You create pipelines in text files written in Pipelines DSL, which we refer to as a **pipeline config**. You can create these files in any text editor of your choice.
You must store your pipeline config file(s) in a source code repository (for example, GitHub). When Pipelines has been configured to use this repo as a pipeline source, the config files will be automatically read and the workflows loaded into Pipelines and run.
[/INST]#### Pipeline Config Structure
There are two top-level sections that can be defined in a pipeline config:
  * A `resources` section that specifies the Resources used by the automated pipeline.
  * A `pipelines` section that specifies the pipeline execution environment and the Steps to execute.


For ease of illustration, we'll describe how this looks in a single pipeline config file (e.g., `pipelines.yml`).
#### Resources Section
Resources provide the information that steps need in order to execute or store information generated by a step. For example, a resource might point to a source code repository, a Docker image, or a Helm chart. A list of all supported resources is available in Resources overview.
The basic format of each `resources` declaration is:
Tag
Description  
`name`
A globally unique friendly name for the resource.  
`type`
A predefined string that specifies the type of resource.
For more information, see Resource Types.  
`configuration`
Begins the section of settings required by the resource `type`. This typically includes the name of the integration that connects the resource to the external service.  
### Note
Resource definitions are global and can be used by all pipelines in a Project that is in at least one of the same environments. This means that resource names must be unique across all pipeline config files in a Project.
For example, here is a `resources` section that defines two resources, a GitRepo and a Docker Image:GitRepo
[YAML]
    resources:
      - name: my_Git_Repository
        type: GitRepo
        configuration:
          gitProvider: my_GitHub_Integration
          path: ~johndoe/demo
          branches:
            include: master
    
      - name: my_Docker_Image
        type: Image
        configuration:
          registry: my_Docker_Registry_Integration
          imageName: johndoe/demo_image
          imageTag: latest
[/YAML]
#### Pipelines Section
The `pipelines` section defines the workflow, consisting of steps and the dependencies between them.
The basic format of each `pipelines` declaration is:
Tag
Description  
`name`
A friendly name for the resource, unique within the project.  
`configuration`
An optional section to specify environment variables and/or a `runtime` image for the pipeline to execute in.  
preRun
This is an optional step. When configured, this step will always run at the beginning of a pipeline. This is useful when you want to run some checks at the beginning of a run.
Like Pipelines Steps, preRun must include at least one execution command, such as onExecute, onStart, onSuccess, onFailure, or onComplete. In addition, it can include input and output resources.
[YAML]
    pipelines: 
      - name: PreAndPostRuns 
        preRun: 
          execution: 
            onExecute: 
              - printenv 
              - echo ""Executing Pre Run""
[/YAML]  
postRun
This is an optional step. When configured, this step will always run at the end of a pipeline. This is useful when you want to run some checks at the end of a run.
Like Pipelines Steps, postRun must include at least one execution command, such as onExecute, onStart, onSuccess, onFailure, or onComplete. In addition, it can include input and output resources.
[YAML]
    pipelines: 
      - name: PreAndPostRuns 
        postRun: 
          execution: 
            onExecute: 
              - printenv 
              - echo ""Executing Post Run""
[/YAML]  
This is followed by a collection of` step` sections that specifies the steps to execute.
### Tip
The `name` of the pipeline will be available in the environment variable `$pipeline_name`, which can be used to construct the base name for builds.
##### Pipeline Configuration
The optional `configuration` section can specify an execution environment for all steps in the pipeline. While this configuration can be defined per step, it is sometimes more convenient to define it at a pipeline level if it's going to be the same for all steps in the pipeline.
The basic format of each `configuration` section is:
Tag
Description  
`environmentVariables`
Variables defined here are available for use in every step in the pipeline. These variables are read-only; they cannot be redefined in a step.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME` : If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER` : If set, the pipeline uses this value instead of the default run number for the build info collected.
  * ` USE_LOCAL_JFROG_CLI` : If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.
  * `JFROG_XRAY_URL` : If `jfrogCliVersion` is set to 2, this variable may be used to specify an Xray URL for use when configuring the JFrog CLI with an Artifactory integration. In most cases, the platform URL will be correct and `JFROG_XRAY_URL` is not required.

  
`nodePool`
Optionally specify a specific node pool where your steps will execute. If not specified, then the node pool set as the default will be used.
For more information, see Choosing Node Pools.  
`affinityGroup`
Optionally specify an Affinity Group name to specify that all steps in this Pipeline are part of one Affinity Group. This means that all steps will run on the same build node.
For more information, see Running multiple steps on the same build node.  
`runtime`
This section allows you to specify the default runtime environment for steps in the pipeline. The options are:
  * Run steps directly on the host machine
  * Run steps inside the node pool's default Docker container or one of its language-specific variants
  * Run steps inside a custom Docker container of your choice


For more information, see Choosing your Runtime Image.  
`chronological`
Any runs of the pipeline will not start running while another run of the same pipeline is processing if `chronological` is set to true. The default is false, allowing runs to execute in parallel if there are nodes available.  
`dependencyMode`
Specifies when the pipeline may run relative to other pipelines connected by resources. If any of these three settings are true, new runs will not be created for resources updated by other pipelines if there is already a waiting run with the same resources and steps. So if a pipeline runs twice consecutively and the following pipeline has waitOnParentComplete set to true, the following pipeline will only run once. When the pipelines do run, they will use the latest resource versions.
The optional settings are:
  * `waitOnParentComplete` : If true, the pipeline will not start running when a pipeline that outputs a resource that is an input to this pipeline has a waiting or processing run.
  * `waitOnParentSuccess` : If true, the pipeline will not start running when a pipeline that outputs a resource that is an input to this pipeline has a processing run or the last complete run was not successful.
  * `waitOnChildComplete` : If true, the pipeline will not start running when a pipeline that has an input resource that is the output of this pipeline has a waiting or processing run unless that child pipeline is waiting for this pipeline to complete.

  
`retentionPolicy`
Optionally specify if the pipeline run data should be deleted after a specific number of days. Also, provides the ability to keep a minimum number of pipeline runs data:
  * `maxAgeDays` : Specifies number of days after which the pipeline run data will be deleted (cannot exceed the system level setting). Setting this value to 0 means infinite retention.
  * `minRuns` : Specifies the minimum number of pipeline runs data to keep, regardless of their age (cannot exceed the system level setting).


For more information, see Setting Retention Policy.  
`jfrogCliVersion`
Optionally specify either 1 to use JFrog CLI v1 or 2 to use JFrog CLI v2 in the steps in the pipeline. The default is currently v1.  
`integrations `
Specifies Integrations, similar to the `integrations` section in a step, to be input to all steps in the Pipeline. Integrations listed here may not be listed in the `integrations` section of a step in this Pipeline.
For more information, see Pipelines Integrations.
### Note
Pipeline-level declarations for integrations are applicable for all steps. You cannot choose the steps that will not use those integrations.  
`inputResources`
Specifies Resources, similar to the `inputResources` section in a step, to be inputs to all steps in the Pipeline. Resources listed here may not be listed in the `inputResources` or out `putResources` sections of any step in this Pipeline. Each Resource in `inputResources` should be specified by `name` and `branch` (if from a multi-branch Pipeline Sources) and the `trigger` option may be set to control if updates to the resource should trigger steps in the Pipeline.
For more information, see Using Resources.
### Note
Pipeline-level declarations for input resources are applicable for all steps. You cannot choose the steps that will not use those input resources.  
`outputResources`
Specifies Resources, similar to the out `putResources` section in a step, to be inputs to all steps in the Pipeline. Resources listed here may not be listed in the `inputResources` or out `putResources` sections of any step in this Pipeline. Each Resource in `inputResources` should be specified by `name` and `branch` (if from a multi-branch Pipeline Sources).
For more information, see Using Resources.
### Note
Pipeline-level declarations for output resources are applicable for all steps. You cannot choose the steps that will not use those output resources.  
Any step can override the pipeline's default `runtime` configuration if needed to configure its own runtime selection.
##### Pipeline Steps
Each named pipeline declares a collection of named `step` blocks the pipeline will execute.
The basic format of each `step` declaration is:
Tag
Description  
`name`
A friendly name for the step that may be referenced in other steps. Step names must be unique within the same pipeline.  
`type`
A predefined string that specifies the type of step.
For more information, see Pipelines Steps.  
`configuration`
Begins the section of settings required by the step `type`. This may include:
  * Environment variables local to the step
  * Any runtime configuration for the step
  * Any triggering input steps or resources
  * Any resources output by the step
  * Any integrations used by the step
  * All settings required by the step type

  
`execution`
Specifies the actions to perform for each execution phase of the step.  
For example, here is a simple sequence of two steps. Each uses the generic Bash step to output text to the console:
[YAML]
        steps:
        - name: step_1
          type: Bash
          configuration:
            inputResources:
              - name: my_Git_Repository     # Trigger execution on code commit
          execution:
            onExecute:
              - echo ""Hello World!""
     
        - name: step_2
          type: Bash
          configuration:
            inputSteps:
              - name: step_1               # Execute this step after the prior step
          execution:
            onExecute:
              - echo ""Goodbye World!""
[/YAML]
#### Pipeline Config File Strategies
A pipeline config file can have one or more pipelines defined in it, but the definition of a single pipeline cannot be fragmented across multiple files. Pipeline config filenames can be any form you choose, although the convention for a single file is `pipelines.yml`
Some things to note about pipelines:
  * You can have as many pipeline config files as you want. For example, our customers manage config in the following different ways:
    * Maintain a central DevOps repository and keep all pipeline config files for all projects in that repository.
    * Keep pipeline config files that build each microservice with the source code for that microservice.
    * Separating out pipeline steps and resources into separate config files (for example, `pipelines.steps.yml `and `pipelines.resources.yml` respectively).



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
24,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>An **Integration** connects Pipelines to an external service/tool. Each integration type defines the endpoint, credentials and any other configuration detail required for Pipelines to exchange information with the service. All credential information is encrypted and held in secure storage, in conformance with best security practices.
[/INST]#### Using Integrations
Integrations are most commonly used in resources to connect inputs and outputs to the external facility. They can also be referenced directly in many utility functions. Integrations can also be directly used as an input to a step.
Integrations are added through the JFrog Platform. For more information, see Managing Pipelines Integrations.
### Note
All passwords and keys used in integrations are secure and they are masked when printed.
##### Environment Variables
A step that uses an integration can access the integration and its properties through environment variables. The form of the environment variable is:
`int_<integration name>_<tag>`
For example, if an Artifactory Integration is named `myArtifactory`, then the environment variable `int_myArtifactory_url` might be used to send a notification:
[YAML]
    steps:
      . . .
    
      - name: publish_go_binary
      type: GoPublishBinary
      configuration:
        inputSteps:
        - name: build_go
        targetRepository: my-go
        integrations:
        - name: myArtifactory
       execution:
         onSuccess:
           - send_notification notifySlack --text ""Published Go binary to $int_myArtifactory_url""
    
[/YAML]
For more information about using environment variables, see Pipelines Environment Variables.
#### Integration Types
The available integrations for JFrog Pipelines are:
  * Airbrake Integration
  * Artifactory Integration
  * AWS Keys Integration
  * Azure Keys Integration
  * Bitbucket Integration
  * Bitbucket Server Integration
  * Digital Ocean Integration
  * Distribution Integration
  * Docker Registry Integration
  * File Server Integration
  * Generic Integration
  * GitHub Enterprise Integration
  * GitHub IntegrationGitHub Integration
  * GitLab Integration
  * Google Cloud Integration
  * Incoming Webhook Integration
  * Jenkins Integration
  * Jenkins Server Integration
  * JFrog Platform Access Token Integration
  * Jira Integration
  * Kubernetes Integration
  * NewRelic Integration
  * Outgoing Webhook Integration
  * PagerDuty Events Integration
  * PEM Key Integration
  * Slack Integration
  * SMTP Credentials Integration
  * SSH Key Integration



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
25,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The Airbrake integration is used to connect Jfrog Pipelines to Airbrake for notifying deployments performed through Pipelines.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create an Airbrake integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **Url** \-- Airbrake API endpoint. This is already hard-coded to https://airbrake.io/api/v4
  *  **Token** \-- Airbrake API Token


##### Usage
The Airbrake integration can be used in the send_notification utility function.
###### Default Environment Variables
When you create a step that uses the integration in an `inputIntegrations` entity, a set of environment variables is automatically made available that you can use in your scripts.
Environment variable
Description  
`int_<integration_name>_url`
The API Endpoint used to connect to Airbrake  
`int_<integration_name>_token`
The Token used to connect to Airbrake  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
26,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The Artifactory Integration connects your JFrog Pipeline Automation platform to an installation of Artifactory to push artifacts including Docker images, and maintain build information.
[/INST]##### Creating an Integration
Add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- Choose a friendly name for the integration
  *  **Artifactory URL** \-- The Artifactory URL is automatically added for the user adding the integration. If required, you can manually enter the HTTP Endpoint (URL) for your JFrog Artifactory.
  *  **User** \-- The Artifactory username is automatically added for the user adding the integration. If required, you can manually enter the username for your JFrog Artifactory.
  *  **API key** \-- Your Artifactory API Key.Introduction to the User Profile
Click **Get API Key** to automatically fetch API key for the user adding the integration. If the API key does not exist, a new one is generated. If required, you can manually enter the API key for your JFrog Artifactory.


##### Usage
 **With native steps** : An Artifactory integration is mandatory in the `integrations` section of the following native steps:
  * DockerBuild
  * DockerPush
  * GradleBuild
  * MvnBuild
  * NpmBuild
  * NpmPublish
  * PromoteBuild
  * PublishBuildInfo
  * GoBuild
  * GoPublishBinary
  * GoPublishModule


 **In resources** : An Artifactory integration must/may be used in the following resources:
 **Mandatory**
  * Aql
  * BuildInfo


 **Optional**
  * FileSpec
  * Image
  * RemoteFile


###### Default Environment Variables
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration_name>_url`
URL of Artifactory  
`int_<integration_name>_user`
Username supplied in the integration  
`int_<integration_name>_apikey`
Access token supplied in the integration  
When you create an Aql, BuildInfo, or FileSpec resource with this integration, a set of environment variables is automatically made available in steps using that resource.
Environment variable
Description  
`res_<resource_name>_sourceArtifactory_url`
URL of Artifactory  
`res_<resource_name>_sourceArtifactory_user`
Username supplied in the integration  
`res_<resource_name>_sourceArtifactory_apikey`
Access token supplied in the integration  
When you create an Image resource with this integration, a set of environment variables is automatically made available in steps using that resource.
Environment variable
Description  
`res_<resource_name>_registry_url`
URL of Artifactory  
`res_<resource_name>_registry_user`
Username supplied in the integration  
`res_<resource_name>_registry_apikey`
Access token supplied in the integration  
When you create a RemoteFile resource with this integration, a set of environment variables is automatically made available in steps using that resource.
Environment variable
Description  
`res_<resource_name>_source_url`
URL of Artifactory  
`res_<resource_name>_source_user`
Username supplied in the integration  
`res_<resource_name>_source_apikey`
Access token supplied in the integration  
##### Examples
The following example creates a Docker image that is published to Artifactory. It assumes that an Artifactory integration named `myArtifactory` has been created.
  * This example uses a values.yml file to store the pipeline definitions.
  * In addition to the Artifactory integration, this example uses a GitHub Integration.
  * The YAML for this example is available in this repository in the JFrog GitHub account.
  * For a full tutorial, see Pipeline Example: Docker Build and Push.


[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: dbp_repo
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.gitIntegration }}                      
          path: {{ .Values.gitRepositoryPath }} 
          branches:
            include: master
    
      - name: dbp_image
        type: Image
        configuration:
          registry: {{ .Values.artifactoryIntegration }}
          sourceRepository:  {{ .Values.sourceRespository }}
          imageName:  {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}
          imageTag: latest
          autoPull: true
          
      - name: dbp_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
          buildName: dbp_build
          buildNumber: 1    
    
      - name: dbp_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
          buildName: dbp_build
          buildNumber: 1
          
    
    pipelines:
      - name: pipeline_dbp
        steps:
          - name: docker_build
            type: DockerBuild
            configuration:
              affinityGroup: dbp_group          # affinity group string that is the same as specified in the DockerPush step
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName:  {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}
              dockerImageTag: ${run_number}
              inputResources:
                - name: dbp_repo
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
    
          - name: docker_push
            type: DockerPush
            configuration:
              affinityGroup: dbp_group          # affinity group string that is the same as specified in the DockerBuild step
              targetRepository: docker-local
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
              inputSteps:
                - name: docker_build
              outputResources:
                - name: dbp_image
    
          - name: publish_dbp_build
            type: PublishBuildInfo
            configuration:
              inputSteps:
                - name: docker_push
              outputResources:
                - name: dbp_build_info
    
          - name: promote_dbp_build
            type: PromoteBuild
            configuration:
              targetRepository: demo-pipelines
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
              inputResources:
                - name: dbp_build_info
              outputResources:
                - name: dbp_promoted_build_info
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
27,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **AWS Keys** integration is used to provide access and secret keys to AWS services in order to connect to JFrog Pipelines.
The AWS Keys integration is used to connect to AWS services that include:
  * ECR
  * ECS
  * EC2
  * S3
  * Any other service that needs AWS Access and Secret keys to connect


[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Access Key Id** \-- access key Id obtained from AWS
  * **Secret Access Key** \-- secret access key obtained from AWS


##### Usage
The AWS Keys integration can be used in the following resources:
  * Image
  * RemoteFile


An AWS Keys integration can also be added directly to a step in the `integrations` section. This automatically configures the AWS CLI with the access key and secret key.
You can use this integration in any workflow where you need to connect to AWS for some reason, such as provisioning infrastructure, deploying to AWS EC2 or Amazon ECS, etc.
###### Default Environment Variables
When you create an Image resource with this integration, a set of environment variables is automatically made available in steps using that resource.
Environment variable
Description  
`res_<resource_name>_registry_accessKeyId`
Access key supplied in the integration  
`res_<resource_name>_registry_secretAccessKey`
Secret key supplied in the integration  
When you create a Remote File resource with this integration, a set of environment variables is automatically made available in steps using that resource.
Environment variable
Description  
`res_<resource_name>_source_accessKeyId`
Access key supplied in the integration  
`res_<resource_name>_source_secretAccessKey`
Secret key supplied in the integration  
When you add this integration to a step directly, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration_name>_accessKeyId`
Access key supplied in the integration  
`int_<integration_name>_secretAccessKey`
Secret key supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
28,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The Microsoft Azure Integration is used to connect JFrog Pipelines to Microsoft Azure to manage cloud services and entities.
This integration uses a password-based authentication service principal for connecting to and commanding your Azure cloud service. You must create an Azure service principal for this purpose, and provide the generated keys to this integration.
For example, using the Azure CLI service principal creation function:
`az ad sp create-for-rbac --role=""Contributor"" --scopes=""/subscriptions/SUBSCRIPTION_ID""`
will produce the needed key values:  
Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **App Id** \-- the `appId` key used for service principal authentication.
  * **Password** \-- the `password` key used for service principal authentication.
  * **Tenant** \-- the `tenant` key used for service principal authentication.


[/INST]##### Usage
An Azure Keys integration can be used in Bash steps for access to its properties through environment variables. This integraton cannot currently be used with any resources.
Make sure that your service principal has adequate permission to perform whatever actions you plan to take in your custom script. This could mean ACR reader role to pull images, AKS contributor to create deployments, etc.
### Required for Dynamic Nodes
In order for Pipelines to run dynamic node pools on Azure, you will need to accept Azure Marketplace image terms so that the JFrog Pipelines buildplane VM images can be used. The following Azure CLI commands must be run with the Azure service principal used by this integration:
[YAML]
    az vm image terms accept --plan x86_64-ubuntu_16_04 --offer pipelines-buildplane --publisher jfrog
    az vm image terms accept --plan x86_64-ubuntu_18_04 --offer pipelines-buildplane --publisher jfrog
    az vm image terms accept --plan x86_64-centos_7 --offer pipelines-buildplane --publisher jfrog
    az vm image terms accept --plan x86_64-windows_server_2019 --offer pipelines-buildplane --publisher jfrog
[/YAML]
###### Default Environment Variables
When you create a resource with this integration, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_appId`
AppId supplied in the integration  
`int_<integration-name>_password`
Password supplied in the integration  
`int_<integration-name>_tenant`
Tenant supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
29,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Digital Ocean Integration is used to connect JFrog Pipelines to Digital Ocean to interact with its cloud services in order to provision machines.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Token** \-- Token to connect to the Digital Ocean API


##### Usage
The Digital Ocean integration can be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`name_<integration-name>_apitoken`
API Token supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
30,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A Bitbucket Integration is used to connect the JFrog Pipelines platform to bitbucket.org.
[/INST]##### Creating an Integration
You can add this integration by following the steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **URL** \-- Bitbucket API endpoint. This is already hard-coded to https://api.bitbucket.org
  *  **User Name** \-- Username to connect to Bitbucket. This is the username found on the account settings page ( **Personal Settings** → **Bitbucket profile settings** ), and not your email Id or display name.
  *  **Token** \-- Bitbucket token with the permissions needed to run your commands


### Note
Bitbucket integration requires the following permissions:
  *  **Account** : Read
  *  **Pull requests** : Read
  *  **Workspace membership** : Read
  *  **Projects** : Read
  *  **Repositories** : Admin
  *  **Webhooks** : Read and Write


##### Usage
The Bitbucket integration can be used with the GitRepo resource.
The Bitbucket integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it in the `inputResources` section, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_gitProvider_url`
Bitbucket API location  
`res_<resource-name>_gitProvider_username`
Username used to connect to Bitbucket  
`res_<resource-name>_gitProvider_token`
The token used to connect to Bitbucket  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
Bitbucket API location  
`int_<integration-name>_username`
Username used to connect to Bitbucket  
`int_<integration-name>_token`
The Token used to connect to Bitbucket  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
31,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A Bitbucket Server Integration is used to connect the JFrog Pipelines platform to an installation of Bitbucket Server.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **URL** \-- Bitbucket Server API endpoint. For example: `https://bitbucket.myserver.com/`
  *  **User Name** \-- username to connect to Bitbucket Server
  *  **Password** \-- password for the user account on Bitbucket Server


### Note
Bitbucket Server integration requires Admin permission for repositories.
##### Usage
The Bitbucket Server integration is most commonly used with the GitRepo resource or while adding a Pipeline source.
The Bitbucket Server integration can also be added directly to a step in the `integrations` section.
### URL Specification
When a Bitbucket Server repository is part of a project, the URL uses the project key rather than the project name. So if the ""Teams in Space"" project key is TIS, then the ""buildPipe"" repository in that project is addressed as TIS/buildpipe.
When a Bitbucket Server repository is a personal repository, the '~' (tilde) character must precede the user slug in the URL. For example: ~janedoe/test.
For additional information, see Atlassian Stash REST API Reference.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it in the `inputResources` section, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_gitProvider_url`
Bitbucket Server API location  
`res_<resource-name>_gitProvider_username`
Username used to connect to a Bitbucket Server user account  
`res_<resource-name>_gitProvider_password`
The password used to connect to the Bitbucket Server user account  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
Bitbucket Server API location  
`int_<integration-name>_username`
Username used to connect to the Bitbucket Server user account  
`int_<integration-name>_password`
The password used to connect to the Bitbucket Server user account  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
32,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The Distribution Integration connects your JFrog Pipeline Automation platform to a Distribution instance.
[/INST]##### Creating an Integration
Add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- Choose a friendly name for the integration
  *  **Distribution URL** \-- The Distribution URL is automatically added for the user adding the integration. If required, you can manually enter the HTTP Endpoint (URL) for your JFrog Distribution.
  *  **User** \-- The Distribution username is automatically added for the user adding the integration. If required, you can manually enter the username for your JFrog Distribution.
  *  **API Key** \-- Your Distribution API key.
Click **Get API Key** to automatically fetch API key for the user adding the integration. If the API key does not exist, a new one is generated. If required, you can manually enter the API key for your JFrog Distribution.
  *  **Signing Key Passphrase** \-- Your optional GPG signing key passphase.


##### Usage
A Distribution integration is used in the following resources:
  * DistributionRule
  * ReleaseBundle


###### Default Environment Variables
When you create a DistributionRule or ReleaseBundle resource with this integration, a set of environment variables is automatically made available that you can use in your scripts.
Environment variable
Description  
`res_<resource_name>_sourceDistribution_url`
Distribution URL  
`res_<resource_name>_sourceDistribution_user`
Username supplied in the integration  
`res_<resource_name>_sourceDistribution_apikey`
API Key supplied in the integration  
`res_<resource_name>_sourceDistribution_signingKeyPassphrase`
GPG signing key passphrase supplied in the integration  
When you add this integration directly to a step, a set of environment variables is automatically made available that you can use in your scripts.
Environment variable
Description  
int_<integration_name>_url
Distribution URL  
int_<integration_name>_user
Username supplied in the integration  
`int_<integration_name>_apikey`
API Key supplied in the integration  
`int_<integration_name>_signingKeyPassphrase`
GPG signing key passphrase supplied in the integration  
##### Examples
This example uses a BuildInfo resource to create, sign and distribute a release bundle. It assumes that a Distribution integration named `myDist` has been created.
  * In addition to the Distribution integration, this example requires an Artifactory integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


 **CreateReleaseBundle**
[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.myArtifactoryIntegration }}
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.demoArtifactoryIntegration }}
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
33,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The Docker Registry Integration is used to connect the JFrog Pipelines platform to Docker Hub, Docker Trusted Registry, or a Docker registry in Artifactory so that you can pull and push Docker images.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **url** \-- an optional field that defaults to Docker Hub if left empty. To use with Docker Trusted Registry or Docker Private Registry, enter the location of your private registry. Format `https://foo.com`
  *  **User Name** \-- username of your Docker Registry Account
  *  **Password** \-- password of your Docker Registry Account


##### Usage
The Docker Registry integration can be used in the Image resource.
The Docker Registry integration can also be added directly to a step in the `integrations` section. This automatically configures the Docker CLI with access to the registry.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_registry_url`
URL supplied in the integration  
`res_<resource-name>_registry_username`
Username supplied in the integration  
`res_<resource-name>_registry_password`
Password supplied in the integration  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
URL supplied in the integration  
`int_<integration-name>_username`
Username supplied in the integration  
`int_<integration-name>_password`
Password supplied in the integration  
##### Example
  * This example uses a Docker Registry integration named `myDocker`, which is used in the `Image_1` resource.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:  
      - name: Image_1
        type: Image 
        configuration:         
          registry: myDocker              
          imageName: docker/jfreq_win             
          imageTag: latest  
          autoPull: true
    
    pipelines:
      - name: pipelines_Image_1
        steps:
          - name: step_Image_1
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: Image_1
            execution:
              onExecute:
                - write_output ""executing step...""
                - if($Image_1_isTrigger -ne ""true""){
                  Write-Error ""Stop"" -ErrorAction Stop
                  }
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
34,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **File Server** integration is used to connect JFrog Pipelines a remote File server to download and upload files.
[/INST]##### Creating an Integration
You can create this from the integrations page by following instructions here: Managing Pipelines Integrations.
This is the information you would require to create this integration:
  *  ** **Name**** \-- choose a friendly name for the integration
  *  **Protocol** \-- select FTP, SFTP, or SMB
  *  **url** \-- the URL of your file server
  *  **User Name** \-- username for access to the file server
  *  **Password** \-- password for access to the file server


##### Usage
A File Server integration is required for the `source` tag of the RemoteFile resource.
###### Default Environment Variables
When you create a resource with this integration, a set of environment variables is automatically made available that you can use in your scripts.
Environment variable
Description  
`int_<integration_name>_url`
URL of the file server  
`int_<integration_name>_username`
Username supplied in the integration  
`<int_<integration_name>_password`
Password supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
35,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A Generic integration is used to store key/value information that can be accessed in steps as environment variables. Because all integration properties are stored in a secure, encrypted vault, this is suitable for storing secrets such as username/password credentials.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **Custom Environment Variables** \-- the _Key_ and _Value_ pair to store. Select **Secure** to secure the custom environment variable.


You can add as many key-value pairs as you need in the integration by clicking **Add** for each.
##### Usage
The Generic integration can be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you add this integration to a step, All key-value pairs are made available as environment variables.
Environment variable
Description  
`int_<integration-name>_<key>`
The value stored for the custom environment variable.  
####### Example
You can define a Generic integration with the following entries:
  *  **Name** : `myCredentials`
  *  **Custom Environment Variables** :
    * Key: `username`
    * Value: `janedoe`
    * Key: `password`
    * Value: `nAm30fMyp3t`


When `myCredentials` is specified in a step's `integrations` block, the key-value pairs stored there can then accessed in the step as environment variables:
[YAML]
    pipelines:
      - name: generic_integration_example
        steps:
        - name: step_1
          type: Bash
          configuration:
            integrations:
              - name: myCredentials
          execution:
            onExecute:
              - printenv $int_myCredentials_username
              - printenv $int_myCredentials_password
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
36,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The GitHub Integration is used to connect the JFrog Pipelines platform to GitHub.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- Choose a friendly name for the integration
  *  **url** \-- Your GitHub API endpoint. This is already hard-coded to https://api.github.com
  *  **Token** \-- Your GitHub personal access token with the permissions needed to run your jobs
### Token permissions
To use this integration as a pipeline source, your token must be granted these permissions at minimum:
    *  **repo** (all)
    *  **admin:repo_hook** (read, write)
    *  **admin:public_key** (read, write)
Additionally, if your organization uses SAML single sign-on, you will need to authorize your token.
  *  **HTTP Header** \-- Optional. Use the key and value fields to provide the name and value for the custom HTTP header you would like to pass in the HTTP requests for Git clone events.


##### Usage
The GitHub integration can be used in the GitRepoGitRepo resource.GitRepo
The GitHub integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_gitProvider_url`
GitHub API location  
`res_<resource-name>_gitProvider_token`
The Token used to connect to GitHub  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
GitHub API location  
`int_<integration-name>_token`
The Token used to connect to GitHub  
##### Examples
This example shows how to configure an NpmBuild step using default locations and default commands. It assumes that a GitHub integration named m`yGithub` has been created.
  * This example uses a values.yml file to store the pipeline definitions.
  * The YAML for this example is available in this repository in the JFrog GitHub account.
  * For more information about running this example, see Pipeline Example: Npm Build.


 **NpmBuild**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: npm_repo_jfp_example
        type: GitRepo
        configuration:
          # SCM integration where the repository is located
          gitProvider: {{ .Values.myRepo.gitProvider }}
          # Repository path, including org name/repo name
          path: {{ .Values.myRepo.path }}
          branches:
            # Specifies which branches will trigger dependent steps
            include: master
    
      - name: npm_buildinfo_jfp_example
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
    
    pipelines:
      - name: npm_pipeline_jfp_example
        steps:
          - name: npm_build_step
            type: NpmBuild
            configuration:
              repositoryName: npm-virtual      # required, npm repository name on artifacctory
              sourceLocation: .      # required, location of package.json file
              integrations:
                - name:  demoArt  # required
              inputResources:
                - name: npm_repo_jfp_example         # required
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
37,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The GitHub Enterprise Integration is used to connect JFrog Pipelines to your instance of GitHub Enterprise Server.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **url** \-- location of your GHE server API. The format is `https://(GitHub Enterprise URL)/api/v3`
  * **Token** \-- your GitHub personal access tokenwith the permissions needed to run your job
### Token permissions
To use this integration as a pipeline source, your token must be granted these permissions at minimum:
    * **repo** (all)
    * **admin:repo_hook** (read, write)
    * **admin:public_key** (read, write)
Additionally, if your organization uses SAML single sign-on, you will need to authorize your token.


##### Usage
The GitHub Enterprise integration can be used in the GitRepo resource.
The GitHub Enterprise integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses the resource, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_gitProvider_url`
GitHub Enterprise API location  
`res_<resource-name>_gitProvider_token`
Token used to connect to GitHub Enterprise  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
GitHub Enterprise API location  
`int_<integration-name>_token`
Token used to connect to GitHub Enterprise  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
38,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The GitLab Integration is used to connect JFrog Pipelines to your instance of GitLab so that you can run builds for your repositories hosted there.
[/INST]### Note
Pipelines does not support GitLab subgroups.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **URL** \-- location of your GitLab API. The URL should be in the format `https://(GitLab URL)/api/(api version)`. For example, if you're using gitlab.com, this will be `https://gitlab.com/api/v4`. Please note that if you're using Gitlab version 9.0 or later, you should use `v4` for api version. If you're using Gitlab version 8.17 or earlier, you should use `v3` for api version. API v3 is unsupported from Gitlab 9.5 according to this Gitlab notice
  *  **Token** \-- GitLab private token with the right levels of permission
### Note
 **Token Permissions**
To use this integration as a pipeline source, your token must be granted these scopes at minimum:
    *  **api**
    *  **write_repository**
Additionally, if your organization uses SAML single sign-on, and you want your tokens to work with the SAML, you will have to configure it.


##### Usage
The GitLab integration can be used in the GitRepo resource.
The GitLab integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_gitProvider_url`
GitLab API location  
`res_<resource-name>_gitProvider_token`
The token used to connect to GitLab  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`name_<integration-name>_url`
GitLab API location  
`name_<integration-name>`_token
The Token used to connect to GitLab  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
39,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The Google Cloud Integration is used to connect JFrog Pipelines to Google Cloud and manage entities and services provided by Google Cloud.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
This is the information you would require to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **JSON Key** \-- JSON Security Key for Google Cloud


##### Usage
The Google Cloud integration can be used in the following resources:
  * Image
  * RemoteFile


The Google Cloud integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create an Image resource with this integration, and create a step that uses the resource, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_registry_jsonKey`
JSON key supplied in the integration  
When you create a Remote File resource with this integration, and create a step that uses the resource, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_source_jsonKey`
JSON key supplied in the integration  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_jsonKey`
JSON key supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
40,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The Jenkins Integration is used to connect the JFrog Pipelines platform to Jenkins. This will enable a Jenkins build job to trigger execution of a pipeline in Pipelines.
[/INST]### Deprecated
As of Pipelines 1.6.0, this integration is deprecated and unavailable for new use. Pipelines using this integration will still function, but it has been retired in favor of the Jenkins Server Integration.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **url** \-- enter the HTTP endpoint (URL) of your Jenkins
  * **callback user** \-- JFrog Platform username to use for basic authentication in the callback URL to Pipelines
  * **callback password** \-- JFrog Platform user password


##### Usage
The Jenkins integration can be used in the BuildInfo resource.
A Jenkins integration can be added directly to a step in the `integrations` section to access its environment variables.
Once the integration has been created, it provides the format of the callback URL and curl command lines that your Jenkins job must invoke to trigger execution of a pipeline in Pipelines.  
###### Default Environment Variables
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
HTTP endpoint of Jenkins  
`int_<integration-name>_hookUsername`
Callback Username  
`int_<integration-name>_hookPassword`
Callback Password  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
41,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The Jenkins Integration is used to connect the JFrog Pipelines platform to Jenkins through the Jenkins Artifactory Plugin.Configuring Jenkins Artifactory Plug-in
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **url** \-- enter the HTTP endpoint (URL) of your Jenkins server
  * **Jenkins user** \-- the Jenkins account user name
  * **Jenkins API token** \-- the Jenkins user's API token for authentication
  * **Token to configure Jenkins Plugin** \-- generate the required **Token to configure Jenkins Plugin** to complete the integration.


### Note
For information about using the Jenkins server integration, see Using Jenkins With Pipelines.
##### Usage
The Jenkins Server integration can be used with the Jenkins native step. Operation requires that the Jenkins Artifactory Plugin be installed in your Jenkins server.Configuring Jenkins Artifactory Plug-in
For Jenkins to successfully interoperate with Pipelines, a bearer token generated by Pipelines must be provided to the required plugin for authentication of your JPD user account.
###### Configuration
To complete the Jenkins Server integration and connect it to the Jenkins Artifactory Plugin:
  1. In the Jenkins Server Integration UI, enter the required configuration information.
  2. Next to the field **Token to configure Jenkins Plugin** , click **Generate**.  
  3. Once the token is generated, click **Copy to Clipboard**.  
  4. In the Jenkins Artifactory Plugin configuration, in the **JFrog Pipelines server** section, **Add** the token to the _Credentials **.** _, pasting the value from your clipboard.
  5. Return to Pipelines, and open the Jenkins Server Integration again for editing. You should now see a **Callback URL** presented. Click **Copy webhook url to clipboard**.  
  6. In the Jenkins Artifactory Plugin configuration, past this URL into the _Integration URL_ field.  


###### Default Environment Variables
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
HTTP endpoint of Jenkins  
`int_<integration-name>_jenkinsUsername`
Jenkins username  
`int_<integration-name>_jenkinsToken`
Jenkins API token  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
42,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The JFrog Platform Access Token integrationgives steps the ability to utilize an access token to interact with different pieces of the JFrog platform. These tokens can be used for many of JFrog's REST APIs, including the Pipelines API. This integration is also required when using the TriggerPipeline native step.
[/INST]##### Creating an Integration
Add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- Choose a friendly name for the integration
  *  **Platform URL** \-- The Platform URL is automatically added for the user adding the integration. If required, you can manually enter the HTTP Endpoint (URL) for your JFrog Platform.
  *  **Access Token** \-- Your Artifactory API Key. You can automatically generate a token that represents the logged in user from the User Profile page, or you can follow the Access Tokens documentation to create a more nuanced token with particular scopes.Introduction to the User ProfileIntroduction to the User ProfileIntroduction to Access Tokens
When creating an Access Token, ensure that **Token scope** and **User name** are `Admin`.  


##### Usage
 **With native steps** : An Artifactory integration is mandatory in the `integrations` section of the TriggerPipeline native step:
 **Default Environment Variables**
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration_name>_url`
The API Endpoint of the JFrog Platform  
`int_<integration_name>_accessToken`
The token used to access the JFrog Platform  
##### Examples
[YAML]
    pipelines:
      steps:
        - name: scan_controller
          type: TriggerPipeline
          configuration:
            pipelineName: steptarget
            stepName: scanIt
            integrations:
              - name: myPlatformToken
            environmentVariables:
              scan_target:
                default: ""hello-world""
                allowCustom: true
                values:
                  - ""vault""
                  - ""redis""
                  - ""postgresql""
          execution:
            onStart:
              - set_trigger_payload pipelineVariables ""scan_target=${scan_target}""
              - set_trigger_payload stepVariables ""notify=email"" ""uploadReport=true""
            onComplete:
              - echo ""Final status is $nested_run_status""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
43,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A Jira integration is required to create issues on Jira server from JFrog Pipelines steps.
[/INST]##### Adding an integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Url** \-- Jira URL (for example, `https://jira.mycompany.com`)
  * **Username** \-- account username/email used for login
  * **Token/Password** \-- either a Jira API Tokenfor the account with permissions on the Jira REST APIs(for Jira Cloud), or the account password (for Jira Server)


##### Usage
The Jira integration can be added directly to a step in the `integrations` section.
The  send_notificationutility function can be used with a Jira integration to create an issue in Jira. For example, if a deployment fails, a ticket can be created in Jira through this integration. For more information, see Creating Jira Issues From Pipelines.
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
Jira API endpoint URL  
`int_<integration-name>_username`
Username or email to use to login  
`int_<integration-name>_token`
Authorization to use for invoking the APIs  
##### Example
The following example pipeline demonstrates the use of the `send_notification` utility function to create a Jira issue (ticket).
  * The YAML for this example is available in this repository in the JFrog GitHub account.
  * In addition to the Jira integration, this example uses an Artifactory Integration.
  * This example sets environment variables for the project-id and type options.
  * This example performs a MvnBuild native step, which produces a log file.
  * On failure, this example uses the `send_notification` utility function to create the JIra ticket. The command line:
    * Relies on the environment variables for the project-id and type options
    * Attaches the log file produced by Maven to the Jira issue
    * Specifies the summary and description options using  standard environment variables


[YAML]
    resources:
      - name: my_repo
        type: GitRepo
        configuration:
          gitProvider: MyGithub
          path: myrepo/myproject
    
    pipelines:
      - name: jira_ticket_example
        configuration:
          environmentVariables:
            readOnly:
              NOTIFY_PROJECT_ID: ""DEMO""            # Jira project key for all tickets we create
              NOTIFY_TYPE: ""Bug""                   # Jira issue type for all tickets we create
        steps:
        ### with jira for log attachment
          - name: BuildSample
            type: MvnBuild
            configuration:
              sourceLocation: artifactory-maven-plugin-example
              configFileLocation: .
              configFileName: config
              mvnCommand: ""install -P release --log-file ${step_tmp_dir}/log.txt""
              inputResources:
                - name: my_repo
              integrations:
                - name: myArtifactory
                - name: myJira
            execution:
              onFailure:
                - send_notification myJira --attach-file ""$step_tmp_dir/log.txt"" --description ""Failure occured in $pipeline_name - $step_name"" --summary ""$step_name has failed"" 
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
44,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>An Incoming Webhook Integration is used to receive a webhook from an external system (for example, GitHub) to trigger a pipeline step.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Authorization Type** -
select **hmac** , **basic** , or **authorization**
  * **Secret** \-- secret to use with hmac authType
  * **User Name** \-- username to use with basic authType to trigger the webhook
  * **Password** \-- password for the user to trigger the webhook for basic authType
  * **Authorization** \-- authorization to use with authorization authType to trigger the webhook


###### Incoming Webhook URL
When an Incoming Webhook integration is created, it generates a unique webhook URL that can be used to post information into any resource that is associated with the integration.  
##### Usage
Incoming Webhook can be used in the Incoming Webhook resource.
The Incoming Webhook integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_webhookName_authType`
Select hmac, basic, or authorization  
`res_<resource-name>_webhookName_secret`
Secret to use with hmac  
`res_<resource-name>_webhookName_username`
Username to use to trigger the webhook  
`res_<resource-name>_webhookName_password`
Password for the user  
`res_<resource-name>_webhookName_authorization`
Authorization to trigger the webhook  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_authType`
Select hmac, basic, or authorization  
`int_<integration-name>_secret`
Secret to use with hmac  
`int_<integration-name>_username`
Username to use to trigger the webhook  
`int_<integration-name>_password`
Password for the user  
`int_<integration-name>_authorization`
Authorization to trigger the webhook  
##### Example
  * This example uses an Incoming Webhook Integration named `MyIncomingWebhookIntegration`. The unique Webhook URL this integration creates can be used to post information into any resource that is associated with the integration. In this example, this integration is used in the resource named `MyIncomingWebhookResource`. To trigger the `ExampleStep`step, the resource`MyIncomingWebhookResource` is used as an input resource in the step.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      - name:               MyIncomingWebhookResource
        type:               IncomingWebhook
        configuration:
          webhookName: MyIncomingWebhookIntegration
    
    pipelines:
      - name: ExamplePipeline
        steps:
        - name: ExampleStep
          type: Bash
          configuration:
            inputResources:
              - name: MyIncomingWebhookResource
          execution:
              onExecute:
                - echo ""$res_MyIncomingWebhookResource_payload"" | jq '.' > payload.json
                - read_json payload.json ""LESSER_SECRET_FORMULA""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
45,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The Kubernetes Integration is used to connect JFrog Pipelines platform to self-hosted Kubernetes clusters so that you can deploy Docker based applications.
A Kubernetes integration can be used for these purposes:
  * To deploy builds to nodes in Kubernetes clusters from pipelines using a HelmChart resource with the HelmDeploy step.
  * To enable Pipelines to create and destroy build nodes in Kubernetes clusters on demand through a  dynamic node pool.


[/INST]##### Creating an Integration
Kubernetes Integration can be added as:
  * A Pipeline integration, to use Kubernetes for deploying builds from a pipeline
  * An Admin integration, to use Kubernetes for a dynamic node pool


For more information, see Adding an Integration.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Kube Config** \-- configuration YAML to access Kubernetes cluster
### Note
The Kube Config file should include the following permissions:
    * Create/scale/destroy Deployments
    * Create/delete PVCs
    * Create/delete Secrets
    * Create/delete ConfigMaps
Note that ConfigMaps is required only when user is using the  insecure docker registries feature.


##### Usage
The Kubernetes integration can also be added directly to a step in the `integrations` section. This automatically configures the kubectl command line interface with access to the Kubernetes cluster.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_kubernetesConfig_kubeconfig`
Kubernetes cluster configuration  
When add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_kubeconfig`
Kubernetes cluster configuration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
46,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A NewRelic integration is used to connect JFrog Pipelines to NewRelic for recording deployments.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** : choose a friendly name for the integration
  * **URL** : NewRelic API endpoint. This is already hard-coded to `https://api.newrelic.com/v2`.
  * **Token** : NewRelic API Token


##### Usage
The NewRelic integration can be added directly to a step in the `integrations` section.
The send_notification utility function can be used with a NewRelic integration to send a notification message.
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
NewRelic API endpoint  
`int_<integration-name>_token`
Token used to communicate with NewRelic  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
47,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>An Outgoing Webhook Integration is used by JFrog Pipelines to send webhook to an external system to trigger events on that system.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Webhook URL** \-- URL of endpoint to receive the payload
  * **Authorization Type** -
select hmac, basic, or authorization
  * **Secret** \-- secret to use with hmac authType
  * **User Name** \-- username to use with basic authType for invoking the webhook
  * **Password** \-- password for the user to use with basic authType
  * **Authorization** \-- authorization to use with authorization authType for invoking the webhook in the external system


##### Usage
Outgoing Webhook integration can be used in the Outgoing Webhook resource.
The Outgoing Webhook integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_webhookName_webhookURL`
Webhook URL to receive the outgoing payload  
`res_<resource-name>_webhookName_authType`
AuthType: hmac, basic, or authorization  
`res_<resource-name>_webhookName_secret`
Secret to use with hmac  
`res_<resource-name>_webhookName_username`
Username to use for invoking the webhook  
`res_<resource-name>_webhookName_password`
Password for the user  
`res_<resource-name>_webhookName_authorization`
Authorization to use for invoking the webhook  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_webhookURL`
Webhook URL to receive the outgoing payload  
`int_<integration-name>_authType`
AuthType: hmac, basic, or authorization  
`int_<integration-name>_secret`
Secret to use with hmac  
`int_<integration-name>_username`
Username to use for invoking the webhook  
`int_<integration-name>_password`
Password for the user  
`int_<integration-name>_authorization`
Authorization to use for invoking the webhook  
##### Example
  * This example uses an Outgoing Webhook Integration named `MyOutgoingWebhookIntegration`, which is used in the resource named `My_OutHook_formurl_Resource`. The step `my_send_hook_formurl` uses this resource as an output resource to send a payload to the endpoint specified in the integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


**Outgoing Form URL Webhook**
[YAML] 
    resources:
      - name: My_OutHook_formurl_Resource
        type: OutgoingWebhook
        configuration:
          webhookName: MyOutgoingWebhookIntegration
          path: /url
          headers:
            content-type: application/x-www-form-urlencoded
    
    pipelines:
      - name: ExamplePipeline
        steps:
          - name: my_send_hook_formurl
            type: Bash
            configuration:
              outputResources:
                - name: My_OutHook_formurl_Resource
            execution:
              onExecute:
                - echo ""hello world""
                - formUrlEncodedPayload=""foo=bar&runNumber=$run_number&stepName=$step_name""
                - set_payload My_OutHook_formurl_Resource ""$formUrlEncodedPayload""
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
48,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The PagerDuty Events integration is used to connect JFrog Pipelines platform to PagerDuty for notifying events.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create an PagerDuty Events integration:
  * **Name** \-- choose a friendly name for the integration.
  * **Token** \-- PagerDuty integration key/token.


##### Usage
The PagerDuty integration can be used in the  send_notification utility function.
###### Default Environment Variables
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_token`
The integration key that identifies your PagerDuty service  
##### Example
  * This example uses a PagerDuty integration named `myPagerDuty`.
  * The send_notification utility function is used to send an `onExecute`notification.
  * The YAML for this example is available in this repository in the JFrog GitHub account.


**Sending notifications**
[YAML] 
    pipelines:
      - name: pipeline_pd
        steps:
          - name: bash_pagerduty_success
            type: Bash
            configuration:
              integrations:
                - name: myPagerDuty
            execution:
              onStart:
                - echo ""starting""
              onExecute:
                - send_notification myPagerDuty -text ""hello world from JFrog Pipelines run number $run_number""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
49,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The PEM Key Integration is used to connect JFrog Pipelines to VMs that allow PEM-based authorization. This is typically used to SSH in and then run commands on the machine. Tools like Terraform and Ansible use this to execute scripts on a machine.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **Key** \-- encrypted Key in PEM format


 **Note:** The PEM Key must be generated without passphrase.
##### Usage
The PEM Key integration can be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_key`
PEM key  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
50,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A Slack integration is required to send notifications to Slack channels or rooms for pipeline events (to learn more about the Slack integration, see Artifactory and Xray Integration with Slack).Artifactory and Xray Integration with Slack
[/INST]##### Creating an Integration
  * Follow instructions in the Slack docs to create an incoming webhook. While you should specify a default or popular channel while creating the webhook, it can be overriden in your pipeline source when needed.
  * Copy the text in the **Webhook URL** section. It looks like this:
`https://hooks.slack.com/services/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`
  * To add your integration, follow steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
    *  **Name** \-- choose a friendly name for the integration
    *  **Webhook url** \-- Webhook URL to your Slack channel


##### Usage
The Slack integration can be added directly to a step in the `integrations` section.
The send_notification utility function can be used with a Slack integration to send a notification message.
For more information, see Sending Notifications from Pipelines.
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
Webhook URL to your Slack channel  
##### Example
  * This example uses a Slack integration named `mySlack`.
  * The send_notification utility function is used to send the `onStart`and`onExecute`notifications.
  * The YAML for this example is available in this repository in the JFrog GitHub account.


 **Sending notifications**
[YAML]
    pipelines:
      - name: pipe1
        steps:
          - name: test_step_1
            type: Bash
            configuration:
              integrations:
                - name: mySlack       # This is the name of the Slack integration you created in your test instance
                    execution:
                      onStart:
                        - send_notification mySlack --text ""starting test_step_1""         # replace 'mySlack' with the name of your Slack integration
              onExecute:
                - send_notification mySlack --text ""onExecute, past test commands""  # replace 'mySlack' with the name of your Slack integration
                      onSuccess:
                        - send_notification mySlack      # without '--text' to send a generic notification that includes the pipeline name and step name
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
51,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>An SMTP Credentials integration is used to send notifications for pipeline events using SMTP email.
[/INST]##### Creating an Integration
To add this integration, follow the steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Host** \-- endpoint to receive the payload
  * **Port** \-- port to use to send the email
  * **Host Name** \-- optional identifying label for the email
  * **Email Sender** \-- optional sender email address
  * **Email Auth User** \-- optional username for authorization to send the email
  * **Email Auth Password** \-- optional password for authorization to send the email
  * **Proxy** \-- optional proxy setting for the email server
  * **Secure** \-- optional secure option to connect to the email server
  * **ignoreTLS** \-- optional TLS option to connect to the email server without using TLS


##### Usage
The SMTP Credentials integration can be added directly to a step in the `integrations` section.
The send_notification utility function can be used with a SMTP Credentials integration to send a notification message.
Example:
`send_notification notifySMTP --body ` `""built docker image 112.32.101.173:8081/docker-local/demo:$PIPELINE_NAME.$RUN_NUMBER""`  
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_name`
Name of the integration  
`int_<integration-name>_host`
Endpoint to receive the payload  
`int_<integration-name>_port`
Port to use to send the email  
`int_<integration-name>_hostname`
Identifying label for the email  
`int_<integration-name>_senderEmail`
Sender email address  
`int_<integration-name>_username`
Username for authorization to send the email  
`int_<integration-name>_password`
Password for authorization to send the email  
`int_<integration-name>_proxy`
Proxy setting for the email server  
`int_<integration-name>_secure`
Secure option to connect to the email server  
`int_<integration-name>_ignoreTLS`
TLS option to connect to the email server without using TLS  
##### Example
  * This example uses an SMTP Credentials integration named notifySMTP.
  * The send_notification utility function is used to send the `onSuccess` and `onFailure` notifications.
  * The YAML for this example is available in this repository in the JFrog GitHub account.


**Sending notifications**
[YAML] 
    pipelines:
      - name: smtp_pipe
        steps:
          - name: test_step_1
            type: Bash
            configuration:
              integrations:
                - name: notifySMTP       # replace with friendly name of your SMTP Credentials integration
            execution:
              onSuccess:
                # replace 'notifySMTP' with the name of your SMTP Credentials integration
                - send_notification notifySMTP --recipients ""buildalerts@mycompany.com"" --subject ""Build Succeeded"" --body ""built docker image 112.32.101.173:8081/docker-local/demo:$PIPELINE_NAME.$RUN_NUMBER""
              onFailure:
                # replace 'notifySMTP' with the name of your SMTP Credentials integration
                - send_notification notifySMTP --recipients ""buildalerts@mycompany.com"" --subject ""Build Failed"" --body ""Failed docker image 112.32.101.173:8081/docker-local/demo:$PIPELINE_NAME.$RUN_NUMBER"" 
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
52,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**SSH Key** Integration is used to connect JFrog Pipelines to VMs that allow SSH based auth. This is typically used to SSH in and then run commands on the machine. Tools like Terraform and Ansible use this to execute scripts on a machine.
[/INST]##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Public Key** \-- Public SSH Key
  * **Private Key** \-- Private SSH Key


**Note:** The SSH Key must be generated without passphrase.
##### Usage
The SSH Key integration can be used in the following resources:
  * VmCluster


The SSH Key integration can also be added directly to a step in the `integrations` section. This automatically adds the public and private SSH keys to the SSH client configuration file (~/.ssh) to enable a secure connection through the `ssh` command.
###### Default Environment Variables
When you create a VmCluster resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_sshKey_publicKey`
Public Key supplied in the integration  
`res_<resource-name>_sshKey_privateKey`
Private Key supplied in the integration  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_publicKey`
Public Key supplied in the integration  
`int_<integration-name>_privateKey`
Private Key supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
53,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Resources are one of the key building blocks of all pipelines. They are information entities that are used for storing and exchanging information across steps and pipelines.
Resources are versioned and each version is immutable. They are also global and depending on the scope defined for the pipeline source, they can be available across pipelines, which enable you to connect multiple pipelines together to create a pipeline of pipelines.
Resources are pointers and they can be used to reference:
  * A repository in your source code control system, such as GitHub
  * A file on a remote file server
  * A Docker image
  * A release bundle for JFrog Distribution
  * A cluster for container orchestration

  
Using resources in your pipeline involves two main steps:
  1. In the pipeline's YAML, in the `resources` section, define all the resources required for running the pipeline.
After a resource is defined, it is available for use in the pipeline, based on the scope defined for the pipeline source.
  2. In the `steps` section, as per your workflow, add these resources as input and/or output.


 **Resource Types**
  * Aql
  * Artifact
  * BuildInfo
  * CronTrigger
  * DistributionRule
  * FileSpec
  * GitRepo
  * HelmChart
  * Image
  * IncomingWebhook
  * OutgoingWebhook
  * PropertyBag
  * ReleaseBundle
  * RemoteFile
  * VmCluster


[/INST]### See it Live
Click  **here**  to see some of these resources in action.
##### Resource Scope and Visibility
Pipeline resources have the same scope as the pipeline source where they are defined.
When defining resources, the recommended approach is to define them in a pipeline source that is shared across environments in the same Project. This ensures that the resources are available across environments in a project. For more information, see Creating a Project - Pipelines Resources.Creating a Project
### Note
Currently, pipeline resources cannot be shared across Projects.
##### Resource Types
Resources play different roles based on where and how they are used:
  *  **Triggering Resource** : Whenever there is a change in this type of resource, it triggers the dependent step. Examples: CronTrigger, GitRepo, IncomingWebhook.
  *  **Generated Resource** : These are resources that are generated by a step and they can trigger successive downstream steps. Examples: Aql, BuildInfo, FileSpec, Image.
  *  **Webhook Resource** : IncomingWebhook and OutgoingWebhook are resources that can be used to integrate a pipeline with third-party services.


Pipelines supports several types of resources, with each resource capable of serving a specific activity:
Type
Description  
Aql
An Aql resource specifies an Artifactory query using Artifactory Query Language.  
BuildInfo
A BuildInfo resource is the metadata associated with a build in Artifactory.  
CronTrigger
A CronTrigger is used as an input resource as a step to trigger execution of the step at a scheduled time.  
DistributionRule
A DistributionRule resource is the set of Destination rules that can be applied to distributing a release bundle using JFrog Distribution.  
FileSpec
A FileSpec resource specifies a File Spec, which provides the details of files to upload or download from Artifactory.  
GitRepo
A GitRepo is used to connect JFrog Pipelines to a source control repository. It creates a webhook to the repo so that future commits will automatically create a new version with the webhook payload.  
HelmChart
The HelmChart resource maps to a specific chart in an Artifactory Helm Repository.  
Image
An Image resource is used to add a reference to a Docker image to your pipeline.  
IncomingWebhook
An IncomingWebhook resource can trigger one or more jobs in your pipeline whenever the associated URL is called using the HTTP POST method.  
OutgoingWebhook
An OutgoingWebhook resource uses HTTP to send information from a step to an external API endpoint through an Outgoing Webhook Integration.  
PropertyBag
A PropertyBag resource is used to pass information from one pipeline to another and provide environment variables to a step in the format of a resource.  
ReleaseBundle
A ReleaseBundle resource specifies a set of artifacts in Artifactory that are distributed to Artifactory Edge nodes as a JFrog Distribution Release Bundle.  
RemoteFile
A RemoteFile resource enables using a file on a remote file server.  
VmCluster
A VmCluster is used to represent a set of virtual machines. It is mainly used to deploy services/apps to the specified clusters and in some cases, it can be used to run certain maintenance activities on the clusters as a whole.  
##### Resource Versions
One of the key features of Pipelines resource is versioning. Resource versions are useful for controlling the flow of pipelines and tracking the changes that a resource undergoes over time. You can trigger runs using specific versions or skip steps in the same run when input resources are not updated.
Every resource starts with an initial version, which is updated every time the resource changes. For example, the version of an Image resource used in a pipeline is updated whenever a new tag is pushed to a Docker image. Pipelines tracks these changes by updating the resource version, which is based on the metadata received for that particular resource.
A new resource version is created when:
  * The resource definition is updated in the pipelines YAML file.
  * An output resource is updated during a run.
  * There is an external event, such as pushing a commit to a git repository.


### Note
No new version is created for an unchanged resource. However, if you need the resource to always change (for example, to trigger another pipeline), you can add $run_id environment variable as the value of an additional property.
Each version of a resource is immutable and returns the same result every time a specific version is used for a run. By default, steps always run using the latest version of an input resource. However, because a resource is versioned and holds the entire history of all the available versions, a run can be customized to use a specific version of a resource. For more information, see Triggering a Run with Custom Parameters.
### Default Behavior of Resource Versions
Resource versions have the following default behavior:
  *  **Latest version** : When a run is triggered, the latest version of the input resource is used in that run.
  *  **Dependent step in the same pipeline** : During a run, even when an input resource is not updated (no new version), the dependent step is triggered. While this is the default behavior, it can be changed by setting the `newVersionOnly` tag as `true`.
### Note
This is applicable only for generated resources, which are resources that connect two steps of a pipeline.
  *  **Dependent step in another pipeline** : During a run, the dependent pipeline is triggered only when there is a new version of the resource that connects the pipelines.


 **Example - Resource Versions**
This is a simple, single-step pipeline, with GitRepo as the input resource for the step.  
In this example:
  *  **First version** : When the pipeline syncs and loads for the first time, the latest commit on the Git repository is seeded as the very first version for the Gitrepo resource.
When the first run is triggered, this resource version is used for the run.
  *  **Second version** : When a new commit is pushed to the Git repository, the Gitrepo resource is updated with the new version, which becomes the latest version of the resource.
The Git commit automatically triggers the run and the second version of the resource, which is now the latest, is used for the run.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
54,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>All resources are defined in a pipeline YAML under the `resources` tag, as shown below. After a resource is defined and committed to a source control, it is consumed within a pipeline, based on the scope defined for the pipeline source.
[/INST]### Sharing resources across environments
When defining resources, the recommended approach is to define them in a pipeline source that is shared across environments in the same Project. This ensures that the resources are available across environments in a project. For more information, see  Creating a Project - Pipelines Resources.Creating a Project
While each resource has its own specific configuration, they all require a `name` and a `type`.
 **YAML Schema**
[YAML]
    resources:
      - name:               <string>
        type:               <resource type name>
        configuration:
          <as required by type>
[/YAML]
 **Tag**
 **Description of usage**
Required/Optional  
`name`
An alphanumeric string (underscores are permitted) that makes it easy to infer what the resource represents. This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines Project.
 **Example** : `aws_creds` to represent AWS keys.
### Note
  * After the pipeline performs a sync, it is recommended to not change the name of the resource. If you change the name of the resource, it is treated as a new resource and all its version history is lost. In addition, the name of the resource will need to be updated in all the places it appears in the pipeline.
  * Within the scope of a pipeline (across a Project or environment), no two resources can share the same name.


Required  
`type`
Name of the resource type that this resource is an instance of.
### Note
After the pipeline performs a sync, its type cannot be modified.
Required  
`configuration`
Specifies configuration settings, which vary for each `type` of resource.
Commonly included in this block is a setting that assigns an integrationthrough which the resource will be authenticated and accessed. The integration must be compatible with the `type`of the resource. The name of the integration field will vary by the resource.
Required  
 **Examples - Resource Definition**
These examples show the YAML definition for GitRepo and Image resources:
  *  **Example 1 - GitRepo Resource**
[YAML]    resources:
      - name: gitrepo_trigger
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          branches:
            include: master
[/YAML]
  *  **Example 2 - Image Resource**
[YAML]    resources: 
      - name: Image_1
        type: Image
        configuration:    
          registry: PSSG_DockerRegistry      
          imageName: docker/jfreq_win        
          imageTag: latest 
          autoPull: true
[/YAML]


##### Modifying Resources
In Pipelines, resources and their versions are tightly coupled. Therefore, when a resource is deleted, its historical data is permanently affected. This can mess up your DevOps Assembly Lines as it is a connected inter-dependent workflow.
The following rules apply when editing resources:
  * If you modify a resource's name, it is treated as new resource.
  * A resource's ` type` cannot be modified.


If your pipeline is failing because of a modified resource tag,the only option to recover your pipeline is to delete the resource definition. Deletion of a resource is a two-step process:

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
55,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>In a pipeline, steps can use resources as:
  * Inputs: When a resource is an input for a step, it is called an input resource. The input resource of one step can be the output resource of other steps.
  * Outputs: When a resource is an output of a step, it is called an output resource. The output resource of one step can be the input resource of other steps.


Input and output resources can originate from the same pipeline source as the pipeline or another pipeline source in the same project and environment.
[/INST]##### Input Resources
Input resources enable you to create dependencies between steps and pipelines. Steps that have input resources from other pipelines trigger a run of the pipeline when the resource is updated. By default, input resources that were an output resource of another step in the same run will run whether or not the resource is updated. A resource can also be referred to by its name as an argument in the shell commands that the step executes.
###### Input Resource Definiton
A resource can be specified as an input for a step by adding it to the `inputResources` section of a step.
 **YAML Schema**
[YAML]
        steps:
          - name: <step_name>
            type: <step_type>
            configuration:          
              inputResources:
                - name:         <resource name>
                  trigger:      <true/false>    # default true
                  newVersionOnly: <true/false>  # default false
                  branch:       <string>        # see description of defaults below
[/YAML]
 **Tag**
 **Description of usage**
Required/Optional  
`name`
Name of the declared resource that is to be a used as an input for the step.
Required  
`trigger`
`Trigger` is set as `true` by default and any change to a named resources triggers the execution of a step. This can be changed by declaring `trigger` as `false`.
Optional  
`newVersionOnly`
Setting `newVersionOnly`as `true` for an input resource will cause a step to be skipped if the input resource is not updated in the current run.
If there are multiple `inputResources` with `newVersionOnly: true`, the step is skipped only when none of the resources are updated. If at least one of the resources is updated, the step is not skipped.
Optional  
`branch`
A `branch` is required only when using a resource from another branch of a multi-branch source. By default, the resource from a single-branch source or the same branch of a multi-branch pipeline is used. To use a multi-branch resource in a single-branch pipeline or another branch in a multi-branch pipeline, `branch` should be used to specify the branch to which the resource belongs.
Optional  
 **Example - Input Resource Definition**
[YAML]
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: my_app_repo
                  trigger: false   # optional; default true
                  newVersionOnly: true  # optional; default false
                  branch: master   # optional 
[/YAML]
###### Using Input Resources
This section provides information about the various ways in which input resources can be used to manipulate pipeline runs.
 **Skip Automatic Trigger for All Commits**
By default, changes to an input resource triggers the execution of the dependent steps. For example, when a step specifies a GitRepo resource, any new code committed to that Git repository automatically causes that step to execute. However, this behavior can be changed by declaring `trigger` as `false` (see below). Now, even when the resource is updated, the dependent step is not triggered. This is especially useful for a production pipeline, when you do not want to deploy every new build.
### Note
For a step to not be triggered automatically, `trigger: ` `false` must be set for all input resources in the step.
### Note
Even if `trigger` is set as `false`, the step still receives webhook updates. This ensures that when it is manually triggered, it uses the latest commit.
 **Example - Automatic Trigger Off**
[YAML]
    pipelines:
      - name: java_pipeline
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: my_app_repo
                  trigger: false
                - name: cron_trigger
                  trigger: false
            execution:
               onExecute:
                 - pushd $res_my_app_repo_resourcePath
                 - ./execute.sh
                 - popd
[/YAML]
When `trigger` is set as `false`, the line linking the input resource and the step appears as a dashed line.
`trigger: false`
`trigger: true`  
 **Trigger Automatically on New Version Only**
Whenever a resource undergoes a change, its version is updated and the dependent step is triggered. This is the default behavior for all input resources. To skip steps in a run when input resources are not updated, add the `newVersionOnly` tag and set it as `true`. During a run, the step is triggered only when the resource is updated. If the resource is not updated, the step is skipped and all the downstream steps are skipped as well.
 **Example 1 - newVersionOnly**
[YAML]
    pipelines:
      - name: java_pipeline
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: my_app_repo
                - newVersionOnly: true
            execution:
              onExecute:
                - pushd $res_my_app_repo_resourcePath
                - ./execute.sh
                - popd
[/YAML]
 **Example 2 - newVersionOnly**
[YAML]
    resources:
      - name: new_resource
        type: PropertyBag
        configuration:
          runNumber: 0
    
    pipelines:
      - name: pipeline_01
        steps:
          - name: step_input
            type: Bash
            configuration:
              outputResources:
                - name: new_resource
            execution:
              onExecute:
                #- write_output new_resource runNumber=${run_number}
                - echo ""test""
    
          - name: step1      
            type: Bash
            configuration:
              inputResources:
                - name: new_resource
                  newVersionOnly: true
            execution:
              onExecute:
                - echo ""test""
    
          - name: step2
            type: Bash
            configuration:
              inputResources:
                - name: new_resource
            execution:
              onExecute:
                - echo ""test""
[/YAML]  
 **Trigger Manually using Specific Versions**
A run can be customized by selecting a specific version for an input resource. For more information, see Triggering a Run with Custom Parameters.
###### Pinning Resource Versions
By default, Pipelines uses the most recent or latest version of an input resource when running a job. However, there could be cases where you want to use a specific version of an input resource for a run. This is called pinning and input versions can be pinned using the YAML configuration. When a resource version is unpinned, it switches to using the latest version for all subsequent runs.
Resource version Ids have a global sequence, which can be found on the **Resource** tab. For more information, see Viewing Resources.
####### Pinning Resource Versions in YAML
You can use the `pin` tag to pin a specific input version as shown below below:
 **Example**
[YAML]
    resources:
      - name:                       <string>
        type:                       DistributionRule
        configuration:
          pin:
            versionId:  <number>
[/YAML]
The following resources support version pinning:
  * BuildInfo
  * GitRepo
  * HelmChart
  * Image
  * PropertyBag
  * ReleaseBundle


##### Output Resources
Output resources are resources that are either generated or changed by a step. When specified as the output of a step, the resource receives the output of the step. If required, this output resource can then be used as an input resource in a subsequent step in the same pipeline or another pipeline. The output resource can also be referred to by its name as an argument in the shell commands that the step executes.
###### Output Resource Definition
A resource can be specified as an output for a step by adding it in the `outputResources` section of a step.
 **YAML Schema**
[YAML]
        steps:
          - name: <step_name>
            type: <step_type>
            configuration:          
              outputResources:
                - name:         <resource name>
                  branch:       <string>        # see description of defaults below
[/YAML]
 **Tag**
 **Description of usage**
Required/Optional  
`name`
Name of the declared resource that is to be used as an input for the step.
Required  
`branch`
A `branch`is only required for resources from another branch of a multi-branch source. By default, the resource from a single-branch source or the same branch of a multi-branch pipeline is assumed. To update a multi-branch resource in a single-branch pipeline or another branch in a multi-branch pipeline, `branch` should be used to specify the branch to which the resource belongs.
Optional  
 **Example - Output Resource Definition**
[YAML]
        steps:
          - name: step_2
            type: Bash
            configuration:
              outputResources:
                - name: my_repo
                  branch: master
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
56,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>After a pipeline's YAML file is committed to a repository, add the repository to Pipelines through the UI. The Pipelines platform then watches for changes (job additions, edits or deletes) through source control webhooks. YAML changes are automatically synced and are reflected in the UI immediately.
After the pipeline source successfully syncs the YAML file, select **Applications** | **My Pipelines** to view the the pipeline.
In the Pipelines view:
  * Each resource is shown as a circular icon
  * Clicking a resource displays information specific to that resource
  * Clicking the YAML  icon → **Resources** tab displays the `resources` definition for the pipeline

  
[/INST]### Note
A resource that automatically triggers a step is represented by a solid line (see below). This changes to a dashed line when the `trigger` tag for an input resource is set as `false`. For more information, see Automatic Trigger.  
##### Resetting Resource Versions
If a resource is in an invalid state, you have the option to reset it, which delete all previous versions of the resource. For example, when a source repository invalidates the SHA that is used by a GitRepo resource.
To reset a resource:
  1. In the Pipelines view, click the resource and then click the **Reset** button.
  2. Click **Confirm**.

  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
57,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>In a pipeline, the role of a resource goes beyond inputs and outputs. Here are several other ways in which you can use resources in your pipeline.
[/INST]##### Using Resources Values in Environment Variables
A step that specifies a resource can access the resource and its attributes through environment variables.
  * These environment variables have a standard naming convention: `res_<resource name>_<tag>`
  * This can be extended further to access an integration that is specified in the resource: res_<resource_name>_<integration tag name>_<tag>


 **Example**
  * If the definition of a GitRepo resource named `app_gitrepo` is as follows:GitRepo
 **GitRepo Resource Definition**
[YAML]    resources:
      - name: app_gitRepo
        type: GitRepo
        configuration:
          path: user1/repo1
          gitProvider: myGitProvider
[/YAML]
  * The environment variable definition would be as follows:
Environment Variable
Result
Description  
`res_app_gitRepo_path`
user1/repo1
Returns the `path` attribute of the `app_gitRepo` resource  
`res_app_gitRepo_gitProvider_url`
Returns `url` of the Git Provider of the `app_gitRepo` resource  
  * Then the following environment variables will be available to a step that uses the `app_gitRepo` resource as an input:
 **Environment Variables**
[YAML]    steps:
      - name: build_app
        type: MvnBuild
        configuration:
          sourceLocation: .
          mvnCommand: clean install
          configFileLocation: .
          inputResources:
            - name: app_gitRepo             # Use the app_gitRepo resource
       execution:
         onSuccess:
           - send_notification notifySlack --text ""Maven build completed for $res_app_gitRepo_path at $res_app_gitRepo_gitProvider_url""
         onFailure:
           - send_notification notifySlack --text ""Maven build FAILED for $res_app_gitRepo_path at $res_app_gitRepo_gitProvider_url""
[/YAML]


###### Using Arrays
Certain resources, such as Aql, DistributionRule, and VmCluster have arrays in their configuration.
  * For arrays within resources ( VmCluster):


`res_<resource name>_<array heading>_len (this tells you how many entries in array)`
`res_<resource name>_<array heading>_0`
`res_<resource name>_<array heading>_1`
`res_<resource name>_<array heading>_2`
  * For object arrays within resources ( DistributionRule):


`res_<resource name>_<array heading>_len (this tells you how many entries in array)`
`res_<resource name>_<array heading>_0_<tag>`
`res_<resource name>_<array heading>_1_<tag>`
##### Using Stateful Resources
Resources are stateful entities and persist across pipelines, enabling passing of information between pipelines. This is especially useful when creating a pipeline of pipelines.
Steps can store any key-value pair data in a resource using the ` write_output`utility function. These values can then be referenced as environment variables by any subsequent cuting step that uses that resource as an input. Therefore, a step can pass information to another step in the run of the pipeline.
The environment variable for the stored value is of the form `res_<resource name>_<key name>`.
 **Example**
The following example creates three properties in the resource `myImage`.
 **Using write_output**
[YAML]
    write_output myImage sport=""baseball"" equipment=""bat"" field=""diamond""
[/YAML]
When the resource is specified in a step's `inputResources`, these properties can be accessed as the following environment variables:
 **Environment Variables**
[YAML]
    $ printenv res_myImage_sport
    baseball
    $ printenv res_myImage_equipment
    bat
    $ printenv res_myImage_field
    diamond
[/YAML]
For more information, see  Creating Stateful Pipelines.
##### Using Extension Resources
Extension resources enable Pipelines users to extend the Pipelines DSL by specifying their own resource types. After an extension resource is loaded, it can be used by any step in a pipeline. For more information, see Pipelines Extension Resource Model.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
58,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>An `Aql` resource specifies an Artifactory query using Artifactory Query Language.Artifactory Query Language
An Aql resource can be used in the `configuration` of a FileSpec resource to specify file properties to match against.
An Aql resource can be used as an `inputresource` for the CreateReleaseBundle step. It defines the query that is used to create a release bundle.
[/INST]##### YAML Schema
The YAML schema for Aql:
[YAML]
    resources:
      - name:                       <string>
        type: Aql
        configuration:
          sourceArtifactory:   <artifactory integration name>
          query:               <string>
          addedProperties:
            key1:              val1
            key2:              val2
          mappings: 
            - name: mapping1
              input:    
              output: 
            - name: mapping2
              input:    
              output: 
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `Aql `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceArtifactory `
The name of an Artifactory Integration.
Required  
`query `
An Artifactory Query Language query string.Artifactory Query Language
Required  
`addedProperties`
List of added properties that will be added to the artifacts after distribution of the release bundle.
  * `key` \- Property key to be created or updated after distribution of the release bundle.
  * `value` \- List of values to be added to the property key after distribution of the release bundle.


Optional  
`mappings`
List of mappings that are applied to the artifact paths after distribution of the release bundle.
  * `name` \- The friendly name of the mapping entry.
  * `input` \- Regex match string for artifact paths.
  * `output` \- Replacement for artifact paths matched by the ""input"" match string. Capture groups can be used as ""$1"".


Optional  
##### Examples
  * These examples may require an Artifactory integration and/or a Distribution integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


###### Example 1: AQL with build name and number
[YAML]
    resources:
      - name: myAqlResource
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: items.find({""@build.name"":{""$eq"":""buildName""}})
      - name: myReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistributionIntegration
          name: name
          version: '1'
    
    pipelines:
      - name: pipelineToUpdateResource
        steps:
          - name: myBashStep
            type: Bash
            configuration:
              integrations:
                - name: myArtifactoryIntegration
              outputResources:
                - name: myAqlResource
            execution:
              onExecute:
                - write_output myAqlResource query='items.find({""@build.name"":{""$eq"":""myBuildName""}})'
    
      - name: CreateReleaseBundlePipeline
        configuration:
          inputResources:
            - name: myAqlResource
          outputResources:
            - name: myReleaseBundle
        steps:
          - name: myCreateReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: ${JFROG_CLI_BUILD_NAME}
              releaseBundleVersion: ${run_id}
              dryRun: true
[/YAML]
###### Example 2: AQL with file name
[YAML]
    resources:
      - name: myAqlResource
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: 'items.find({""$and"": [{""repo"": {""$eq"": ""example-repo-local""}}, {""name"": {""$match"": ""myFile.txt""}}]})'
    
      - name: myReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistributionIntegration
          name: name
          version: '1'
    
    pipelines:
      - name: myCreateReleaseBundlePipeline
        steps:
          - name: myCreateReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: ${JFROG_CLI_BUILD_NAME}
              releaseBundleVersion: ${run_id}
              dryRun: false
              inputResources:
                - name: myAqlResource
              outputResources:
                - name: myReleaseBundle
[/YAML]
###### Example 3: AQL to add an additional property to the artifacts in the release bundle
[YAML]
    resources:
      - name: myAqlResource
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: 'items.find({""$and"": [{""repo"": {""$eq"": ""example-repo-local""}}, {""name"": {""$match"": ""myFile.txt""}}]})'
          addedProperties:
            myRunNumber: ${run_id}
    
      - name: myReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistributionIntegration
          name: name
          version: '1'
    
    pipelines:
      - name: myCreateReleaseBundlePipeline
        steps:
          - name: myCreateReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: ${JFROG_CLI_BUILD_NAME}
              releaseBundleVersion: ${run_id}
              dryRun: false
              inputResources:
                - name: myAqlResource
              outputResources:
                - name: myReleaseBundle
[/YAML]
###### Example 4: AQL to map one path to another
[YAML]
    resources:
      - name: myAqlResource
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: 'items.find({""$and"": [{""repo"": {""$eq"": ""example-repo-local""}}, {""name"": {""$match"": ""myFile.txt""}}]})'
          mappings:
            - name: myMapping
              input: example-repo-local/myFile.txt
              output: example-repo-local/myFile_2.txt
    
      - name: myReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistributionIntegration
          name: name
          version: '1'
    
    pipelines:
      - name: myCreateReleaseBundlePipeline
        steps:
          - name: myCreateReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: ${JFROG_CLI_BUILD_NAME}
              releaseBundleVersion: ${run_id}
              dryRun: false
              inputResources:
                - name: myAqlResource
              outputResources:
                - name: myReleaseBundle
[/YAML]
###### Example 5: AQL mapping paths using capturing groups in the input
[YAML]
    resources:
      - name: myAqlResource
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: 'items.find({""$and"": [{""repo"": {""$eq"": ""example-repo-local""}}, {""name"": {""$match"": ""myFile.txt""}}]})'
          mappings:
            - name: myMapping
              input: example-repo-local/myFile.(.*)
              output: example-repo-local/myFile_2.$1
    
      - name: myReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistributionIntegration
          name: name
          version: '1'
    
    pipelines:
      - name: myCreateReleaseBundlePipeline
        steps:
          - name: myCreateReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: ${JFROG_CLI_BUILD_NAME}
              releaseBundleVersion: ${run_id}
              dryRun: false
              inputResources:
                - name: myAqlResource
              outputResources:
                - name: myReleaseBundle
[/YAML]
###### Example 6
 **pipelines.yml**
[YAML]
    resources:
      - name: Aql_ART_1
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: items.find({""@build.name"":{""$eq"":""art""}})
          addedProperties:
            key1: value1
            key2: value2
          mappings:
            - name: mapping1
              input: IN
              output: test1
            - name: mapping2
              input: IN
              output: test2
    
    pipelines:
      - name: pipelines_ART_1
        steps:
          - name: step_ART_1
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: Aql_ART_1
                  trigger: true
            execution:
              onExecute:
                - write_output ""executing step...""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
59,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>An `Artifact` resource can be used to trigger a step or pipeline on any of the following Artifactory events:
  * Deploy
  * Delete
  * Copy
  * Move
  * Cache


When this resource is added as an input resource to any step, on resource update, the step or the pipeline is triggered.
[/INST]### Important
If there are issues while using Artifact resource, see the Troubleshooting section below for possible solutions.
### Note
This resource cannot be added as an output to a step.
### See it Live!
Click here to see a live example that uses this resource.
##### YAML Schema
### Rename Resource Name
In your pipeline YAML, whenever you make a change to any tag in the Configuration section, the name of the resource must be renamed. If the resource name is not renamed, any changes to the Configuration section are not taken into consideration during pipeline sync.
The YAML schema for Artifact:
[YAML]
    resources:
      name:         <string>
      type: Artifact
      configuration:
        jfrogPlatformProvider:     <JFrogPlatformAccessToken_integration>    # required
        repositories:                                # required
          - repository_name
          - repository_name
        artifactPath:                                # optional
          include:
            - pattern_1
            - pattern_2
          exclude:
            - pattern_1
            - pattern_2
          branches:                                    # required
            include:     <includePattern>            
            exclude:     <excludePattern>   
          buildOn:                                     # required
            deployed:    <boolean>
            deleted:     <boolean>
            copied:      <boolean>
            moved:       <boolean>
            cached:      <boolean>
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `Artifact` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`jfrogPlatformProvider`
The name of the JFrog Platform Access Token integration.
Required  
`repositories`
Used to specify the Artifactory repositories on which the event is applied.
### Note
If this tag is not specified, all tags are matched by default.
Required  
`branches`
For use in  multibranch pipelines, though it will also work with single branch pipelines. An object where `include` and `exclude` regex patterns are used to control which branches are triggered by the resource.
  *  **Required** : When an `include` pattern is supplied, any branch not matching the pattern will not trigger.
  *  **Optional** : If an `exclude` pattern is supplied, any branch matching the pattern will not trigger.


### Supported regex
The `branches` tag supports the following regex:
`^ $ . * & ` **? @ = ; :** **+**
 **Examples** :
`^gitBranch$` (to trigger for that branch)
`example.*`
`example+.&`
`example+:&@=.*$`
`^._example+:&.@=+;?.*$`
`example+.:&@=*;?1661337986460$`
`branches.include` is required
`branches.exclude` is optional  
`buildOn`
Used to control whether the resource will be updated on specified events. These are set to `false` by default.
  *  **Deployed** : Trigger when an artifact is deployed to a repository.
  *  **Deleted** : Trigger when an artifact is deleted from a repository.
  *  **Moved** : Trigger when an artifact is moved from a repository. This will apply on the repositories from which the artifact is moved.
  *  **Copied** : Trigger when an artifact is copied from a repository. This will apply on the repositories from which the artifact is copied.
  *  **Cached** : Trigger when an artifact is cached.


Required  
`artifactPath`
  * `include` \-- (optional) Simple comma separated wildcard patterns for repository artifact paths (with no leading slash). Ant-style path expressions are supported (*, **, ?). For example: ""org/apache/**""
  * `exclude` \-- (optional) Simple comma separated wildcard patterns for repository artifact paths (with no leading slash). Ant-style path expressions are supported (*, **, ?). For example: ""org/apache/**""


Optional  
##### Environment Variables
Whenever `Artifact` is used in a step, a set of environment variables is automatically made available that can be used in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_event_type
[/YAML]  
The type of the event.
[YAML]
    res_<resource_name>_event_path
[/YAML]  
The artifact path that triggered the event.  
##### Examples
  * These examples require a GitRepo and JFrog Platform Access Token integration.
  * The Pipelines DSL for these examples are available in this repository in the JFrog GitHub account.
  * Click here to see a live example that uses this resource.


 **Example 1**
### Rename Resource Name
In your pipeline YAML, whenever you make a change to any tag in the Configuration section, the name of the resource must be renamed. If the resource name is not renamed, any changes to the Configuration section are not taken into consideration during pipeline sync.
[YAML]
    resources:
      - name: artifact_1
        type: Artifact
        configuration:
          jfrogPlatformProvider: jfrogPlafromTokenIntegration    # required
          repositories:                            # required
            - example-repo-local
          artifactPath:                            # optional
            include:
              - tools/*
            exclude:
              - tools/exclude/*
          buildOn:                                 # required
            deployed: true
            cached: true
            deleted: true
          branches:                                # required
            include: artifactResource              # required
      - name: artifact_2
        type: Artifact
        configuration:
          jfrogPlatformProvider: myIntegration    # JFrog Platform Access Token integration
          repositories:
            - example-repo-local
          artifactPath:
            include:
              - default/*
              - tools/*
          buildOn:
            deployed: true
          branches:
            include: artifactResource
    
    pipelines:
      - name: my_pipe_1
        configuration:
          jfrogCliVersion: 2
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: artifact_1
            execution:
              onExecute:
                - echo ""executing step_1""
      - name: my_pipe_2
        steps:
          - name: step_2
            type: Bash
            configuration:
              inputResources:
                - name: artifact_2
            execution:
              onExecute:
                - echo ""executing step_2""
                - echo ""sync started""
    
[/YAML]
##### Troubleshooting
When using the Artifact resource in Pipelines, if you see that the pipeline or step is not getting triggered on artifact deployment or event update as configured in the resource, check the following:
  * Ensure that the integration used in the Artifact resource has administrator privileges on the JPD (on that Artifactory). You can either:
    * Create a access token with **admin** permissions
or
    * Use the identity token of the user who has administrator privileges
  * Ensure that there are source branches that match the branch pattern mentioned in the Artifact resource’s `branches.include`.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
60,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A `BuildInfo` resource is the metadata associated with a build in Artifactory.
JFrog Pipelines automatically creates BuildInfo when any of the Build steps like MvnBuild, NpmBuild, GradleBuild, or DockerBuild are used to generate packages.
BuildInfo is published to Artifactory:
  * Through the PublishBuildInfo step, by setting a flag in Build steps.
or
  * By providing that resource in steps like NpmPublish, GoPublishModule, GoPublishBinary and DockerPush.


[/INST]##### YAML Schema
The YAML schema for BuildInfo:
[YAML]
    resources:
      - name:               <string>
        type: BuildInfo
        configuration:
          sourceArtifactory:        artifactoryIntegration
          buildName:                        <string>
          buildNumber:                      <number>
          pin:
            versionId:          <number>
            buildName:          <string>
            buildNumber:        <number>
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `BuildInfo` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceArtifactory`
The name of the Artifactory Integration.
Required  
`buildName `
The name of the build to associate with this BuildInfo. Recommend using the environment variable available from the step that generated the build.
Optional  
`buildNumber `
The number of the build to associate with this BuildInfo. Can be an integer, a semantic version, or a string.
Recommend using the environment variable available from the step that generated the build.
Optional  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the BuildInfo resource version to be pinned:
  * `versionId`: Resource version Id (can be obtained through the UI)


Or
  * `buildName`: The name of the build to be pinned
  * `buildNumber`: The number of the build to be pinned


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
##### Environment Variables
Whenever `Buildinfo` is used in a step, a set of environment variables is automatically made available that can be used in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_buildName
[/YAML]  
The name of the build associated with this BuildInfo.
[YAML]
    res_<resource_name>_buildNumber
[/YAML]  
The number of the build associated with this BuildInfo.
[YAML]
    res_<resource_name>_targetRepo
[/YAML]  
When set by PromoteBuild, the name of a repository in Artifactory for the BuildInfo.  
##### Examples
  * These examples require a GitHub Integration and an Artifactory Integration.
  * The Pipelines DSL for these examples are available in this repository in the JFrog GitHub account.


 **Example 1**
[YAML]
    resources:
      - name: myGitRepo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: jfrog/jfrog-pipelines-simple-example
          branches:
            include: master
      - name: myBuildInfo
        type: BuildInfo
        configuration:
          sourceArtifactory: art
          buildName: SampleApp
          buildNumber: 1          
    pipelines:  
      - name: basic_pipeline
        configuration:
          environmentVariables:
            readOnly:
              my_env_var: ""hello""  
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: myGitRepo
              outputResources:
                - name: myBuildInfo             
            execution:
              onExecute:
                - echo $my_env_var
                - if [ -z $app_version ]; then add_pipeline_variables app_version=1.0.0; else app_version=$(bump_semver $app_version minor); fi
                - write_output myBuildInfo ""buildNumber=$app_version""
                - add_run_variables run_var=""hello""
          - name: step_2
            type: Bash
            configuration:
              inputResources:
                - name: myBuildInfo            
              runtime:
                type: image
                image:
                  auto:
                    language: java
                    versions: ['11']
            execution:
              onExecute:
                - echo $run_var  
                - echo $res_myBuildInfo_buildNumber
          - name: step_3
            type: Bash
            configuration:
              environmentVariables:
                step_env_var1: ""hello""
              inputSteps:
                - name: step_2
            execution:
              onExecute:
                - echo $step_env_var1
          - name: step_4
            type: Bash
            configuration:
              inputSteps:
                - name: step_2
            execution:
              onExecute:
                - echo $app_version
          - name: step_5
            type: Bash
            configuration:
              inputSteps:
                - name: step_3
                - name: step_4  
              integrations:
                - name: my_github            
            execution:
              onExecute:
                - 'curl -H ""Authorization: token ${int_my_github_token}"" ${int_my_github_url}/rate_limit'
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
61,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A `CronTrigger` is used as an `inputResource` to a step to trigger execution of the step at a scheduled time or at a recurring interval. The minimum recurring interval for `CronTrigger` is one minute and it supports the cron syntaxand UTC time.
[/INST]##### YAML Schema
[YAML]
    resources:
      - name:               <string>
         type:              CronTrigger
         configuration:
           interval:    <string>  # should be a cron expression
           branches:
             include: ""^dev.+$"" <string>
             exclude: ""^dev-test.+$"" <string>
           environmentVariables:
             key: value
             key: value
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `CronTrigger` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`interval`
A cron expression string that specifies the time(s) or interval to trigger the step.
The `interval` tag uses standard Cron expression and takes the following format:
 **Format**
[YAML]
    interval: * * * * *                                 
              │ │ │ │ │_________ day of the week (1-7 or SUN-SAT)
              | | | |__________ month (1 - 12)
              | | |__________ day of the month (1 - 31)
              | |__________ hour (0 - 23)
              |__________ minute (0 - 59)
[/YAML]
 **Examples**
 **CRON Expression**
 **Meaning**  
`0 12 * * *`
12 PM (noon) every day.  
`25 10 * * *`
10:25 AM every day.  
`15 22 * * *`
10:15 PM every day.  
`* 16 * * *`
Every minute starting at 4 PM and ending at 4:59 PM, every day.  
`0/5 13 * * *`
Every 5 minutes starting at 1 PM and ending at 1:55 PM, every day.  
`0/5 15,18 * * *`
Every 5 minutes starting at 3 PM and ending at 3:55 PM, and every 5 minutes starting at 6 PM and ending at 6:55 PM, every day.  
`0-5 18 * * *`
Every minute starting at 6 PM and ending at 6:05 PM, every day.  
`10,44 15 * 5 MON`
3:10 PM and at 3:44 PM every Monday in the month of May.  
`45 10 * * MON-FRI `
10:45 AM every Monday, Tuesday, Wednesday, Thursday and Friday.  
`30 10 10 * *`
10:30 AM on the 10th day of every month.  
`0 12 1/5 * *`
12 PM (noon) on every 5th day-of-month from 1 through 31.  
`11 11 11 11 *`
Every November 11th at 11:11 AM.  
### Note
  * For more information, see Cron expression.
  * It is recommended that you first test your Cron expression using an online tool before using it.


Required  
`branches`
For use in  multibranch pipelines, though it will also work with single branch pipelines. An object where `include` and `exclude` regex patterns are used to control which branches are triggered by the resource.
  * If an `include` pattern is supplied, any branch not matching the pattern will not trigger.
  * If an `exclude` pattern is supplied, any branch matching the pattern will not trigger.


### Note
When `include` or `exclude` is not provided, the cronTrigger resource will trigger every branch in a multibranch pipeline. Most multibranch pipelines should use this configuration.
Optional  
`environmentVariables`
Add environment variables as key-value pairs.
 **Example**
[YAML]
        configuration:
          interval: '*/30 * * * *' # Every 10 minutes
          environmentVariables:
            first: env
            second: env
[/YAML]  
##### Environment Variables
Whenever `CronTrigger`is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_interval
[/YAML]  
The cron string that specifies the time(s) or interval  
##### Examples
The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
[YAML]
    resources:
      - name: cron_trigger
        type: CronTrigger
        configuration:
          interval: '*/5 * * * *'     # Every 5 minutes
          branches:
            include: ""^dev.+$""
            exclude: ""^dev-test.+$""
          environmentVariables:
            first: env
            second: env
    
    pipelines:
      - name: pipeline_scheduled_triggers
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: cron_trigger
            execution:
              onExecute:
                - printenv
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
62,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A `DistributionRule `resource is the set of Destination rules that can be applied to distributing a release bundle using JFrog Distribution.Distributing Release Bundles
[/INST]##### YAML Schema
[YAML]
    resources:
      - name:                       <string>
        type:                       DistributionRule
        configuration:
          sourceDistribution:       <distribution integration name>
          serviceName:                      <string>                                          # required, only when used with Distribution 1.x
          siteName:                         <string>
          cityName:                         <string>
          countryCodes:
            - <string>
            - <string>
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `DistributionRule `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceDistribution`
The name of a Distribution Integration.
Required  
`siteName`
The Artifactory Edge Node site name specified with a wildcard pattern.
Required  
`cityName`
The Artifactory Edge Node city name with a wildcard pattern.
Required  
`countryCodes`
The Artifactory Edge Node country codes.
Required  
`serviceName `
The Artifactory Edge Node service name with a wildcard pattern.
Required only when used with Distribution v.1.  
##### Environment Variables
Whenever DistributionRule is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description  
res_<resource_name>_name
Name of the resource  
res_<resource_name>_servicename
The Artifactory Edge Node service name  
res_<resource_name>_sitename
The Artifactory Edge Node site name  
res_<resource_name>_cityname
The Artifactory Edge Node city name  
res_<resource_name>_countrycodes
The Artifactory Edge Node country codes  
##### Example
  * This examples requires an Artifactory Integration and Distribution Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: myArtifactory
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDist
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDist
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: myDist
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
63,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A `FileSpec` resource specifies a File Spec, which provides the details of files to upload or download from Artifactory.Using File Specs
Specifying a `FileSpec` in a step's `inputResources` downloads matching files and makes them available to that step.
Specifying a `FileSpec` in a step's `outputResources` signals a subsequent step that is dependent on that `FileSpec` to trigger. No file upload occurs.
[/INST]### Remote Repositories
FileSpecs will not currently download artifacts from remote repositories in Artifactory. To do so, you must use the API in a step. The recommended way to do this is to use curl command in the JFrog CLI in a step. For example:
[YAML]
    jfrog rt curl ""my-github-remote-repo/mikefarah/yq/releases/download/v4.24.4/yq_linux_amd64.tar.gz"" -O
[/YAML]
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               FileSpec
        configuration:      
          sourceArtifactory:        artifactoryIntegration
          pattern:                          <string>   # required, either this or aql
          aql:                                      <string>   # required, either this or pattern
          target:                           <string>   # optional
          props:                            <string>   # optional
          recursive:                        <boolean>  # optional
          flat:                             <boolean>  # optional
          excludePatterns:          <string>   # optional
          archiveEntries:           <string>   # optional
          buildName:                        <string>   # optional
          buildNumber:                      <integer>  # optional
          sortBy:                           <string>   # optional
          sortOrder:                        <string>   # optional
          limit:                            <integer>  # optional
          offset:                           <integer>  # optional
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `FileSpec `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceArtifactory`
The name of the Artifactory Integration.
Required  
`pattern`
Specifies the source path in Artifactory, from which the artifacts should be downloaded, in the following format: `[repository name]/[repository path]`. You can use wildcards to specify multiple artifacts.
Required if `aql` is not specified  
`aql`
An AQL query that specified the artifacts to be downloaded.
### Note
  * AQL has to be in JSON format.
  * Ensure that there are no spaces in the AQL query.
  * The query does not support inputs such as limits and sorting. These are supported as inputs in filespec resource


Required if `pattern` is not specified  
`target`
Specifies the local file system path to which artifacts should be downloaded.
For flexibility in specifying the target path, you can include placeholders in the form of `{1`}, `{2`}, `{3`}....These are replaced by corresponding tokens in the pattern property that are enclosed in parenthesis. For more information, see Using Placeholders.Using File Specs
Optional  
`props`
List of ""key=value"" pairs separated by a semi-colon. (For example, ""`key1=value1;key2=value2;key3=` `value3`""). Only artifacts with all the specified properties and values will be downloaded.
Optional  
`flat`
[Default: false]
If true, artifacts are downloaded to the exact target path specified and their hierarchy in the source repository is ignored.
If false, artifacts are downloaded to the target path in the file system while maintaining their hierarchy in the source repository.
Optional  
`recursive`
[Default: true]
If true, artifacts are also downloaded from sub-paths under the specified path in the source repository.
If false, only artifacts in the specified source path directory are downloaded.
Optional  
`excludePatterns`
[Applicable only when '`pattern`' is specified]
An array (enclosed with square brackets) of patterns to be excluded from downloading. Unlike the ""pattern"" property, ""excludePatterns"" must NOT include the repository as part of the pattern's path. You can use wildcards to specify multiple artifacts.
For example: [""*.sha1"",""*.md5""]
Optional  
`archiveEntries`
If specified, only archive artifacts containing entries matching this pattern are matched. You can use wildcards to specify multiple artifacts.
Optional  
`buildName `
The name of the build to associate with this BuildInfo. Recommend using the environment variable available from the step that generated the build.
Optional  
`buildNumber `
The number of the build to associate with this BuildInfo. Recommend using the environment variable available from the step that generated the build.
Optional  
`sortBy`
A list of semicolon-separated fields to sort by. The fields must be part of the 'items' AQL domain. For more information read the AQL documentation.Artifactory Query Language
Optional  
`sortOrder`
[Default: asc]
The order by which fields in the _sortBy_ option should be sorted. Accepts _asc_ or _desc_.
Optional  
`limit`
The maximum number of items to fetch. Usually used with the _sortBy_ option.
Optional  
`offset`
The offset from which to fetch items (i.e. how many items should be skipped). Usually used with the _sortBy_ option.
Optional  
For the remaining values, see the Artifactory topic Using File Specs.Using File Specs
##### Environment Variables
Whenever `FileSpec` is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_resourcePath
[/YAML]  
Path to where the File Spec is downloaded to
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_sourceArtifactory
[/YAML]  
The name of the Artifactory integration  
##### Examples
  * These examples require an Artifactory Integration.
  * The Pipelines DSL for these examples are available in this repository in the JFrog GitHub account.


 **Example 1**
This example uses the following tags:
  * `sourceArtifactory`
  * `pattern`
  * `recursive`
  * `offset`


[YAML]
    resources:
      - name: FileSpec_0014
        type: FileSpec
        configuration:
          sourceArtifactory: s_artifactory
          pattern: ""test-local/setup/""
          recursive: false
          offset: 1@3
    
    pipelines:
      - name: pipelines_FLSPEC_0014
        steps:
          - name: step_FLSPEC_0014
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: FileSpec_0014
            execution:
              onExecute:
                - write_output ""executing step...""
[/YAML]
 **Example 2**
This example uses the following tags:
  * `sourceArtifactory`
  * `pattern`
  * `recursive`
  * `limit`
  * `sortBy`
  * `sortOrder`
  * `target`


[YAML]
    resources:
      - name: FileSpec_0013
        type: FileSpec
        configuration:
          sourceArtifactory: s_artifactory
          pattern: ""test-local/setup/""
          recursive: false
          limit: 1@3
          sortBy:
            - created
          sortOrder: invalid
          target: 'C:\Users\pipelines\Desktop\'
    
    pipelines:
    
      - name: pipelines_FLSPEC_0013
        steps:
          - name: step_FLSPEC_0013
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: FileSpec_0013
            execution:
              onExecute:
                - write_output ""executing step...""
[/YAML]
 **Example 3**
This example uses the following tags:
  * `sourceArtifactory`
  * `pattern`
  * `recursive`
  * `buildName`
  * `buildNumber`


[YAML]
    resources:
      - name: FileSpec_0010
        type: FileSpec
        configuration:
          sourceArtifactory: s_artifactory
          pattern: ""test-local/setup/""
          recursive: false
              buildName: pipelines_api
          buildNumber: 1@3
    
    pipelines:
      - name: pipelines_FLSPEC_0010
        steps:
          - name: step_FLSPEC_0010
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: FileSpec_0010
            execution:
              onExecute:
                - write_output ""executing step...""
[/YAML]
 **Example 4**
This example uses the following tags:
  * `sourceArtifactory`
  * `pattern`
  * `recursive`
  * `archiveEntries`


[YAML]
    resources:
      - name: FileSpec_0007
        type: FileSpec
        configuration:
          sourceArtifactory: s_artifactory
          pattern: ""test-local/setup/""
          recursive: false
          archiveEntries: ""#/artifactory""
    
    pipelines:
      - name: pipelines_FLSPEC_0007
        steps:
          - name: step_FLSPEC_0007
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: FileSpec_0007
            execution:
              onExecute:
                - write_output ""executing step...""
[/YAML]
 **Example 5**
This example uses the following tags:
  * `sourceArtifactory`
  * `aql`


[YAML]
    resources:
      - name: Get_Files
        type: FileSpec
        configuration:
          sourceArtifactory: s_artifactory
          aql: ""{'repo': 'docker-local'}""
        
    pipelines:
      - name: PropertyDemo
        steps:
          - name: ArtifactoryFiles
            type: Bash
            configuration:
              inputResources:
                - name: Get_Files
            execution:
              onExecute:
                - cat $HOME/.jfrog/logs/jfrog-cli*
                - ls
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
64,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A `GitRepo `is used to connect JFrog Pipelines to a source control repository. Adding it creates a webhook to the repo so that future commits will automatically create a new version with the webhook payload.
The first version of the resource, used if it is an input to a step that runs before any webhooks are received, will be for the latest commit in the default branch on the source provider at the time the resource is created. Later resource versions can be restricted by the `branches`, `tags`, and `buildOn` options.
[/INST]### Note
To suppress triggering for an individual commit, include the text `skipRun` in the commit message.
By default the`GitRepo`resource uses `ssh` protocol with the created deploy key to fetch the source repository content. However, this only provides Pipelines with read-only access to the source repository. Optionally, users can configure a `GitRepo` resource to use `https` protocol. In this case Pipelines will have privileges to the source repository according to the credentials used to set up the Git provider integration.
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               GitRepo
        configuration:
          gitProvider:      <git integration name>
          path:             <string>
          files:
            include:        <regular expression>
            exclude:        <regular expression>
          branches:
            include:        <regular expression>
            exclude:        <regular expression>
          recurseSubmodules:    <Boolean>  
          pullRequestSourceBranches:
            include: <regular expression>
            exclude: <regular expression>
          pullRequestTargetBranches:
            include: <regular expression>
            exclude: <regular expression>
          tags:
            include:        <regular expression>
            exclude:        <regular expression>
          buildOn:
             branchCreate:      <Boolean>
             branchDelete:      <Boolean>
             commit:            <Boolean>
             pullRequestCreate: <Boolean>
             pullRequestClose:      <Boolean>
             releaseCreate:         <Boolean>
             tagCreate:                     <Boolean>
          cancelPendingRunsOn:
             newCommit:         <Boolean>
             pullRequestUpdate: <Boolean>
          shallowDepth:     <Positive Integer>        # used to set the depth at which the repo is (shallow)cloned/fetched
          cloneProtocol:    <ssh|https>               # Optional. Defaults to ssh
          pin:
            versionId:  <number>
            sha:        <string> 
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `GitRepo` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`gitProvider `
The name of the source control integration. Currently supported integration types are:
  * Bitbucket Integration
  * Bitbucket Server Integration
  * GitHub Integration
  * GitHub Enterprise Integration
  * GitLab Integration


Required  
`path`
The path of the repository from the integration root.
The path can be hard coded or you can use `{{.jfrog-pipelines.sourceRepository}}` variable to reference the current pipeline source path.
 **Example**
[YAML]
      - name: myFirstRepo
        type: GitRepo
        configuration:
          gitProvider: myGitHub
          path: {{.jfrog-pipelines.sourceRepository}}
[/YAML]
Required  
`files`
  * `include `\-- (optional) Regular expression to include files meeting the pattern from the repo
  * `exclude`\-- (optional) Regular expression to exclude files meeting the pattern from the repo


### Note
  * If this tag is not specified, all files are matched by default.
  * This filter only applies to commit events, and not pull requests, tags, or releases.


Optional  
`branches `
  * `include `\-- (optional) Regular expression to include branches from the repo
  * `exclude`\-- (optional) Regular expression to exclude branches from the repo


### Supported regex
The `branches` tag supports the following regex:
`^ $ . * & ` **? @ = ; :** **+**
 **Examples** :
`^gitBranch$` (to trigger for that branch)
`example.*`
`example+.&`
`example+:&@=.*$`
`^._example+:&.@=+;?.*$`
`example+.:&@=*;?1661337986460$`
### Note
If the tag is not specified, the default branch will be triggered.
Optional  
`recurseSubmodules`
Set as `true` to clone all submodules in the repository. Default value is `false`.
Optional  
`pullRequestSourceBranches`
  * `include `\-- (optional) Regular expression to include pull requests from branches matching the pattern
  * `exclude `\-- (optional) Regular expression to exclude pull requests from branches matching the pattern


### Note
If this tag is not specified, all branches are matched by default.
Optional  
`pullRequestTargetBranches`
  * `include `\-- (optional) Regular expression to include pull requests to branches matching the pattern
  * `exclude`\-- (optional) Regular expression to exclude pull requests to branches matching the pattern


### Note
If this tag is not specified, the `branches` tag setting will apply to the target branch. If neither is specified, all branches are matched by default.
Optional  
`tags `
Used to specify a collection of tags and releases upon which a new version is created
  * `include `\-- (optional) Regular expression to include tags meeting the pattern from the repo
  * `exclude`\-- (optional) Regular expression to exclude tags meeting the pattern from the repo


### Note
If this tag is not specified, all tags are matched by default.
Optional  
`buildOn `
Used to control whether the resource will be updated on specified events
  * `branchCreate` \-- (default is false) used to control whether the resource will be updated when a branch is created. Note that when this is set as `false`, the resource is not updated and the resource version is not created.
  * `branchDelete` \-- (default is false) used to control whether the resource will be updated when a branch is deleted. Note that when this is set as `false`, the resource is not updated and the resource version is not created.
  * `commit`\-- (default is true) used to control whether the resource will be updated for commit webhooks. Note that when this is set as `false`, the resource is not updated and the resource version is not created.
  * `pullRequestCreate` \-- (default is false) used to control whether the resource will be updated for pull request webhooks
  * `pullRequestClose` \-- (default is false) used to control whether the resource will be updated for closing a pull request webhook
  * `releaseCreate` \-- (default is false) used to control whether the resource will be updated for release webhooks
  * `tagCreate` \-- (default is false) used to control whether the resource will be updated for tag webhooks
### Note
In a multi-branch pipeline, when `tagCreate` is set as `true` and when you perform a commit or Git release with a specific tag, buildOn.commit will trigger only those branches that have the associated Git tag.
This is applicable for GitHub only.


### Note
If this tag is not specified, `commit:` `true` is the default setting.
For more information about using this tag, see Triggering On a Git Repository Change.
Optional  
`cancelPendingRunsOn`
Used to control whether previously triggered runs are canceled for new webhooks
  * `newCommit`\-- (default is false) Cancels previous processing or waiting runs for the same branch for commits, same tag name for tags, and same release name for releases.
  * `pullRequestUpdate` \-- (default is false) Cancels previous processing or waiting runs for the same pull request number when a webhook is received for a pull request.


### Note
All values for this tag are false by default.
For more information about using this tag, see Cancelling Previous Runs On a Git Repository Change.
Optional  
`shallowDepth`
Uses a positive integer to set the depth at which the repo is (shallow) cloned/fetched
### Note
If `shallowDepth` is not specified, no shallow clone options are passed to git. The full repo is cloned in its entirety.
Optional  
`cloneProtocol`
Used to select the protocol to be used when cloning the repo. Supported values are `ssh` or `https`. Defaults to `ssh`.
Optional  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the GitRepo resource version to be pinned:
  * `versionId` \-- Resource version Id (can be obtained through the UI)


Or
  * `sha` \-- The commit sha to be pinned


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
###### Using include and exclude Patterns
When both include and exclude patterns are used, the following rules apply on the list of all the files in the commit change list :
  * All files that do not match the include pattern are removed
  * All files that do match the exclude pattern are removed
  * If any files are left, the resource is updated


##### Environment Variables
Whenever `GitRepo` is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_path
[/YAML]  
Relative path of the resource
[YAML]
    res_<resource_name>_branchName
[/YAML]  
Name of branch on which the commit occurred. If it was created for a pull request, this is the base branch.
This environment variable is useful for getting the name of the branch.
[YAML]
    res_<resource_name>_commitMessage
[/YAML]  
Commit message of the version being used
[YAML]
    res_<resource_name>_commitSha
[/YAML]  
SHA of the commit of the version being used
[YAML]
    res_<resource_name>_commitUrl
[/YAML]  
URL to the commit
[YAML]
    res_<resource_name>_committerLogin
[/YAML]  
Name of the committer for the SHA being used
[YAML]
    res_<resource_name>_gitProvider_name
[/YAML]  
Git integration name
[YAML]
    res_<resource_name>_gitProvider_url
[/YAML]  
URL of the Git Provider integration
[YAML]
    res_<resource_name>_gitRepoFullName
[/YAML]  
Name of the Git repo
[YAML]
    res_<resource_name>_gitRepoRepositoryHttpsUrl
[/YAML]  
HTTPS URL for the Git repository
[YAML]
    res_<resource_name>_gitRepoRepositorySshUrl
[/YAML]  
SSH URL for the Git repository.
[YAML]
    res_<resource_name>_gitRepoRepositoryUrl
[/YAML]  
URL for the Git repository
[YAML]
    res_<resource_name>_gitRepoSourceDefaultBranch
[/YAML]  
Default branch of the Git repository
[YAML]
    res_<resource_name>_gitTagName
[/YAML]  
If a tag name was present in the current version, this will be the tag name
[YAML]
    res_<resource_name>_integrationName
[/YAML]  
The `name` string of the integration used by the GitRepo
[YAML]
    res_<resource_name>_isGitTag
[/YAML]  
`True` if the version is a git tag based build
[YAML]
    res_<resource_name>_isPrerelease
[/YAML]  
True if the version is a git release based build. Supported only if the integration is GitHub
[YAML]
    res_<resource_name>_isPullRequest
[/YAML]  
`True` if the version is a git open pull request based build
[YAML]
    res_<resource_name>_isPullRequestClose
[/YAML]  
`True`, if the version is a git closed pull request based step. Supported only if the integration is GitHub or Bitbucket or Gitlab.
[YAML]
    res_<resource_name>_isRelease
[/YAML]  
`True` if the version is a git release based build. Supported only if the integration is GitHub
[YAML]
    res_<resource_name>_pullRequestNumber
[/YAML]  
The ID number of the pull request
[YAML]
    res_<resource_name>_pullRequestBaseBranch
[/YAML]  
Name of the base branch into which the pull request changes will be merged
[YAML]
    res_<resource_name>_pullRequestSourceUrl
[/YAML]  
Source URL of the pull request
[YAML]
    res_<resource_name>_pullRequestSourceHttpsUrl
[/YAML]  
Source HTTPS URL of the pull request
[YAML]
    res_<resource_name>_pullRequestSourceSshUrl
[/YAML]  
Source SSH URL of the pull request
[YAML]
    res_<resource_name>_resourcePath
[/YAML]  
Full path of the resource folder on the node
[YAML]
    res_<resource_name>_operation
[/YAML]  
Input or output resource
[YAML]
    res_<resource_name>_shaData
[/YAML]  
JSON object containing the details of the SHA
[YAML]
    res_<resource_name>_beforeCommitSha
[/YAML]  
SHA before the commit was made
[YAML]
    res_<resource_name>_baseCommitRef
[/YAML]  
The commit SHA of base branch in a PR
[YAML]
    res_<resource_name>_headCommitRef
[/YAML]  
The source branch name in a PR
[YAML]
    res_<resource_name>_compareUrl
[/YAML]  
Compare URL for the commit
[YAML]
    res_<resource_name>_gitTagMessage
[/YAML]  
If a git tag is created, this holds the message if it is present
[YAML]
    res_<resource_name>_releaseName
[/YAML]  
If a release is created, this holds the name of the release
[YAML]
    res_<resource_name>_releaseBody
[/YAML]  
If a release is created, this holds the description of the release
[YAML]
    res_<resource_name>_releasedAt
[/YAML]  
If a release is created, this holds the date of the release
[YAML]
    res_<resource_name>_lastAuthorLogin
[/YAML]  
Details of when the last author logged in
[YAML]
    res_<resource_name>_lastAuthorEmail
[/YAML]  
Email of the author
[YAML]
    res_<resource_name>_isBranchCreate
[/YAML]  
`True` if a branch was created
[YAML]
    res_<resource_name>_isBranchDelete
[/YAML]  
`True` if a branch was deleted  
##### Examples
 **Example 1**
 **GitRepo resource listening to the master branch**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          branches:
            include: master
[/YAML]
 **Example 2**
 **GitRepo resource listening to all branches except master and release**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          branches:
            exclude: ^(master|release)$
    
[/YAML]
**Example 3**
The following example may be useful to reference a single directory in a source repository that contains source code for a single microservice within a larger project.
 **GitRepo resource listening only to certain files in a directory**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          branches:
            include: master
          files:
            include: ^folder1\/package\/docker\/micro1\/.+
    
[/YAML]
**Example 4**
By default, a GitRepo triggers on a commit, but this can be changed in the `buildOn` attributes. For more information about triggering, see Triggering Pipelines and Steps.
 **GitRepo triggers only for a pull request and not for commit**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          branches:
            include: master
          buildOn:
            commit: false
            pullRequestCreate: true
    
[/YAML]
**Example 5**
The following example pipeline triggers the first step on the GitRepo resource. The step uses the environment variables exposed by the GitRepo resource to navigate to the path where the Git repository is cloned on the build node, and to execute a script in that directory.
 **Usage in a Pipeline**
[YAML]
    pipelines:
      - name: java_pipeline
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: my_app_repo
            execution:
              onExecute:
                - pushd $res_my_app_repo_resourcePath
                - ./execute.sh
                - popd
[/YAML]
 **Example 6**
 **GitRepo resource using HTTPS protocol during repo clone**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          branches:
            include: master
          cloneProtocol: https
[/YAML]
 **Example 7**
In this example, the GitRepo is only updated for pull requests to master.
 **GitRepo triggers only for a pull requests to master**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          pullRequestTargetBranches:
            include: ^master$
          buildOn:
            commit: false
            pullRequestCreate: true
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
65,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The `HelmChart` resource maps to a specific chart in an  Artifactory Helm Repository.Kubernetes Helm Chart Repositories
The HelmChart resource can be used in two native steps. The HelmPublish step takes a HelmChart as a required `outputResource` . HelmPublish will create a new version of the chart, push it to Artifactory, and store the new version in the HelmChart resource. Then that HelmChart resource can be used as an `inputResource` to the HelmDeploy native step. This will automatically download the HelmChart, and use the Helm CLI to deploy it to the configured cluster.
[/INST]##### YAML Schema
[YAML]
    resources:
      - name:               <string>
         type:              HelmChart
         configuration:
           sourceArtifactory:    artifactoryIntegration
           repository:           <string>
           chart:                <string>
           version:              <version string>
           pin:
             versionId:          <number>
             version:            <version string> 
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `HelmChart `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceArtifactory `
The name of an Artifactory Integration
Required  
`repository `
The name of the Helm repository in Artifactory
Required  
`chart`
The name of the Helm chart
Required  
`version `
A string that identifies the version number of the Helm chart
Required  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the HelmChart resource version to be pinned:
  * `versionId` \-- Resource version Id (can be obtained through the UI)


Or
  * `version` \-- A string that identifies the version to be pinned


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
##### Environment Variables
Whenever `HelmChart `is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_sourceArtifactory
[/YAML]  
The name of the Helm repository in Artifactory
[YAML]
    res_<resource_name>_chart
[/YAML]  
The name of the Helm chart
[YAML]
    res_<resource_name>_version 
[/YAML]  
A string that identifies the version number of the Helm chart  
##### Example
  * This examples requires an Artifactory Integration and GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      - name: git_repo
        type: GitRepo
        configuration:
          gitProvider: git_r
          path: jfrog/charts
       
      - name: helm_repo
        type: HelmChart
        configuration: 
          sourceArtifactory: art_r
          repository: helm-local/
          chart: test
          version: 1.0.0
    
    pipelines:
      - name: test_helm
        configuration:
          nodePool: win_2019
    
        steps:
          - name: helm_pub
            type: HelmPublish
            configuration:
              chartPath: ./simplehelm
     
              inputResources:
                - name: git_repo                    
              outputResources:
                - name: helm_repo                  # required
     
            execution:
              onStart:
                - write_output ""executing step...""
                - Get-Variable | format-table
              onSuccess:
                - write_output ""executing step...""
                - Get-Variable | format-table
              onFailure:
                - write_output ""executing step...""
                - Get-Variable | format-table
    
              onComplete: #always
                - write_output ""executing step...""
                - Get-Variable | format-table
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
66,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>An `Image `resource is used to add a reference to a Docker image to your pipeline.
An Image resource holds information of a docker image that is stored through a Docker Registry Integration that may in one of the following registry sources:
  * JFrog Artifactory / JFrog Container Registry
  * Amazon Container Registry
  * Google Container Registry
  * Azure Container Registry
  * Docker Hub


An Image resource can be used as both input and output. Step like DockerBuild will generate it as an output and steps like DockerPublish will use it as input to publish to a Docker registry.
[/INST]##### YAML Schema
[YAML]
    resources:
      - name:               <string>
         type:              Image
         configuration:
           registry:                <docker integration name>
           sourceRepository:        <name of the artifactory repo>   # required if registry is Artifactory
           region:              <string>                         # required if registry is on AWS
           imageName:               <imageRepo/imageName>
           imageTag:                        <string>
           autoPull:                        <boolean> #(default true)
           pin:
             versionId:         <number>
             imageTag:          <string> 
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `Image `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`registry`
The name of a Docker Registry Integration
Required  
`sourceRepository `
The name of the repository in Artifactory to which the images will be published. Required for Docker images in Artifactory when the pipeline is configured to use JFrog CLI v1.
This is not used when the pipeline is configured to use JFrog CLI v2.
Required if registry is in JFrog Artifactory with JFrog CLI v1  
`region`
An AWS region identifier
Required if registry is on AWS  
`imageName`
The file path of the Docker image, of the form imageRepo/imageName
Required  
`imageTag`
The version tag of the initial version of the Docker image
Required  
`autoPull `
Defaults to **true**. The image is automatically pulled to the machine running the step.
Optional  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the Image resource version to be pinned:
  * `versionId` \-- Resource version Id (can be obtained through the UI)


Or
  * `imageTag` \-- The version tag to be pinned


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
##### Environment Variables
Whenever `Image `is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_sourceRepository
[/YAML]  
The name of the repository in Artifactory  
res_<resource_name>_region
The AWS region identifier  
res_<resource_name>_imageName
The file path of the Docker image  
res_<resource_name>_imageTag
The version tag of the initial version of the Docker image  
res_<resource_name>_autoPull
Value of the autopull flag  
##### Examples
The Pipelines DSL for this example is available in t his repository in the JFrog GitHub account.
 **Example 1**
[YAML]
    resources:  
      - name: Image_1
        type: Image 
        configuration:         
          registry: PSSG_DockerRegistry           
          imageName: docker/jfreq_win             
          imageTag: latest
    
    pipelines:
      - name: pipelines_Image_1
        steps:
          - name: step_Image_1
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: Image_1
            execution:
              onExecute:
                - write_output ""executing step...""
                - if($Image_1_isTrigger -ne ""true""){
                  Write-Error ""Stop"" -ErrorAction Stop
                  }
[/YAML]
 **Example 2**
[YAML]
    resources:
      - name: Image_2
        type: Image
        configuration:
          registry: s_artifactory
          sourceRepository: docker-local
          imageName: ubuntu
          imageTag: latest
    
    pipelines:
      - name: pipelines_IMAGE_2
        steps:
          - name: step_IMAGE_2
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: Image_2
            execution:
              onExecute:
                - write_output ""executing step...""
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
67,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>An `IncomingWebhook` resource can trigger one or more jobs in your pipeline whenever the associated URL is called using the HTTP POST method.
When an `IncomingWebhook` resource is created with an Incoming Webhook Integration, it can be specified as an input to any step in that step's `inputResources` section. By default, any incoming payload from the webhook will trigger execution of the step, but this can be changed by setting `trigger` to `false`.
Any JSON payload passed in by the received POST call is stored in the `payload` property of the resource.The step can access the JSON payload through the `res_<resource_name>_payload` environment variable. The value of the environment variable can be written to a file, and individual elements can be read from that file using the read_json utility function.
When using Webhook as an output resource, you can construct the outgoing payload through the write_output utility function to add key/value pairs.
You can use an `IncomingWebhook`resource to trigger execution of another pipeline. When specified as an output resource of a step in one pipeline and an input resource of a step in a different pipeline, setting a payload for the `IncomingWebhook` in the first step will trigger execution of the other step. In this way, completion of a step in one pipeline can trigger execution of a different pipeline.
[/INST]### Incoming Webhooks and Multi Branch Pipelines
When working with  Multi Branch Pipelines, to trigger an `IncomingWebhook` resource for a specific Pipelines Source branch, you need to add the _branch_ query parameter to your  Incoming Webhook URL as follows:
`http://host:port/pipelines/api/v1/projectIntegrations/1/hook?branch=>`
 **Example**
`http://host:port/pipelines/api/v1/projectIntegrations/1/hook?branch=foo`
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               IncomingWebhook
        configuration:
          webhookName:      <string>
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `IncomingWebhook` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`webhookName`
The name of an Incoming Webhook Integration
Required  
##### Environment Variables
Whenever`IncomingWebhook`is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_payload
[/YAML]  
Webhook payload that was received in the incoming POST or created by the step for output  
##### Example
  * This example requires an Incoming Webhook Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      - name:               MyIncomingWebhookResource
        type:               IncomingWebhook
        configuration:
          webhookName: MyIncomingWebhookIntegration
    
    pipelines:
      - name: ExamplePipeline
            steps:
        - name: ExampleStep
          type: Bash
          configuration:
            inputResources:
              - name: MyIncomingWebhookResource
          execution:
              onExecute:
                - echo ""$res_MyIncomingWebhookResource_payload"" | jq '.' > payload.json
                - read_json payload.json ""LESSER_SECRET_FORMULA""
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
68,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>An `OutgoingWebhook` resource uses HTTP to send information from a step to an external API endpoint through an Outgoing Webhook Integration.
When an `OutgoingWebhook` resource is specified as an`outputResource`for a step and has an attached payload (including an empty payload), at the completion of the step's execution, the payload is sent to the endpoint specified in the Outgoing Webhook Integration. If required, this can be overridden by setting the resource's `skipWebhook` environment variable as `true`.
To attach the outgoing payload, use the set_payload utility function. If no payload is attached, no message is sent through the webhook.
[/INST]##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               OutgoingWebhook
        configuration:
          webhookName: <string>                           # name of an Outgoing Webhook Integration
          method: <GET|PUT|POST|DELETE|PATCH>             # default POST
          Parameters: <string>                            # query string to be appended to the outgoing URL
          path:       <string>
          headers:
            Content-type: <valid request content types> 
            accept: xyc
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `OutgoingWebhook` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`webhookName`
The name of an Outgoing Webhook Integration
Required  
`method`
The HTTP method to use: GET | PUT | POST | DELETE | PATCH
The default is POST.
Optional  
`Parameters`
A query string to be appended to the outgoing URL
Optional  
`path`
Path for the outgoing URL
Optional  
`headers`
Any HTTP entity headers to include. For example, `content-type` to specify any the type of content returned.
Optional  
##### Environment Variables
Whenever `Webhook` is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
                                           res_<resource_name>_name
                                        
[/YAML]  
Name of the resource  
`res_<resource_name>_payload`
The JSON payload attached to the resource  
`res_<resource_name>_skipWebhook`
When set to **true** , the outgoing webhook will not be sent. This is useful to inhibit sending of a prepared webhook in a step's `onFailure`section.
Defaults to **false**.  
##### Examples
The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
 **Example 1**
 **Outgoing Form URL Webhook**
[YAML]
    resources:
      - name: My_OutHook_formurl_Resource
        type: OutgoingWebhook
        configuration:
          webhookName: My_OutgoingWebhookIntegration
          path: /url
          headers:
            content-type: application/x-www-form-urlencoded
    
    pipelines:
      - name: ExamplePipeline
        steps:
          - name: my_send_hook_formurl
            type: Bash
            configuration:
              outputResources:
                - name: My_OutHook_formurl_Resource
            execution:
              onExecute:
                - echo ""hello world""
                - formUrlEncodedPayload=""foo=bar&runNumber=$run_number&stepName=$step_name""
                - set_payload My_OutHook_formurl_Resource ""$formUrlEncodedPayload""
    
[/YAML]
**Example 2**
 **Outgoing JSON Webhook**
[YAML]
    resources:
      - name:               MyOutgoingJSONResource
        type:               OutgoingWebhook
        configuration:
          webhookName: MyOutgoingJSONResource
          path: /json
    
    pipelines:
      - name: ExamplePipeline
            steps:
        - name: my_send_hook_json
          type: Bash
          configuration:
            outputResources:
              - name: MyOutgoingJSONResource
          execution:
              onExecute:
                - jsonPayload=""{\""runNumber\"":\""$run_number\"", \""stepName\"":\""$step_name\""}""
                - set_payload MyOutgoingJSONResource ""$jsonPayload""
              onFailure:
                - export res_MyOutgoingJSONResource_skipWebHook=true
              onSuccess:
                - echo ""Payload sent by ""$res_MyOutgoingJSONResource_name"":""
                - echo $res_MyOutgoingJSONResource_payload
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
69,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A `PropertyBag` resource is used to pass information from one pipeline to another and provide environment variables to a step in the format of a resource.
A PropertyBag resource can have any string as property, which is then available as environment variables when the key is an input to a step. When it is an output, steps can change the values of properties or add new ones.
[/INST]##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               PropertyBag
        configuration:
          <string>: <string>
          <string>: <string>
          pin:
            versionId: <number>
            <string>: <string>
            <string>: <string> 
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `PropertyBag `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`<string>`
A property for the PropertyBag resource. The tag should be a valid variable name (Bash or PowerShell) for the steps where it is to be an input or output and the value a string. At least one is required, multiple properties are allowed.
Required  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the PropertyBag resource version to be pinned:
  * `versionId` \-- Resource version Id (can be obtained through the UI)


Or
  * `<string>` \-- The collection of properties to be pinned. At least one is required, multiple properties are allowed.


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
##### Environment Variables
Whenever `PropertyBag `is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_<property>
[/YAML]  
Each property in the PropertyBag resource will be available as an environment variable in this format.
[YAML]
    <property>
[/YAML]  
If a PropertyBag resource is in inputResources, the properties will also be exported as environment variables with the tag as the variable name.  
##### Examples
The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
 **Example 1**
[YAML]
    resources:
      - name: propertyBag_1
        type: PropertyBag
        configuration:
          property1: ""jfrog""
          property2: ""pipelines""
    
    pipelines:
      - name: pipeline_PropertyBag_1
        steps:
          - name: step_propertyBag_1
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: propertyBag_1
            execution:
              onExecute:
                - write_output ""onExecute""
                - get-variable -name property1
                - get-variable -name property2
              onSuccess:
                - write_output ""onSuccess""
[/YAML]
 **Example 2**
This example uses two steps, `step1` and `step2`. When the pipeline runs, the step `step2` is triggered, but the step `step1` is skipped. This is because, the `newVersionOnly` tag is set as `true` for the `myResource` input resource that is used in this step. During a run, if an input resource's version is not updated and if the `newVersionOnly` tag is set as true, it causes the step using that input resource to be skipped.
[YAML]
    resources:
      - name: myResource
        type: PropertyBag
        configuration:
          runNumber: 0
    
    pipelines:
      - name: myPipeline
        steps:
          - name: input1
            type: Bash
            configuration:
              outputResources:
                - name: myResource
            execution:
              onExecute:
                - write_output myResource runNumber=${run_number}
                - echo ""test""
          - name: step1
            type: Bash
            configuration:
              inputResources:
                - name: myResource
                  newVersionOnly: true
            execution:
              onExecute:
                - echo ""test""
          - name: step2
            type: Bash
            configuration:
              inputResources:
                - name: myResource
            execution:
              onExecute:
                - echo ""test""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
70,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A `ReleaseBundle` resource specifies a set of artifacts in Artifactory that are distributed to Artifactory Edge nodes as a JFrog Distribution Release Bundle.Distributing Release Bundles
This resource can be used with CreateReleaseBundle or DistributeReleaseBundle. This resource can be used to represent both signed and unsigned Release Bundle.Distributing Release Bundles
[/INST]##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               ReleaseBundle
        configuration:
          sourceDistribution:   distributionIntegration
          name:                             <string>
          version:                          <string>
          isSigned:                         <boolean>
          pin:
            versionId:          <number>
            name:               <string>
            version:            <string>
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `ReleaseBundle `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceDistribution`
The name of a Distribution Integration
Required  
`name`
Name of the release bundle
Required  
`version`
Version number of the release bundle
Required  
`isSigned`
When **true** , the release bundle is signed.
Defaults to **false**.
Optional  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the ReleaseBundle resource version to be pinned:
  * `versionId` \-- Resource version Id (can be obtained through the UI)


Or
  * `name` \-- The name of the release bundle to be pinned
  * `version` \-- The version of the release bundle to be pinned


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
##### Environment Variables
Whenever `ReleaseBundle `is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the release bundle
[YAML]
    res_<resource_name>_sourceDistribution
[/YAML]  
Name of the Distribution integration
[YAML]
    res_<resource_name>_version 
[/YAML]  
Version number of the release bundle
[YAML]
    res_<resource_name>_isSigned 
[/YAML]  
Indicates whether the release bundle is signed  
##### Examples
  * This example requires an Artifactory Integration and a Distribution Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: myArtifactory
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDist
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDist
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: myDist
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
71,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A `RemoteFile` resource enables using a file on a remote file server.
This resource holds information of a RemoteFile that is stored on the following registries:
  * JFrog Artifactory
  * AWS S3
  * Google Cloud Storage
  * File Server


[/INST]##### YAML Schema
[YAML]
    resources:
      - name:                       <string>
        type:                       RemoteFile
        configuration:      
          source:                   <file server integration name>
          fileLocation:     <string>  # optional
          fileName:                 <string>  # optional
          fileDateTime:     <string>  # optional
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `RemoteFile `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`source`
The name of an integration of one the following types:
  * Artifactory Integration
  * AWS Keys Integration
  * Google Cloud Integration
  * File Server Integration


Required  
`fileLocation `
Directory path on the remote file server where the file is located
Optional  
`fileName`
The name of the file
Optional  
`fileDateTime`
The date and time of the file
Optional  
##### Environment Variables
Whenever `RemoteFile` is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description  
`res_<resource_name>_name`
Name of the resource  
`res_<resource_name>_source_name`
Name of the integration the file is associated with  
`res_<resource_name>_fileLocation`
The directory containing files for the resource  
`res_<resource_name>_fileName`
The name of the file  
`res_<resource_name>_fileDateTime`
The date and time of the file  
##### Examples
  * This examples requires an Artifactory Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      - name: RemoteFile_1
        type: RemoteFile
        configuration:
          source: s_artifactory
          fileLocation: example-repo-local/setup/help # optional
          fileName: something.txt # optional
              fileDateTime: 19/12/19 # optional
    
    
    pipelines:
      - name: pipelines_1
        steps:
          - name: REFILE_1
            type: PowerShell
            configuration:
              nodePool: win_2019
              integrations:
                - name: s_artifactory
              outputResources:
                - name: RemoteFile_1
            execution:
              onExecute:
                - write_output ""executing step...""
                - jfrog rt dl example-repo-local/setup/test.sh
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
72,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A `VmCluster` is used to represent a set of virtual machines. It is mainly used to deploy services/apps to the specified clusters and in some cases, it can be used to run certain maintenance activities on the clusters as a whole.
[/INST]##### YAML Schema
[YAML]
    - name:             <string>
      type:             VmCluster
      configuration:
        sshKey:         <sshKey integration name>
        targets:
          - <IP address>
          - <IP address>
          - <IP address>
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `VmCluster `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sshKey`
The name of an SSH Key Integration
Required  
`targets`
Declares a collection of IP addresses of Virtual Machines in the clusters
Required  
##### Environment Variables
Whenever `VmCluster `is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_sshkey
[/YAML]  
Name of the SSH Key integration
[YAML]
    res_<resource_name>_targets
[/YAML]  
The collection of IP addresses  
##### Example
The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
[YAML]
    resources:
       - name: vmCluster_1
         type: VmCluster
         configuration:
           sshKey: SR_SSHKEYS
           targets:
             - 1.2.3.4
    
    pipelines:
      - name: pipelines_1
        steps:
          - name: VM_0001
            type: PowerShell
            configuration:
              inputResources:
                - name: vmCluster_1
            execution:
              onExecute:
                - write-output ""executing step...""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
73,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A **Step** is a unit of execution in a pipeline. It is triggered by some event and uses resources to perform an action as part of the pipeline.
Steps take Inputs in the form of Integrations or Resources, execute tasks that perform the operations necessary and then produce a result i.e. Output(s). Now these Outputs can become Inputs to other steps and so on forming a dependency-based, event-driven pipeline.  
A trigger for a step is some external event, or combination of events, such as:
  * Completion of another step
  * A change to a resource, such as a commit to a source code repository or the generation of a new Docker image


 **Step Types**
  * Bash
  * CreateReleaseBundle
  * DistributeReleaseBundle
  * DockerBuild
  * DockerPush
  * GoBuild
  * GoPublishBinary
  * GoPublishModule
  * GradleBuild
  * HelmBlueGreenCleanup
  * HelmBlueGreenDeploy
  * HelmBlueGreenRoleSwitch
  * HelmDeploy
  * HelmPublish
  * Jenkins
  * LinuxVMDeploy
  * Matrix
  * MvnBuild
  * NpmBuild
  * NpmPublish
  * PostMatrix
  * PowerShell
  * PreMatrix
  * PromoteBuild
  * PublishBuildInfo
  * SignReleaseBundle
  * TriggerPipeline
  * UploadArtifact
  * XrayScan


[/INST]### See it Live
Click  **here**  to see some of these steps in action.
#### Generic and Native Steps
Steps are categorized as one of the following:
  *  **Generic step** : A generic step is for general-purpose execution. The Bash and PowerShell steps, which execute any series of shell commands you specify, are the single generic steps for Linux and Windows runtimes, respectively.
  *  **Native Steps**: A native step performs a specific set of actions as an encapsulated unit. Native steps inherently know what shell commands to execute to perform their action. With native steps, you can create complex workflows that push, publish, and promote your builds in Artifactory using simple step definitions.
All native steps derive from the base generic step definition for their respective runtime environment, so all steps have the same base tag:entity structure in a pipeline config. Each native step defines additional tags that are specific to its function.


##### Generic Steps  
Bash
The Bash is a generic step type that enables executing any shell command. This general-purpose step can be used to execute any action that can be scripted, even with tools and services that haven't been integrated with JFrog Pipelines. This is the most versatile of the steps while taking full advantage of what the lifecycle offers.  
Matrix
The Matrix step commences multiple parallel build processes across multiple containers, with different settings for each.  
PostMatrix
The PostMatrix generic step may be used to perform post-execution tasks following a Matrix step.  
PowerShell
The PowerShell step type is a generic type that enables executing PowerShell commands. PowerShell steps can only run on Windows node pools and are similar to the Bash step on other node pools. As a general-purpose step that can execute any action that can be scripted, even with tools and services that haven't been integrated with JFrog Pipelines, it can be used to perform actions where complete control is required.  
PreMatrix
The PreMatrix generic step may be used to prepare a build environment for execution of a Matrix step.  
##### Native Steps  
CreateReleaseBundle
The CreateReleaseBundle is a native step that produces a Release Bundle for distribution to an Artifactory Edge Node. The step can be used to create a signed or unsigned release bundle.  
DistributeReleaseBundle
The DistributeReleaseBundle native step triggers the distribution of a Release Bundle to an Artifactory Edge Node. This step requires a signed release bundle and one or more distribution rules to successfully execute.  
DockerBuild
The DockerBuild native step performs a build to produce a Docker image from a Dockerfile in a Git source repository.  
DockerPush
The DockerPush native step pushes a Docker Image to a Docker registry.  
GoBuild
The GoBuild native step performs a build from Go (GoLang) source.  
GoPublishBinary
The GoPublishBinary native step publishes the GO (GoLang) binaries to Artifactory.  
GoPublishModule
The GoPublishModule native step publishes the GO (GoLang) modules to an Artifactory. This step should be used in conjunction with the GoBuild step.  
GradleBuild
The GradleBuild native step performs a Gradle build on files in a Git repository. Optionally, it can also publish build information to Artifactory.  
HelmBlueGreenCleanup
The HelmBlueGreenCleanup step uninstalls an Idle release previously deployed by a HelmBlueGreenDeploy step.  
HelmBlueGreenDeploy
The HelmBlueGreenDeploy step implements a Blue/Green strategy to deploy a Docker image to a Kubernetes cluster using a Helm chart.  
HelmBlueGreenRoleSwitch
The HelmBlueGreenRoleSwitch step flips the roles played by the Helm releases deployed through a HelmBlueGreenDeploy step.  
HelmDeploy
The HelmDeploy step deploys a Docker image to a Kubernetes cluster using a Helm chart.  
HelmPublish
The HelmPublish step publishes a Helm chart and associated build info from a location in a Git repo to a Helm repository in Artifactory.  
Jenkins
The Jenkins native step transfers execution to a Jenkins pipeline.  
MvnBuild
The MvnBuild native step performs a Maven project build on files in a Git repository. Optionally, it can also publish build information to Artifactory.  
NpmBuild
The NpmBuild native step builds an npm source. This step automatically performs npm-install on the source in a Git repository.  
NpmPublish
The NpmPublish step publishes an npm package to the registry in Artifactory following an NpmBuild step.  
PromoteBuild
The PromoteBuild native step promotes a BuildInfo and moves or copies the related artifacts from one Artifactory repository to another.  
PublishBuildInfo
The PublishBuildInfo step publishes BuildInfo to Artifactory. BuildInfo provides a manifest for the build and includes metadata about the modules, dependencies and other environment variables.  
SignReleaseBundle
The SignReleaseBundle native step signs a Release Bundle in preparation for distributing it to Edge nodes.  
XrayScan
The XrayScan native step triggers a scan by JFrog Xray for security vulnerabilities and license compliance. If there was a watch created that covers the selected build, Xray will scan the indexed build artifacts.  
#### Step Definition
Steps are defined in a pipeline config under the `steps` tag, as shown below.
[YAML]
    pipelines:
      - name: pipe1
        configuration:
          <optional configuration settings>
        steps:
          <collection of step types>
    
[/YAML]
All step are composed of these top-level tags:
Tag
 **Description**
Required/Optional  
`name`
An alphanumeric string (underscores are permitted) that identifies the step. This is the name that will be used when the step is assigned as an input to other steps. The name should be chosen to accurately describe what the step does, e.g. `prov_test_env` to represent a job that provisions a test environment. Names of steps must be unique within a pipeline.
Required  
`type`
A predefined step type.
### Note
After the pipeline performs a sync, a step's type should not be modified, as it can cause pipeline sync errors.
Though not recommended, if you do want to change a step's type, perform the following steps:
  1. Change the step's name and type
  2. Wait for pipeline sync
  3. After the sync completes, change the step's name back to the old name


Required  
`configuration`
Specifies all configuration settings for the step's execution environment. These settings may include:
  * Environment variable definitions
  * Runtime the step will execute in
  * Integrations
  * Input and output resources
  * Timeout duration
  * Queuing priority
  * Settings for native steps


While most configuration tags are optional, in practice you will at least need to define the integrations, steps, and/or resources that trigger the execution of the step. Many native steps will also require some settings in this tag section.
Optional for the generic steps
Required for most native steps  
`execution`
Specifies the actions to perform for each execution phase of the step. These phases may include:
  * the base set of commands
  * successful execution of the base set
  * failed execution of the base set
  * commands to execute on any completion (e.g., for cleanup)


Optional  
##### Step Configuration
When coding your steps, no matter what their function, these are some of the `configuration` section settings you will likely need to consider.
  * Environment variables
  * Runtime
  * Node Pool Assignment
  * Affinity Group
  * Queuing Priority
  * Timeout Duration
  * Integrations
  * Inputs/Triggers
  * Outputs


###### Environment variables
You may define or override a set of `environmentVariables` that will be available only for the duration of the execution of the step.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


###### Runtime
You may choose a runtime for the step to execute in.
###### Node Pool Assignment
You can designate a particular `nodePool` that you want your steps to draw execution nodes from. You may wish to do this to ensure your steps execute in a particular architecture/OS combination, and/or draw from a node pool that provides enough execution nodes to process multiple steps in your pipeline at once.
If you don't specify a `nodePool` then your steps will run in the node pool that has been set as the default.
###### Affinity Group
You can bind steps together in a named `affinityGroup` to ensure that they execute on the same node. This helps when several steps need access to files or artifacts that are created on a node. For example, DockerBuild and DockerPush steps must be assigned to the same `affinityGroup` so that the image built by DockerBuild is available for DockerPush.
###### Queuing Priority
You can assign a queuing `priority` to a step to ensure it gains appropriately ordered access to a build node when there are parallel steps in a pipeline or multiple pipelines are executing.
###### Timeout Duration
You can set a time limit for the step to complete execution, using the timeoutSeconds tag. If the step does not complete in the given number of seconds, the step will be forced to a completion state of `failed`.
###### Integrations
Your native step may require an integration to be declared, or you may wish to issue commands in your step that reference integrations, such as  sending notifications to Slack. If so, you must declare the `integrations` in your step.
###### Inputs/Triggers
When defining the first step in your pipeline, you will likely need to define triggering `inputResources` (such as GitRepo or a Webhook) that will automatically commence a new run of the pipeline. Resources may be from any pipeline source. If the resource is part of a multi-branch pipeline source and from a different branch than the step, the branch must be specified for the resource in `inputResources`.
You should define the remainder of your steps so they execute in an interdependent sequence. This means that each step is configured so that its execution will be triggered by the successful completion of a prior, prerequisite step (or steps). In this way, step 1's completion will trigger the execution of step 2, completion of step 2 triggers execution of step 3, and so on until all steps in the pipeline are executed.
To do this, each step will need to define the `inputSteps` that trigger it in order to build the desired execution sequence.
###### Outputs
Your step will likely produce changes to a resource as a result of performing its task. You must declare these as `outputResources`. If the updated resource is part of a multi-branch pipeline source and from a different branch than the step, the branch must be specified for the resource in `outputResources`.
##### Step Execution
Your automated pipeline issues shell commands to the execution node to perform the work. These are specified in each step's `execution` section during the specified execution stage of that step.
For example, in the generic Bash step:
[YAML]
            . . .
            execution:
              onStart:
                - echo ""prepare for main execution""
              onExecute:
                - echo ""executing task command 1""
                - echo ""executing task command 2""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: 
                - echo ""Cleaning up some stuff""
[/YAML]
### Note
You may not declare an `onExecute` section in a native step as they provide their own `onExecute` to perform their native function.
In addition to the conventional shell commands, you can also make use of built-in Utility Functionsto perform actions through integrations or preserve state information.
You can also leverage environment variables that are available during pipeline execution to flexibly automate any of your step execution actions.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
74,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **Bash** is a generic step type that enables executing any shell command. This general-purpose step can be used to execute any action that can be scripted, even with tools and services that haven't been integrated with JFrog Pipelines. This is the most versatile of the steps while taking full advantage of what the lifecycle offers.
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
[/INST]##### Usage
 **Bash**
[YAML]
    pipelines:
      - name:                       <string>
        steps:
          - name:           <string>
            type:           Bash
            configuration:
              affinityGroup:        bldGroup 
              priority:             <[0-10000]>
              timeoutSeconds:       <job timeout limit>
              nodePool:             <name of the nodePool>
              chronological:        <true/false>
              allowFailure:         <true/false>
              environmentVariables:
                env1:                       <string>
                env2:                       <string>
                env3:
                  default:      <string>
                  description:  <string>
                  values:       <array>
                  allowCustom:  <true/false>
              integrations:
                - name:             <integration name>
              inputSteps:
                - name:             <step name>
                  status:
                    - <terminal_status>
                    - <terminal_status>
                    - <terminal_status>
              inputResources:
                - name:             <resource name>
                  trigger:          <true/false>    # default true
                  newVersionOnly: <true/false>  # default false
                  branch:       <string>        # see description of defaults below
              outputResources:
                - name:             <resource name>
                  branch:       <string>        # see description of defaults below
              runtime:            
                type:                       <image/host>
                image:
                  auto:
                    language: <string>
                     version: <string>  # specifies a single version. Cannot be used if ""versions"" is defined.
                     versions:          # specifies multiple versions. Cannot be used if ""version"" is defined.
                                   - <string>
                  custom:
                    name:                  <string>
                    tag:                       <string>
                    options:           <string>
                    registry:          <integration>   # optional integration for private registry
                    sourceRepository:  <path>          # required if registry is Artifactory. e.g. docker-local
                    region:                            # required if registry is AWS. e.g. us-east-1
                    autoPull:       <true/false>             # default true; pulls image before run
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onExecute:
                - echo ""executing task command 1""
                - echo ""executing task command 2""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step. The name should be chosen to accurately describe what the step does, e.g. `prov_test_env` to represent a job that provisions a test environment. Names of steps must be unique within a pipeline.
###### type
Must be `Bash` for this step type.
###### configuration
Specifies all optional configuration selections for the step's execution environment.
Tag
 **Description of usage**
Required/Optional  
`affinityGroup`
Label that controls affinity to a Node. All the steps with the same affinityGroup will be executed on the same node. This will allow sharing state between the steps. An example is having the same affinityGroup for DockerBuild and DockerPush steps in a Pipeline so that Image being built in the DockerBuild step can be used to published in the DockerPush step.
Optional  
`priority`
Controls the priority of a step when there are parallel steps in a pipeline or multiple pipelines executing. It determines which step will run first across all steps that could run if there were no constraints on the number of steps running. Steps with a lower number will run before steps with higher numbers. For example, priority 10 will run before priority 100. The default priority is 9999.
Priority does not apply to steps that are still waiting for an input to complete or configured to run in a node pool with no available nodes. Also, if there are two steps ready to run and only one available node, the one with the lower priority number runs first, regardless of which pipeline each step belongs to.
Optional  
`timeoutSeconds`
Time limit, in the number of seconds, for the step to complete. If the step does not complete in the given time limit, the step will be forced to a completion state of failed.
Optional  
`nodePool`
Assigns the node pool the step executes on. If node pool isn't specified, a step will execute on the default node pool.  See here to learn more about node pool
Optional  
`chronological`
Specifies the step must execute in chronological order, to ensure receipt of all state updates from preceding steps.
A step with `chronological:true` will only execute when that step is complete in all previously triggered runs.
Optional  
`allowFailure`
If you do not want a step to contribute to the final status of the run, add allowFailure: true to the configuration section of that step. When this option is used, even when a step fails or is skipped, the final status of the run is not affected.
For more information, see Conditional Workflows.
Optional  
`condition`
Create a condition based on the values of add_run_variables environment variable, so that a step can be skipped based on dynamically set variables before it gets assigned to a node.
For more information, see Run Variable Conditional Workflow.
Optional  
`environmentVariables`
Assigns any environment variables and their values in key:value format. All environment variables assigned within a step definition are active only for the scope of the execution of that step.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


Optional  
`integrations`
A collection of integrations that will be used by this step. Integrations can be used directly in step without a resource.
Optional  
`inputSteps`
A collection of named steps whose completion will trigger execution of this step.
In addition, you can set status conditional workflow for input steps. When configured for a step, it executes only if an input step’s status, during its current run, is satisfied. You can configure any number of statuses for an input step.
### Note
It is important to note that the status of an input step in the current run only is considered for conditional workflows. If a step is not part of the current run, it is always assumed that the condition for that input step is met.
For more information, see Step Status Conditional Workflow.
Optional  
`inputResources`
A collection of named Pipelines Resources that will be used by this step as inputs.
  *  **trigger** : By default, changes to these named resources will trigger execution of this step. This can be changed by declaring `trigger` as `false`.
  * `newVersionOnly` : Setting `newVersionOnly` as `true` on one or more inputs will cause the step to be skipped if one or more of those resources is an output resource of another step in the same run and none of the `inputResources` have been updated in the current run.
  * `branch` : A `branch` is only required when using a resource from another branch of a multi-branch source. By default, the resource from a single-branch source or from the same branch of a multi-branch pipeline is used. To use a multi-branch resource in a single-branch pipeline or another branch in a multi-branch pipeline, `branch` should be used to specify the branch to which the resource belongs.


Optional  
`outputResources`
A collection of named Pipelines Resources that will be generated or changed by this step.
`branch`: A `branch` is only required for resources from another branch of a multi-branch source. By default, the resource from a single-branch source or from the same branch of a multi-branch pipeline is assumed. To update a multi-branch resource in a single-branch pipeline or another branch in a multi-branch pipeline, `branch` should be used to specify the branch to which the resource belongs.
Optional  
`runtime`
Specifies the runtime for the execution node.
Optional  
###### execution
Declare sets of shell command sequences to perform for different execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of `onExecute`
Optional  
`onExecute`
Main commands to execute for the step
Optional  
`onSuccess`
Commands to execute on successful completion of `onExecute`
Optional  
`onFailure`
Commands to execute on failed completion of `onExecute`
Optional  
`onComplete`
Commands to execute on any completion of `onExecute`
Optional  
### Note
`onExecute`, `onStart`, `onSuccess`, `onFailure`, and `onComplete` are reserved keywords. Using these keywords in any other context in your execution scripts can cause unexpected behavior.
##### Examples
The Pipelines DSL for these examples is available in this repository in the JFrog GitHub account.
###### Perform a build activity
This is an example of how to use the Bash step to perform a build activity.
 **Bash step to build**
[YAML]
    - name: build
      type: Bash
      configuration:
        nodePool: my_node_pool
        environmentVariables:
          env1: value1
          env2:
            default: value2
            description: Example Variable
            values:
              - value2
              - value3
            allowCustom: false
        runtime:
          type: image
          image:
            auto:
              language: node
              versions:
                - ""16""
        inputResources:
          - name: src
      execution:
        onExecute:
          - cd $res_src_resourcePath
          - npm install
          - mkdir -p testresults && mkdir -p codecoverage
          - $res_src_resourcePath/node_modules/.bin/mocha --recursive ""tests/**/*.spec.js"" -R mocha-junit-reporter --reporter-options mochaFile=testresults/testresults.xml
          - $res_src_resourcePath/node_modules/.bin/istanbul --include-all-sources cover -root ""routes"" node_modules/mocha/bin/_mocha -- -R spec-xunit-file --recursive ""tests/**/*.spec.js""
          - $res_src_resourcePath/node_modules/.bin/istanbul report cobertura --dir codecoverage
          - save_tests $res_src_resourcePath/testresults/testresults.xml
        onSuccess:
          - send_notification mySlack ""build completed""
[/YAML]
###### Python in bash step
This is an example of how to use Python in a bash step.
 **Python**
[YAML]
    resources:
      - name: script
        type: GitRepo
        configuration:
          path: jfrog/sample-script
          gitProvider: myGithub
    pipelines:
      - name: test_stepTestReports
        steps:
          - name: testReport
            type: Bash
            configuration:
              inputResources:
                - name: script
            execution:
              onExecute:
                - cd $res_script_resourcePath
                - ls
                - python -m py_compile calc.py
                - pip install --upgrade pip
                - hash -d pip
                - pip install pytest           
                - py.test --verbose --junit-xml test-reports/results.xml test_calc.py
              onComplete:
                - save_tests $res_script_resourcePath/test-reports/results.xml
[/YAML]
###### runtime, environmentVariables, and inputSteps tags
This example uses the `runtime`, `environmentVariables`, and `inputSteps` tags:
[YAML]
    pipelines:
      - name: api_steps
        steps:
          - name: api_steps
            type: Bash
            configuration:
              runtime:
                type: host
              environmentVariables:
                env1: value1
                env2: value2
            execution:
              onExecute:
                - touch cachefile.txt
                - add_cache_files cachefile.txt my_file
    
          - name: api_steps_2
            type: Bash
            configuration:
              runtime:
                type: host
              inputSteps:
                - name: api_steps
            execution:
              onExecute:
                - echo ""step 2..""
    
      - name: api_steps_ProjectAdmin
        steps:
          - name: api_steps_ProjectAdmin
            type: Bash
            configuration:
              runtime:
                type: host
              environmentVariables:
                env1: value1
                env2: value2
            execution:
              onExecute:
                - touch cachefile.txt
                - add_cache_files cachefile.txt my_file
    
          - name: api_steps_ProjectAdmin_2
            type: Bash
            configuration:
              runtime:
                type: host
              inputSteps:
                - name: api_steps_ProjectAdmin
            execution:
              onExecute:
                - echo ""step 2..""
    
    
[/YAML]
###### affinityGroup and priority tags
This example uses the `affinityGroup` and `priority` tags:
[YAML]
    pipelines:
      - name: S_WF_019
        steps:
          - name: S_WF_019_001
            type: Bash
            execution:
              onStart:
                - add_run_variables step_1_var=""step_1""
              onExecute:
                - echo ""step 1 is running""
    
          - name: S_WF_019_002
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_001
              affinityGroup: ag_foo
              priority: 4
            execution:
              onStart:
                - echo ""step_4_var - ${step_4_var}""
                - if [ ""$step_4_var"" != ""step_4"" ]; then exit 1; fi
                - add_run_variables step_2_var=""step_2""
              onExecute:
                - echo ""step 2 is running""
    
          - name: S_WF_019_003
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_001
              affinityGroup: ag_foo
              priority: 1
            execution:
              onStart:
                - echo ""step_1_var - ${step_1_var}""
                - if [ ""$step_1_var"" != ""step_1"" ]; then exit 1; fi
                - add_run_variables step_3_var=""step_3""
              onExecute:
                - echo ""step 3 is running""
    
          - name: S_WF_019_004
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_001
              affinityGroup: ag_foo
              priority: 3
            execution:
              onStart:
                - echo ""step_3_var - ${step_3_var}""
                - if [ ""$step_3_var"" != ""step_3"" ]; then exit 1; fi
                - add_run_variables step_4_var=""step_4""
              onExecute:
                - echo ""step 4 is running""
    
          - name: S_WF_019_005
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_002
                - name: S_WF_019_003
                - name: S_WF_019_004
              affinityGroup: ag_foo
              priority: 4
            execution:
              onStart:
                - echo ""step_6_var - ${step_6_var}""
                - if [ ""$step_6_var"" != ""step_6"" ]; then exit 1; fi
                - add_run_variables step_5_var=""step_5""
              onExecute:
                - echo ""step 5 is running""
    
          - name: S_WF_019_006
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_002
                - name: S_WF_019_003
                - name: S_WF_019_004
              affinityGroup: ag_foo
              priority: 2
            execution:
              onStart:
                - echo ""step_2_var - ${step_2_var}""
                - echo ""step_3_var - ${step_3_var}""
                - echo ""step_4_var - ${step_4_var}""
                - if [ ""$step_2_var"" != ""step_2"" ]; then exit 1; fi
                - if [ ""$step_3_var"" != ""step_3"" ]; then exit 1; fi
                - if [ ""$step_4_var"" != ""step_4"" ]; then exit 1; fi
                - add_run_variables step_6_var=""step_6""
              onExecute:
                - echo ""step 6 is running""
    
          - name: S_WF_019_007
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_005
                - name: S_WF_019_006
              affinityGroup: ag_foo
              priority: 2
            execution:
              onStart:
                - echo ""step_1_var - ${step_1_var}""
                - echo ""step_2_var - ${step_2_var}""
                - echo ""step_3_var - ${step_3_var}""
                - echo ""step_4_var - ${step_4_var}""
                - echo ""step_5_var - ${step_5_var}""
                - echo ""step_6_var - ${step_6_var}""
                - if [ ""$step_1_var"" != ""step_1"" ]; then exit 1; fi
                - if [ ""$step_2_var"" != ""step_2"" ]; then exit 1; fi
                - if [ ""$step_3_var"" != ""step_3"" ]; then exit 1; fi
                - if [ ""$step_4_var"" != ""step_4"" ]; then exit 1; fi
                - if [ ""$step_5_var"" != ""step_5"" ]; then exit 1; fi
                - if [ ""$step_6_var"" != ""step_6"" ]; then exit 1; fi
              onExecute:
                - echo ""step 7 is running""
    
    
[/YAML]
###### chronological tag
This examples uses the `chronological` tag:
[YAML]
    pipelines: 
      - name: bash_chronological
        steps: 
          - name: Start
            type: Bash
            execution: 
              onExecute: 
                - echo ""It's a start.""
          - name: Step1
            type: Bash
            configuration:
              chronological: true
              inputSteps: 
                - name: Start
            execution: 
              onExecute: 
                - add_run_variables step1=foo
          - name: Step2
            type: Bash
            configuration: 
              chronological: true
              inputSteps: 
                - name: Start
            execution: 
              onExecute: 
                - add_run_variables step2=bar
          - name: Step3
            type: Bash
            configuration: 
              chronological: true
              inputSteps: 
                - name: Start
            execution: 
              onExecute: 
                - add_run_variables step3=baz
          - name: Finish
            type: Bash
            configuration: 
              inputSteps: 
                - name: Step1
                - name: Step2
                - name: Step3
            execution: 
              onExecute: 
                - |
                  echo ""Step1: $step1""
                  echo ""Step2: $step2""
                  echo ""Step3: $step3""
    
    
[/YAML]
###### timeoutSeconds tag
This example uses the `timeoutSeconds` tag:
[YAML]
    pipelines:
      - name: pipelines_S_Bash_0023
        steps:
          - name: S_Bash_0023
            type: Bash
            configuration:
              timeoutSeconds: 10
            execution:
              onExecute:
                - sleep 3m
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
75,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **CreateReleaseBundle** is a native step that produces a Release Bundlefor distribution to an Artifactory Edge Node.The step can be used to create a signed or unsigned release bundle. When the `sign` tag is set as `true` in your yaml, this step saves artifact information to support signed pipelines.Release Bundle RepositoriesJFrog Artifactory Edge
[/INST]##### YAML Schema
The YAML schema for DockerBuild native step is as follows:
 **CreateReleaseBundle**
[YAML]
    pipelines:
      - name:    <string>
        steps:
          - name: <string>
            type: CreateReleaseBundle
            configuration:
              #inherits all the tags from bash
              releaseBundleName:        <string>
              releaseBundleVersion:     <string>
              dryRun:                   <boolean>   # default true
              sign:                     <boolean>   # default false
              description:              <string>    # optional
              failOnValidate:           <boolean>   # optional (Signed Pipelines must be enabled)
              releaseNotes:                         # optional
                content:                <string>    # ""markdown|asciidoc|plain_text""
                syntax:                 <string>    # required in releaseNotes
     
              inputResources:
                - name:                 <BuildInfo resource>  # one or more BuildInfo, or
                - name:                 <Aql resource>        # one Aql
              outputResources:
                - name:                 <ReleaseBundle resource>
     
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `CreateReleaseBundle `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify either a named BuildInfo resource(s) or an Aql resource. CreateReleaseBundle step does not accept other input resources.
If BuildInfo `inputResources` are provided, the query for the release bundle is constructed using the `buildName`, `buildNumber`, and `targetRepo` of each BuildInfo input.
Required  
`outputResources`
Must specify a ReleaseBundleresource.
The `name`, `version`, and `isSigned` settings in the output ReleaseBundle are updated to the step's `releaseBundleName`, `releaseBundleVersion`, and `sign` values respectively (or any environment variable values that replaced environment variable placeholders for those values).
Required  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`releaseBundleName`
An alphanumeric name for the release bundle.
Required  
`releaseBundleVersion`
Version string for the release bundle.
Required  
`dryRun`
When set to `true` parse and validate only to test whether a release bundle version can be created.
Default is true.
Optional  
`sign`
Specifies whether the release bundle version will be signed as part of this step.
Default is false.
### GPG Signing Key Passphrase
When configuring `sign=true`, and when your Distribution release bundle GPG signing keyis passphrase protected, remember to provide the GPG Signing Key Passphrase when creating/updating your  Distribution Integration.GPG Signing
Optional  
`description`
Description of the release bundle.
Optional  
`releaseNotes`
Describes the release notes for the release bundle version.
`syntax` specifies the format of release notes: `plain_text`, `markdown`, or `asciidoc`. Default is `plain_text`.
`content` is the release notes string in the specified syntax format. Use the `|` character to denote a string preserving newlines.
Optional  
`failOnValidate`
Fail the step if one of the signatures of the BuildInfo input resource artifacts cannot be verified.
Default is `false`.
Optional ( Signed Pipelines must be enabled)  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
### Note
`onExecute`, `onStart`, `onSuccess`, `onFailure`, and `onComplete` are reserved keywords. Using these keywords in any other context in your execution scripts can cause unexpected behavior.
##### Examples
The following examples show which settings to configure for a few different release bundles.
###### Unsigned Release Bundle Created using BuildInfo Resource
A simple, unsigned release bundle created using a BuildInfo resource. In this case, the release bundle version will be the run number and will have no description or release notes.
  * This example requires an Artifactory Integration and a Distribution Integration.
  * The Pipelines DSL for this example is available in this repositoryin the JFrog GitHub account.


 **CreateReleaseBundle**
[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.myArtifactoryIntegration }}
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.demoArtifactoryIntegration }}
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]
###### Create and Sign Release Bundle
Create and immediately sign a release bundle, with a description and release notes added to the release bundle.
 **CreateReleaseBundle**
[YAML]
    pipelines: 
      - name: createReleaseBundlePipeline
        steps:
          - name: createReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: myReleaseBundle
              releaseBundleVersion: ""${run_number}""
              dryRun: false
              sign: true
              description: ""My release bundle""
              releaseNotes:
                syntax: plain_text
                content: ""Release of ${run_number} by ${step_name}""
              inputResources:
                - name: myBuildInfo
              outputResources:
                - name: myReleaseBundle
    
[/YAML]
###### Trigger a Dry Run
Trigger a dry run of the release bundle creation. No release bundle will be created.
 **CreateReleaseBundle**
[YAML]
    pipelines: 
      - name: createReleaseBundlePipeline
        steps:
          - name: createReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: myReleaseBundle
              releaseBundleVersion: ""${run_number}""
              dryRun: true
              inputResources:
                - name: myBuildInfo
              outputResources:
                - name: myReleaseBundle
    
[/YAML]
##### How it Works
When you use the **CreateReleaseBundle** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (configure cli with the integration in the input resource)
  * jfrog rt curl (get the Artifactory service_id)
  * validate_artifact (use the signed pipelines feature to verify the incoming BuildInfo)
  * Create the release bundle creation payload (the JSON object that will be in the request to Distribution)
  * curl (send the release bundle creation payload to Distribution)
  * write_output (update the output ReleaseBundle resource)
  * save_artifact_info (if the bundle was signed, save the data for use with signed pipelines)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
76,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **DistributeReleaseBundle** native step triggers the distribution of Release Bundles to an Artifactory Edge Node. This step requires a signed release bundle and one or more distribution rules to successfully execute.Distributing Release BundlesJFrog Artifactory Edge
[/INST]##### YAML Schema
The YAML schema for DistributeReleaseBundle native step is as follows:
 **DistributeReleaseBundle**
[YAML]
    pipelines:
      - name:    <string>
        steps:
          - name: my_distribute
            type: DistributeReleaseBundle
            configuration:
                    #inherits all the tags from bash; 
              dryRun: <boolean>                # optional
              inputResources:
                - name: my_releaseBundle      # one ReleaseBundle is required
                  trigger: false   
                - name: my_distributionRule   # one DistributionRule is required
                  trigger: false    # default true
              outputResources:
                - name: my_releaseBundleOutput # one ReleaseBundle is optional
     
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `DistributeReleaseBundle `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify a ReleaseBundle resource and one DistributionRule resource.
Required  
`outputResources`
May specify a ReleaseBundle resource to be updated with the `name` and `version` of the input ReleaseBundle.
Optional  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`dryRun`
Controls whether this should be a dry run to test if the release bundle can distribute to the Edge nodes matching the distribution rule.
The default is true.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a DistributeReleaseBundle step to distribute or for a dry run.
###### Distribute Input Release Bundle Edge Node
Distributes the input release bundle to the edge nodes defined in the distribution rule.
  * This example requires an Artifactory Integration and a Distribution Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


 **DistributeReleaseBundle**
[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.myArtifactoryIntegration }}
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.demoArtifactoryIntegration }}
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]
###### Trigger a Dry Run
Triggers a dry run of the distribution.
 **DistributeReleaseBundle**
[YAML]
    pipelines: 
      - name: distributeReleaseBundlePipeline
        steps:
          - name: distributeReleaseBundleDryRun
            type: DistributeReleaseBundle
            configuration:
              dryRun: true
              inputResources:
                - name: myReleaseBundle
                - name: myInputDistributionRule
    
[/YAML]
###### Triggers Distribution and Updates Output Resource
Triggers a distribution and updates the output resource with the name and version of the input.
 **DistributeReleaseBundle**
[YAML]
    pipelines: 
      - name: distributeReleaseBundlePipeline
        steps:
          - name: distributeReleaseBundleDryRun
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: myReleaseBundle
                - name: myInputDistributionRule
              outputResources:
                - name: myOutputReleaseBundle
    
[/YAML]
###### Same Step for Dry Runs and to Distribute
In this example, the same step is used for both dry runs and to distribute the release bundle to Edge Nodes. The dry_run variable may be set in the  pipeline configuration section or  step configuration or added as a run variable by an earlier step in the pipeline using add_run_variable.
 **DistributeReleaseBundle**
[YAML]
    pipelines: 
      - name: distributeReleaseBundlePipeline
        steps:
          - name: distributeReleaseBundleStep
            type: DistributeReleaseBundle
            configuration:
              dryRun: ${dry_run}
              inputResources:
                - name: myReleaseBundle
                - name: myInputDistributionRule
    
[/YAML]
##### How it Works
When you use the **DistributeReleaseBundle** native step in a pipeline, it performs the following functions in the background:
  * Create the distribution payload (the JSON object that will be in the request to Distribution)
  * curl $distributionUrl/api/v1/distribution/$releaseBundleName/$releaseBundleVersion (send the distribution or dry run payload to Distribution)
  * curl $distributionUrl/api/v1/release_bundle/$releaseBundleName/$releaseBundleVersion/distribution/$trackerId (if not a dry run, using the tracker ID returned by Distribution, check if the distribution is complete)
  * write_output (update the output ReleaseBundle resource)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
77,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The `DockerPush `native step pushes a Docker Image to a Docker registry.
[/INST]### Note
  * Currently, only Docker registries in Artifactory are supported.
  * DockerBuild and DockerPush steps must be assigned to the same `affinityGroup` to share state. If they are not, the output of DockerBuild will not be available to DockerPush. For more information on `affinityGroup` see Pipelines Steps.


### Docker Build and Push Quickstart
This Docker Build and Push quickstart demonstrates the definition of a pipeline that uses the DockerBuild and DockerPush native steps to build a single Docker Image, push it to Artifactory, and then publish the BuildInfo.
##### YAML Schema
The YAML schema for DockerPush native step is as follows:
 **DockerPush**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: DockerPush
            configuration:
              #inherits all the tags from bash
              affinityGroup:        <string>
              targetRepository:     <string>        #  may be required. Must be a local repository. Virtual repositories are not supported.
              forceXrayScan:        <boolean>       # default false
              failOnScan:           <boolean>       # default true
              autoPublishBuildInfo: <boolean>       # default false
    
              integrations:
                - name:             <artifactory integration>  # required
              inputSteps:
                - name:             <DockerBuild step>         # required
              outputResources:
                - name:                 <Image resource>           # optional
                - name:                 <BuildInfo resource>       # required if autoPublishBuildInfo is true
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: 
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `DockerPush` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`affinityGroup`
Must specify an affinity group string that is the same as specified in a prior DockerBuild step.
Optional  
`integrations`
Must specify an Artifactory Integration.
Required  
`inputSteps`
Typically the DockerBuild step that built the image. The DockerBuild step must always be in the same affinity group, but other steps, such as Bash or PowerShell, are also permitted in the same affinity group, between DockerBuild and DockerPush.
May be Required  
`outputResources`
May specify an Image resource. If one is specified, the `imageTag` property of that resource will be updated with the `dockerImageTag` of the preceding DockerBuild step.
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set to `true`. If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the input DockerBuild step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName`and `buildNumber` are `$pipeline_name` and `$run_number` respectively.
Optional
May be required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well as that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`targetRepository`
The name of the Docker repository in Artifactory. Required when using JFrog CLI v1 and not used when the pipeline is configured to use JFrog CLI v2.
### Note
Must be a local repository. Virtual repositories are not supported.
May be required  
`forceXrayScan`
When `true`, forces a scan of the pushed image by JFrog Xray.
Default is `false`.
Optional  
`failOnScan`
When set to `true`, and when the Xray Policy Rule  Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
`autoPublishBuildInfo`
When set to `true`, publishes build info with the Docker image.
Default is `false`.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a DockerPush step to push a Docker image.
###### Push Image to Artifactory
Pushes the image created by the DockerBuild input step to Artifactory. Does not trigger a scan.
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For a full tutorial, see Pipeline Example: Docker Build and Push.


 **DockerPush**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: app_repo1
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.gitIntegration }}                      
          path: {{ .Values.gitRepositoryPath }}    
          branches:
            include: master
    
      - name: app_buildinfo1
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
      - name: app_promoted_buildinfo1
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
    pipelines:
      - name: app_dev_pipeline
        steps:
          - name: app_build
            type: DockerBuild
            configuration:
              affinityGroup: docker_group
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}  
              dockerImageTag: ${run_number}
              inputResources:
                - name: app_repo
              integrations:
                - name: {{ .Values.artifactoryIntegration }}         
                
          - name: app_push
            type: DockerPush
            configuration:
              affinityGroup: docker_group
              targetRepository: {{ .Values.sourceRepository }}
              integrations:
                - name: {{ .Values.artifactoryIntegration }}                            
              inputSteps:
                - name: app_build
    
          - name: publish_app_build
            type: PublishBuildInfo
            configuration:
              affinityGroup: docker_group
              inputSteps:
                - name: app_push
              outputResources:
                - name: app_buildinfo
    
[/YAML]
###### Affinity Group
This extends one of the DockerBuild examples, pushing that image to Artifactory. Note that an affinity group has been specified in both steps.
 **DockerPush**
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              affinityGroup: dockerGroup
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc   # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              dockerOptions: --build-arg build_number_env_variable=${run_number}          
              inputResources:
                - name: gosvc_app
              integrations:
                - name: MyArtifactory
    
          - name: dockerPushStep
            type: DockerPush
            configuration:
              affinityGroup: dockerGroup
              targetRepository: dockerRepo
              inputSteps:
                - name: bld_image
              outputResources:
                - name: outputBuildInfo
              integrations:
                - name: MyArtifactory
    
[/YAML]
###### Publish Build Info, Trigger Xray Scan, Update Output Image Resource
In this, publishing build info, triggering an Xray scan, and updating an output Image resource has been added to the previous example.
 **DockerPush**
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              affinityGroup: dockerGroup
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc   # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              dockerOptions: --build-arg build_number_env_variable=${run_number}          
              inputResources:
                - name: gosvc_app
              integrations:
                - name: MyArtifactory
    
          - name: dockerPushStep
            type: DockerPush
            configuration:
              affinityGroup: dockerGroup
              targetRepository: dockerRepo
              autoPublishBuildInfo: true
              forceXrayScan: true
              inputSteps:
                - name: bld_image
              outputResources:
                - name: outputBuildInfo
                - name: outputImage
              integrations:
                - name: MyArtifactory
    
[/YAML]
##### How it Works
When you use the **DockerPush** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt use (to set the current default Artifactory configuration to the one set up for the integration in `integrations`)
  * restore_run_files (copy the build information saved from the DockerBuild step)
  * jfrog rt docker-push (push the image to Artifactory)
  * jfrog rt build-publish (if `autoPublishBuildInfo` is true, publish the build info)
  * write_output (if `autoPublishBuildInfo` is true, update the BuildInfo buildName and buildNumber)
  * write_output (if there is an output Image resource, update the Image imageTag)
  * jfrog rt build-scan (if forceXrayScan is true, trigger a scan)
  * add_run_files (save/update the build information in the run state for later publish steps)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
78,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **DockerBuild** native step performs a build to produce a Docker image from a Dockerfile in a GitRepo source repository resource.
In the step configuration, you must provide the name ( `dockerFileName` ) and directory ( `dockerFileLocation` ) of the Dockerfile that contains the command to be processed by a `docker build` command, as well as the name ( `dockerImageName` ) and tag ( `dockerImageTag` ) of the resulting image. The image is built on the build node, and information about that image is stored in the run state.
To build a Docker image that relies on a private base image:
  1. Define the base image as an Image resource, with `autoPull` set to `true`.
  2. Specify the Image resource as one of the `inputResources` of the DockerBuild step.


To include artifacts in the Docker image that are not part of the GitRepo source repository:
  1. Define a FileSpec resource that specifies the files to include from Artifactory.
  2. Specify the FileSpec resource as one of the `inputResources`of the DockerBuild step.


[/INST]### Proper usage of DockerBuild step
DockerBuild and DockerPush steps must be assigned to the same `affinityGroup` to share state. If this is not done, the output of DockerBuild will not be available for DockerPush. For more information on using `affinityGroup`, see Running multiple steps on the same build node.
### Docker Build and Push Quickstart
This Docker Build and Push quickstart demonstrates the definition of a pipeline that uses the DockerBuild and DockerPush native steps to build a single Docker Image, push it to Artifactory, and then publish the BuildInfo.
##### YAML Schema
The YAML schema for DockerBuild native step is as follows:
 **DockerBuild**
[YAML]
    pipelines: 
      - name: <string>
        steps:
          - name: <string>
            type: DockerBuild
            configuration:
              #inherits all the tags from bash
              affinityGroup:       <string>
              dockerFileLocation:  <string>
              dockerFileName:      <string>
              dockerImageName:     <string>
              dockerImageTag:      <string>
              dockerOptions:       <string>
    
              integrations:
                - name:            <artifactory or docker registry integration>  # required
    
              inputResources:
                - name:             <GitRepo resource>        # required, git repository containing your Dockerfile
                - name:             <Image resource>          # optional base image
                - name:             <FileSpec resource>       # optional
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `DockerBuild` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`affinityGroup`
Must specify an affinity group string that is the same as specified in a subsequent DockerPush step.
Optional  
`inputResources`
Must specify:
  * a GitRepo resource (that contains the Dockerfile)


Optionally, you may also specify:
  * One or more Image resources to pull base images used in the build or to trigger this build.
  * One or more FileSpec resources that specify what files to include in the build context. These files are automatically copied to `dockerFileLocation`.


Required/Optional  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`dockerFileLocation`
Directory containing the Dockerfile, which is the file that has Docker build configuration. This file is also used as the context for the Docker build. The path provided should be relative to the root of the input GitRepo repository. If no location is provided, the default is the root of the GitRepo repository.
Required  
`dockerFileName`
Name of the Dockerfile.
Required  
`dockerImageName`
The name of the Docker image to create. This can be set using environment variables or triggering a run using parameters.
Required  
`dockerImageTag`
The tag for the Docker image to create. This can be set using environment variables or triggering a run using parameters.
Required  
`dockerOptions`
Additional options for the docker build command.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples use a GoLang Git repository represented by a GitRepo resource named `gosvc_app` to create a Docker image that is published to Artifactory. They assume that an Artifactory integration named `MyArtifactory` has been created, and that the Artifactory instance has a Docker repository mapped to `docker.artprod.company`.
  * These examples require an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for a similar example is available in this repository in the JFrog GitHub account.
  * For a full tutorial, see Pipeline Example: Docker Build and Push.


The following resources declarations support these examples. Not all of these resources are used in all examples.
###### Resources
[YAML]
    resources:
    # Application source repository
      - name: gosvc_app
        type: GitRepo
        configuration:
          gitProvider: myGithub
          path: myuser/myrepo                   # replace with your repository name
          branches:
            include: master
    
    # Docker image in an Artifactory repository
      - name: base_image
        type: Image
        configuration:
          registry: myArtifactory
          sourceRepository: docker-local        # replace with your repository name
          imageName: docker.artprod.mycompany.com/baseimage
          imageTag: latest
          autoPull: true
    
    # Files in an Artifactory repository
      - name: icon_files
        type: FileSpec
        configuration:
          sourceArtifactory: myArtifactory
          pattern: my-local-repo/all-my-images/
          target: icons/
[/YAML]
###### Build a Docker image from a source repository
This example builds a Docker image to a Docker registry in Artifactory. The tag for the image is set to the pipeline's run number.
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc    # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              inputResources:
                - name: gosvc_app
              integrations:
                - name: MyArtifactory
[/YAML]
###### Build a Docker image with dockerOptions
This example demonstrates use of the `dockerOptions` tag to set the `build-arg` option for the Docker command. An environment variable named `build_number_env_variable` is dynamically set to the pipeline's run number. The example assumes the environment variable is used in the Dockerfile commands.
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc   # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              dockerOptions: --build-arg build_number_env_variable=${run_number}           
              inputResources:
                - name: gosvc_app
              integrations:
                - name: MyArtifactory
[/YAML]
###### Build a Docker image with a private base image
This example builds a Docker image that relies on a private base image stored in an Artifactory Docker repository.
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc       # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              inputResources:
                - name: gosvc_app
                - name: base_image
              integrations:
                - name: MyArtifactory
[/YAML]
###### Build a Docker image with files outside the current path
This example demonstrates building a Docker image that includes files outside of the current path. It pulls icon files stored in an Artifactory repository for integration art named `my-local-repo`. It is assumed that the Dockerfile has a command that will include the files in `/icons` into the image.
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc         # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              inputResources:
                - name: gosvc_app
                - name: icon_files
              integrations:
                - name: MyArtifactory
[/YAML]
##### How it Works
When you use the **DockerBuild** native step in a pipeline, it performs the following functions in the background:
  * cp (if there is a FileSpec input, copy those files to the root of the cloned GitRepo input)
  * docker build
  * add_run_variables (add several variables that are later used when pushing the Docker image or publishing build info)
  * jfrog rt build-collect-env (collect environment information to be later published as part of build info)
  * add_run_files (save information collected for build info)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
79,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **GoBuild** native step performs a build from Go (GoLang) source.
[/INST]##### YAML Schema
The YAML schema for GoBuild native step is as follows:
 **GoBuild**
[YAML]
    pipelines: 
      - name: <string>
        steps:
          - name: <string>
            type: GoBuild
            configuration:
              #inherits all the tags from bash
              sourceLocation:   <string>  # optional
              outputLocation:   <string>  # optional
              outputFile:       <string>  # optional
              resolverRepo:     <string>  # optional
              repository:       <string>  # optional
              goCommand:        <string>  # optional
    
              integrations:
                - name:         <artifactory integration>  # may be required
              inputResources:
                - name:         <GitRepo resource>         # required
                - name:         <FileSpec resource>        # optional
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete:                                  #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `GoBuild `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Specifies an Artifactory Integration where modules will be published. If a FileSpec resource is specified in `inputResources` then this is optional. Otherwise, it is required.
May be required  
`inputResources`
Must specify a GitRepo resource that has Go source files in `sourceLocation`.
Also may specify an optional FileSpec resource that specifies what files to copy to `sourceLocation`to build.
Required
Optional  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`sourceLocation`
Location where the Go source files are available, relative to the root of the GitRepo repository. If not specified, the default is the root of the GitRepo repository.
Optional  
`outputLocation`
Location where the built Go modules should be published.
Optional  
`outputFile`
File that has the output of the Go command. The default filename is the name of the step.
Optional  
`resolverRepo`
Name of the Artifactory repository to be used to resolve dependencies.
Optional  
`repository`
Alternative to `resolverRepo`. Only one of these options may be specified.
Optional  
`goCommand`
Specifies a command line string of options to use with the Go client.
 **Default** : `build -o $outputLocation/$outputFile`
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a GoBuild step.
###### Full Pipeline Example
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For a full tutorial, see Pipeline Example: Go Build.


[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file. For more information, see the 'Pipeline Example: Go Build' quickstart.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Sample Go app in a GitRepo
      - name: go_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repoPath }}
          branches:
            include: main
          gitProvider: {{ .Values.gitProvider }}
    
      # Build info for the published Go app
      - name: go_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactory }}
    
    
    pipelines:
      - name: go_build_pipeline_example
        steps:
          # Build the Go sample app from the GitRepo
          - name: build_go
            type: GoBuild
            configuration:
              sourceLocation: .
              resolverRepo: go-virtual
              noRegistry: true
              inputResources:
                - name: go_repo
              integrations:
                - name: {{ .Values.artifactory }}
    
                  # Publish the Go sample app binary to Artifactory
          - name: publish_go_binary
            type: GoPublishBinary
            configuration:
              inputSteps:
                - name: build_go
              targetRepository: go-local
              integrations:
                - name: {{ .Values.artifactory }}
    
          # Publish the Go sample app build info
          - name: publish_build
            type: PublishBuildInfo
            configuration:
              inputSteps:
                - name: publish_go_binary
              outputResources:
                - name: go_buildinfo
[/YAML]
###### Default Locations
A GoBuild step using default locations.
 **GoBuild**
[YAML]
    pipelines: 
      - name: goBuildPipeline
        steps:
          - name: goBuildStep
            type: GoBuild
            configuration: 
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
[/YAML]
###### FileSpec Input
A GoBuild step with a FileSpec input providing files for the build that are not in the GitRepo and resolverRepo specifying an Artifactory repository to use when resolving dependencies.
 **GoBuild**
[YAML]
    pipelines: 
      - name: goBuildPipeline
        steps:
          - name: goBuildStep
            type: GoBuild
            configuration:
              resolverRepo: repo
              inputResources:
                - name: gitRepoResource
                - name: fileSpec
    
[/YAML]
###### Alternative Source Location in Git Repo
A GoBuild step with an alternative source location in the GitRepo and an alternative Go command for the build.
 **GoBuild**
[YAML]
    pipelines: 
      - name: goBuildPipeline
        steps:
          - name: goBuildStep
            type: GoBuild
            configuration: 
              sourceLocation: ""app/go""
              goCommand: ""build -insecure -o output/outputFile""
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
##### How it Works
When you use the **GoBuild** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (to configure the JFrog CLI with the Artifactory credentials in the input FileSpec if there is no input integration)
  * jfrog rt use (to set the current default Artifactory configuration)
  * cp (if there is an input FileSpec, copy the files to the root of the cloned GitRepo)
  * jfrog rt go-config (configure the repository to resolve dependencies)
  * jfrog rt go (build)
  * add_run_variables (save information about this step for future steps)
  * add_run_files (save the output and the build information in the run state for later publish steps)
  * jfrog rt build-collect-env (collect environment variables)


##### Related Topics
Go Build Quickstart

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
80,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **GoPublishBinary** native step publishes the GO (GoLang) binaries built in a GoBuild step to Artifactory.
[/INST]### Tip
It is recommended, but not required, that the GoBuild and GoPublishBinary steps be in the same affinity group to optimize sharing files between the two steps.
##### YAML Schema
The YAML schema for GoPublishBinary native step is as follows:
 **GoPublishBinary**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: GoPublishBinary
            configuration:
              #inherits all the tags from bash
    
              forceXrayScan:                <boolean>  # optional
              failOnScan:           <boolean>  # default true
              autoPublishBuildInfo: <boolean>  # optional
              targetRepository:     <string>   # required
                      failOnValidate:       <boolean>  # optional (Signed Pipelines must be enabled)
    
              integrations:
                - name:         <artifactory integration>  # required if autoPublishBuildInfo is false
              outputResources:
                - name:         <BuildInfo resource>       # required if autoPublishBuildInfo is true
              inputSteps:
                - name:         <GoBuild, Bash, or PowerShell step> # required
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete:                                  #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `GoPublishBinary `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Specifies an Artifactory Integration. Required when `autoPublishBuildInfo`is set to false. Otherwise, the artifactory integration specified in the BuildInfo resource will be used.
May be required  
`outputResources`
Must specify a BuildInfo resource when`autoPublishBuildInfo`is set to true.
If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the input GoBuild step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number `respectively.
May be required  
`inputSteps`
Must specify a GoBuild, Bash, or PowerShell step.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`forceXrayScan`
When true, forces a scan of the pushed image by JFrog Xray.
Default is false.
Optional  
`failOnScan`
When set to t`rue`, and when the Xray Policy Rule Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
`autoPublishBuildInfo`
When set to `true`, publishes build info with the Docker image.
Default is `false`.
Optional  
`targetRepository`
Repository in Artifactory where the module will be published.
Required  
`failOnValidate`
Fail the step if signatures of build artifacts cannot be verified.
Default is `false`.
Optional (Signed Pipelines must be enabled)  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a GoPublishBinary step.
###### Upload to Artifactory
Uploads the binary built by a GoBuild step to an Artifactory repository named go-local.
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For more information about running this example, see Pipeline Example: Go Build.


[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file. For more information, see the 'Pipeline Example: Go Build' quickstart.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Sample Go app in a GitRepo
      - name: go_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repoPath }}
          branches:
            include: main
          gitProvider: {{ .Values.gitProvider }}
    
      # Build info for the published Go app
      - name: go_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactory }}
    
    
    pipelines:
      - name: go_build_pipeline_example
        steps:
          # Build the Go sample app from the GitRepo
          - name: build_go
            type: GoBuild
            configuration:
              sourceLocation: .
              resolverRepo: go-virtual
              noRegistry: true
              inputResources:
                - name: go_repo
              integrations:
                - name: {{ .Values.artifactory }}
    
          # Publish the Go sample app binary to Artifactory
          - name: publish_go_binary
            type: GoPublishBinary
            configuration:
              inputSteps:
                - name: build_go
              targetRepository: go-local
              integrations:
                - name: {{ .Values.artifactory }}
[/YAML]
###### Upload Build Info
Uploads the binary built by a GoBuild step to an Artifactory repository named go-repo and uploads build info. This extends example 1 in the GoBuild documentation.
 **GoPublishBinary**
[YAML]
    pipelines: 
      - name: goBuildPipeline
        steps:
          - name: goBuildStep
            type: GoBuild
            configuration:
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: goPublishBinaryStep
            type: GoPublishBinary
            configuration:
              targetRepository: go-repo
              autoPublishBuildInfo: true
              inputSteps:
                - name: goBuildStep
              outputResources:
                - name: outputBuildInfo
    
[/YAML]
###### Publish Build Info and Trigger Xray Scan
In this example, build info is published and an Xray scan triggered.
 **GoPublishBinary**
[YAML]
    pipelines: 
      - name: goBuildPipeline
        steps:
          - name: goBuildStep
            type: GoBuild
            configuration:
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: goPublishBinaryStep
            type: GoPublishBinary
            configuration:
              targetRepository: go-repo
              autoPublishBuildInfo: true
              forceXrayScan: true
              inputSteps:
                - name: goBuildStep
              outputResources:
                - name: outputBuildInfo
    
[/YAML]
##### How it Works
When you use the **GoPublishBinary** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (if there is a BuildInfo output, configure the JFrog CLI with those credentials)
  * jfrog rt use (specify the configured credentials to use)
  * restore_run_files (copy the output and build info from the GoBuild step)
  * jfrog rt upload (upload the binary)
  * jfrog rt build-publish (if autoPublishBuildInfo is true, publish the build info)
  * write_output (if autoPublishBuildInfo is true, update the output BuildInfo resource)
  * jfrog rt build-scan (if forceXrayScan is true, trigger a scan)
  * add_run_files (update the build info saved in run state)


##### Related Topics
Go Build Quickstart

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
81,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **GoPublishModule** native step publishes GO (GoLang) modules to an Artifactory repository.
[/INST]##### YAML Schema
The YAML schema for GoPublishModule native step is as follows:
 **GoPublishModule**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: GoPublishModule
            configuration:
              #inherits all the tags from bash
    
              forceXrayScan:        <boolean>         # optional
              failOnScan:           <boolean>         # default true
              autoPublishBuildInfo: <boolean>         # optional
              
              # for payloadType module:
              sourceLocation: <string>                # optional
              version: <string>                       # required
              targetRepository: <string>              # required
              self: <boolean>                         # optional
              deps: <string>                          # optional 
              resolverRepo: <string>                  # optional
    
              integrations:
                - name:         <artifactory integration>  # required if autoPublishBuildInfo is false
              inputResources:
                - name:         <gitrepo resource>         # required
                - name:         <filespec resource>        # optional
              outputResources:
                - name:         <buildinfo resource>       # required if autoPublishBuildInfo is true
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete:                                  #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `GoPublishModule `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Specifies an Artifactory Integration. Required when `autoPublishBuildInfo` is set to `false`. Otherwise, the Artifactory integration specified in the BuildInfo resource will be used.
May be required  
`inputResources`
Must specify a GitReporesource. The publish commands will run on the Git repository at `sourceLocation`.
Also may specify an optional FileSpec resource that specifies what files to copy to `sourceLocation`.
Required
Optional  
`outputResources`
Must specify a BuildInfo resource when`autoPublishBuildInfo`is set to `true`.
If `JFROG_CLI_BUILD_NAME`or `JFROG_CLI_BUILD_NUMBER`is set as an environment variable for the pipeline or the step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName`and `buildNumber`are `$pipeline_name` and `$run_number` respectively.
May be required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`sourceLocation`
Location of the Go source files relative to the root of the input GitRepo repository. If not specified, the default is the root of the GitRepo repository.
Optional  
`version`
Version of the module to build.
Required  
`targetRepository`
Repository in the Artifactory where the module will be published.
Required  
`resolverRepo`
Name of the Artifactory repository to be used to resolve dependencies.
Optional  
`self`
When `true`, uses the `--self` option to publish the Project to Artifactory. For more information, see the CLI for JFrog Artifactory documentation. Not supported with JFrog CLI v2.
Optional  
`deps`
When specified, uses the `--deps` option to specify a list of project dependencies to Artifactory. For more information, see the CLI for JFrog Artifactory documentation. Not supported with JFrog CLI v2.
Optional  
`forceXrayScan`
When set to true, force an Xray scan after publishing to Artifactory.
Default is false.
Optional  
`failOnScan`
When set to **true** , and when the Xray Policy Rule **Fail Build Creating Xray Policies and Rules** checkbox is checked, a failed Xray scan will result in a failure of the step.
Default is true.
Optional  
`autoPublishBuildInfo`
When set to true, automatically publish the implicitly created BuildInfo.
Default is false.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a GoPublishModule step.
###### Full Pipeline Example
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Sample Go app in a GitRepo
      - name: go_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repoPath }}
          branches:
            include: main
          gitProvider: {{ .Values.gitProvider }}
    
      # Build info for the published Go app
      - name: go_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactory }}    
    
    pipelines:
      - name: go_publish_binary_example
        steps:
          # Build the Go sample app from the GitRepo
          - name: build_go
            type: GoBuild
            configuration:
              sourceLocation: .
              resolverRepo: go-virtual
              noRegistry: true
              inputResources:
                - name: go_repo
              integrations:
                - name: {{ .Values.artifactory }}
                 
          # Publish the Go sample app binary to Artifactory
          - name: cicd_go_publish_binary
            type: GoPublishBinary
            configuration:
              # forceXrayScan: true
              # failOnScan: false
              autoPublishBuildInfo: true
              inputSteps:
                - name: build_go
              targetRepository: go-local
              integrations:
                - name: myArtifactory
              outputResources:
                - name: go_buildinfo
[/YAML]
###### Using Default Locations
A GoPublishModule step using default locations and publishing version v0.0.0 to an Artifactory repository named go-repo.
 **GoPublishModule**
[YAML]
    pipelines: 
      - name: goPublishModulePipeline
        steps:
          - name: goPublishModuleStep
            type: GoPublishModule
            configuration: 
              version: ""v0.0.0""
              targetRepository: go-repo
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
###### Different Source Location in the GitRepo
A GoPublishModule step specifying a different source location in the GitRepo and publishing the project and dependencies to the Artifactory repository named go-repo as well.
 **GoPublishModule**
[YAML]
    pipelines: 
      - name: goPublishModulePipeline
        steps:
          - name: goPublishModuleStep
            type: GoPublishModule
            configuration: 
              version: ""v0.0.${run_number}""
              targetRepository: go-repo
                      resolverRepo: go-virtual
              self: true
              deps: ALL
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
###### Publish Build Info and Trigger Xray Scan
A GoPublishModule step that publishes the build info and triggers an Xray scan.
 **GoPublishModule**
[YAML]
    pipelines: 
      - name: goPublishModulePipeline
        steps:
          - name: goPublishModuleStep
            type: GoPublishModule
            configuration: 
              version: ""v0.0.${run_number}""
              targetRepository: go-repo
              autoPublishBuildInfo: true
              forceXrayScan: true
              inputResources:
                - name: gitRepoResource
              outputResources:
                - name: outputBuildInfo
              integrations:
                - name: artifactory_integration
    
[/YAML]
##### How it Works
When you use the **GoPublishModule** native step in a pipeline, it performs the following functions in the background:
  * jfrog config add (if there is an output BuildInfo resource, configure the JFrog CLI with the Artifactory credentials in that resource)
  * jfrog config use (to set the current default Artifactory configuration)
  * cp (if there is an input FileSpec, copy the files to the root of the cloned GitRepo)
  * jfrog rt go-config (configure the repository to resolve dependencies)
  * jfrog rt go-publish (publish)
  * add_run_variables (save information about this step for future steps)
  * jfrog rt build-collect-env (collect environment variables)
  * jfrog rt build-publish (if autoPublishBuildInfo is true, publish the build info)
  * write_output (if autoPublishBuildInfo is true, update the output resource)
  * jfrog rt build-scan (if forceXrayScan is true, trigger a scan)
  * add_run_files (save the build information in the run state for later publish steps)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
82,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **GradleBuild** native step performs a Gradle build on files in a Git repository. Optionally, it can also publish build information to Artifactory.
This step utilizes the JFrog CLI to run a Gradle build for your project. The CLI config file can be created during execution, or you can generate it ahead of time with the settings you require and commit it to your repository. For information about gradle-config, see CLI for JFrog Artifactory documentation.
[/INST]##### YAML Schema
The YAML schema for GradleBuild native step is as follows:
 **GradleBuild**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: GradleBuild
            configuration:
                      #inherits all the tags from bash
    
              gradleCommand:                <string>  #required
              sourceLocation:       <string>  #optional
              configFileLocation:   <string>  #optional
                      configFileName:               <string>  #optional
              resolverRepo:         <string>  #optional
                      deployerRepo:             <string>      #optional
                      usesPlugin:           <string>  #optional
                      useWrapper:               <string>      #optional
                      forceXrayScan:                <boolean> #optional
              failOnScan:           <boolean>   #optional; default true
                      autoPublishBuildInfo: <boolean> #optional
              inputResources:
                            - name: myGitRepo       
                            - name: artifactoryFileSpec     
                      outputResources:
                            - name: myBuildInfo
              integrations:
                            - name: myArtifactory 
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `GradleBuild` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify an  Artifactory Integration.
Required  
`inputResources`
Must specify a GitRepo resource. The Gradle command will perform the build on files in the Git repository at `sourceLocation`.
May also optionally specify a FileSpec resource that specifies what files to copy to `sourceLocation` to build.
Required  
`outputResources`
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set to `true`.
If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number.`
May be required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`gradleCommand`
Specifies a command line string of options to use with the Gradle build tool.
Required  
`sourceLocation`
Location of the source files to build relative to the root of the GitRepo repository. If no path is specified, the root of the repository will be used.
Optional  
`resolverRepo`
Artifactory repository to be used to resolve dependencies. May not be used with `configFileName` and `configFileLocation`.
Optional  
`deployerRepo`
Artifactory repository to which to deploy. May not be used with `configFileName` and `configFileLocation`.
Optional  
`usesPlugin`
Set to `true` if the Gradle Artifactory Plugin is already applied in the build script. Default is `false`.
Do not use with `configFileName` and`configFileLocation` (you can enable this option directly in your existing config file).
Optional  
`useWrapper`
If set to `true`, use the Gradle wrapper. Default is `false`.
Do not use with `configFileName` and `configFileLocation` (you can enable this option directly in your existing config file).
Optional  
`forceXrayScan`
When set to `true`, forces an Xray scan after publishing to Artifactory.
Default is `false`.
Optional  
`failOnScan`
When set to `true`, and when the Xray Policy Rule  Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
`autoPublishBuildInfo`
When set to `true`, publishes build info to Artifactory.
Default is `false`.
Optional  
`configFileName`
Gradle configuration file. If specified, the `configFileLocation` tag is also required. This file is generated using the JFrog CLI when you run `jfrog rt gradle-config` command.
The `serverID` in the configuration file should match the Artifactory integration name.
### Note
Use this option only if you have committed this configuration file to your source.
Optional  
`configFileLocation`
Gradle configuration file location. A `configFileName` tag is also required. Typically stored in the `.jfrog` directory in the repository.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show a few ways in which a GradleBuild step can be configured.
###### Using Default Values
The most basic form of GradleBuild. Uses all default values. This step will navigate to the root of the repo specified in the inputResources array, and run the specified Gradle command.
 **GradleBuild**
[YAML]
    pipelines: 
      - name: gradleBuildPipeline
        steps:
          - name: gradleBuildStep
            type: GradleBuild
            configuration: 
              gradleCommand: ""build""
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
###### Source Location is a Sub-directory of Git Repo
A more complicated GradleBuild step, in this one the source location is a subdirectory of the GitRepo (gradle-example-minimal) and the project uses the Gradle Artifactory Plugin. Resolver and deployer repositories have also been configured, resolving from jcenter and deploying to gradle-local.
 **GradleBuild**
[YAML]
    pipelines: 
      - name: gradleBuildPipeline
        steps:
          - name: gradleBuildStep
            type: GradleBuild
            configuration: 
              gradleCommand: ""artifactoryPublish clean -b build.gradle""
              sourceLocation: gradle-example-minimal
              deployerRepo: gradle-local
              resolverRepo: jcenter
              usesPlugin: true
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
###### Using Config File
This GradleBuild step is similar to the previous example, but has aconfig file committed to the repository as gradle-example-minimal/gradle-art-config. The config file contains the repository and plugin settings.
 **GradleBuild**
[YAML]
    pipelines: 
      - name: gradleBuildPipeline
        steps:
          - name: gradleBuildStep
            type: GradleBuild
            configuration: 
              gradleCommand: ""artifactoryPublish clean -b build.gradle""
              sourceLocation: gradle-example-minimal
              configFileLocation: .
              configFileName: gradle-art-config
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
###### Publish Build Info and Trigger Xray Scan
In this example, build info is published as part of the GradleBuild step and an Xray scan is triggered.
 **GradleBuild**
[YAML]
    pipelines: 
      - name: gradleBuildPipeline
        steps:
          - name: gradleBuildStep
            type: GradleBuild
            configuration: 
              gradleCommand: ""build""
              autoPublishBuildInfo: true
              forceXrayScan: true
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
              outputResources:
                            - name: myBuildInfo
    
[/YAML]
##### How it Works
When you use the **GradleBuild** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (configure JFrog CLI with the integration listed in the yaml)
  * jfrog rt gradle-config (if configFileName and configFileLocation are not set in yaml)
  * jfrog rt gradle $gradleCommand (the main gradle build command)
  * add_run_variables (save information in run state for future steps to reference)
  * jfrog rt build-collect-env (collect the build environment, preparing for build publish)
  * jfrog rt build-publish (publish the build, only if autoPublishBuildInfo is true)
  * write_output (update the BuildInfo output resource with the published name/number)
  * jfrog rt build-scan (if forceXrayScan is true)
  * add_run_files (adds build info to run state)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
83,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **HelmBlueGreenCleanup** step uninstalls an Idle release previously deployed by a HelmBlueGreenDeploy step.
[/INST]##### Blue/Green Deployment Strategy
Blue/Green deployment is a strategy of releasing new software that uses two production environments, as identical as possible, that take turns on serving users requests while the other one is used to perform the final stage of testing.
The **HelmBlueGreenDeploy** step refers to those two environments as **blue** and **green**. Those environments can be either playing **Idle** or **Live** roles, depending on which environment the users' requests are being forwarded to. The environment that is currently handling user requests is considered Live while the other one is considered Idle. To rollout a new release, we first deploy it to the environment playing the Idle role. There we can validate the new version to check if it is good to become available to users. If that is true, we can flip the environment roles so the previously Idle environment becomes Live and starts to handle user requests, while the previously Live environment goes Idle. One of the main advantages of this strategy is that rolling back new releases is as easy as flipping the environments roles again.
Both blue and green environments are represented in the context of Helm as two individual releases that co-exist in the same namespace. The **HelmBlueGreenDeploy** step assign Idle or Live roles to those releases by creating copies of their public Services dedicated to each role. By doing that we can easily change the releases role by updating those Services to be a copy of the Services from a specific environment. Those role Services created by Pipelines can be used to provision additional entrypoint components like DNS Records, Ingress Rules or Service Meshes.
To implement the complete Blue/Green deployment strategy workflow, Pipelines provides three native steps:
  *  **HelmBlueGreenDeploy** discovers from runtime which release is playing Idle role, deploys the chart to it and creates or updates Idle role Services.
  *  **HelmBlueGreenRoleSwitch** discovers from runtime which release is playing each role and flips them by creating or updating the role Services.
  *  **HelmBlueGreenCleanup** discovers from runtime which release is playing Idle role and uninstalls it.


##### YAML Schema
The YAML schema for HelmBlueGreenCleanup native step is as follows:
 **HelmBlueGreenCleanup**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: HelmBlueGreenCleanup
            configuration:
              #inherits all the tags from bash
              deployStep:       <string>
              flags:            <string>                               # optional
              
              integrations:
                - name:         <kubernetes integration>                         # required          
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `HelmBlueGreenCleanup` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a Kubernetes Integration.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`deployStep`
HelmBlueGreenDeploy step name that deployed the Helm Chart and where the blue/green strategy was configured. The referenced deploy step must be declared in the same Pipeline.
Required  
`flags`
String containing global flags to be included in the Helm command when uninstalling the release. For example: ""--debug""
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a HelmBlueGreenCleanup step.
###### Full Pipeline Example
This pipeline shows how to deploy a Helm chart using the blue/green strategy, promote it to the Live role and uninstall the previous deployed version (if present).
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmBlueGreenDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
         
         - name: helmRoleSwitch
           type: HelmBlueGreenRoleSwitch
           configuration: 
             deployStep: helmDeployStep
             integrations:
               - name: kubernetes_integration
             inputSteps:
               - name: helmDeployStep   
    
         - name: helmCleanup
           type: HelmBlueGreenCleanup
           configuration: 
             deployStep: helmDeployStep
             integrations:
               - name: kubernetes_integration
             inputSteps:
               - name: helmRoleSwitch
[/YAML]
##### How it Works
When you use the **HelmBlueGreenCleanup** native step in a pipeline, it performs the following functions in the background:
  * restore_pipeline_files (restores referenced HelmBlueGreenDeploy step configuration)
  * kubectl get service (fetch Live Services to read annotations and discover current Idle environment)
  * helm status (check if Idle release exists)
  * helm uninstall (if Idle release is present)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
84,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **HelmBlueGreenDeploy** step implements a Blue/Green strategy to deploy a Docker image to a Kubernetes cluster using a Helm chart.
The Helm chart used to deploy the image through this native step can be either:
  * A HelmChart resource that identifies a Helm chart stored in an Artifactory repository by HelmPublish.
  * A Helm chart in a source code repository identified by a GitRepo resource.


When properly configured, the HelmBlueGreenDeploy step also performs a replace_envs operation on all files specified under the `blueValueFilePaths` or `greenValueFilePaths` tag to replace all environment variables in those files with their values. This enables the step to derive information from a resource, such as the image name and tag from an Image, and use it to perform the deployment.
The HelmBlueGreenDeploy step uses the `helm upgrade` command to perform the deployment.
[/INST]##### Blue/Green Deployment Strategy
Blue/Green deployment is a strategy for releasing new software that uses two production environments, as identical as possible, where they take turns in serving user requests and performing final stages of testing.
The **HelmBlueGreenDeploy** step refers to those two environments as **blue** and **green**. Those environments can play either **Idle** or **Live** roles, depending on which environment the users' requests are being forwarded to. The environment that is currently handling user requests is considered Live while the other one is considered Idle. To rollout a new release, it is first deployed to the environment playing the Idle role. There we can validate the new version to check if it is good to become available to users. If that is true, the environment roles can be flipped so that the previously Idle environment becomes Live and starts handling user requests, while the previously Live environment becomes Idle. One of the main advantages of this strategy is that rolling back new releases is as easy as flipping the environments roles again.
Both blue and green environments are represented in the context of Helm as two individual releases that co-exist in the same namespace. The **HelmBlueGreenDeploy** step assign Idle or Live roles to those releases by creating copies of their public Services dedicated to each role. By doing that we can easily change the release's role by updating those Services to be a copy of the Services from a specific environment. Those role Services created by Pipelines can be used to provision additional entrypoint components like DNS Records, Ingress Rules or Service Meshes.
To implement the complete Blue/Green deployment strategy workflow, Pipelines provides three native steps:
  *  **HelmBlueGreenDeploy** : From runtime, this step discovers which release is playing Idle role, deploys the chart to it and creates or updates Idle role Services.
  *  **elmBlueGreenRoleSwitch** : From runtime, this step discovers which release is playing each role and flips them by creating or updating the role Services.
  *  **HelmBlueGreenCleanup** : From runtime, this step discovers which release is playing Idle role and uninstalls it.


##### YAML Schema
The YAML schema for HelmBlueGreenDeploy native step is as follows:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: HelmBlueGreenDeploy
            configuration:
              #inherits all the tags from bash
              helmVersion:                  <2 | 3>         # optional, defaults to 3
              namespace:                    <string>
              blueReleaseName:              <string>
              blueFlags:                    <string>        # optional
              blueValueFilePaths:                           # optional
                - <path and filename>          
              greenReleaseName:             <string>
              greenFlags:                   <string>        # optional
              greenValueFilePaths:                      # optional
                - <path and filename>          
              roleServices:
                - liveName:             <string>
                  liveCustomProperties: <string>        # optional
                  idleName:             <string>
                  idleCustomProperties: <string>        # optional
                  blueReferenceName:    <string>
                              greenReferenceName:   <string>              
              chartPath:                    <path string>   # optional, may be required for certain input resources
                      chartName:                <path string>   # optional, may be required for certain input resources
                      chartVersion:             <path string>   # optional, may be required for certain input resources
              dryRun:                   <true | false>  # optional, only deploys if the dryrun is success
              lint:                     <true | false>  # lints chart before upgrade, default false
              lintFlags: “--strict”
              test:                     <true | false>  # runs helm test after upgrade, default false
              testFlags: “--parallel”
    
              integrations:
                - name:         <kubernetes integration>                         # required
              inputResources:
                - name:         <HelmChart, GitRepo, FileSpec or BuildInfo resource>        # required, only one input resource is allowed
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `HelmBlueGreenDeploy` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a Kubernetes Integration.
and
Must specify an  Artifactory Integration when `ReleaseBundle `is the `inputResource`for the step.
Required
May be required  
`inputResources`
Must specify HelmChart, GitRepo, FileSpec, BuildInfo, or ReleaseBundle resource that contains a Helm chart.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`helmVersion`
A number representing the major version of Helm to use. Can be either 2 or 3. Defaults to 3.
Optional  
`namespace`
Target kubernetes namespace for the deployment.
Required  
`blueReleaseName`
The release name for the blue environment. Equivalent to the --name (-n) option of the helm install command.
Required  
`blueFlags`
A flag string to be included in the Helm command when deploying the blue release. For example: `--set key=’value’ -f myTestValues.yaml`
Optional  
`blueValueFilePaths`
Specifies values YAML file(s) for use with a `--values (-f)` option of the helm install command when deploying the blue release.
All environment variables referenced in the specified file(s) are automatically replaced with the values of matching runtime environment variables.
Optional  
`greenReleaseName`
The release name for the green environment. Equivalent to the `--name (-n)` option of the helm install command.
Required  
`greenFlags`
A flag string to be included in the Helm command when deploying the green release. For example: `--set key=’value’ -f myTestValues.yaml`
Optional  
`greenValueFilePaths`
Specifies values YAML file(s) for use with a `--values (-f)` option of the helm install command when deploying the green release.
All environment variable referenced in the specified file(s) are automatically replaced with values of matching runtime environment variables.
Optional  
`roleServices`
List of services that should be created by Pipelines to assign Idle and Live roles to the releases.
Tag
Description of usage
Required/Optional  
`liveName`
Name of the Service that points to the environment playing Live role.
Required  
`liveCustomProperties`
`jq` expression to be executed on the Live Service after the content is copied from the reference Service and before applying it to the kubernetes cluster. Can be used to customize the Live Service content. For instance, to assign a custom label use the following expression: `"".metadata.labels.my-label = my-value"".`
Optional  
`idleName`
Name of the Service that points to the environment playing Idle role.
Required  
`idleCustomProperties`
`jq` expression to be executed on this Idle Service after the content is copied from the reference Service and before applying it to the kubernetes cluster. Can be used to customize the Idle Service content. For instance, to assign a custom label use the following expression: `"". metadata.labels.my-label = my-value"".`
Optional  
`blueReferenceName`
Name of the Service deployed by the blue release that will be used as reference when creating or updating this role Service. When blue is playing Live role, `liveName` will be a copy of this Service. If blue is playing Idle role, `idleName` will be a copy of this Service.
Required  
`greenReferenceName`
Name of the Service deployed by the green release that will be used as reference when creating or updating this role Service. When green is playing Live role, `liveName` will be a copy of this Service. If green is playing Idle role, `idleName` will be a copy of this Service.  
Required  
`chartPath`
The path to the Helm chart in the `GitRepo/FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart resource is specified.
May be required  
`chartName`
The name of the Helm chart in the `FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart/GitRepo resource is specified or when your input resource is not an Artifactory Helm repository.
May be required  
`chartVersion`
The name of the Helm chart in the `FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart/GitRepo resource is specified or when your input resource is not an Artifactory Helm repository.
May be required  
`dryRun`
When **true** , only deploys if the --dry-run install simulation is successful.
Defaults to `false`.
Optional  
`lint`
When set to **true** , performs a lint to examine a chart for possible issues.
Defaults to `false`.
Optional  
`lintFlags`
Flag string to pass to the `helm lint` command.
Optional  
`test`
When set to **true** , performs a test to run the tests for release.
Defaults to `false`.
Optional  
`testFlags`
Flag string to pass to the `helm test` command.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a HelmBlueGreenDeploy step.
###### Helm 3 and HelmChart Resource Input
A HelmBlueGreenDeploy step using Helm 3 and a HelmChart resource input.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
    
[/YAML]
###### Dry Run
A HelmBlueGreenDeploy step using Helm 3 and a HelmChart resource input with values files and a dry run before the deployment.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              greenReleaseName: app-green
              greenValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              dryRun: true          
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
    
[/YAML]
###### GitRepo Input
A HelmBlueGreenDeploy step with a GitRepo input instead of a HelmChart, lint and test commands.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueFlags: ""--set key=value""
              greenReleaseName: app-green
              greenFlags: ""--set key=value""
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: gitRepoResource
    
[/YAML]
###### FileSpec Input
A HelmBlueGreenDeploy step with a FileSpec input resource. When using the FileSpec input resource, use the `chartPath` property to specify where the Helm chart file is in the FileSpec resource. If the FileSpec represents a Generic repository in Artifactory that contains the Helm chart, use the following configuration as a reference:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""          
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: fileSpecResource
[/YAML]
If the FileSpec input resource represents a Helm repository in Artifactory, you will also need to provide the `chartName`and`chartVersion`configuration properties:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""
                      chartName: ""myChartName""
                      chartVersion: 0.0.1          
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: fileSpecResource
[/YAML]
###### BuildInfo Input
A HelmBlueGreenDeploy step with a BuildInfo input resource. When using the BuildInfo input resource, use the `chartPath` property to specify where the Helm chart file is in the BuildInfo resource. If the BuildInfo represents a Generic repository in Artifactory that contains the Helm chart, use the following configuration as a reference:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              greenReleaseName: app-green
              greenValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""          
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""          
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: buildInfoResource
[/YAML]
If the BuildInfo input resource represents a Helm repository in Artifactory, you will also need to provide the `chartName` and `chartVersion`configuration properties:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              greenReleaseName: app-green
              greenValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""
                      chartName: ""myChartName""
                      chartVersion: 0.0.1
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""          
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: buildInfoResource
[/YAML]
###### ReleaseBundle Input
A HelmBlueGreenDeploy step with a ReleaseBundle input resource. When using the ReleaseBundle input resource you will need to specify where the Helm chart file is in the ReleaseBundle resource using the `chartPath` property.
If the ReleaseBundle represents a Generic repository in Artifactory that contains the Helm chart, please use the following configuration as a reference:
### Artifactory integration is required
You must specify an Artifactory Integration when `ReleaseBundle `is the `inputResource` to `HelmBlueGreenDeploy` step. This integration will point to the Artifactory that will be the source to download the ReleaseBundle that will be used in this step.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines:
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration:
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              greenReleaseName: app-green
              greenValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""          
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              integrations:
                - name: kubernetes_integration
                - name: artifactory_integration
              inputResources:
                - name: releaseBundleResource
[/YAML]
If the ReleaseBundle input resource represents a Helm repository in Artifactory, you will also have to specify `chartName` and `chartVersion` configuration properties:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines:
      - name: helmDeployPipeline   
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration:
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              greenReleaseName: app-green
              greenValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""
              chartName: ""myChartName""
              chartVersion: 0.0.1          
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              integrations:
                - name: kubernetes_integration
                            - name: artifactory_integration 
              inputResources:
                - name: releaseBundleResource
[/YAML]
###### Customizing Role Services
A HelmBlueGreenDeploy step with two custom labels assigned to a role Service.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  idleCustomProperties: "".metadata.labels.mylabel = \""value\"" | .metadata.labels.role = \""idle\""""
                  liveName: app-service
                  liveCustomProperties: "".metadata.labels.mylabel = \""value\"" | .metadata.labels.role = \""live\""""
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
[/YAML]
###### Multiple Public Services
A HelmBlueGreenDeploy step that deploys a Helm Chart that exposes two Services.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service              
                  liveName: app-service
                - blueReferenceName: app-blue-admin-service
                  greenReferenceName: app-green-admin-service 
                  idleName: app-idle-admin-service              
                  liveName: app-admin-service
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
[/YAML]
###### Helm 2
A HelmBlueGreenDeploy step using Helm 2.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 2
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service          
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
[/YAML]
##### How it Works
When you use the **HelmBlueGreenDeploy** native step in a pipeline, it performs the following functions in the background:
  * helm init --client-only (if the Helm version is 2)
  * jfrog rt config (if there is a HelmChart resource input, to configure the JFrog CLI with the Artifactory credentials)
  * jfrog rt use (if there is a HelmChart resource input, to set the current default Artifactory configuration)
  * kubectl get service (fetch Live Services to read annotations and discover current Idle environment)
  * helm fetch (if there is a HelmChart resource input and the Helm version is 2, to fetch the chart)
  * helm pull (if there is a HelmChart resource input and the Helm version is 3, to fetch the chart)
  * replace_envs (if there are valueFilePaths, to replace variable placeholders)
  * helm lint (if lint is true)
  * helm --dry-run (if dryRun is true)
  * helm (deploy)
  * kubectl get service (fetch reference Services deployed by helm to create or update role Services)
  * Cleanup and update role Services definition (uses `jq` on Bash nodes and `jq` and `ConvertFrom-Json` on PowerShell nodes)
  * kubectl apply (create or update role Services)
  * add_pipeline_files (saves HelmBlueGreenDeploy configuration to be used by HelmBlueGreenRoleSwitch and HelmBlueGreenCleanup steps later)
  * helm test (if test is true)


##### Pipelines Managed Services
The role Services managed by Pipelines can be identified by the following labels and annotations metadata:
Name
Type
Value  
app.kubernetes.io/managed-by
Label
""Pipelines""  
pipelines.jfrog.com/pipeline
Annotation
Pipeline name  
pipelines.jfrog.com/deploy-step
Annotation
Step name  
pipelines.jfrog.com/run
Annotation
Number of the last run to update the Service  
pipelines.jfrog.com/reference
Annotation
Reference Service name  
pipelines.jfrog.com/environment
Annotation
Environment currently playing that role  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
85,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **HelmBlueGreenRoleSwitch** step flips the roles played by the Helm releases deployed by a HelmBlueGreenDeploy step.
This role switch can be used to either make a new release available to users or to roll back a previously published release.
[/INST]##### Blue/Green Deployment Strategy
Blue/Green deployment is a strategy of releasing new software that uses two production environments, as identical as possible, that take turns on serving users requests while the other one is used to perform the final stage of testing.
The **HelmBlueGreenDeploy** step refers to those two environments as **blue** and **green**. Those environments can be either playing **Idle** or **Live** roles, depending on which environment the users' requests are being forwarded to. The environment that is currently handling user requests is considered Live while the other one is considered Idle. To rollout a new release, we first deploy it to the environment playing the Idle role. There we can validate the new version to check if it is good to become available to users. If that is true, we can flip the environment roles so the previously Idle environment becomes Live and starts to handle user requests, while the previously Live environment goes Idle. One of the main advantages of this strategy is that rolling back new releases is as easy as flipping the environments roles again.
Both blue and green environments are represented in the context of Helm as two individual releases that co-exist in the same namespace. The **HelmBlueGreenDeploy** step assign Idle or Live roles to those releases by creating copies of their public Services dedicated to each role. By doing that we can easily change the releases role by updating those Services to be a copy of the Services from a specific environment. Those role Services created by Pipelines can be used to provision additional entrypoint components like DNS Records, Ingress Rules or Service Meshes.
To implement the complete Blue/Green deployment strategy workflow, Pipelines provides three native steps:
  *  **HelmBlueGreenDeploy** discovers from runtime which release is playing Idle role, deploys the chart to it and creates or updates Idle role Services.
  *  **HelmBlueGreenRoleSwitch** discovers from runtime which release is playing each role and flips them by creating or updating the role Services.
  *  **HelmBlueGreenCleanup** discovers from runtime which release is playing Idle role and uninstalls it.


##### YAML Schema
The YAML schema for HelmBlueGreenRoleSwitch native step is as follows:
 **HelmBlueGreenRoleSwitch**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: HelmBlueGreenRoleSwitch
            configuration:
              #inherits all the tags from bash
              deployStep:       <string>
              
              integrations:
                - name:         <kubernetes integration>                         # required          
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `HelmBlueGreenRoleSwitch` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a Kubernetes Integration.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`deployStep`
HelmBlueGreenDeploy step name that deployed the Helm Chart and where the blue/green strategy was configured. The referenced deploy step must be declared in the same Pipeline.
Required  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a HelmBlueGreenRoleSwitch step.
 **Example 1**
This pipeline shows how to deploy a Helm chart using the blue/green strategy and promote it to the Live role.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmBlueGreenDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
         
         - name: helmRoleSwitch
           type: HelmBlueGreenRoleSwitch
           configuration: 
             deployStep: helmDeployStep
             integrations:
               - name: kubernetes_integration
             inputSteps:
               - name: helmDeployStep   
[/YAML]
##### How it Works
When you use the **HelmBlueGreenRoleSwitch** native step in a pipeline, it performs the following functions in the background:
  * restore_pipeline_files (restores referenced HelmBlueGreenDeploy step configuration)
  * kubectl get service (fetch Live Services to read annotations and discover current role played by the environments)
  * kubectl get service (fetch reference Services deployed by helm to create or update role Services)
  * Cleanup and update role Services definition (uses `jq` on Bash nodes and `jq` and `ConvertFrom-Json` on PowerShell nodes)
  * kubectl apply (create or update role Services)


##### Pipelines Managed Services
The role Services managed by Pipelines can be identified by the following labels and annotations metadata:
Name
Type
Value  
app.kubernetes.io/managed-by
Label
""Pipelines""  
pipelines.jfrog.com/pipeline
Annotation
Pipeline name  
pipelines.jfrog.com/deploy-step
Annotation
Step name  
pipelines.jfrog.com/run
Annotation
Number of the last run to update the Service  
pipelines.jfrog.com/reference
Annotation
Reference Service name  
pipelines.jfrog.com/environment
Annotation
Environment currently playing that role  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
86,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **HelmDeploy** step deploys a Docker image to a Kubernetes cluster using a Helm chart.
The Helm chart used to deploy the image through this native step can be either:
  * A HelmChart resource that identifies a Helm chart stored in an Artifactory repository by HelmPublish.
  * A Helm chart in a source code repository identified by a GitRepo resource.


When properly configured, the HelmDeploy step also performs a replace_envs operation on all files specified under the `valueFilePaths` tag to replace all environment variables in those files with their values. This enables the step to derive information from a resource, such as the image name and tag from an Image, and use it to perform the deployment.
The HelmDeploy step uses the `helm upgrade` command to perform the deployment.
[/INST]##### YAML Schema
The YAML schema for HelmDeploy native step is as follows:
 **HelmDeploy**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: HelmDeploy
            configuration:
              #inherits all the tags from bash
              helmVersion:         <2 | 3>          # optional, defaults to 2
              namespace:               <string>     # optional, namespace to which to deploy
              flags:               <string>         # optional
              valueFilePaths:                       # optional
                - <path and filename>
              releaseName:         <string> 
              chartPath:           <path string>    # optional, may be required for certain input resources
                      chartName:           <path string>    # optional, may be required for certain input resources
                      chartVersion:        <path string>    # optional, may be required for certain input resources
              dryRun:              <true | false>   # optional, only deploys if the dryrun is success
              lint:                <true | false>   # lints chart before upgrade, default false
              lintFlags: “--strict”
              test:                <true | false>   # runs helm test after upgrade, default false
              testFlags: “--parallel”
    
              integrations:
                - name:         <kubernetes integration>                         # required            
              inputResources:
                - name:         <HelmChart, GitRepo, FileSpec, BuildInfo or ReleaseBundle resource>    # required, one input resource from this list is required
                                                                                                                                                                                               # HelmChart and GitRepo input resources are allowed together       
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `HelmDeploy `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a Kubernetes Integration.
or
Must specify an  Artifactory Integration when `ReleaseBundle `is the `inputResource` to the step.
Required
May be required  
`inputResources`
Must specify HelmChart, GitRepo, FileSpec , BuildInfo or ReleaseBundle resource that contains a Helm chart.
Can also specify a second input resource of type GitRepo containing  Helm values files, if HelmChart was provided as the other input resource.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`helmVersion`
A number representing the major version of Helm to use. Can be 2 or 3. Defaults to 2.
Optional  
`namespace`
The namespace to which to deploy. This will be added to the deploy command as a `--namespace` parameter.
Optional  
`flags`
A flag string to be included in the Helm command. For example: ""--set key=’value’ -f myTestValues.yaml""
Optional  
`valueFilePaths`
Specifies values YAML file(s) for use with a--values (-f) option of the helm install command.
All environment variable referenced in the specified file(s) are automatically replaced with values of matching runtime environment variables.
Optional  
`releaseName`
The release name. Equivalent to the --name (-n) option of the helm install command.
Required  
`chartPath`
The path to the Helm chart in the `GitRepo/FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart resource is specified.
May be required  
`chartName`
The name of the Helm chart in the `FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart/GitRepo resource is specified or when your input resource is not an Artifactory Helm repository.
May be required  
`chartVersion`
The name of the Helm chart in the `FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart/GitRepo resource is specified or when your input resource is not an Artifactory Helm repository.
May be required  
`dryRun`
When **true** , only deploys if the --dry-run install simulation is successful.
Defaults to `false`.
Optional  
`lint`
When set to **true** , performs a lint to examine a chart for possible issues.
Defaults to `false`.
Optional  
`lintFlags`
Flag string to pass to the `helm lint` command.
Optional  
`test`
When set to **true** , performs a test to run the tests for release.
Defaults to `false`.
Optional  
`testFlags`
Flag string to pass to the `helm test` command.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a HelmDeploy step.
###### Helm 3 and HelmChart Resource Input
A HelmDeploy step using Helm 3 and a HelmChart resource input.
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              releaseName: myHelmRelease
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
    
[/YAML]
###### Dry Run
A HelmDeploy step using Helm 3 and a HelmChart resource input with values files and a dry run before the deployment.
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              releaseName: myHelmRelease
              dryRun: true
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
    
[/YAML]
###### GitRepo Input
A HelmDeploy step with a GitRepo input instead of a HelmChart and lint and test commands.
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: gitRepoResource
    
[/YAML]
###### FileSpec Input
A HelmDeploy step with a FileSpec input resource. When using the FileSpec input resource, use the `chartPath` property to specify where the Helm chart file is in the FileSpec resource. If the FileSpec represents a Generic repository in Artifactory that contains the Helm chart, use the following configuration as a reference:
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: fileSpecResource
[/YAML]
If the FileSpec input resource represents a Helm repository in Artifactory, you will also need toprovide the `chartName`and`chartVersion` configuration properties:
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
                      chartName: ""myChartName""
                      chartVersion: 0.0.1
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: fileSpecResource
[/YAML]
###### BuildInfo Input
A HelmDeploy step with a BuildInfo input resource. When using the BuildInfo input resource, use the `chartPath` property to specify where the Helm chart file is in the BuildInfo resource. If the BuildInfo represents a Generic repository in Artifactory that contains the Helm chart, use the following configuration as a reference:
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: buildInfoResource
[/YAML]
If the BuildInfo input resource represents a Helm repository in Artifactory, you will also need toprovide the `chartName` and `chartVersion` configuration properties:
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
                      chartName: ""myChartName""
                      chartVersion: 0.0.1
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: buildInfoResource
[/YAML]
###### ReleaseBundle Input
A HelmDeploy step with a ReleaseBundle input resource. When using the ReleaseBundle input resource you will need to specify where the Helm chart file is in the ReleaseBundle resource using the _chartPath_ property.
If the ReleaseBundle represents a Generic repository in Artifactory that contains the Helm chart, please use the following configuration as a reference:
### Artifactory integration is required
You must specify an Artifactory Integration when `ReleaseBundle `is the `inputResource` to `HelmDeploy` step. This integration will point to the Artifacoty that will be the source to download the ReleaseBundle that will be used in this step.
 **HelmDeploy**
[YAML]
    pipelines:
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration:
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
                - name: artifactory_integration
              inputResources:
                - name: releaseBundleResource
[/YAML]
If the ReleaseBundle input resource represents a Helm repository in Artifactory, you will also have to specify _chartName_ and _chartVersion_ configuration properties:
 **HelmDeploy**
[YAML]
    pipelines:
      - name: helmDeployPipeline   
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration:
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
              chartName: ""myChartName""
              chartVersion: 0.0.1
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
                            - name: artifactory_integration 
              inputResources:
                - name: releaseBundleResource
[/YAML]
###### Helm 2
A HelmDeploy step using Helm 2.
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 2
              namespace: my-namespace
              releaseName: myHelmRelease
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
[/YAML]
##### How it Works
When you use the **HelmDeploy** native step in a pipeline, it performs the following functions in the background:
  * helm init --client-only (if the Helm version is 2)
  * jfrog rt config (if there is a HelmChart resource input, to configure the JFrog CLI with the Artifactory credentials)
  * jfrog rt use (if there is a HelmChart resource input, to set the current default Artifactory configuration)
  * helm fetch (if there is a HelmChart resource input and the Helm version is 2, to fetch the chart)
  * helm pull (if there is a HelmChart resource input and the Helm version is 3, to fetch the chart)
  * replace_envs (if there are valueFilePaths, to replace variable placeholders)
  * helm lint (if lint is true)
  * helm --dry-run (if dryRun is true)
  * helm (deploy)
  * helm test (if test is true)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
87,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **HelmPublish** step publishes a Helm chart and associated build info from a location in a Git repo to a Helm repository in Artifactory.
This step uses the `helm package` command to publish the collection of Helm files in the GitRepo to Artifactory.
The step can optionally publish build info with the Helm chart to Artifactory using the `autoPublishBuildInfo` flag.
Once the Helm chart is published to Artifactory, it can be deployed to a Kubernetes cluster using HelmDeploy.
[/INST]##### YAML Schema
The YAML schema for HelmPublish native step is as follows:
 **HelmPublish**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: HelmPublish
            configuration:
              #inherits all the tags from bash
              helmVersion:       <2 | 3>                               # optional, defaults to 2
              chartPath:         <path string>
              flags:             <option string>                       # optional
              lint:              <true | false>                        # optional, default false
              lintFlags:         <string>                              # optional
              valueFilePaths:                                          # optional
                - <path and filename>
              autoPublishBuildInfo: <true | false>                     # default false          
              inputResources:
                - name:         <GitRepo resource>                     # required
              outputResources:
                - name:         <HelmChart resource>                   # required
                - name:         <BuildInfo resource>                   # optional
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `HelmPublish `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify a GitRepo resource that contains the Helm chart.
The step will package the chart into a versioned chart archive file.
Required  
`outputResources`
Must specify a HelmChart resource to update. The `version` property of the HelmChart is automatically set to the new chart version.
Must specify a BuildInfo resource when `autoPublishBuildInfo` is set to true. If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number.`
Required
May be required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`helmVersion`
A number representing the major version of Helm to use. Can be 2 or 3. Defaults to 2.
Optional  
`chartPath`
The path to the Helm chart in the GitRepo
Required  
`flags`
Command line options to pass to the helm package command.
Optional  
`lint`
When true, performs a lint test to verify that the chart is well formed.
Defaults to false.
Optional  
`lintFlags`
Options string to pass to the `helm lint`command.
Optional  
`valueFilePaths`
Specifies values YAML file(s) that will be used in the lint command  helm lint command.
All environment variable references in the specified file(s) will be automatically replaced with values of matching runtime environment variables.
Optional  
`autoPublishBuildInfo`
When true, publishes build info with the Helm chart.
Default is false.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a HelmPublish step.
###### Publish to Artifactory using Helm 3
A HelmPublish step using Helm 3 to publish to Artifactory.
 **HelmPublish**
[YAML]
    pipelines: 
      - name: helmPublishPipeline
        steps:
          - name: helmPublishStep
            type: HelmPublish
            configuration: 
              helmVersion: 3
              chartPath: ""./myChart""
              inputResources:
                - name: gitRepoResource
              outputResources:
                - name: helmChartResource
    
[/YAML]
###### Using lint and values.yaml
A HelmPublish step with lint, using a values.yaml file, added before publishing the Helm chart.
 **HelmPublish**
[YAML]
    pipelines: 
      - name: helmPublishPipeline
        steps:
          - name: helmPublishStep
            type: HelmPublish
            configuration: 
              helmVersion: 3
              chartPath: ""./myChart""
              lint: true
              lintFlags: ""--strict""
              valueFilePaths:
                - values.yaml
              inputResources:
                - name: gitRepoResource
              outputResources:
                - name: helmChartResource
    
[/YAML]
###### Publish Build Info
A HelmPublish step that publishes build info.
 **HelmPublish**
[YAML]
    pipelines: 
      - name: helmPublishPipeline
        steps:
          - name: helmPublishStep
            type: HelmPublish
            configuration: 
              helmVersion: 3
              chartPath: ""./myChart""
              autoPublishBuildInfo: true
              inputResources:
                - name: gitRepoResource
              outputResources:
                - name: helmChartResource
                - name: buildInfoResource
    
[/YAML]
###### Publish to Artifactory using Helm 2
A HelmPublish step using Helm 2 to publish to Artifactory.
 **HelmPublish**
[YAML]
    pipelines: 
      - name: helmPublishPipeline
        steps:
          - name: helmPublishStep
            type: HelmPublish
            configuration: 
              helmVersion: 2
              chartPath: ""./myChart""
              inputResources:
                - name: gitRepoResource
              outputResources:
                - name: helmChartResource
    
[/YAML]
##### How it Works
When you use the **HelmPublish** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (to configure the JFrog CLI with the Artifactory credentials)
  * jfrog rt use (to set the current default Artifactory configuration)
  * helm init --client-only (if the Helm version is 2)
  * replace_envs (if lint is true and there are valueFilePaths, to replace variable placeholders)
  * helm lint (if lint is true)
  * helm package
  * jfrog rt upload (upload the Helm chart)
  * write_output (to update the version in the output HelmChart resource)
  * jfrog rt build-collect-env (if autoPublishBuildInfo is true, collect environment variables)
  * jfrog rt build-publish (if autoPublishBuildInfo is true, publish the build info)
  * write_output (if autoPublishBuildInfo is true, to update the version in the output BuildInfo resource)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
88,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **Jenkins** native step transfers execution to a Jenkins pipeline. For more information, see Using Jenkins With Pipelines.
[/INST]##### YAML Schema
The YAML schema for Jenkins native step is as follows:
 **Jenkins**
[YAML]
    pipelines:
      - name: <string>
        steps:
          - name: <string>
            type: Jenkins
            configuration:
                      #inherits all the tags from bash
              jenkinsJobName: <string>            # Jenkins job/pipeline name
              jenkinsBranchName: <string>         # required for Jenkins Multibranch
              buildParameters:
                key1: <string>
                key2: <string>   
              integrations:
                - name: <jenkins integration>    # required Jenkins integration
              outputResources:                   # optional
                - name: <resource name>          
    
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `Jenkins` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash step configuration tags, with these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a named Jenkins Integration.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`jenkinsJobName`
The name of the Jenkins job/pipeline
Required  
### Important
When adding the `jenkinsJobName`, do not include the `job` keyword. The only exception is when `job` is the name of a folder.
  *  **Correct** : `jenkinsJobName: infra/test/frontend-trigger-env`
  *  **Incorrect** : `jenkinsJobName: infra/job/test/job/frontend-trigger-env`

  
`jenkinsBranchName`
Required field when the Jenkins Job is configured for multibranch. This tells pipelines which branch to execute.
Optional  
`buildParameters`
Assigns any build parameters and their strings in key:value format. These parameters will be passed to the Jenkins job.
Optional  
###### execution
Execution takes place within the Jenkins job only. This section is not supported for this type of step.
##### Examples
The following examples show how to configure a Jenkins step for a few different Jenkins jobs.
###### Freestyle Jenkins Project or Pipeline
A Jenkins step for a freestyle Jenkins project or Pipeline. Note that this example will not work with Jenkins Multibranch Pipelines, see the next example for those.
This example uses `timeoutSeconds` Bash tag. For more information about other Bash tags that can be used, see Bash.
 **Jenkins**
[YAML]
    pipelines: 
      - name: jenkinsPipeline
        steps:
          - name: jenkinsStep
            type: Jenkins
            configuration: 
                      timeoutSeconds: 30 # uses Bash tag to set time limit, in seconds, for the step to complete
              jenkinsJobName: myJenkinsJob
              integrations:
                - name: jenkins_integration
    
[/YAML]
###### Jenkins Multibranch Pipeline
A Jenkins step for a Jenkins Multibranch Pipeline.
 **Jenkins**
[YAML]
    pipelines: 
      - name: jenkinsPipeline
        steps:
          - name: jenkinsStep
            type: Jenkins
            configuration: 
              jenkinsJobName: myJenkinsMultibranchJob
              jenkinsBranchName: myBranchName
              integrations:
                - name: jenkins_integration
    
[/YAML]
###### Trigger Jenkins Jobs using an Input Image Resource
A Jenkins step providing an image name and tag and run number as build parameters to trigger Jenkins jobs using an input Image resource.
 **Jenkins**
[YAML]
    pipelines: 
      - name: jenkinsPipeline
        steps:
          - name: jenkinsStep
            type: Jenkins
            configuration: 
              jenkinsJobName: myJenkinsMultibranchJob
              jenkinsBranchName: myBranchName
              buildParameters:
                imageName: ""${res_myImageResource_imageName}""
                imageTag: ""${res_myImageResource_imageTag}""
                jfrogPipelineRun: ""${run_number}""
              inputResources:
                - name: myImageResource
              integrations:
                - name: jenkins_integration
    
[/YAML]
###### Two Jenkins Jobs using Priority Tag
Two Jenkins jobs that use the `Priority` Bash tag. For more information about other Bash tags that can be used, see Bash.
[YAML]
    pipelines:
      - name: pipelines_Jen
        steps:
          - name: S_B_Jen_0022_1
            type: Jenkins
            configuration:
              jenkinsJobName: var_output_job    
              priority: 1 # bash tag
              buildParameters:
                key1: 1
                key2: 2
              integrations:
                - name: i_testJenkins
    
          - name: S_B_Jen_0022_2
            type: Jenkins
            configuration:
              jenkinsJobName: var_output_job   
              priority: 2 @ bash tag
              buildParameters:
                key1: 1
                key2: 2
              integrations:
                - name: i_testJenkins
              inputSteps:
                - name : S_B_Jen_0022_1
[/YAML]
##### Related Topics
Using Jenkins With Pipelines

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
89,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **LinuxVMDeploy** native step uploads files to VMs in a VmCluster resource and runs commands on the VMs. If all deployments are successful, a copy of the deploy artifacts are stored in the `${HOME}/${step_name}/rollback` on each VM. The file(s) may be provided in a FileSpec, BuildInfo or ReleaseBundle.
[/INST]##### YAML Schema
The YAML schema for LinuxVMDeploy native step is as follows:
 **LinuxVMDeploy**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: myLinuxVMDeployStep
            type: LinuxVMDeploy
            configuration:
              environmentVariables:
                dry_run:                 <string> # optional
                            deploy_targets_override: <string> # optional
              targetDirectory:           <string> # optional
              rolloutDelaySecs:          <integer> # optional
              fastFail:                  <boolean> # optional
              sshUser:                   <string> # required
              vmEnvironmentVariables:    <string[]> # optional
              strategy:                  <""rollingUpdate""|""blueGreen""> # optional; default ""rollingUpdate""
              inputResources: 
                - name:                  <VMCluster resource> # 1 required (exacly 1, exactly 2 for blueGreen strategy).
                - name:                  <BuildInfo resource|FileSpec resource|ReleaseBundle resource> # required (exactly 1).
              scripts:
                - name:                  <string> # required
                  context:               <""buildNode""|""targetCluster""|""currentCluster""> # required, ""currentCluster"" supported for blueGreen only.
                  commands:              <string[]> # optional
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `LinuxVMDeploy` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a VmCluster resource.
Required  
`inputResources`
Must specify a FileSpec, BuildInfo or ReleaseBundle resource containing the file(s) to be uploaded. Exactly one must be specified.
Required  
In addition, these tags can be defined to support the step's native operation:
Tag
 **Description of usage**
Required/Optional  
`sshUser`
The ssh user used to connect to target VMs.
Required  
`targetDirectory`
Path of the directory where the deploy artifacts will be uploaded. The directory will be created if it does not exist.
Default directory is `<sshUser's $HOME>/$step_name/$run_id`.
Optional  
`vmEnvironmentVariables`
These variables will be exported on target VMs.
Optional  
`rolloutDelaySecs`
Time in seconds to wait between deploys.
Optional  
`fastFail`
If `true`, the step will not deploy to additional VMs after the first failure.
Default is `true`.
Optional  
strategy
The release strategy to be used. It can be either `rollingUpdate` or `blueGreen`.
  * `rollingUpdate`: Deploy to each VM in one cluster, one after the other.
  * `blueGreen`: Deploy to one of two VmCluster resources, swapping clusters each time the LinuxVMDeploy step runs.


Optional  
`scripts`
User-defined commands to run on a given context.
  * `name:` name for the command.
  * `context:` where the script should run. `buildNode` runs on the build node. `targetCluster` runs on the supplied vmCluster.


``
Required  
###### environment variables
The following environment variables are available for user-defined scripts written in the LinuxVMDeploy step.
Name
 **Description of usage**  
`target_cluster `
The name of the VM cluster being deployed to.  
`current_cluster`
The name of the VM cluster that was last deployed to.
### Note
Supported for `blueGreen` strategy only.  
`current_command`
The name of the command that was running when the step exited. This should be blank if no user-defined command was running.  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show a few ways in which a LinuxVMDeploy step can be configured.
###### Uploading an App in a FileSpec Resource to VMs and Running It
The most basic form of LinuxVMDeploy. Uses all default values. The step will upload files to the default `targetDirectory` and run a deploy command.
 **LinuxVMDeploy**
[YAML]
    resources:
      - name: myApplication
        type: FileSpec
        configuration:
          sourceArtifactory: myArtifactory
          pattern: ""example-repo-local/myApp""
      - name: myVM
        type: VmCluster
        configuration:
          sshKey: mySSHKey
          targets:
            - 0.0.0.0
            - 1.1.1.1
    pipelines: 
      - name: linuxVMDeployPipeline
        steps:
          - name: linuxVMDeploy
            type: LinuxVMDeploy
            configuration:
              inputResources:
                - name: myVM
                - name: myApplication
              scripts:
                - name: deployCommand
                  context: targetCluster 
                  commands:
                    - ""./myApp""
    
[/YAML]
###### Uploading an App in a BuildInfo Resource to VMs, Running postDeploy command, and Handling Rollback.
Upload a BuildInfo resource and try to run `deployCommand` and `postDeploy` commands. If `deployCommand` succeeds, then postDeploy will run. If there is a failure on a VM (in case a command or `postDeploy` did not succeed), then `rollbackCommand` will run on any VM that deployed successfully.
 **LinuxVMDeploy**
[YAML]
    resources:
      - name: myApplication
        type: BuildInfo
        configuration:
          sourceArtifactory: myArtifactory
          buildName: myBuild
          buildNumber: 1
      - name: myVM
        type: VmCluster
        configuration:
          sshKey: mySSHKey
          targets:
            - 0.0.0.0
            - 1.1.1.1
    pipelines: 
      - name: linuxVMDeployPipeline
        steps:
          - name: linuxVMDeploy
            type: LinuxVMDeploy
            configuration:
              inputResources:
                - name: myVM
                - name: myApplication
              sshUser: myUser
                      scripts:
                - name: deployCommand
                  context: targetCluster 
                  commands:
                    - ""./myApp""
              onFailure:
                - 'if [ ""$current_command"" == ""deployCommand"" ]; then ./doRollback.sh fi'
    
[/YAML]
###### Overriding VmCluster Targets
In this example, a VmCluster resource is supplied alongside `deploy_targets_override` environment variable. Artifacts will be deployed to the targets specified in `deploy_targets_override`. The ssh keys from the VmCluster resource will still be used.
 **LinuxVMDeploy**
[YAML]
    resources:
      - name: myApplication
        type: FileSpec
        configuration:
          sourceArtifactory: myArtifactory
          pattern: ""example-repo-local/myApp""
      - name: myVM
        type: VmCluster
        configuration:
          sshKey: s_VM_DEPLOY_SSHKEY
          targets:
            - 0.0.0.0
            - 1.1.1.1
    pipelines:
      - name: linuxVMDeployPipeline
        steps:
          - name: linuxVMDeploy
            type: LinuxVMDeploy
            configuration:
              environmentVariables:
                            deploy_targets_override: ""123.456.78.90,127.0.0.1""
              inputResources:
                - name: myVM
                - name: myApplication
              sshUser: myUser
              scripts:
                - name: deployCommand
                  context: targetCluster 
                  commands:
                    - ""./myApp""
    
[/YAML]
###### Passing Environment Variables to Commands Run on the VM
In this example, a VmCluster resource is supplied with `vmEnvironmentVariables`. These variables will be available to user-defined commands running on the `targetCluster`.
 **LinuxVMDeploy**
[YAML]
    resources:
      - name: myApplication
        type: FileSpec
        configuration:
          sourceArtifactory: myArtifactory
          pattern: ""example-repo-local/myApp""
      - name: myVM
        type: VmCluster
        configuration:
          sshKey: s_VM_DEPLOY_SSHKEY
          targets:
            - 0.0.0.0
            - 1.1.1.1
    pipelines: 
      - name: linuxVMDeployPipeline
        steps:
          - name: linuxVMDeploy
            type: LinuxVMDeploy
            configuration:
              vmEnvironmentVariables:
                - ""LOG_LEVEL=DEBUG""
              inputResources:
                - name: myVM
                - name: myApplication
              sshUser: myUser
              scripts:
                - name: deployCommand
                  context: targetCluster 
                  commands:
                    - ""./myApp $LOG_LEVEL""
    
[/YAML]
###### Setting up a blueGreen Deploy Strategy
In this example, two VmCluster resources are supplied and a strategy of `blueGreen` is specified. Each time the step runs, `targetCluster` and `currentCluster` swap between the two supplied VmCluster resources.
 **LinuxVMDeploy**
[YAML]
    resources:
      - name: myApplication
        type: FileSpec
        configuration:
          sourceArtifactory: myArtifactory
          pattern: ""example-repo-local/myApp""
      - name: myVM
        type: VmCluster
        configuration:
          sshKey: s_VM_DEPLOY_SSHKEY
          targets:
            - 0.0.0.0
            - 1.1.1.1
    pipelines: 
      - name: linuxVMDeployPipeline
        steps:
          - name: linuxVMDeploy
            type: LinuxVMDeploy
            configuration:
              vmEnvironmentVariables:
                - ""LOG_LEVEL=DEBUG""
              inputResources:
                - name: myBlueCluster
                - name: myGreenCluster
                - name: myApplication
              sshUser: myUser
              strategy: blueGreen
              scripts:
                - name: deployCommand
                  context: targetCluster 
                  commands:
                    - ""./myApp $LOG_LEVEL""
                            - name: cleanupCommand
                  context: currentCluster 
                  commands:
                    - ""./stopApp""
                - name: swapCluster
                  context: buildNode
                  commands:
                    - ""./swapLoadBalancer $target_cluster""
    
    
[/YAML]
##### How it Works
When you use the **LinuxVMDeploy** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt download (download files from FileSpec, BuildInfo or ReleaseBundle)
  * tar (compress all files to be uploaded to VMs)
  * scp (upload compressed artifacts to VMs)
  * ssh""mkdir -p"" (create the targetDirectory if it does not exist)
  * Script commands run on target VMs via ssh or on the build node itself if context is ""buildNode"".
    * Commands run in the order they were written in the yaml.
    * Commands running on the target cluster will have access to variables defined in vmEnvironmentVariables.
    * A rollback archive is created on the targetCluster (only happens when entire step succeeds)
      * Rollback directory location: /home/$ssh_user/LinuxVMDeploy/rollback
      * mkdir -p (create rollback directory if it does not exist).
      * rm -rf $rollbackDirectory/* (remove any old rollback artifacts)
      * cp -r $targetDirectory/* $rollbackDirectory (copy all uploaded artifacts to rollback archive)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
90,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **Matrix** step commences multiple parallel build processes across multiple containers, with different settings for each.
Common use cases for Matrix steps are:
  * Splitting a large test suite into smaller units, and executing them in parallel to reduce total execution time.
  * Testing against multiple values of environment variables or multiple runtime images.
  * Testing against multiple base operating system versions.


The matrix configuration can specify:
  * Multiple sets of environment variable definitions
  * Multiple runtime images
  * Multiple node pools
  * Multiple operating systems


The Matrix step executes the specified shell scripts multiple times in parallel steplets, in each specified runtime for each set of environment variables on each specified platform. For example, if a Matrix step specifies 3 sets of environment variables and 2 runtime images, it will run a total of 6 steplets (3 times in runtime 1, 3 times in runtime 2). If the Matrix step also specifies 2 node pools, it will run 12 steplets.
A Matrix step can be optionally preceded by a PreMatrix step to prepare the build node environment for the steplets, and a PostMatrix step to aggregate information produced by the steplets. For more information, see Using the Matrix Step.
[/INST]### Note
Matrix step is available for Linux nodes only.
##### Usage
 **Matrix**
[YAML]
    pipelines:
      - name: <string>
        steps:
          - name: <string>
            type: Matrix
            stepMode: Bash
            configuration: 
              #inherits from Bash 
              concurrency: <string>       
              multiNode: <boolean>        # optional, only needed if steplets 
                                          # need to execute on separate nodes
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onExecute:                 # required     
                - echo ""Executing steplet $step_name""
                - echo ""env1 = $env1""
                - echo ""env2 = $env2""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: # always
                - echo ""Cleaning up some stuff""
    
            stepletMultipliers:
               nodePools:                 # wants to execute step against       
                - windows                 # multiple operating systems
                - ubuntu_18
                - ubuntu_16
     
              environmentVariables:       # optional, only needed if user
                - env1: one               # wants to execute step against
                  env2: two               # multiple values of env
                - env1: abc                
                  env2: xyz
    
              runtimes:                   # optional, only needed if user
                - type: image             # wants to execute step against 
                    image:                # multiple images
                    auto:                 
                      language:
                      version:
                    custom:
                      <same config as Bash>
                  image:               
                    auto:
                      language:
                      version:
                    custom:
                      <same config as Bash>
    
              fastFail: <true/false>     # default false. If specified, matrix
                                         # step fails when any steplet fails, 
                                         # unless specified in allowFailures
    
              allowFailures:             # optional, array of combinations
                - nodePool:              # that are allowed to fail  
                  environmentVariables:
                    env1: abc                              
                    env2: xyz
                  runtime:               
    
              exclude:                   # optional, array of combinations
                - nodePool:              # that are not executed  
                  environmentVariables:
                    env1: one
                    env2: two
                  runtime:  
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `Matrix` for this step type.
###### stepmode
Specifies the runtime OS mode. May be either `Bash` or `PowerShell`. If not specified, defaults to `Bash`.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`inputStep`
May specify a PreMatrix preamble step.
Optional  
In addition, these tags can be defined to support the step's native operation:
Tag
 **Description of usage**
Required/Optional  
`multiNode`
When `true`, steplets will be required to run on separate nodes. Default is `false`.
For more information, see Multi-node Matrix.
### Note
When both `multiNode` and `concurrency` are used, `multiNode` takes precedence.
Optional  
`concurrency`
Define the number of steplets that can concurrently run on a node. This helps in optimizing resource usage and ensures that the node efficiently handles steplets without being overwhelmed.
### Note
When both `multiNode` and `concurrency` are used, `multiNode` takes precedence.
Optional  
###### stepletMultipliers
In addition, these tags can be defined to support the step's native operation:
Tag
 **Description of usage**
Required/Optional  
`nodePools`
A collection of node pool names. Recognized only when `multiNode` is set to true. When specified, the step will be executed in each of the defined `runtimes` for each set of `environmentVariables` on each node pool.
If not defined, the step will be executed in the default node pool, or the node pool specified in the `configuration` section.
 **Example**
[YAML]
    steps:
    - name: step_1
      type: Matrix
      stepMode: Bash
      configuration:
        multiNode: true
      stepletMultipliers:
        environmentVariables:
          - foo: foo
          - bar: bar
        nodePools:
          - gcp
          - aws 
[/YAML]
Optional  
`environmentVariables`
A collection of sets of environment variable definitions. Each set of definitions will be used in an execution of a steplet in each of the defined `runtimes`.
If not defined, then a single steplet will execute for each of the defined `runtimes` on each of the specified `nodePools`.
Optional  
`runtimes`
A collection of runtime definitions. The step will be executed in each defined runtime, on each of the specified `nodePools`, for each set of `environmentVariables`.
If not defined, then each steplet will be executed in the default runtime, or the runtime specified in the `configuration` section.
Optional  
`fastFail`
When set to `true` the step fails when any steplet fails, unless permitted to fail in `allowFailures.`
Default is false.
Optional  
`allowFailures`
A collection that specifies the combinations of `nodepools`, `environmentVariables`, and `runtimes` that are permitted to fail without terminating the entire Matrix step. This only has effect when `fastFail` is set to `true`.
Optional  
`exclude`
A collection that specifies the combinations of `nodepools`, `environmentVariables`, and `runtimes` that will not be executed.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of `onExecute`
Optional  
`onExecute`
Main commands to execute for the step
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
### Caution
`onExecute`, `onStart`, `onSuccess`, `onFailure`, and `onComplete` are reserved keywords. Using these keywords in any other context in your execution scripts can cause unexpected behavior.
##### Examples
###### Example 1
This is an example of how to use the Matrix step to perform a build activity.
 **Matrix Step Example**
[YAML]
          - name: matrix_example
            type: Matrix
            stepMode: Bash
            configuration:
              inputSteps: 
                - name: matrix_fan_out_example
    
            stepletMultipliers:
              environmentVariables:             # Sets of environment variables for steplets
                - animal:    dog                            # - Set 1
                  mineral:   copper
                  vegetable: carrot
                - animal:    goat                           # - Set 2
                  mineral:   iron
                  vegetable: broccoli
                - animal:    lizard             # - Set 3
                  mineral:   lead        
    
              nodePools:
                    - gcp
                    - aws 
              
              runtimes:                         # Runtimes for steplets
                - type: image
                  image:
                    auto:
                      language: java
                      versions:
                        - 13.0
                - type: image
                  image:
                    auto:
                      language: node
                      versions:
                        - 8.17.0
            execution:
              onExecute:
                - echo ""I am executing matrix steplet ${steplet_id}""
                - echo ""Envs animal= $animal , mineral = $mineral , vegetable = $vegetable""
                - echo ""Runtime image name: $step_image_name | Runtime image tag: $step_image_tag""
    
[/YAML]
When run, the above example will execute in a total of 6 steplets: in 2 runtimes for each of 3 sets of environment variables:
Runtime
Set 1
Set 2
Set 3  
java 13.0
dog, copper, carrrot
goat, iron, broccoli
lizard, lead, <null>  
node 8.17.0
dog, copper, carrrot
goat, iron, broccoli
lizard, lead, <null>  
###### Example 2 - allowFailures, Exclude, fastFail
[YAML]
    pipelines:
      - name: S_Matrix_0052
        steps:           
          - name: S_Matrix_0052
            type: Matrix
            stepMode: Bash
            stepletMultipliers:
              fastFail: true
              allowFailures:
                - environmentVariables:
                    delay: 1
                    fail: 'true'
                  nodePool: windows
                  runtime: host
                - environmentVariables:
                    delay: 1
                    fail: 'true'
                  nodePool: ubuntu
                  runtime: host
              exclude:
                - environmentVariables:
                    delay: 1
                    fail: 'true'
                  nodePool: windows
                  runtime: host
                - environmentVariables:
                    delay: 1
                    fail: 'true'
                  nodePool: ubuntu
                  runtime: host 
              environmentVariables:
                - delay: 1
                  fail: 'true'
                - delay: 2
                  fail: 'false'
                - delay: 3
                  fail: 'false'
                - delay: 5
                  fail: 'false' 
            execution:
              onStart:
                - sleep ""$delay""
              onExecute:
                - echo ""I am on matrix step""
                - |
                  [ $fail == 'false' ] 
                - ls -l .
[/YAML]
###### Example 3 - concurrency
[YAML]
    pipelines:
      - name: matrix_check
        steps:
          - name: step1
            type: Matrix
            stepMode: Bash
            configuration: 
              concurrency: 50
            stepletMultipliers:
              environmentVariables:            # Sets of environment variables for steplets
              - animal:    dog                 # - Set 1
                mineral:   copper
                vegetable: carrot
              - animal:    goat                # - Set 2
                mineral:   iron
                vegetable: broccoli
              - animal:    lizar1d             # - Set 3
                mineral:   lea1d
              - animal:    liza2rd             # - Set 3
                mineral:   le2ad  
              - animal:    liza3rd             # - Set 3
                mineral:   lea3d
              - animal:    liza4rd             # - Set 3
                mineral:   lea4d
              - animal:    liza5rd             # - Set 3
                mineral:   le5ad  
              - animal:    liz6ard             # - Set 3
                mineral:   lea6d  
              - animal:    lizar7d             # - Set 3
                mineral:   le7ad  
              - animal:    lizar8d             # - Set 3
                mineral:   lea8d 
              - animal:    liza9rd             # - Set 3
                mineral:   lea9d 
              - animal:    liza10rd            # - Set 3
                mineral:   lead10 
              - animal:    liza11rd            # - Set 3
                mineral:   lead11  
              - animal:    liza12rd            # - Set 3
                mineral:   lead12  
              - animal:    lizar13d            # - Set 3
                mineral:   lead13
              - animal:    d1og14              # - Set 1
                mineral:   copper14
                vegetable: carrot14
              - animal:    go1at15             # - Set 2
                mineral:   ir1on15
                vegetable: broccoli15
              - animal:    lizard16            # - Set 3
                mineral:   lead16
              - animal:    lizard17            # - Set 3
                mineral:   lead17
              - animal:    lizard18            # - Set 3
                mineral:   lead18
              - animal:    lizard19            # - Set 3
                mineral:   lead20
              - animal:    lizard20            # - Set 3
                mineral:   lead20
              - animal:    lizard21            # - Set 3
                mineral:   lead21
              - animal:    lizard22            # - Set 3
                mineral:   lead22
              - animal:    lizard23            # - Set 3
                mineral:   lead23
              - animal:    lizard24            # - Set 3
                mineral:   lead24
              - animal:    lizard25            # - Set 3
                mineral:   lead25 
              - animal:    lizard26            # - Set 3
                mineral:   lead26
              - animal:    lizard27            # - Set 3
                mineral:   lead27
              - animal:    lizard28            # - Set 3
                mineral:   lead28
              - animal:    dog29               # - Set 1
                mineral:   copper29
                vegetable: carrot29
              - animal:    goat30              # - Set 2
                mineral:   iron30
                vegetable: broccoli30
              - animal:    lizard31            # - Set 3
                mineral:   lead31
              - animal:    lizard32            # - Set 3
                mineral:   lead32  
              - animal:    lizard33            # - Set 3
                mineral:   lead33
              - animal:    lizard34            # - Set 3
                mineral:   lead34
              - animal:    lizard35            # - Set 3
                mineral:   lead35  
              - animal:    lizard36            # - Set 3
                mineral:   lead36  
              - animal:    lizard37            # - Set 3
                mineral:   lead37  
              - animal:    lizard38            # - Set 3
                mineral:   lead38
              - animal:    lizard39            # - Set 3
                mineral:   lead39
              - animal:    lizard40            # - Set 3
                mineral:   lead40  
              - animal:    lizard41            # - Set 3
                mineral:   lead41  
              - animal:    lizard42            # - Set 3
                mineral:   lead42
              - animal:    lizard43            # - Set 3
                mineral:   lead43     
              - animal:    dog44               # - Set 1
                mineral:   copper44
                vegetable: carrot44
              - animal:    goat45              # - Set 2
                mineral:   iron45
                vegetable: broccoli45
              - animal:    lizard46            # - Set 3
                mineral:   lead46
              - animal:    lizard47            # - Set 3
                mineral:   lead47  
              - animal:    lizard48            # - Set 3
                mineral:   lead48
              - animal:    lizard49            # - Set 3
                mineral:   lead49
              - animal:    lizard50            # - Set 3
                mineral:   lead50
     
            execution:
              onExecute:
                - echo ""I am executing matrix steplet ${steplet_id}""
                - echo ""Envs animal= $animal , mineral = $mineral , vegetable = $vegetable
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
91,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **MvnBuild** native step performs a Maven project build on files in a Git repository. Optionally, it can also publish build information to Artifactory.
[/INST]### Note
If your maven project is already using the Maven Artifactory plugin, see  Setting Up Maven Builds for Pipelines for information about configuring your Maven settings for use in JFrog Pipelines.Ecosystem Integration
Using this step automatically selects Java as the language and bootstraps the runtime environment in the node appropriately.
##### YAML Schema
 **MvnBuild**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: MvnBuild
            configuration:
              #inherits all the tags from bash
              mvnCommand:           <string>      # optional
              sourceLocation:       <string>      # optional
              resolverSnapshotRepo: <string>      # optional
              resolverReleaseRepo:  <string>      # optional
              deployerSnapshotRepo: <string>      # optional
              deployerReleaseRepo:  <string>      # optional
              forceXrayScan:        <string>      # optional
              failOnScan:           <boolean>     # default true
              autoPublishBuildInfo: <string>      # optional
              configFileLocation:   <string>      # optional
              configFileName:       <string>      # optional
    
              integrations:
                - name:         <Artifactory integration>  # required
              inputResources:
                - name:         <GitRepo resource>         # required
                - name:         <FileSpec resource>        # optional
              outputResources:
                - name:         <BuildInfo resource>       # if autoPublishBuildInfo is true
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete:                                  #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `MvnBuild `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify an  Artifactory Integration
Required  
`inputResources`
Must specify a GitReporesource. The mvn command performs the build on files in the Git repository at `sourceLocation`.
May also optionally specify a FileSpec resource that specifies what files to copy to `sourceLocation` to build.
Required  
`outputResources`
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set to `true`. If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the step, that name and/or number will be used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number.`
May be required  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`mvnCommand`
Specifies a command line string of options to use with Maven.
Default is `clean install`.
Optional  
`sourceLocation`
Specifies the location of the source files. By default this is set to the root directory of the GitRepo resource. Defaults to the root of the GitRepo.
Required if the source files are not in the root directory.
May be required  
`resolverSnapshotRepo`
Artifactory repository to resolve snapshot dependencies. If set, `resolverReleaseRepo` is also required.
### Note
Do not use with `configFileName` and `configFileLocation`.
Optional  
`resolverReleaseRepo`
Artifactory repository to resolve release dependencies. If set, `resolverSnapshotRepo` is also required.
### Note
Do not use with `configFileName` and `configFileLocation`.
Optional  
`deployerSnapshotRepo`
Snapshot artifacts created by the MvnBuild are uploaded to this Artifactory repository. If set, `deployerReleaseRepo` is also required.
### Note
Do not use with `configFileName` and `configFileLocation`.
Optional  
`deployerReleaseRepo`
Release artifacts created by the MvnBuild are uploaded to this Artifactory repository. If set, deployerSnapshot`Repo` is also required.
### Note
Do not use with `configFileName` and `configFileLocation`.
Optional  
`forceXrayScan`
When set to `true`, forces an Xray scan after publishing to Artifactory. Default is `false`.
Optional  
`failOnScan`
When set to `true`, and when the Xray Policy Rule Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step. Default is `true`.Creating Xray Policies and Rules
Optional  
`autoPublishBuildInfo`
When set to `true`, publishes build info to Artifactory. Requires a BuildInfo resources in the outputResources list. Default is `false`.
Optional  
`configFileName`
Specifies the name of the JFrog CLI mvn-config file. The `serverID` in the configuration file should match the Artifactory integration name. If set, `configFileLocation`is also required.
### Note
This option should be used only if you have committed this configuration file to your source
Optional  
`configFileLocation`
Specifies the directory containing the JFrog CLI mvn-config file, relative to the `sourceLocation` . If set, `configFileName` is also required.
### Note
This option should be used only if you have committed this configuration file to your source
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show a few ways in which a MvnBuild step can be configured.
###### Full Pipeline Example
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For a full tutorial, see Pipeline Example: Maven Build.


[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: mvn_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repoPath }}
          gitProvider: {{ .Values.gitProvider }}
    
      - name: mvn_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactory }}
    
    pipelines:
      - name: demo_maven
        steps:
          - name: mvn_build_step
            type: MvnBuild
            configuration:
              deployerSnapshotRepo: {{ .Values.deployerRepo }}
              deployerReleaseRepo: {{ .Values.deployerRepo }}
              inputResources:
                - name: mvn_repo
              integrations:
                - name: {{ .Values.artifactory }}
    
          - name: publish_build
            type: PublishBuildInfo
            configuration:
              forceXrayScan: false
              inputSteps:
                - name: mvn_build_step
              outputResources:
                - name: mvn_build_info
    
[/YAML]
###### Basic Example using Default Values
The most basic form of MvnBuild. Uses all default values. This step navigates to the root of the repo specified in the `inputResources` array, and performs an `mvn clean install`. It does not publish a build or request an Xray scan.
 **MvnBuild**
[YAML]
    pipelines: 
      - name: MyMavenPipeline
        steps:
          - name: MavenWithArtifactory
            type: MvnBuild
            configuration:
              integrations:
                - name: art
              inputResources:
                - name: mvn_repo
[/YAML]
###### Using Artifactory as Resolver and Deployer
This example uses Artifactory as a resolver and a deployer, so that your project's dependencies are pulled from the specified repo, and the resulting artifacts are pushed back up to Artifactory. It also publishes build info to Artifactory and requests an Xray scan.
 **MvnBuild**
[YAML]
    pipelines: 
      - name: MyMavenPipeline
        steps:
          - name: MavenWithAll
            type: MvnBuild
            configuration:
              integrations:
                - name: art
              inputResources:
                - name: mvn_repo
              outputResources:
                - name: mvn_build_info
              mvnCommand: clean install test
              sourceLocation: mvnproject
              resolverSnapshotRepo: snapshots-remote-maven
              resolverReleaseRepo: releases-remote-maven
              deployerSnapshotRepo: snapshots-local-maven
              deployerReleaseRepo: releases-local-maven
              forceXrayScan: true
              failOnScan: true
              autoPublishBuildInfo: true
    
[/YAML]
###### Using a Config File
This example uses a committed config file to accomplish the same thing that example 2 does.
 **MvnBuild**
[YAML]
    pipelines: 
      - name: MyMavenPipeline
        steps:
          - name: MvnWithConfig
            type: MvnBuild
            configuration:
              integrations:
                - name: art
              inputResources:
                - name: mvn_repo
              outputResources:
                - name: mvn_build_info
              mvnCommand: clean install test
              sourceLocation: mvnproject
              configFileLocation: "".""
              configFileName: mvn-art-config
              forceXrayScan: true
              failOnScan: true
              autoPublishBuildInfo: true
    
[/YAML]
##### How it Works
When you use the **MvnBuild** native step in a pipeline, it performs the following actions in the background:
 **MvnBuildCommands**
[YAML]
    jfrog rt config                        # configures JFrog CLI with the integration listed in the yaml
    jfrog rt mvn-config             # if configFileName and configFileLocation are not set in yaml
    jfrog rt mvn $mvnCommand        # the main maven build command
    add_run_variables                       # saves some information in run state for future steps to reference
    jfrog rt build-collect-env      # collects build environment, preparing for build publish
    jfrog rt build-publish          # only if autoPublishBuildInfo is true
    write_output                            # updates the buildinfo resource, if present
    jfrog rt build-scan             # if forceXrayScan is true
    add_run_files                           # adds BuildInfo to run state
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
92,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **NpmBuild** native step builds an npm source. This step automatically performs `npm-install` on the source in a Git repository.
If you want to run tests, add a Bash step to trigger them.
[/INST]### Important
The NpmBuild native step may fail on W19 nodes when JFrog CLI v1 is used. Workaround is to use JFrog CLI v2.
##### YAML Schema
The YAML schema for NpmBuild native step is as follows:
 **NpmBuild**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: NpmBuild
            configuration:
              #inherits all the tags from bash
              # for payloadType npm:
              npmArgs:          <string>      # optional
              repositoryName:   <string>      # optional, npm repository name on artifactory
              resolverRepo:     <string>      # optional, npm repository name on artifactory
              sourceLocation:   <string>      # optional, directory containing package.json file
    
              integrations:
                - name:         <artifactory integration>  # required, unless there is a FileSpec input
              inputResources:
                - name:         <gitrepo resource>         # required
                - name:         <filespec resource>        # optional
      
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete:                                  #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `NpmBuild `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Specifies an  Artifactory Integrationwhere modules will be published. If a FileSpec resource is specified in `inputResources` then this is optional. Otherwise, it is required.
May be required  
`inputResources`
Must specify a GitReporesource. The `npm-install` runs on the Git repository at `sourceLocation`.
May also optionally specify a FileSpec resource that specifies what files to copy to `sourceLocation` prior to running `npm-install`.
Required
Optional  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`npmArgs`
Specifies a command line string of arguments to use with npm-install.
Optional  
`resolverRepo`
The name of the npm repository in Artifactory.
Optional  
`repositoryName`
Alternative to `resolverRepo`. Do not set both tags.
Optional  
`sourceLocation`
The directory containing the `package.json` file, relative to the GitRepo `path`.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure an NpmBuild step.
###### Using Default Settings
An NpmBuild step using default locations and default commands.
  * This example requires a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For more information about running this example, see Pipeline Example: Npm Build.


 **NpmBuild**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: npm_repo_jfp_example
        type: GitRepo
        configuration:
          # SCM integration where the repository is located
          gitProvider: {{ .Values.myRepo.gitProvider }}
          # Repository path, including org name/repo name
          path: {{ .Values.myRepo.path }}
          branches:
            # Specifies which branches will trigger dependent steps
            include: master
    
      - name: npm_buildinfo_jfp_example
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
    
    pipelines:
      - name: npm_pipeline_jfp_example
        steps:
          - name: npm_build_step
            type: NpmBuild
            configuration:
              repositoryName: npm-virtual      # required, npm repository name on artifacctory
              sourceLocation: .      # required, location of package.json file
              integrations:
                - name:  demoArt  # required
              inputResources:
                - name: npm_repo_jfp_example         # required
[/YAML]
###### Using FileSpec Input
An NpmBuild step with a FileSpec input providing files for the build that are not in the GitRepo and resolverRepo specifying an Artifactory repository to use when resolving dependencies.
 **NpmBuild**
[YAML]
    pipelines: 
      - name: npmBuildPipeline
        steps:
          - name: npmBuildStep
            type: NpmBuild
            configuration: 
              sourceLocation: ""source""
              resolverRepo: remote-npm
              inputResources:
                - name: gitRepoResource
                - name: fileSpecResource
              integrations:
                - name: artifactory_integration
[/YAML]
##### How it Works
When you use the **NpmBuild** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (if there is a FileSpec input, to configure the JFrog CLI with the Artifactory credentials in the input FileSpec)
  * jfrog rt use (to set the current default Artifactory configuration)
  * cp (if there is an input FileSpec, copy the files to the root of the cloned GitRepo)
  * jfrog rt npm-config (configure the repository to resolve dependencies)
  * jfrog rt npm-install
  * add_run_variables (save information about this step for future steps)
  * jfrog rt build-collect-env (collect environment variables)
  * add_run_files (save the output and the build information in the run state for later publish steps)


##### Related Topics
Npm Build Quickstart

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
93,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The NpmPublish step publishes an npm package to the registry in Artifactory following an NpmBuild step. It is recommended, but not required, that the NpmBuild and NpmPublish steps be in the same affinity group to optimize sharing files between the two steps.
[/INST]### Important
The NpmBuild native step may fail on W19 nodes when JFrog CLI v1 is used. Workaround is to use JFrog CLI v2.
##### YAML Schema
The YAML schema for NpmPublish native step is as follows:
 **NpmPublish**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: NpmPublish
            configuration:
                      #inherits all the tags from bash
              # for payloadType npm:
              deployerRepo:         <string>   # optional, npm repository name on artifactory
              repositoryName:       <string>   # optional, npm repository name on artifactory
              forceXrayScan:                <boolean>  # optional
              failOnScan:           <boolean>  # default true
              autoPublishBuildInfo: <boolean>  # optional
    
              integrations:
                - name:         <artifactory integration>      # required
              inputSteps:
                - name:         <NpmBuild step>                # required
                      outputResources:
                            - name:                 <BuildInfo resource>           # optional
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `NpmPublish ` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify an  Artifactory Integration.
Required  
`inputSteps`
Must specify a named NpmBuild or Bash step.
Required  
`outputResources`
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set to `true`.
If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the input NpmBuild step, that name and/or number will be used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number.`
May be required  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`deployerRepo`
The name of the npm repository to publish in Artifactory.
Optional  
`repositoryName`
Alternative to `deployerRepo`. Do not set both tags.
Optional  
`forceXrayScan`
When set to `true`, forces an Xray scan after publishing to Artifactory.
Default is `false`.
Optional  
`failOnScan`
When set to `true`, and when the Xray Policy Rule  Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
`autoPublishBuildInfo`
When set to `true`, publishes build info to Artifactory.
Default is `false`.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure an NpmPublish step.
###### Full Pipeline Example
  * This example requires a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For more information about running this example, see Pipeline Example: Npm Build.


 **NpmBuild**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: npm_repo_jfp_example
        type: GitRepo
        configuration:
          # SCM integration where the repository is located
          gitProvider: {{ .Values.myRepo.gitProvider }}
          # Repository path, including org name/repo name
          path: {{ .Values.myRepo.path }}
          branches:
            # Specifies which branches will trigger dependent steps
            include: master
    
      - name: npm_buildinfo_jfp_example
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
    
    pipelines:
      - name: npm_pipeline_jfp_example
        steps:
          - name: npm_build_step
            type: NpmBuild
            configuration:
              affinityGroup: npm_pipeline_jfp_example
              repositoryName: npm-virtual      # required, npm repository name on artifacctory
              sourceLocation: .      # required, location of package.json file
              integrations:
                - name:  demoArt  # required
              inputResources:
                - name: npm_repo_jfp_example         # required
    
          - name: npm_publish_step
            type: NpmPublish
            configuration:
              affinityGroup: npm_pipeline_jfp_example
              # for payloadType npm:
              repositoryName: npm-virtual        # required, npm repository name on artifactory
              autoPublishBuildInfo: true       # optional
              integrations:
                - name: demoArt      # required
              inputSteps:
                - name: npm_build_step                # required
              outputResources:
                - name: npm_buildinfo_jfp_example                 # optional
[/YAML]
###### Upload Artifactory Repo
Uploads to an Artifactory repository named npm-repo.
### Note
This example is not a complete pipeline. An NpmBuild step must be part of the same pipeline.
 **NpmPublish**
[YAML]
    pipelines: 
      - name: npmPublishPipeline
        steps:
          - name: npmPublishStep
            type: NpmPublish
            configuration:
              deployerRepo: npm-repo
              integrations:
                - name: artifactory_integration
              inputSteps:
                - name: npmBuildStep
[/YAML]
###### Upload the Result of NpmBuild Step to Artifactory Repo
Uploads the result of the NpmBuild step to an Artifactory repository named npm-repo and uploads build info. This extends example 1 in the NpmBuild documentation.
 **NpmPublish**
[YAML]
    pipelines: 
      - name: npmBuildPipeline
        steps:
          - name: npmBuildStep
            type: NpmBuild
            configuration:
              affinityGroup: npmBuildPipeline
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: npmPublishStep
            type: NpmPublish
            configuration:
              affinityGroup: npmBuildPipeline
              deployerRepo: npm-repo
              autoPublishBuildInfo: true
              inputSteps:
                - name: npmBuildStep
              outputResources:
                - name: outputBuildInfo
    
[/YAML]
###### Publish Build Info and Trigger Xray Scan
In this example, build info is published and an Xray scan triggered.
 **NpmPublish**
[YAML]
    pipelines: 
      - name: npmBuildPipeline
        steps:
          - name: npmBuildStep
            type: NpmBuild
            configuration:
              affinityGroup: npmBuildPipeline
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: npmPublishStep
            type: NpmPublish
            configuration:
              affinityGroup: npmBuildPipeline
              deployerRepo: npm-repo
              autoPublishBuildInfo: true
              forceXrayScan: true
              inputSteps:
                - name: npmBuildStep
              outputResources:
                - name: outputBuildInfo
    
[/YAML]
##### How it Works
When you use the **NpmPublish** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt use (specify the configured credentials to use)
  * restore_run_files (copy the output and build info from the NpmBuild step)
  * jfrog rt npm-config (configure the deploy repo)
  * jfrog rt npm-publish
  * jfrog rt build-publish (if autoPublishBuildInfo is true, publish the build info)
  * write_output (if autoPublishBuildInfo is true, update the output BuildInfo resource)
  * jfrog rt build-scan (if forceXrayScan is true, trigger a scan)
  * add_run_files (update the build info saved in run state)


##### Related Topics
Pipeline Example: Npm Build

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
94,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **PreMatrix** generic step may be used to prepare a build environment for execution of a Matrix step.
The PreMatrix step is optional for use with Matrix builds. For more information, see Using the Matrix Step.
[/INST]##### Usage
 **FanOut**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: PreMatrix
            configuration: 
              # inherits Bash config, except affinityGroup and outputResources
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onExecute:                                                    # required     
                - echo ""this is a PreMatrix step""           
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `PreMatrix` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, except for thesetags, which are disallowed:
  * `affinityGroup`
  * `outputResources`


###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of `onExecute`
Optional  
`onExecute`
Main commands to execute for the step
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
95,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **PostMatrix** generic step may be used to perform post-execution tasks following a Matrix step.
The PostMatrix step is optional for use with Matrix builds. For more information, see Using the Matrix Step.
[/INST]##### Usage
 **FanIn**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: PostMatrix
            configuration: 
              # inherits Bash config, except affinityGroup and outputResources
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onExecute:                                                    # required     
                - echo ""this is a PostMatrix step""           
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `PostMatrix` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, except for thesetags, which are disallowed:
  * `affinityGroup`
  * `outputResources`


###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of `onExecute`
Optional  
`onExecute`
Main commands to execute for the step
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
96,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **PowerShell** step type is a generic type that enables executing PowerShell commands. PowerShell steps can only run on Windows node pools and are similar to the Bash step on other node pools. As a general-purpose step that can execute any action that can be scripted, even with tools and services that haven't been integrated with JFrog Pipelines, it can be used to perform actions where complete control is required.
All native steps running in a Windows node pool derive from the PowerShell step. This means that all steps share the same base set of tags from PowerShell, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the PowerShell step definition when running steps on Windows, since it's the core of the definition of all other steps.
[/INST]##### Usage
 **PowerShell**
[YAML]
    pipelines:
      - name:                       <string>
        steps:
          - name:           <string>
            type:           PowerShell
            configuration:
              affinityGroup:        bldGroup 
              priority:             <[0-10000]>
              timeoutSeconds:       <job timeout limit>
              nodePool:             <name of the nodePool>
              chronological:        <true/false>
              environmentVariables:
                env1:                       <string>
                env2:                       <string>
                env3:
                  default:      <string>
                  description:  <string>
                  values:       <array>
                  allowCustom:  <true/false>
              integrations:
                - name:             <integration name>
              inputSteps:
                - name:             <step name>
              inputResources:
                - name:             <resource name>
                  trigger:          <true/false>    # default true
                  branch:       <string>        # see description of defaults below
              outputResources:
                - name:             <resource name>
                  branch:       <string>        # see description of defaults below
              runtime:            
                type:                       <image/host>
                image:
                  auto:
                    language:
                    versions:
                  custom:
                    name:
                    tag:
                    options:
                    registry:          <integration>   # optional integration for a private registry
                    sourceRepository:  <path>          # required if registry is Artifactory. e.g. docker-local
                    region:                            # required if registry is AWS. e.g. us-east-1
                    autoPull:       <true/false>             # default true; pulls image before run
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onExecute:
                - echo ""executing task command 1""
                - echo ""executing task command 2""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step. The name should be chosen to accurately describe what the step does, e.g. `prov_test_env` to represent a job that provisions a test environment. Names of steps must be unique within a pipeline.
###### type
Must be `PowerShell` for this step type.
###### configuration
Specifies all optional configuration selections for the step's execution environment.
Tag
 **Description of usage**
Required/Optional  
`affinityGroup`
Label that controls affinity to a Node. All the steps with the same affinityGroup will be executed on the same node. This will allow sharing state between the steps. An example is having the same affinityGroup for DockerBuild and DockerPush steps in a Pipeline so that Image being built in the DockerBuild step can be used to published in the DockerPush step
Optional  
`priority`
Controls the priority of a step when there are parallel steps in a pipeline or multiple pipelines executing. It determines which step will run first across all steps that could run if there were no constraints on the number of steps running. Steps with a lower number will run before steps with higher numbers. For example, priority 10 will run before priority 100. The default priority is 9999.
Priority does not apply to steps that are still waiting for an input to complete or configured to run in a node pool with no available nodes. Also, if there are two steps ready to run and only one available node, the one with the lower priority number runs first, regardless of which pipeline each step belongs to.
Optional  
`timeoutSeconds`
Time limit, in the number of seconds, for the step to complete. If the step does not complete in the given time limit, the step will be forced to a completion state of failed.
Optional  
`nodePool`
Assigns the node pool on which the step executes. If node pool isn't specified, a step will execute on the default node pool.  See here to learn more about node pools.
Optional  
`chronological`
Specifies that the step must execute in chronological order, to ensure receipt of all state updates from preceding steps.
A step with `chronological: true` will only execute when that step is complete in all previously triggered runs.
Optional  
`environmentVariables`
Assigns any environment variables and their values in key:value format. All environment variables assigned within a step definition are active only for the scope of the execution of that step. Variables will be available in global scope and in the `env` drive.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


Optional  
`integrations`
A collection of integrations that will be used by this step. Integrations can be used directly in step without a resource.
Optional  
`inputSteps`
A collection of named steps whose completion will trigger execution of this step.
Optional  
`inputResources`
A collection of named resources that will be used by this step as inputs.
By default, changes to these named resources will trigger execution of this step. This can be changed by declaring `trigger` to be `false`.
A `branch` is only required when using a resource from another branch of a multi-branch source. By default, the resource from a single-branch source or from the same branch of a multi-branch pipeline is used. To use a multi-branch resource in a single-branch pipeline or another branch in a multi-branch pipeline, `branch` should be used to specify the branch to which the resource belongs.
Optional  
`outputResources`
A collection of named resources that will be generated or changed by this step.
A `branch` is only required for resources from another branch of a multi-branch source. By default, the resource from a single-branch source or from the same branch of a multi-branch pipeline is assumed. To update a multi-branch resource in a single-branch pipeline or another branch in a multi-branch pipeline, `branch` should be used to specify the branch to which the resource belongs.
Optional  
`runtime`
Specifies the runtime for the execution node.
Optional  
###### execution
Declare sets of shell command sequences to perform for different execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of `onExecute`
Optional  
`onExecute`
Main commands to execute for the step
Optional  
`onSuccess`
Commands to execute on successful completion of `onExecute`
Optional  
`onFailure`
Commands to execute on failed completion of `onExecute`
Optional  
`onComplete`
Commands to execute on any completion of `onExecute`
Optional  
##### Example
This is an example of how to use the PowerShell step to perform a build activity.
 **PowerShell step to build**
[YAML]
    - name: build
      type: PowerShell
      configuration:
        nodePool: my_windows_node_pool
        environmentVariables:
          env1: value1
          env2:
            default: value2
            description: Example Variable
            values:
              - value2
              - value3
            allowCustom: false
        runtime:
          type: image
          image:
            auto:
              language: node
              versions:
                - ""10.18""
        inputResources:
          - name: src
        integrations:
          - name: mySlack
      execution:
        onExecute:
          - Push-Location $res_src_resourcePath
          - npm install
          - New-Item testresults -Type Directory
          - New-Item codecoverage -Type Directory
          - .\node_modules\.bin\mocha --recursive ""tests/**/*.spec.js"" -R mocha-junit-reporter --reporter-options mochaFile=testresults/testresults.xml
          - .\node_modules\.bin\istanbul --include-all-sources cover -root ""routes"" node_modules/mocha/bin/_mocha -- -R spec-xunit-file --recursive ""tests/**/*.spec.js""
          - .\node_modules\.bin\istanbul report cobertura --dir codecoverage
          - save_tests testresults/testresults.xml
        onSuccess:
          - send_notification mySlack ""build completed""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
97,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **PromoteBuild** native step promotes a BuildInfo and moves or copies the related artifacts from one Artifactory repository to another.
[/INST]### Note
This step requires a BuildInfo resource as an input to successfully execute.
##### YAML Schema
The YAML schema for PromoteBuild native step is as follows:
 **PromoteBuild**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: PromoteBuild
            configuration:
              #inherits all the tags from bash
              targetRepository:      <string>
              includeDependencies:   <boolean>    # default false
              status:                                <string>  
              comment:                               <string>
              copy:                                  <boolean>
    
              inputResources:
                - name:         <BuildInfo resource>     # required
                  trigger: false
              outputResources:
                - name:         <BuildInfo resource>     # required
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `PromoteBuild` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify a named BuildInfo resource whose `buildName` and `buildNumber` properties identify the build to promote.
Required  
`outputResources`
Must specify a named BuildInfo resource to map to the promoted build. The BuildInfo will be updated with the `buildName` and `buildNumber` of the input BuildInfo resource, and its `targetRepo` with the value of `targetRepository`.
Required  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`targetRepository`
The name of the repository in Artifactory to promote the build to.
Required  
`includeDependencies`
When set to true, promotion will include the build dependencies.
Defaults to false.
Optional  
`status`
Name of the status to promote the build to.
Optional  
`comment`
Comment to include for the promotion.
Optional  
`copy`
When set to true, copies the artifacts to the targetRepository vs moving them to the targetRepository.
Defaults to false.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a PromoteBuild step.
###### Promote Build to Artifactory
Promotes a build specified by an input BuildInfo to an Artifactory repository named `docker-prod-local`. In this example, `trigger` is set as `false` on the input resource so that the step will not be automatically triggered.
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


 **PromoteBuild**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: app_repo
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.gitIntegration }}                      
          path: {{ .Values.gitRepositoryPath }}    
          branches:
            include: master
    
      - name: app_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
      - name: app_promoted_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
    pipelines:
      - name: app_dev_pipeline_su204
        steps:
          - name: app_build
            type: DockerBuild
            configuration:
              affinityGroup: docker_group
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}  
              dockerImageTag: ${run_number}
              inputResources:
                - name: app_repo
              integrations:
                - name: {{ .Values.artifactoryIntegration }}         
                
          - name: app_push
            type: DockerPush
            configuration:
              affinityGroup: docker_group
              targetRepository: {{ .Values.sourceRepository }}
              integrations:
                - name: {{ .Values.artifactoryIntegration }}                            
              inputSteps:
                - name: app_build
    
          - name: publish_app_build
            type: PublishBuildInfo
            configuration:
              affinityGroup: docker_group
              inputSteps:
                - name: app_push
              outputResources:
                - name: app_buildinfo
                
          - name: xrayScanStep
            type: XrayScan
            configuration:
              inputResources:
                - name: app_buildinfo   
    
      - name: app_prod_pipeline_su204
        steps:
          - name: app_promote_build
            type: PromoteBuild
            configuration:
              targetRepository: {{ .Values.prodRepository }}
              inputResources:
                - name: app_buildinfo
                  trigger: false
              outputResources:
                - name: app_promoted_buildinfo
    
          - name: app_deploy
            type: Bash
            configuration:
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
              inputResources:
                - name: app_promoted_buildinfo
            execution:
              onStart:
                - jfrog rt docker-pull {{ .Values.artifactoryUrl }}/{{ .Values.prodRepository }}/{{ .Values.imageName }}:$res_app_promoted_buildinfo_buildNumber {{ .Values.prodRepository }}
                #- jfrog rt dl prod-go-local/app
                #- ls -latr
              onExecute:
                - docker run {{ .Values.artifactoryUrl }}/{{ .Values.prodRepository }}/{{ .Values.imageName }}:$res_app_promoted_buildinfo_buildNumber
[/YAML]
###### Promote Build with Status and Comment
Promotes the build with a status and comment.
 **PromoteBuild**
[YAML]
    pipelines: 
      - name: promoteBuildPipeline
        steps:
          - name: promoteBuildStep
            type: PromoteBuild
            configuration:
              targetRepository: my-promoted-builds
              status: TEST
              comment: ""Promoted by run number ${run_number}.""
              integrations:
                - name: artifactory_integration
              inputResources:
                - name: inputBuildInfo
                  trigger: false
              outputResources:
                - name: outputBuildInfo
    
[/YAML]
###### Promote Build and Copy
In this example, when the build is promoted, it is copied so that a copy remains in the previous repository and the dependencies are copied as well as the build.
 **PromoteBuild**
[YAML]
    pipelines: 
      - name: promoteBuildPipeline
        steps:
          - name: promoteBuildStep
            type: PromoteBuild
            configuration:
              targetRepository: my-promoted-builds
              includeDependencies: true
              copy: true
              integrations:
                - name: artifactory_integration
              inputResources:
                - name: inputBuildInfo
                  trigger: false
              outputResources:
                - name: outputBuildInfo
[/YAML]
##### How it Works
When you use the **PromoteBuild** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (configure the JFrog CLI with the credentials in the BuildInfo input)
  * jfrog rt build-promote
  * write_output (update the output BuildInfo resource)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
98,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **PublishBuildInfo** step publishes BuildInfo to Artifactory. BuildInfo provides a manifest for the build and includes metadata about the modules, dependencies and other environment variables.
BuildInfo can also be published by any of the language-specific publish steps, when its `autoPublishBuildInfo` tag is set to `true`.
[/INST]### Note
A base exclude pattern always applies to published build info from Pipelines, to prevent environment variables containing security information (e.g., secrets) and other unnecessary data from being published. The base exclude pattern is:
`buildinfo.env.res_*;buildinfo.env.int_*;buildinfo.env.current_*;*password*;*secret*;*key*;*token*`
##### YAML Schema
The YAML schema for PublishBuildInfo native step is as follows:
 **PublishBuildInfo**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: PublishBuildInfo
            configuration:
              #inherits all the tags from bash
              envInclude:       <string>   
              envExclude:       <string>   
              forceXrayScan:    <boolean>    # default false
              failOnScan:       <boolean>    # default true
    
              inputSteps:
                - name:         <any of the build steps or Bash step>  # required
              outputResources:
                - name:         <BuildInfo resource>                             # required
    
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `PublishBuildInfo` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`inputSteps`
Must specify a named step of one of the following types:
  * Bash
  * PowerShell
  * DockerPush
  * GoPublishModule
  * GoPublishBinary
  * GradleBuild
  * HelmPublish
  * MvnBuild
  * NpmPublish


The step must not have set `autoPublishBuildInfo` to `true`.
Required  
`outputResources`
Must specify a BuildInfo resource to publish. The `buildName` and `buildNumber` properties are automatically set to `$buildName` and `$buildNumber` if defined, or the `JFROG_CLI_BUILD_NAME` and/or `JFROG_CLI_BUILD_NUMBER` from the environment of the named step for native steps. If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` was not set when the input step ran, the defaults are `$pipeline_name` and `$run_number`.
### Note
If `inputSteps` is a Bash step and `$buildName` or `$buildNumber` are not defined, then this step will fail.
Required  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`envInclude`
Pattern for which environment variables to include. Default is to include all the environment variables.
Optional  
`envExclude`
Pattern for which environment variables to exclude. This is applied in addition to the base exclude pattern applied to all build info.
Optional  
`forceXrayScan`
When set to true, forces an Xray scan after publishing to Artifactory.
Default is false.
Optional  
`failOnScan`
When set to **true** , and when the Xray Policy Rule **Fail Build Creating Xray Policies and Rules** checkbox is checked, a failed Xray scan will result in a failure of the step.
Default is true.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a PublishBuildInfo step.
###### Full Pipeline Example
Pushes the image created by the DockerBuild input step and published BuildInfo to Artifactory.
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


 **DockerPush**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: app_repo1
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.gitIntegration }}                      
          path: {{ .Values.gitRepositoryPath }}    
          branches:
            include: master
    
      - name: app_buildinfo1
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
      - name: app_promoted_buildinfo1
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
    pipelines:
      - name: app_dev_pipeline
        steps:
          - name: app_build
            type: DockerBuild
            configuration:
              affinityGroup: docker_group
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}  
              dockerImageTag: ${run_number}
              inputResources:
                - name: app_repo
              integrations:
                - name: {{ .Values.artifactoryIntegration }}         
                
          - name: app_push
            type: DockerPush
            configuration:
              affinityGroup: docker_group
              targetRepository: {{ .Values.sourceRepository }}
              integrations:
                - name: {{ .Values.artifactoryIntegration }}                            
              inputSteps:
                - name: app_build
    
          - name: publish_app_build
            type: PublishBuildInfo
            configuration:
              affinityGroup: docker_group
              inputSteps:
                - name: app_push
              outputResources:
                - name: app_buildinfo
[/YAML]
###### MvnBuild Example
This extends the first MvnBuild example to publish the build info using a PublishBuildInfo step.
 **PublishBuildInfo**
[YAML]
    pipelines:
      - name: MyMavenPipeline
        steps:
          - name: MavenWithArtifactory
            type: MvnBuild
            configuration:
              integrations:
                - name: art
              inputResources:
                - name: mvn_repo
          - name: publishBuildInfoStep
            type: PublishBuildInfo
            configuration:
              inputSteps:
                - name: MavenWithArtifactory
              outputResources:
                - name: buildInfoResource
    
[/YAML]
###### NpmBuild and NpmPublish Example
An NpmBuild and NpmPublish example publishing the build info using a PublishBuildInfo step and triggering an Xray scan in the PublishBuildInfo step.
 **PublishBuildInfo**
[YAML]
    pipelines:
      - name: npmBuildPipeline
        steps:
          - name: npmBuildStep
            type: NpmBuild
            configuration:
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: npmPublishStep
            type: NpmPublish
            configuration:
              deployerRepo: npm-repo
              inputSteps:
                - name: npmBuildStep
              outputResources:
                - name: outputBuildInfo
          - name: publishBuildInfoStep
            type: PublishBuildInfo
            configuration:
              forceXrayScan: true
              inputSteps:
                - name: npmPublishStep
              outputResources:
                - name: buildInfoResource
    
[/YAML]
###### Bash and GoPublishModule Step
A Bash step input to the PublishBuildInfo step following an earlier GoPublishModule step.
 **PublishBuildInfo**
[YAML]
    pipelines:
      - name: goPublishModulePipeline
        steps:
          - name: goPublishModuleStep
            type: GoPublishModule
            configuration:
              version: ""v0.0.${run_number}""
              targetRepository: go-repo
              self: true
              deps: ALL
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: moduleTestStep
            type: Bash
            configuration:
              inputSteps:
                - name: goPublishModuleStep
            execution:
              onExecute:
                - ""echo 'Running some tests.'""
          - name: publishBuildInfoStep
            type: PublishBuildInfo
            configuration:
              environmentVariables:
                buildStepName: goPublishModuleStep
              inputSteps:
                - name: moduleTestStep
              outputResources:
                - name: buildInfoResource
[/YAML]
##### How it Works
When you use the **PublishBuildInfo** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (configure the JFrog CLI with the BuildInfo output credentials)
  * jfrog rt use (specify the configured credentials to use)
  * restore_run_files (copy the output and build info from the input step)
  * jfrog rt build-publish (publish the build info)
  * jfrog rt build-scan (if forceXrayScan is true, trigger a scan)
  * write_output (if autoPublishBuildInfo is true, update the output BuildInfo resource)
  * add_run_files (update the build info saved in run state)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
99,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **SignReleaseBundle** native step signs a Release Bundle in preparation for distributing it to Edge nodes.
[/INST]##### YAML Schema
The YAML schema for SignReleaseBundle native step is as follows:
 **SignReleaseBundle**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: SignReleaseBundle
            configuration:
                      #inherits all the tags from bash
              inputResources:
                - name:            <release bundle to sign>  # required
                      trigger: false
              outputResources:
                - name:            <signed release bundle>  # required
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `SignReleaseBundle` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify a ReleaseBundleresource whose `name` and `version` properties identify the release bundle to be signed.
Required  
`outputResources`
Must specify a ReleaseBundleresource. This ReleaseBundle is updated with the `name` and `version` properties of the input ReleaseBundle, and sets the `isSigned` property to true.
Required  
There is no additional configuration required.
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
### GPG Signing Key Passphrase
When your Distribution release bundle GPG signing keyis passphrase protected, remember to provide the GPG Signing Key Passphrase when creating/updating your Distribution Integration.GPG Signing
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following example shows how to configure a SignReleaseBundle step.
###### Sign a Release Bundle
Signs a ReleaseBundle provided as an input.
  * This example requires an Artifactory Integration and a Distribution Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


 **SignReleaseBundle**
[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.myArtifactoryIntegration }}
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.demoArtifactoryIntegration }}
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]
##### How it Works
When you use the **SignReleaseBundle** native step in a pipeline, it performs the following functions in the background:
  * curl $distUrl/api/v1/release_bundle/$releaseBundleName/$releaseBundleVersion/sign (sign the release bundle)
  * write_output (update the output ReleaseBundle resource)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
100,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **TriggerPipeline** native step will trigger another step and wait for the resulting run to complete before continuing. This allows you to embed another pipeline inside your own.
[/INST]### Note
  * This step type will utilize a build node to poll for the status of the triggered run until it is complete. Ensure that the capacity of your node pool is sufficient to support both pipelines executing in parallel.
  * For more information about using the `TriggerPipeline` native step, see Embedded Pipelines.


You can use the `set_trigger_payload` utility function in the `onStart` section of this step to set customized parameters for the pipeline that you wish to trigger. In addition, you can add `pipelineVariables`, which will apply to all steps in the triggered run, as well as `stepVariables`, which will apply only to the specified step.
### Linux only
This step is supported for execution on Linux machines only. PowerShell support is not yet available.
##### YAML Schema
The YAML schema for the TriggerPipeline native step is as follows:
 **TriggerPipeline**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: TriggerPipeline
            configuration:
              #inherits all the tags from bash
    
              pipelineName:   <string>   # required
              stepName:       <string>   # required
              branchName:     <string>   # optional. recommended if target is multibranch.
              projectKey:     <string>   # optional. recommended if target belongs to a project.
              integrations:
                - name:       <JFrog Platform Access Token integration>  # required
    
            execution:
              onStart:
                - echo ""Preparing for work...""
                - set_trigger_payload stepVariables ""test=true""
                - set_trigger_payload pipelineVariables ""notify=true"" ""version=5.4.3""
                            - export pipelines_poll_interval_seconds=30 # defaults to 10
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""Something went wrong""
                            - echo ""Nested run status is: ${nested_run_status}""
              onComplete:
                - echo ""Cleaning up""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `TriggerPipeline` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Specifies an JFrog Platform Access Token integration. This integration should contain a token that has the appropriate level of access to trigger the specified pipeline.
Required  
`pipelineName`
The name of the pipeline that you want to trigger
Required  
`stepName`
The name of the step that you want to trigger
Required  
`branchName`
The name of the branch that you want to trigger. This should be used if your target pipeline comes from a  multibranch pipeline source.
Optional
May be required  
`projectKey`
The unique key of the Project that contains the pipeline that you want to trigger. This should only be used if your target pipeline belongs to a Project.
Optional
May be required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation. This is the ideal place to utilize the ` set_trigger_payload` utility function to add custom step- and pipeline-level variables to your target.
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
###### environment variables
The TriggerPipeline step utilizes several new environment variables to provide as much information as possible.
These variables can be used to configure the behavior of the `TriggerPipeline` step
variable name
value  
pipelines_poll_interval_seconds
How frequently, in seconds, the step should poll for its nested run status.
Defaults to 10 seconds  
These variables are injected into the environment of the target pipeline
variable name
value  
parent_step_id
The unique ID of the TriggerPipeline step  
parent_step_name
The name of the TriggerPipeline step  
parent_step_url
The URL that links to the TriggerPipeline step  
parent_run_id
The unique ID of the run that the TriggerPipeline step belongs to  
parent_run_number
The run number of the run that the TriggerPipeline step belongs to  
These variables are exported into the `TriggerPipeline` step environment after the target pipeline's run completes.
variable name
value  
nested_run_status
The final status of the triggered run (can be success, failure, error, canceled, timeout, or skipped)  
nested_run_number
The run number of the triggered run.  
nested_run_duration_seconds
The total duration of the triggered run (in seconds)  
nested_run_url
The URL that links to the triggered step  
##### Examples
The following example shows how to configure a `TriggerPipeline` step.
This example requires a JFrog Platform Access Token integration.
[YAML]
    pipelines:
      - name: top_pipeline
        steps:
          - name: scan_controller
            type: TriggerPipeline
            configuration:
              pipelineName: scanner_pipeline
              stepName: scan_it
              integrations:
                - name: myPlatformToken
              environmentVariables:
                scan_target:
                  default: ""hello-world""
                  allowCustom: true
                  values:
                    - ""vault""
                    - ""redis""
                    - ""postgresql""
                    - ""hello-world""
            execution:
              onStart:
                - set_trigger_payload pipelineVariables ""scan_target=${scan_target}""
                - set_trigger_payload stepVariables ""notify=email"" ""uploadReport=true""
              onComplete:
                - echo ""Final status is $nested_run_status""
    
      - name: scanner_pipeline
        steps:
          - name: scan_it
            type: Bash
            execution:
              onExecute:
                - echo ""Image to scan is $scan_target.""
                - echo ""Triggered by parent step at $parent_step_url""
    
    
[/YAML]
##### How it Works
When you use the **TriggerPipeline** native step in a pipeline, it performs the following functions in the background:
  * GET /projects (if projectKey is provided, the projectId is found)
  * GET /pipelines (find the pipeline by its name)
  * GET /pipelineSteps (find the step by its name)
  * POST /pipelineSteps/:id/trigger (trigger a run starting from the specified step)
  * GET /runs/:id (poll the run to check its status)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
101,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **UploadArtifact** native step uploads artifacts to Artifactory. Optionally, it can also publish build information to Artifactory and trigger Xray scans.
This step utilizes the JFrog CLI to upload an artifact to Artifactory. The file(s) may be provided in a FileSpec, if already in Artifactory, or RemoteFile or GitRepo input.
[/INST]##### YAML Schema
The YAML schema for UploadArtifact native step is as follows:
 **UploadArtifact**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: UploadArtifact
            configuration:
              targetPath:           <string>  #required
              sourcePath:           <string>  #optional
              properties:           <string>  #optional
                      regExp:                       <boolean> #optional
              flat:                 <boolean> #optional
                      module:                       <string>  #optional
                      deb:                  <string>  #optional
                      recursive:                <boolean>     #optional
                      dryRun:                       <boolean> #optional
              symlinks:                     <boolean> #optional
              explode:                      <boolean> #optional
              exclusions:               <string>      #optional
              includeDirs:              <boolean>     #optional
              syncDeletes:              <string>      #optional
              forceXrayScan:                <boolean> #optional
              failOnScan:           <boolean>   # default true
                      autoPublishBuildInfo: <boolean> #optional
              inputResources:
                            - name: myGitRepo       
                            - name: artifactoryFileSpec     
                - name: myRemoteFile        
                      outputResources:
                - name: myFileSpec
                            - name: myBuildInfo
              integrations:
                            - name: myArtifactory 
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `UploadArtifact` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify an  Artifactory Integration.
Required  
`inputResources`
May specify a GitRepo, FileSpec, or RemoteFile resource containing the file(s) to be uploaded. One of each type may be specified.
Optional  
`outputResources`
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set as `true`.
If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number.`
May also specify a FileSpec resource to be updated with the pattern and properties of the uploaded Artifact.
May be required  
In addition, these tags can be defined to support the step's native operation:
Tag
 **Description of usage**
Required/Optional  
targetPath
Path to upload the files, including repository name.
Required  
`sourcePath`
Files to upload. If this is a relative path pattern, it is relative to the root of a GitRepo/FileSpec/RemoteFile input.
Default is `*` when `regExp` is `false` and `.*` when `regExp` is `true`.
Optional  
`properties`
Semi-colon separated properties for the uploaded artifact. For example: `myFirstProperty=one;mySecondProperty=two`.
Properties `pipelines_step_name`, `pipelines_run_number`, `pipelines_step_id`, `pipelines_pipeline_name`, `pipelines_step_url`, `pipelines_step_type`, and `pipelines_step_platform` will also be added.
Optional  
`regExp`
When set as `true`, regular expressions are used in other parameters, such as `sourcePath`, instead of wildcards. Expressions must be in parentheses.
Default is `false`.
Optional  
`flat`
When set as `true`, the uploaded files are flattened, removing the directory structure.
Default is `false`.
Optional  
`module`
A module name for the Build Info.
Optional  
`deb`
A `distribution/component/architecture` for Debian packages. If the distribution, component, or architecture includes a / it must be double-escaped, For example: `distribution/my\\\/component/architecture` for a `my/component` component.
Optional  
`recursive`
When set as `false`, do not upload any matches in subdirectories.
Default is true.
Optional  
`dryRun`
When set as `true`, nothing is uploaded.
Default is `false`.
Optional  
`symlinks`
When set as `true`, symlinks matching the other criteria are uploaded.
Default is `false`.
Optional  
`explode`
When set as `true` and the uploaded Artifact is an archive, the archive is expanded.
Default is `false`.
Optional  
`exclusions`
Semi-colon separated patterns to exclude.
Optional  
`includeDirs`
When set as `true`, empty directories matching the criteria are uploaded.
Default is `false`.
Optional  
`syncDeletes`
A path under which to delete any existing files in Artifactory.
Optional  
`forceXrayScan`
When set as `true`, forces an Xray scan after publishing to Artifactory.
Default is `false`.
Optional  
`failOnScan`
When set as `true`, and when the Xray Policy Rule  Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
`autoPublishBuildInfo`
When set as `true`, publishes build info to Artifactory.
Default is `false`.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
### Note
`onExecute`, `onStart`, `onSuccess`, `onFailure`, and `onComplete` are reserved keywords. Using these keywords in any other context in your execution scripts can cause unexpected behavior.
##### Examples
The following examples show a few ways in which a UploadArtifact step can be configured.
###### Uploading an Artifact to Another Repository using a FileSpec Resource
The most basic form of UploadArtifact. Uses all default values. This step will download the file matching the FileSpec and upload it to the location in `targetPath`. The optional output FileSpec resource will be updated with the `targetPath` and the default properties added to the uploaded artifact.
 **UploadArtifact**
[YAML]
    pipelines: 
      - name: uploadArtifactPipeline
        steps:
          - name: uploadArtifactStep
            type: UploadArtifact
            configuration:
              targetPath: my-repository/myDirectory/myFile.txt
              integrations:
                - name: myArtifactoryIntegration
              inputResources:
                - name: myInputFileSpec
              outputResources:
                - name: myOutputFileSpec
    
[/YAML]
###### Uploading an Artifact from a RemoteFile Resource
In this example, the input is a RemoteFile resource. Otherwise, this is very similar to the previous example with an input that downloads a file that is then uploaded and an optional FileSpec output updated for the uploaded file.
 **UploadArtifact**
[YAML]
    pipelines: 
      - name: uploadArtifactPipeline
        steps:
          - name: uploadArtifactStep
            type: UploadArtifact
            configuration:
              targetPath: my-repository/myDirectory/myFile.txt
              integrations:
                - name: myArtifactoryIntegration
              inputResources:
                - name: myInputRemoteFile
              outputResources:
                - name: myOutputFileSpec
    
[/YAML]
###### Publish Build Info and Trigger Xray Scan
In this example, build info is published as part of the UploadArtifact step and an Xray scan is triggered.
 **UploadArtifact**
[YAML]
    pipelines: 
      - name: uploadArtifactPipeline
        steps:
          - name: uploadArtifactStep
            type: UploadArtifact
            configuration:
              targetPath: my-repository/myDirectory/myFile.txt
              autoPublishBuildInfo: true
              forceXrayScan: true
              integrations:
                - name: myArtifactoryIntegration
              inputResources:
                - name: myFileSpec
              outputResources:
                            - name: myBuildInfo
    
[/YAML]
##### How it Works
When you use the **UploadArtifact** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (configure JFrog CLI with the integration listed in the yaml)
  * jfrog rt use (configure JFrog CLI to use the config for the integration listed in the yaml)
  * mkdir (create a directory to use as the root of relative paths in the following actions)
  * cp (copy the FileSpec, RemoteFile, or GitRepo files to the new directory, limit one of each input type)
  * jfrog rt upload (upload the Artifact)
  * write_output (update the FileSpec output resource with the uploaded pattern and properties)
  * add_run_variables (save information in run state for future steps to reference)
  * jfrog rt build-collect-env (collect the build environment, preparing for build publish)
  * jfrog rt build-publish (publish the build, only if autoPublishBuildInfo is true)
  * write_output (update the BuildInfo output resource with the published name/number)
  * jfrog rt build-scan (if forceXrayScan is true)
  * add_run_files (adds build info to run state)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
102,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **XrayScan** native step triggers a JFrog Xray scan for security vulnerabilities and license compliance. If there was a watch created that covers the selected build, Xray will scan the indexed build artifacts.
[/INST]##### YAML Schema
The YAML schema for XrayScan native step is as follows:
 **XrayScan**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: scan
            type: XrayScan
            configuration:
              #inherits all the tags from bash
              failOnScan:       <boolean>                      # default true
              inputResources:
                - name:         <BuildInfo resource>           # required
                  trigger: false
                      outputResources:
                - name:         <BuildInfo resource>           # optional
    
            execution:
              onStart:
                - echo ""Preparing for work...""
                - echo ""Prepping build environment""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `XrayScan` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify a BuildInforesource . The `buildName` and `buildNumber` are used to trigger the scan.
Required  
`outputResources`
May specify a BuildInforesource. The BuildInfo will be updated with the `buildName`and `buildNumber`of the input BuildInfo resource.
Optional  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`failOnScan`
When set to `true`, and when the Xray Policy Rule Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
 **execution**
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
### Note
`onExecute`, `onStart`, `onSuccess`, `onFailure`, and `onComplete` are reserved keywords. Using these keywords in any other context in your execution scripts can cause unexpected behavior.
##### Examples
The following examples show how to configure a XrayScan step.
###### Trigger an Xray Scan for Build Name and Number
Triggers an Xray scan for the build name and number provided by an input BuildInfo resource.
  * This example require a GitHub Integration and an Artifactory Integration.GitHub Integration
  * The Pipelines DSL for this example is available in t his repository in the JFrog GitHub account.


 **XrayScan**
[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: app_repo
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.gitIntegration }}                      
          path: {{ .Values.gitRepositoryPath }}    
          branches:
            include: master
    
      - name: app_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
      - name: app_promoted_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
    pipelines:
      - name: app_dev_pipeline
        steps:
          - name: app_build
            type: DockerBuild
            configuration:
              affinityGroup: docker_group
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}  
              dockerImageTag: ${run_number}
              inputResources:
                - name: app_repo
              integrations:
                - name: {{ .Values.artifactoryIntegration }}         
                
          - name: app_push
            type: DockerPush
            configuration:
              affinityGroup: docker_group
              targetRepository: {{ .Values.sourceRepository }}
              integrations:
                - name: {{ .Values.artifactoryIntegration }}                            
              inputSteps:
                - name: app_build
    
          - name: publish_app_build
            type: PublishBuildInfo
            configuration:
              affinityGroup: docker_group
              inputSteps:
                - name: app_push
              outputResources:
                - name: app_buildinfo
                
          - name: xrayScanStep
            type: XrayScan
            configuration:
              inputResources:
                - name: app_buildinfo 
[/YAML]
###### Update Output BuildInfo Resource
Updates an output BuildInfo resource that can be used as an input to future steps. Using an output resource means that you can be sure that those versions have been scanned when using that resource as an input.
 **XrayScan**
[YAML]
    pipelines: 
      - name: xrayScanPipeline
        steps:
          - name: xrayScanStep
            type: XrayScan
            configuration:
              inputResources:
                - name: inputBuildInfo
              outputResources:
                - name: outputBuildInfo
[/YAML]
##### How it Works
When you use the **XrayScan** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (if there is a BuildInfo output, configure the JFrog CLI with the credentials of the input BuildInfo resource)
  * jfrog rt use (specify the configured credentials to use)
  * jfrog rt build-scan (trigger a scan)
  * replicate_resource (if there is an output BuildInfo, copy the properties of the input BuildInfo resource to the output)
  * write_output (if there is an output BuildInfo, update the output BuildInfo resource buildName and buildNumber)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
103,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **Utility Functions** are built-in shell functions that can be used in steps to interact with the runtime environment.
Most utility functions are available in both Bash (Linux) and Powershell (Windows) OS runtimes.
[/INST]### See it Live
Click  **here**  to see some of these utility functions in action.
#### bump_semver
 **Description**
Increments the provided semver version with the given action.
 **Usage**
[YAML]
    bump_semver <semver string> <action>
    
    
[/YAML]
  * `semver string` is the semver version to be incremented
  * `action` is the type of increment to be applied


The valid actions are:
  * major: Increment the major version. The minor and patch versions are reset to 0.
  * minor: Increment the minor version. The patch version is reset to 0.
  * patch: Increment the patch version.
  * alpha: Increment or add an `alpha` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-alpha` and `v1.1.1-alpha` becomes `v1.1.1-alpha.1`. Any other pre-release tags will be removed.
  * beta: Increment or add a `beta` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-beta` and `v1.1.1-beta` becomes `v1.1.1-beta.1`. Any other pre-release tags will be removed.
  * rc: Increment or add an `rc` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-rc` and `v1.1.1-rc`becomes `v1.1.1-rc.1`. Any other pre-release tags will be removed.
  * final: Remove any pre-release tags, leaving `major.minor.patch`.


Click for examples
`ubuntu:~$ bump_semver v1.0.0 major`
`v2.0.0`
`ubuntu:~$ bump_semver v1.0.0 minor`
`v1.1.0`
`ubuntu:~$ bump_semver v1.0.0 patch`
`v1.0.1`
`ubuntu:~$ bump_semver v1.0.0 rc`
`v1.0.0-rc`
`ubuntu:~$ bump_semver v1.0.0-rc rc`
`v1.0.0-rc.1`
`ubuntu:~$ bump_semver v1.0.0 alpha`
`v1.0.0-alpha`
`ubuntu:~$ bump_semver v1.0.0 beta`
`v1.0.0-beta`
`ubuntu:~$ bump_semver v1.0.0-rc.12 final`
`v1.0.0`
`ubuntu:~$ bump_semver v1. rc`
`error: Invalid semantic version given in the argument.`
`ubuntu:~$ bump_semver v1.0.0 badaction`
`error: Invalid action given in the argument.`
#### replace_envs
 **Description**
Replaces variables in a file with values based on your current shell `env`. This is useful to create config files from templates, for example.
If the file contains placeholders that are not defined in the environment, they will become empty strings (“”). The original file is overwritten with the modified file.
 **Usage**
[YAML]
    replace_envs <filename1> <filename2> <filenameN>
    
                         
[/YAML]
where your files have placeholders in the format `$ENVIRONMENT_VARIABLE_NAME` or `${ENVIRONMENT_VARIABLE_NAME}`.
Click for examples
`replace_envs properties.json deploy.json run.json`
#### retry_command
 **Description**
Execute any command up to three times if it returns a non-zero error code. This is useful when you need to execute a command that can be flaky as a result of network hiccups, for example.
 **Usage**
Shell
Function  
Bash
`retry_command <shell command>`  
PowerShell
`retry_command <shell command>`  
  * `shell command `is the command to be retried


Click for Bash example
`retry_command docker push mydirectory/myImage`
Click for Powershell example
`retry_command docker push mydirectory/myImage`
#### get_uuid
 **Description**
Puts a uuid to stdout. Uses `/proc/sys/kernel/random/uuid` if available and falls back to `uuidgen` if not. The function calls `exit 1` if neither of these are available.
 **Usage**
Shell
Function  
Bash
`get_uuid`  
Click for Bash example
`my_uuid=$(get_uuid) `
#### save_artifact_info
 **Description**
Saves metadata about an artifact. When saved, this metadata is used to enable signed pipelines for the artifacts.
 **Usage**
Shell
Function  
Bash
`save_artifact_info <artifact type> <file path> [--build-name <build name> --build-number <build number> --project-key <project key>]`  
PowerShell
`save_artifact_info <artifact type> <file path> [-build-name <build name> -build-number <build number> -release-bundle-name <name> -release-bundle-version <version> -project-key <project key>]`  
  * `artifact type` : This is the type of artifact. Either `file`, `buildInfo`, or `releaseBundle`.
  * `file path` : This is the path to the metadata file to be saved.


Bash
PowerShell
Description  
 **\--build-name**
 **-build-name**
This is name of the build. Required when artifact type is `buildInfo`.  
 **\--build-number**
 **-build-number**
This is number of the build. Required when artifact type is `buildInfo`.  
 **\--release-bundle-name**
 **-release-bundle-name**
This is name of the Release Bundle. Required when artifact type is `releaseBundle`.  
 **\--release-bundle-version**
 **-release-bundle-version**
This is version of the Release Bundle. Required when artifact type is `releaseBundle`.  
 **\--project-key**
 **-project-key**
(optional) Defaults to the environment's `project_key`. Can be specified to save info for a different project.  
Click for Bash example
`save_artifact_info 'buildInfo' './myBuildDetailedSummary.json' --build-name myBuild --build-number 42 `
`save_artifact_info 'file' './myFileDetailedSummary.json' `
Click for PowerShell example
`save_artifact_info buildInfo myBuildDetailedSummary.json -build-name myBuild -build-number 42 `
`save_artifact_info file myFileDetailedSummary.json `
`save_artifact_info releaseBundle myFileDetailedSummary.json -release-bundle-name myBundle -release-bundle-version 1 `
#### validate_artifact
 **Description**
Validates the signature of an artifact. Requires signed pipelines to be enabled.
 **Usage**
Shell
Function  
Bash
`validate_artifact <artifact type> <file path> [--build-name <build name> --build-number <build number> --project-key <project key>]`  
PowerShell
`validate_artifact <artifact type> <file path> [-build-name <build name> -build-number <build number> -project-key <project key>]`  
  * `artifact type`: This is the type of artifact. Supports `buildinfo`.
  * `file path`: This is the path to the metadata file to be validated.


Bash
PowerShell
Description  
 **\--build-name**
 **-build-name**
This is name of the build. Required when artifact type is `buildInfo`.  
 **\--build-number**
 **-build-number**
This is number of the build. Required when artifact type is `buildInfo.`  
 **\--project-key**
 **-project-key**
(optional) Defaults to the environment's `project_key`. Can be specified to validate info for a different project.  
Click for Bash example
`validate_artifact 'buildInfo' './myBuildDetailedSummary.json' --build-name myBuild --build-number 42 `
Click for PowerShell example
`validate_artifact buildInfo myBuildDetailedSummary.json -build-name myBuild -build-number 42 `
#### configure_jfrog_cli
 **Description**
Configures the JFrog CLI (version 1 or 2 specified as `jfrogCliVersion` in the  pipeline configuration) with the provided credentials, handling the different formats for different minor versions. Artifactory integrations listed in the integrations section of the step will be automatically configured, but this may be useful for resources or if the credentials are provided to the step in another way. When using v2, a non-default Xray URL may also be specified using the environment variable JFROG_XRAY_URL.
 **Usage**
Bash
`configure_jfrog_cli --artifactory-url <url> [--xray-url <url> --user USER --apikey <key> --access-token <token> --server-name <name>]`  
PowerShell
`configure_jfrog_cli -artifactory-url <url> [-xray-url <url> -user USER -apikey <key> -access-token <token> -server-name <name>]`  
  * `artifactory-url` : Required. The Artifactory URL.
  * `xray-url` : Optional. The Xray URL. Only used with CLI v2.
  *  **user** : The user. Required when an API key is provided.
  * `apikey` : An API key. Requires --user and may not be used with --access-token.
  * `access-token` : An Access token. May not be used with --access-token or --user.
  * `server-name` : Defaults to `default`. Can be specified to configure the CLI with that name.


Click for Bash example
`configure_jfrog_cli --artifactory-url  https://my-artifactory.local/artifactory --user me --apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R --server-name myAPIKeyArtifactory`
`configure_jfrog_cli --artifactory-url  https://my-artifactory.local/artifactory --access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg --server-name myAccessTokenArtifactory`
Click for PowerShell example
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -user me -apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R -server-name myAPIKeyArtifactory`
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg -server-name myAccessTokenArtifactory`
#### check_xray_available
 **Description**
With JFrog CLI v2 (version 2 specified as `jfrogCliVersion` in the  pipeline configuration), checks that Xray is available for the specified CLI configuration, or the default configuration if none is specified.
 **Usage**
Bash
`check_xray_available [--server-name <name>]`  
PowerShell
`check_xray_available [-server-name <name>]`  
  * `server-name` : Optional. Specifies a JFrog CLI configuration to check.


Click for Bash example
`check_xray_available --server-name myArtifactoryIntegration`
Click for PowerShell example
`check_xray_available --server-name myArtifactoryIntegration`
#### cleanup_jfrog_cli
 **Description**
Removes configuration for the JFrog CLI (v1), handling the different formats for different minor versions. Artifactory integrations listed in the integrations section of the step will be automatically removed at the end of the step, but this may be useful to remove the credentials earlier or when using `configure_jfrog_cli`.
 **Usage**
Bash
`cleanup_jfrog_cli [--server-name <name>]`  
PowerShell
`cleanup_jfrog_cli [-server-name <name>]`  
  * `server-name` : Defaults to `default`. Can be specified to remove that configuration.


Click for Bash example
`cleanup_jfrog_cli --server-name myArtifactory`
Click for PowerShell example
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -user me -apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R -server-name myAPIKeyArtifactory`
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg -server-name myAccessTokenArtifactory`
#### set_trigger_payload
 **Description**
Adds one or more key=value pairs as custom step or pipeline variables to a JSON payload that can be used to call the pipelines trigger API.
Multiple key=value pairs can be given in a single command or can be split across multiple commands.
If a key is added twice, the original value will be replaced.
 **Usage**
Bash
`set_trigger_payload [stepVariables|pipelineVariables] <key value pairs>`  
PowerShell
Not currently supported in PowerShell  
  * `stepVariables`: These key value pairs will be present in the environment of the triggered step
  *  **pipelineVariables** : These key value pairs will be present in the environment of every step in the run that is triggered.


Click for Bash example
`set_trigger_payload stepVariables ""shouldScan=true"" ""notify=email""`
`set_trigger_payload pipelinesVariables ""releaseVersion=1.2.3""`
`set_trigger_payload pipelinesVariables ""collectData=true""`
#### get_trigger_payload
 **Description**
Prints on stdout a payload that can be used with the pipelines trigger API.
Use `set_trigger_payload` to add additional values to the payload.
 **Usage**
Bash
`get_trigger_payload `  
PowerShell
Not currently supported in PowerShell  
Click for Bash example
curl -XPOST -H ""authorization: Bearer <token>"" -H ""content-type: application/json"" <endpoint>/pipelines/api/v1/pipelineSteps/:id/trigger -d ""$(get_trigger_payload)""
#### end_step
 **Description**
Stops execution of the onStart or onExecute section and immediately continues to the onSuccess, onFailure, or onComplete section setting the specified status. If ""success"" is provided, the onSuccess and onComplete sections will be executed. For ""failure,"" the onFailure and onComplete sections will be executed. And for ""skipped,"" only the onComplete section. The end_step utility function may only be called in the onStart and onExecute sections and the only statuses supported are success, failure, and skipped.
 **Usage**
Bash
`end_step [success|failure|skipped]`  
PowerShell
`end_step [success|failure|skipped]`  
Click for Bash example
`end_step success`
Click for PowerShell example
`end_step success`
#### update_run_description
 **Description**
Provide dynamic description for a run, which will be shown in the UI. This is useful for providing more context about a run. Every step in a pipeline can include a run description.
 **Usage**
Bash
`update_run_description ""description""`  
PowerShell
`update_run_description ""description""`  
Click for Bash example
`update_run_description ""$msg""`
Click for PowerShell example
`update_run_description ""$msg""`
Full YAML Example
[YAML]
    name: UpdateRunDescription
    steps:
            - name: update_l
              type: Bash
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
            - name: update_2
              type: Bash
              configuration:
                    inputSteps:
                            - name: update_l
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
[/YAML]
#### set_run_name
 **Description**
Provide a unique, dynamic name for a run, which will be shown in the UI. This is useful for providing more context about a run. Unlike run description, run name is unique for a run.
In the UI, run name appears in the Run View:  
Run name appears also appears in Pipeline of Pipelines view and Active Board view:  
 **Usage**
Bash
`set_run_name ""1.0.1""`  
PowerShell
`set_run_name ""1.0.1""`  
Click for Bash example
`set_run_name ""1.0.1""`
Click for PowerShell example
`set_run_name ""1.0.1""`
Full YAML Example
[YAML]
    name: UpdateRunDescription
    steps:
            - name: update_l
              type: Bash
              execution:
                    onStart:
                            - set_run_name ""1.0.1""
            - name: update_2
              type: Bash
              configuration:
                    inputSteps:
                            - name: update_l
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
[/YAML]
#### Source Control
#### Test Reports
##### save_tests
 **Description**
Copies test reports given as input to later be parsed and uploaded (if file storage is available).
 **Usage**
`save_tests <file or directory>`
  * `file` or `directory`specifies either a filename for the test report file, or a directory name for a directory of test report files


Click for examples
`save_tests testreport.xml`
#### Encryption
##### encrypt_string
 **Description**
Uses the provided public key to encrypt the specified string.
 **Usage**
Shell
Function  
Bash
`encrypt_string --key <path> <source string> `  
PowerShell
`encrypt_string -key <path> <source string>`  
  * `key` is the fully qualified path of the public key file
  * `source string` is the string to be encrypted


Click for Bash examples
`ubuntu:~$ encrypt_string --key pub.pem""admin:passw0rd"" LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+ 06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6t dVK0X/5pCTWBKk5nIvw=`
Click for Powershell examples
`PS C:\Users\windowsuser> encrypt_string ""admin:passw0rd"" -key pub.pem LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+ 06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6t dVK0X/5pCTWBKk5nIvw=`
##### decrypt_string
 **Description**
Uses the provided private key to decrypt the specified string.
This is typically used to decrypt information that was encrypted using `encrypt_string` with the corresponding public key. It helps you avoid building your own encrypt-decrypt system.
 **Usage**
Shell
Function  
Bash
`decrypt_string --key <path> <encrypted string>`  
PowerShell
`decrypt_string -key <path> <encrypted string> `  
  * `key` is the fully qualified path of the private key file
  * `encrypted string` is the string to be decrypted


Click for Bash examples
ubuntu:~$ decrypt_string --key key.pem LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=
`admin:passw0rd`
`ubuntu:~$ decrypt_string LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=`
`decrypt_string: ERROR - Key file not found`
Click for Powershell examples
`PS C:\Users\windowsuser> decrypt_string -key key.pem LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=`
`admin:passw0rd`
`PS C:\Users\windowsuser> decrypt_string LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=`
`key is mandatory, please provide a value.`
##### encrypt_file
 **Description**
Uses the provided public key to encrypt the specified file to a new file.
 **Usage**
Shell
Function  
Bash
`encrypt_file --key <path> [--output <filename>] <source filename> `  
PowerShell
`encrypt_file -key <path> [-output <filename>] <source filename>`  
  * `key` is the fully qualified path of the public key file
  * `output` is the name of the resulting encrypted file. Defaults to “encrypted”
  * `source filename` is the file to be decrypted


Click for Bash examples
`ubuntu:~$ cat secrets.json {""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""} ubuntu:~$ encrypt_file secrets.json --key pub.pem encrypt_file: Encrypting secrets.json using key pub.pem encrypt_file: Encrypted secrets.json to encrypted ubuntu:~$ cat encrypted dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+ 5xHBCvlenuFanIFLDbo=`
Click for Powershell examples
`PS C:\Users\windowsuser> type secrets.json {""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""} PS C:\Users\windowsuser> encrypt_file secrets.json -key pub.pem encrypt_file: Encrypting secrets.json using key pub.pem encrypt_file: Encrypted secrets.json to encrypted PS C:\Users\windowsuser> type encrypted dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+ 5xHBCvlenuFanIFLDbo=`
##### decrypt_file
 **Description**
Uses the provided private key to decrypt the specified file to a new file.
This is typically used to decrypt information that was encrypted using `encrypt_file` with the corresponding public key. It helps you avoid building your own encrypt-decrypt system.
 **Usage**
Shell
Function  
Bash
`decrypt_file --key <path> [--output <filename>] <source filename>`  
PowerShell
`decrypt_file -key <path> [-output <filename>] <source filename> `  
  * `key` is the fully qualified path of the private key file
  * `output` is the name of the resulting decrypted file. Defaults to “decrypted”
  * `source filename` is the file to be decrypted


Click for Bash examples
`ubuntu:~$ cat encrypted`
`dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ`
`YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+`
`5xHBCvlenuFanIFLDbo=`
`ubuntu:~$ decrypt_file encrypted --key key.pem`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to decrypted`
`ubuntu:~$ cat decrypted`
`{""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""}`
`ubuntu:~$ decrypt_file encrypted --key key.pem --output secrets.json`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to secrets.json`
Click for Powershell examples
`PS C:\Users\windowsuser> type encrypted`
`dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ`
`YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+`
`5xHBCvlenuFanIFLDbo=`
`PS C:\Users\windowsuser> decrypt_file encrypted -key key.pem`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to decrypted`
`PS C:\Users\windowsuser> type decrypted`
`{""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""}`
`PS C:\Users\windowsuser> decrypt_file encrypted -key key.pem -output secrets.json`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to secrets.json`
#### Notifications
##### send_notification
 **Description**
Utilizes notification integration to send custom messages at any time during the build to any recipient.
For more information, see Sending Notifications from Pipelines.
 **Usage**
[YAML]
    send_notification <integration> [options]
    
    
[/YAML]
The options can be specified as part of the command, or defined as environment variables before the command is issued.
The command line arguments take priority over the environment variables.
Click for Bash examples
`- send_notification pipelines_mqtest_slack --text ""Pipeline - $pipeline_name | Step - $step_name \n Committer - <$committer> ""set_payloadmyExternalHookResource""{\""test\"":\""payload\""}""`
###### AirBrake
Creates an AirBrake deployment through an Airbrake Integration. Not supported in PowerShell.
Bash
Option Description  
 **\--project-id**
the project ID to send the notification for  
 **\--environment**
the environment value to use when posting the deployment  
 **\--username**
used when posting an AirBrake deployment  
 **\--email**
the email to be used when posting the AirBrake deployment  
 **\--repository**
the repository to use when posting the AirBrake deployment  
 **\--revision**
the deployment revision  
 **\--version**
the version to use when posting the AirBrake deployment  
 **\--type**
currently only type “deploy” is supported  
 **\--description**
description of the deployment  
 **\--payload**
path to a valid JSON file that contains a payload to use to POST the AirBrake deployment  
###### Jira
Creates a Jira issue (also known as a ticket).
Bash
PowerShell
Option Description  
 **\--project-id**
 **-project-id**
the Project Key of the project to associate the new issue with. The project key is the short string that begins all issue numbers for the project (e.g., ""EXAMPLE-1234"")  
 **\--type**
 **-type**
the issue type for the new issue (e.g., ""Bug"", ""Task"", etc.). This string must be one of the recognized Jira issue types  
 **\--summary**
 **-summary**
a string for the new issue's Summary field (it's title)  
 **\--description**
 **\--description**
(optional) a string for the new issue's Description field  
 **\--attach-file**
 **\--attach-file**
(optional) a path to a file that you’d like to attach to the issue  
###### NewRelic
Creates a NewRelic deployment through a NewRelic Integration. Not supported in PowerShell.
Bash
Option Description  
 **\--type**
the type of object to be posted. At the moment, only “deployment” is supported  
 **\--description**
description of the deployment  
 **\--username**
the user recording the deployment. Defaults to “JFrog Pipelines”  
 **\--changelog**
the changelog value to use in the deployment  
 **\--revision**
the deployment revision (required)  
 **\--appId**
the ID of the app being deployed. If not provided, --appName must be present  
 **\--appName**
the name of the app being deployed. If not provided, --appId must be present  
 **\--payload**
path to a valid JSON file that contains a payload to use to POST the NewRelic deployment  
###### PagerDuty Events
Sends an event through a PagerDuty Events Integration.
Bash
PowerShell
Option Description  
 **\--text**
 **-text**
The main text to display in the event on PagerDuty.  
Slack
Sends a message on Slack through a Slack Integration.
Bash
PowerShell
Option Description  
 **\--payload**
 **-payload**
(optional) A path to a valid json file to act as the payload of the message. If a payload is provided, all other parameters are ignored. This payload is directly sent to Slack, so please view the Slack API documentation for information on how the payload should be formatted.  
 **\--username**
 **-username**
(optional) shows in the heading of the Slack message  
 **\--pretext**
 **-pretext**
(optional) a string that becomes the first part of the Slack message. Defaults to current date/time  
 **\--text**
 **-text**
(optional) the main text to display in the message.  
 **\--color**
 **-color**
(optional) hex string that changes the color of the status bar to the left of the Slack message.  
 **\--recipient**
 **-recipient**
(optional) the target of the message. Should start with “@” or “#” for user or channel, respectively.  
 **\--icon-url**
 **-icon-url**
(optional) the url of the icon to show next to the message  
###### smtpCreds (email)
Sends an email through an SMTP Credentials Integration.
Bash
PowerShell
Option Description  
 **\--recipients**
 **-recipients**
one or more email addresses  
 **\--subject**
 **-subject**
(optional) add a message to the subject. Does not replace the default subject  
 **\--body**
 **-body**
(optional) specify some text to add to the body of the email. Does not replace the existing body information  
 **\--status**
 **-status**
(optional) can be set to a valid status string. By default it will be set based on the section of scrip the command is executed in.  
 **\--attachments**
 **-attachments**
(optional) a list of files to attach to the email. Combined total of all files cannot exceed 5MB  
 **\--attach-logs**
 **-attach-logs**
(optional) ‘true’ or ‘false’. Defaults to false. All available logs for the step will be attached to the email. Note that it can only attach logs that have already been created, so using this option in the onStart section, for example, would not have very detailed logs.  
 **\--show-failing-commands**
 **-show-failing-commands**
(optional) 'true’ or ‘false’. Defaults to false. The existing logs for the step will be parsed. Any failed command that is detected will be added to the body of the email, along with up to 100 preceding lines (if printed from the same command)  
Environment Options
All of the above options can also be included as environment variables instead of arguments. The command line argument will have priority over the environment. Here is the full list of ENVs:
  * `NOTIFY_USERNAME` (--username/-username)
  * `NOTIFY_PASSWORD` (--password/-password)
  * `NOTIFY_RECIPIENT` (--recipient/-recipient)
  * `NOTIFY_PRETEXT` (--pretext/-pretext)
  * `NOTIFY_TEXT` (--text/-text)
  * `NOTIFY_COLOR` (--color/-color)
  * `NOTIFY_ICON_URL` (--icon-url/-icon-url)
  * `NOTIFY_PAYLOAD` (--payload/-payload)
  * `NOTIFY_TYPE` (--type/-type)
  * `NOTIFY_PROJECT_ID` (--project-id/-project-id)
  * `NOTIFY_ENVIRONMENT` (--environment/-environment)
  * `NOTIFY_REVISION` (–revision/-revision)
  * `NOTIFY_SUMMARY` (--summary/-summary)
  * `NOTIFY_ATTACH_FILE` (--attach-file/-attach-file)
  * `NOTIFY_REPOSITORY` (--repository/-repository)
  * `NOTIFY_EMAIL` (–email--email/-email)
  * `NOTIFY_STATUS` (--status/-status)
  * `NOTIFY_VERSION` (--version/-version)
  * `NOTIFY_CHANGELOG` (--changelog/-changelog)
  * `NOTIFY_DESCRIPTION` (–description--description/-description)
  * `NOTIFY_ATTACHMENTS` (–attachments--attachments/-attachments)
  * `NOTIFY_ATTACH_LOGS` (--attach-logs/-attach-logs)
  * `NOTIFY_SHOW_FAILING_COMMANDS` (--show-failing-commands/-show-failing-commands)
  * `NOTIFY_SUBJECT` (--subject/-subject)
  * `NOTIFY_BODY` (–body)


#### JSON
##### set_payload
 **Description**
Sets an optional JSON payload (string or file) for an OutgoingWebhook resource. When the OutgoingWebhook is specified in a step's `outputresources` the payload is sent when the step is complete.
 **Usage**
Shell
Function  
Bash
`set_payload <resource> <payload> [--file]`  
PowerShell
`set_payload <resource> <payload> [-file]`  
  * `resource` is the name of an OutgoingWebhook resource.
  * `payload` is a JSON string or file to attach to the resource that will be sent as part of the outgoing webhook. A file can be specified as a path relative to the current directory, absolute path, or path relative to the step workspace directory.
  * `file` option specifies that the `payload` parameter is a file. If not specified, `payload` will be processed as a string.


Click for Bash examples
`set_payloadmyExternalHookResource""{\""test\"":\""payload\""}""`
`echo""{\""test\"":\""payload\""}""> testpayload.json set_payload myExternalHookResource testpayload.json --file`
Click for Powershell examples
`set_payloadmyExternalHookResource""{`""test`"":`""payload`""}""`
`Set-Content -Path testpayload.json -Value ""{`""test`"":`""payload`""}"" set_payload myExternalHookResource testpayload.json -file`
##### read_json
 **Description**
Extracts the json property value from the specified file.
This simplifies handling of a JSON file to read specific property values that are required for your workflow.
### Not supported in PowerShell
In PowerShell, `ConvertTo-Json` is suggested as an alternative.
 **Usage**
[YAML]
    read_json <path to file> <field name>
    
    
[/YAML]
  * `path to file` is the fully qualified path of the JSON file
  * field name is the field for which you want to read the value. Use dot notation and [n] for arrays.


Click for examples
ubuntu:~$ cat secrets.json
{
""TOP_SECRET_CONFIDENTIAL_FORMULA"":""uuddlrlrbas"",
""LESSER_SECRET_FORMULA"":""dyddy"",
""TOP_5_PASSWORDS_LIST"": [
""admin"",
""passw0rd"",
""testing123"",
""correcthorsebatterystaple""
],
“nesting”: {
“is”: {
“fun”: “yay”
}
}
}
ubuntu:~$ read_json secrets.json""LESSER_SECRET_FORMULA""
dyddy
ubuntu:~$ read_json secrets.json""TOP_5_PASSWORDS_LIST[3]""
correcthorsebatterystaple
ubuntu:~$ read_json secrets.json "" nesting.is.fun""
yay
#### Resources
##### replicate_resource
 **Description**
This command takes an input resource and creates an exact copy. This helps you to transfer metadata from one step to the next.
 **Usage**
Shell
Function  
Bash
`replicate_resource <from_resource> <to_resource> [--options]`  
PowerShell
`replicate_resource <from_resource> <to_resource> [-options] `
[/YAML]
[YAML]  * `from_resource` is the name of the `inputResources` resource that you're copying from.  
  * `to_resource` is the name of the `outputResources` resource that will receive the replicated data from the `from_resource`.Any pre-existing files or key-value pairs in the `to_resource` will be replaced.
  * `match-settings` option should be set when you want the replication to adhere to any branch/tag settings in the `to_resource`. For example, If your `from_resource` gitRepo can trigger on both commits and pull requests, but you only want to update your `to_resource` on commits, you can replicate with --match-settings, and the `to_resource` will only be updated when the `from_resource` had a commit.


Click for Bash examples
`replicate_resource myRepo1 myRepo2 --match-settings replicate_resource myTestImage myStageImage`
Click for Powershell examples
`replicate_resource myRepo1 myRepo2 -match-settings replicate_resource myTestImage myStageImage`
##### write_output
 **Description**
Adds data to an output resource in the form of key/value pairs that will become properties of the resource.
 **Usage**
Bash
`write_output <resource> <key value pair>... [--overwrite]`  
PowerShell
`write_output <resource> <key value pair>... [-overwrite]`  
  * `resource` ``is the resource to update
  * `key value pair` is a single string with a key and a value, separated by an “=”. Multiple of these strings can be supplied as input. A value with spaces should be surrounded by quotes.


Bash
Powershell
Option Description  
 **\--overwrite**
 **-overwrite**
If supplied, all key value pairs will be replaced.  
Click for examples
`write_output myImageimageTag=master sha=$commitSha description=\""hello world\""`
The newly attached properties can be accessed as environment variables of the form `res_{Resource Name}_{Key Name}`.
For example, the above created properties can be accessed as these environment variables:
[YAML]
    $ printenv res_myImage_master
    master
    $ printenv res_myImage_sha
    d6cd1e2bd19e03a81132a23b2025920577f84e37
    $ printenv res_myImage_description
    ""hello world""
[/YAML]
#### Caching
Caching helps you speed up execution of your steps by preserving and restoring packages and dependencies between runs of a step. In this way, you can reduce build times by avoiding repeating the installation or loading of large dependencies.
##### add_cache_files
 **Description**
Copies files given as input to later be uploaded if file storage is available.
For more information about using this utility function, see Caching Step Runtimes.
 **Usage**
`add_cache_files <file or directory> <name>`
  * `file or directory` is a file or directory to store in the cache
  * `name` is a name to give the stored file or directory (without spaces)


Click for examples
`add_cache_files cachefile.txt my_file`
`add_cache_files directory/subdirectory my_directory`
##### restore_cache_files
 **Description**
Copies stored cache (if file storage is available) to the specified location. No error will occur if nothing is available for <name> in the cache.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`restore_cache_files <name> <path>`
  * `name` is the name the file or driectory to be restored was given when cached.
  * `path` is a path at which to place the file or directory.


Click for examples
`restore_cache_files my_file cachefile.txt`
`restore_cache_files my_directory directory/subdirectory`
#### Run State Management
##### add_run_variables
 **Description**
Allows you to add environment variables that will be available in the following steps of the run.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


 **Usage**
`add_run_variables <key value pair>...`
  * `key value pair` ``is a single string with a key and a value, separated by an “=”. Multiple of these strings can be supplied as input. Each value will be exported as an environment variable at the time this command is used and automatically in any later steps within the run.


Click for examples
add_run_variables imageTag=""master""
##### export_run_variables
 **Description**
Sources the file containing the run variables. This will be done automatically, but may also be used to “reset” the environment variables in the current step.
 **Usage**
`export_run_variables`
Click for examples
`export_run_variables`
##### add_run_files
 **Description**
Copies files given as input into the run state for use in later steps in the run, if file storage is available.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`add_run_files <file or directory> <name>`
`file or directory` is a file or directory to store in the run state
`name` is a name to give the stored file or directory (without spaces). This cannot be run.env.
Click for examples
`add_run_files cachefile.txt my_file`
`add_run_files directory/subdirectory my_directory`
`add_run_files directory/*/subdirectory my_directory`
##### restore_run_files
 **Description**
Copies files stored in the run state (if file storage is available) to the specified location. No error will occur if nothing is available for <name> in the run state.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`restore_run_files <name> <path>`
  * `path` is the name the files to be restored were given when added to the run state.
  * `file or directory` ``is a path at which to place the file or files.


Click for examples
`restore_run_files my_file cachefile.txt`
`restore_run_files my_directory directory/subdirectory`
#### Affinity Group State Management
##### add_affinity_group_files
 **Description**
Copies files or directories given as input into the affinity group workspace for use in later steps in the affinity group. Files or directories may be specified using wildcards (*) or as multiple input parameters preceding the name under which the files will be stored.
 **Usage**
`add_affinity_group_files <file or directory> <name>`
`file or directory` is a file or directory to store in the run state.
`name` ``is a name to give the stored file or directory (without spaces).
Click for examples
`add_affinity_group_files cachefile.txt my_file`
`add_affinity_group_files myDirectory myOtherDirectory/subdirectory my_directories`
`add_affinity_group_files myDirectory/*.txt my_text_files`
`add_affinity_group_files ../adjacentDirectory my_adjacent_directory`
##### restore_affinity_group_files
 **Description**
Copies files stored in the affinity group workspace by `add_affinity_group_files` to the specified location. The restore behavior differs from the other state restore functions to better handle wildcard patterns with a variable number of matching files or directories. With `restore_affinity_group_files`, the relative path specified in the `add_affinity_group_files` function will be preserved (excluding any traversal to parent directories) and the path specified should be the base for that relative path. The following examples are to symmetrically restore the files saved in the examples above for `add_affinity_group_files.` No error will occur if no files were saved for <name>.
 **Usage**
`restore_affinity_group_files <name> <path>`
  * `name` is the name the files to be restored were given when storing the files with `add_affinity_group_files`.
  * `path` ``is a path at which to place the file or files.


Click for examples
`restore_affinity_group_files my_file .`
`restore_affinity_group_files my_directories .`
`restore_affinity_group_files my_text_files .`
`restore_affinity_group_files my_adjacent_directory ..`
#### Pipeline State Management
##### add_pipeline_variables
 **Description**
Allows you to add environment variables that will be available in the following steps of the run and in future runs. These variables may be overridden by another variable with the same key added to the current run.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


 **Usage**
`add_pipeline_variables <key value pair>...`
  * `key value pair` ``is a single string with a key and a value, separated by an “=”. Multiple of these strings can be supplied as input. Each value will be exported as an environment variable at the time this command is used and automatically in any steps that start after this run is complete.


Click for examples
`add_pipeline_variables imageTag=""master""`
`add_pipeline_variables imageName=""myimage"" imageTag=""master""`
##### export_pipeline_variables
 **Description**
Sources the file containing the pipeline variables. This will be done automatically, but may also be used to “reset” the environment variables in the current step.
 **Usage**
`export_pipeline_variables`
Click for examples
`export_pipeline_variables`
##### add_pipeline_files
 **Description**
Copies files given as input into the pipeline state for use in later steps in the run and future runs, if file storage is available.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`add_pipeline_files <file or directory> <name>`
  * `file or directory` is a file or directory to store in the pipeline state.
  * `name` is a name to give the stored file or directory (without spaces). This cannot be pipeline.env.


Click for examples
`add_pipeline_files cachefile.txt my_file`
`add_pipeline_files directory/subdirectory my_directory`
##### restore_pipeline_files
 **Description**
Copies files stored in the pipeline state (if file storage is available) to the specified location. No error will occur if nothing is available for <name> in the run state.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`restore_pipeline_files <name> <path>`
  * `name` is the name the file to be restored was given when added to the pipeline state.
  * `path` is a path at which to place the file or files.


Click for examples
`restore_pipeline_files my_file cachefile.txt`
`restore_pipeline_files my_directory directory/subdirectory`
#### Step Properties
##### find_resource_variable
 **Description**
Retrieves the value of the named property of a resource.
 **Usage**
`find_resource_variable <resourceName> <propertyName>`
  * `resourceName` is the name of the resource.
  * `propertyName` is the name of the resource property whose value to retrieve.


Click for examples
`find_resource_variable myGitHub commitSha`
`find_resource_variable myImage imageTag`
##### get_integration_name
 **Description**
Retrieves the name of the first integration found of the type specified. Available to extension steps to get the name of the first input integration of a particular type.
 **Usage**
`get_integration_name --type <integration type>`
  * `integration type` is the name of an Pipelines Integration type


Click for examples
`get_integration_name --type Slack`
`get_integration_name --type ""Docker Registry""`
##### get_resource_name
 **Description**
Retrieves the name of the first resource found of the type specified in `inputResources` or `outputResources`. Available to extension steps to get the name of the first input or output resource of a particular type.
 **Usage**
get_resource_name --type <resource type> \--operation <IN | OUT> \--syntax-version <semver>
  * `resource type` is the name of a Pipelines Resource type
  * `IN | OUT` selects whether the resource is named in `inputResources` or `outputResources`
  * `semver` is the semantic version number of the resource's syntax version


Click for examples
`get_resource_name --type GitRepo --operation IN`
get_resource_name --type BuildInfo --operation OUT
`get_resource_name --type GitRepo --operation IN --syntax-version 1.5.0`
##### get_resource_names
 **Description**
Retrieves an array of names of the type specified in `inputResources` or `outputResources`. Available to extension steps to get the names of input or output resource of a particular type.
 **Usage**
get_resource_names --type <resource type> \--operation <IN | OUT> \--syntax-version <semver>
  * `resource type` is the name of a Pipelines Resource type
  * `IN | OUT` selects whether the resource is named in `inputResources` or `outputResources`
  * `semver` is the semantic version number of the resource's syntax version
  * In PowerShell, a native PowerShell array is returned. In Bash, a JSON array is returned that can be handled with jq.


Click for examples
`get_resource_names --type GitRepo --operation IN`
get_resource_names --type BuildInfo --operation OUT
`get_resource_names --type GitRepo --operation IN --syntax-version 1.5.0`
##### get_affinity_group_step_names
 **Description**
Retrieves a JSON array of names of steps of the type specified in the current affinity group.
 **Usage**
`get_affinity_group_step_names [--type <step type>]  [--syntax-version <semver>] [--namespace <namespace>]find_step_configuration_value <propertyName>`
  * `type` is used to specify the type of the steps to be found. If not specified, steps of all types will be returned.
  * `syntax-version` is used to specify the type of the steps to be found. If not specified, steps of all types will be returned.
  * `namespacetype` is used with extension steps to specify the namespace of the steps to be found.


Click for examples
`get_affinity_group_step_namesfind_step_configuration_value forceXrayScan`
`get_affinity_group_step_names --type GitRepo`
`get_affinity_group_step_names --type MyExtension --syntax-version 1.5.0`
`get_affinity_group_step_names --type MyExtension --namespace myNamespace --syntax-version 1.5.0`
##### find_step_configuration_value
 **Description**
Retrieves the value of the `configuration` property for the currently executing step. If the property is a collection, the first value will be returned. Available to extension steps to get the value of a configuration.
 **Usage**
`find_step_configuration_value <propertyName>`
  * `propertyName` is the name of the step's configuration property whose value to retrieve


Click for examples
`find_step_configuration_value forceXrayScan`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
104,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Increments the provided semver version with the given action.
 **Usage**
[YAML]
    bump_semver <semver string> <action>
    
    
[/YAML]
  * `semver string` is the semver version to be incremented
  * `action` is the type of increment to be applied


The valid actions are:
  * major: Increment the major version. The minor and patch versions are reset to 0.
  * minor: Increment the minor version. The patch version is reset to 0.
  * patch: Increment the patch version.
  * alpha: Increment or add an `alpha` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-alpha` and `v1.1.1-alpha` becomes `v1.1.1-alpha.1`. Any other pre-release tags will be removed.
  * beta: Increment or add a `beta` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-beta` and `v1.1.1-beta` becomes `v1.1.1-beta.1`. Any other pre-release tags will be removed.
  * rc: Increment or add an `rc` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-rc` and `v1.1.1-rc`becomes `v1.1.1-rc.1`. Any other pre-release tags will be removed.
  * final: Remove any pre-release tags, leaving `major.minor.patch`.


Click for examples
`ubuntu:~$ bump_semver v1.0.0 major`
`v2.0.0`
`ubuntu:~$ bump_semver v1.0.0 minor`
`v1.1.0`
`ubuntu:~$ bump_semver v1.0.0 patch`
`v1.0.1`
`ubuntu:~$ bump_semver v1.0.0 rc`
`v1.0.0-rc`
`ubuntu:~$ bump_semver v1.0.0-rc rc`
`v1.0.0-rc.1`
`ubuntu:~$ bump_semver v1.0.0 alpha`
`v1.0.0-alpha`
`ubuntu:~$ bump_semver v1.0.0 beta`
`v1.0.0-beta`
`ubuntu:~$ bump_semver v1.0.0-rc.12 final`
`v1.0.0`
`ubuntu:~$ bump_semver v1. rc`
`error: Invalid semantic version given in the argument.`
`ubuntu:~$ bump_semver v1.0.0 badaction`
`error: Invalid action given in the argument.`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
105,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Replaces variables in a file with values based on your current shell `env`. This is useful to create config files from templates, for example.
If the file contains placeholders that are not defined in the environment, they will become empty strings (“”). The original file is overwritten with the modified file.
 **Usage**
[YAML]
    replace_envs <filename1> <filename2> <filenameN>
    
                         
[/YAML]
where your files have placeholders in the format `$ENVIRONMENT_VARIABLE_NAME` or `${ENVIRONMENT_VARIABLE_NAME}`.
Click for examples
`replace_envs properties.json deploy.json run.json`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
106,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Execute any command up to three times if it returns a non-zero error code. This is useful when you need to execute a command that can be flaky as a result of network hiccups, for example.
 **Usage**
Shell
Function  
Bash
`retry_command <shell command>`  
PowerShell
`retry_command <shell command>`  
  * `shell command `is the command to be retried


Click for Bash example
`retry_command docker push mydirectory/myImage`
Click for Powershell example
`retry_command docker push mydirectory/myImage`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
107,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Puts a uuid to stdout. Uses `/proc/sys/kernel/random/uuid` if available and falls back to `uuidgen` if not. The function calls `exit 1` if neither of these are available.
 **Usage**
Shell
Function  
Bash
`get_uuid`  
Click for Bash example
`my_uuid=$(get_uuid) `

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
108,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Saves metadata about an artifact. When saved, this metadata is used to enable signed pipelines for the artifacts.
 **Usage**
Shell
Function  
Bash
`save_artifact_info <artifact type> <file path> [--build-name <build name> --build-number <build number> --project-key <project key>]`  
PowerShell
`save_artifact_info <artifact type> <file path> [-build-name <build name> -build-number <build number> -release-bundle-name <name> -release-bundle-version <version> -project-key <project key>]`  
  * `artifact type` : This is the type of artifact. Either `file`, `buildInfo`, or `releaseBundle`.
  * `file path` : This is the path to the metadata file to be saved.


Bash
PowerShell
Description  
 **\--build-name**
 **-build-name**
This is name of the build. Required when artifact type is `buildInfo`.  
 **\--build-number**
 **-build-number**
This is number of the build. Required when artifact type is `buildInfo`.  
 **\--release-bundle-name**
 **-release-bundle-name**
This is name of the Release Bundle. Required when artifact type is `releaseBundle`.  
 **\--release-bundle-version**
 **-release-bundle-version**
This is version of the Release Bundle. Required when artifact type is `releaseBundle`.  
 **\--project-key**
 **-project-key**
(optional) Defaults to the environment's `project_key`. Can be specified to save info for a different project.  
Click for Bash example
`save_artifact_info 'buildInfo' './myBuildDetailedSummary.json' --build-name myBuild --build-number 42 `
`save_artifact_info 'file' './myFileDetailedSummary.json' `
Click for PowerShell example
`save_artifact_info buildInfo myBuildDetailedSummary.json -build-name myBuild -build-number 42 `
`save_artifact_info file myFileDetailedSummary.json `
`save_artifact_info releaseBundle myFileDetailedSummary.json -release-bundle-name myBundle -release-bundle-version 1 `

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
109,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Validates the signature of an artifact. Requires signed pipelines to be enabled.
 **Usage**
Shell
Function  
Bash
`validate_artifact <artifact type> <file path> [--build-name <build name> --build-number <build number> --project-key <project key>]`  
PowerShell
`validate_artifact <artifact type> <file path> [-build-name <build name> -build-number <build number> -project-key <project key>]`  
  * `artifact type`: This is the type of artifact. Supports `buildinfo`.
  * `file path`: This is the path to the metadata file to be validated.


Bash
PowerShell
Description  
 **\--build-name**
 **-build-name**
This is name of the build. Required when artifact type is `buildInfo`.  
 **\--build-number**
 **-build-number**
This is number of the build. Required when artifact type is `buildInfo.`  
 **\--project-key**
 **-project-key**
(optional) Defaults to the environment's `project_key`. Can be specified to validate info for a different project.  
Click for Bash example
`validate_artifact 'buildInfo' './myBuildDetailedSummary.json' --build-name myBuild --build-number 42 `
Click for PowerShell example
`validate_artifact buildInfo myBuildDetailedSummary.json -build-name myBuild -build-number 42 `

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
110,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Configures the JFrog CLI (version 1 or 2 specified as `jfrogCliVersion` in the  pipeline configuration) with the provided credentials, handling the different formats for different minor versions. Artifactory integrations listed in the integrations section of the step will be automatically configured, but this may be useful for resources or if the credentials are provided to the step in another way. When using v2, a non-default Xray URL may also be specified using the environment variable JFROG_XRAY_URL.
 **Usage**
Bash
`configure_jfrog_cli --artifactory-url <url> [--xray-url <url> --user USER --apikey <key> --access-token <token> --server-name <name>]`  
PowerShell
`configure_jfrog_cli -artifactory-url <url> [-xray-url <url> -user USER -apikey <key> -access-token <token> -server-name <name>]`  
  * `artifactory-url` : Required. The Artifactory URL.
  * `xray-url` : Optional. The Xray URL. Only used with CLI v2.
  *  **user** : The user. Required when an API key is provided.
  * `apikey` : An API key. Requires --user and may not be used with --access-token.
  * `access-token` : An Access token. May not be used with --access-token or --user.
  * `server-name` : Defaults to `default`. Can be specified to configure the CLI with that name.


Click for Bash example
`configure_jfrog_cli --artifactory-url  https://my-artifactory.local/artifactory --user me --apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R --server-name myAPIKeyArtifactory`
`configure_jfrog_cli --artifactory-url  https://my-artifactory.local/artifactory --access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg --server-name myAccessTokenArtifactory`
Click for PowerShell example
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -user me -apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R -server-name myAPIKeyArtifactory`
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg -server-name myAccessTokenArtifactory`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
111,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Sets a configured Artifactory server as the default.
 **Usage**
Bash
`use_jfrog_cli <server_name>`  
PowerShell
`use_jfrog_cli <server_name>`  
  * `server-name`: Name of the configured Artifactory server to be set as default.


Click for Bash example
`use_jfrog_cli myArtifactoryServer`
[YAML]
    configure_jfrog_cli --server-name Artifactory_Server --artifactory-url  https://my-artifactory.io/artifactory --access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVL
    use_jfrog_cli Artifactory_Server
[/YAML]
Click for PowerShell example
`use_jfrog_cli myArtifactoryServer`
[YAML]
    configure_jfrog_cli --server-name Artifactory_Server --artifactory-url  https://my-artifactory.io/artifactory --access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVL
    use_jfrog_cli Artifactory_Server
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
112,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
With JFrog CLI v2 (version 2 specified as `jfrogCliVersion` in the  pipeline configuration), checks that Xray is available for the specified CLI configuration, or the default configuration if none is specified.
 **Usage**
Bash
`check_xray_available [--server-name <name>]`  
PowerShell
`check_xray_available [-server-name <name>]`  
  * `server-name` : Optional. Specifies a JFrog CLI configuration to check.


Click for Bash example
`check_xray_available --server-name myArtifactoryIntegration`
Click for PowerShell example
`check_xray_available --server-name myArtifactoryIntegration`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
113,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Removes configuration for the JFrog CLI (v1), handling the different formats for different minor versions. Artifactory integrations listed in the integrations section of the step will be automatically removed at the end of the step, but this may be useful to remove the credentials earlier or when using `configure_jfrog_cli`.
 **Usage**
Bash
`cleanup_jfrog_cli [--server-name <name>]`  
PowerShell
`cleanup_jfrog_cli [-server-name <name>]`  
  * `server-name` : Defaults to `default`. Can be specified to remove that configuration.


Click for Bash example
`cleanup_jfrog_cli --server-name myArtifactory`
Click for PowerShell example
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -user me -apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R -server-name myAPIKeyArtifactory`
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg -server-name myAccessTokenArtifactory`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
114,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Adds one or more key=value pairs as custom step or pipeline variables to a JSON payload that can be used to call the pipelines trigger API.
Multiple key=value pairs can be given in a single command or can be split across multiple commands.
If a key is added twice, the original value will be replaced.
 **Usage**
Bash
`set_trigger_payload [stepVariables|pipelineVariables] <key value pairs>`  
PowerShell
Not currently supported in PowerShell  
  * `stepVariables`: These key value pairs will be present in the environment of the triggered step
  *  **pipelineVariables** : These key value pairs will be present in the environment of every step in the run that is triggered.


Click for Bash example
`set_trigger_payload stepVariables ""shouldScan=true"" ""notify=email""`
`set_trigger_payload pipelinesVariables ""releaseVersion=1.2.3""`
`set_trigger_payload pipelinesVariables ""collectData=true""`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
115,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Prints on stdout a payload that can be used with the pipelines trigger API.
Use `set_trigger_payload` to add additional values to the payload.
 **Usage**
Bash
`get_trigger_payload `  
PowerShell
Not currently supported in PowerShell  
Click for Bash example
curl -XPOST -H ""authorization: Bearer <token>"" -H ""content-type: application/json"" <endpoint>/pipelines/api/v1/pipelineSteps/:id/trigger -d ""$(get_trigger_payload)""

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
116,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Stops execution of the onStart or onExecute section and immediately continues to the onSuccess, onFailure, or onComplete section setting the specified status. If ""success"" is provided, the onSuccess and onComplete sections will be executed. For ""failure,"" the onFailure and onComplete sections will be executed. And for ""skipped,"" only the onComplete section. The end_step utility function may only be called in the onStart and onExecute sections and the only statuses supported are success, failure, and skipped.
 **Usage**
Bash
`end_step [success|failure|skipped]`  
PowerShell
`end_step [success|failure|skipped]`  
Click for Bash example
`end_step success`
Click for PowerShell example
`end_step success`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
117,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Provide dynamic description for a run, which will be shown in the UI. This is useful for providing more context about a run. Every step in a pipeline can include a run description.
 **Usage**
Bash
`update_run_description ""description""`  
PowerShell
`update_run_description ""description""`  
Click for Bash example
`update_run_description ""$msg""`
Click for PowerShell example
`update_run_description ""$msg""`
Full YAML Example
[YAML]
    name: UpdateRunDescription
    steps:
            - name: update_l
              type: Bash
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
            - name: update_2
              type: Bash
              configuration:
                    inputSteps:
                            - name: update_l
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
118,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Provide a unique, dynamic name for a run, which will be shown in the UI. This is useful for providing more context about a run. Unlike run description, run name is unique for a run.
In the UI, run name appears in the Run View:  
Run name appears also appears in Pipeline of Pipelines view and Active Board view:  
 **Usage**
Bash
`set_run_name ""1.0.1""`  
PowerShell
`set_run_name ""1.0.1""`  
Click for Bash example
`set_run_name ""1.0.1""`
Click for PowerShell example
`set_run_name ""1.0.1""`
Full YAML Example
[YAML]
    name: UpdateRunDescription
    steps:
            - name: update_l
              type: Bash
              execution:
                    onStart:
                            - set_run_name ""1.0.1""
            - name: update_2
              type: Bash
              configuration:
                    inputSteps:
                            - name: update_l
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
119,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
120,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Lists the files/directories containing changes within a commit range. This function is useful when building a monorepo (monolithic repository) to determine which services have changes.
Shell
Function  
Bash
`compare_git [--path | --resource] [options]`  
PowerShell
`compare_git [-path | -resource] [options] `  
  * `path` is the file system path to a git repository.
  * `resource` is the name of the gitRepo resource.
  * `commit-range` option specifies the range of commits to look for changes (Example: `HEAD~1..HEAD`).
  * `directories-only` option lists only the directories containing changes.
  * `depth` option returns file/folder at certain depth. Root directory has depth value 1.


Click for Bash examples
`ubuntu:~$ compare_git --path ./microservices --commit-range HEAD~2..HEAD api/common/utilities/leftpad.js api/main.js notifier/main.js ubuntu:~$ compare_git --path ./microservices --commit-range HEAD~2..HEAD --directories-only api/common/utilities/ api/ notifier/ ubuntu:~$ compare_git --path ./microservices --commit-range HEAD~2..HEAD --directories-only --depth 1 api notifier`
`ubuntu:~$ compare_git --path ./microserv --commit-range HEAD~2..HEAD --directories-only --depth 1`
`git repository not found at path: ./microserv`
Click for Powershell examples
`PS C:\Users\windowsuser> compare_git -path microservices -commit-range HEAD~2..HEAD api/common/utilities/leftpad.js api/main.js notifier/main.js PS C:\Users\windowsuser> compare_git -path microservices -commit-range HEAD~2..HEAD -directories-only api/common/utilities/ api/ notifier/ PS C:\Users\windowsuser> compare_git -path microservices -commit-range HEAD~2..HEAD -directories-only -depth 1 api notifier`
`PS C:\Users\windowsuser> compare_git -path microserv -commit-range HEAD~2..HEAD -directories-only -depth 1`
`git repository not found at path: ./microserv`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
121,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Updates the statusof the commit on the source provider. Status options are _processing_ , _success_ , and _failure_.
 **Usage**
Shell
Function  
Bash
`update_commit_status <gitRepo resource name> --status <status> --message <message> --context <context>`  
PowerShell
`update_commit_status <gitRepo resource name> -status <status> -message <message> -context <context>`  
update_commit_status <gitRepo resource name> \--status <status> \--message <message> \--context <context>
  * `gitRepo resource name` is the name of the gitRepo resource.
  * `status` is the status to be set on the source provider: _processing_ , _success_ , or _failure_.
If no status is specified:
    *  _processing_ will be assumed in `onStart` and `onExecute`
    *  _success_ will be assumed in `onSuccess`
    *  _failure_ will be assumed in `onFailure`
  * `message` is the message (description) string to send with the status.
If no message is specified, the default message will be ""Step <status> in pipeline $pipeline_name""
  * `context` is the context (key) for the `status`. The source provider will retain only the latest status received for that context.
If no context is specified, the default is ""$pipeline_name_$step_name""


Click for Bash examples
update_commit_status myGitRepoResource
update_commit_status myGitRepoResource --status processing --message “my description” --context $step_name
Click for PowerShell examples
`update_commit_status myGitRepoResource`
`update_commit_status myGitRepoResource -status processing -message “my description” -context $step_name`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
122,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>[/INST]##### save_tests
 **Description**
Copies test reports given as input to later be parsed and uploaded (if file storage is available).
 **Usage**
`save_tests <file or directory>`
  * `file` or `directory`specifies either a filename for the test report file, or a directory name for a directory of test report files


Click for examples
`save_tests testreport.xml`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
123,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
124,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Uses the provided public key to encrypt the specified string.
 **Usage**
Shell
Function  
Bash
`encrypt_string --key <path> <source string> `  
PowerShell
`encrypt_string -key <path> <source string>`  
  * `key` is the fully qualified path of the public key file
  * `source string` is the string to be encrypted


Click for Bash examples
`ubuntu:~$ encrypt_string --key pub.pem""admin:passw0rd"" LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+ 06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6t dVK0X/5pCTWBKk5nIvw=`
Click for Powershell examples
`PS C:\Users\windowsuser> encrypt_string ""admin:passw0rd"" -key pub.pem LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+ 06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6t dVK0X/5pCTWBKk5nIvw=`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
125,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Uses the provided private key to decrypt the specified string.
This is typically used to decrypt information that was encrypted using `encrypt_string` with the corresponding public key. It helps you avoid building your own encrypt-decrypt system.
 **Usage**
Shell
Function  
Bash
`decrypt_string --key <path> <encrypted string>`  
PowerShell
`decrypt_string -key <path> <encrypted string> `  
  * `key` is the fully qualified path of the private key file
  * `encrypted string` is the string to be decrypted


Click for Bash examples
ubuntu:~$ decrypt_string --key key.pem LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=
`admin:passw0rd`
`ubuntu:~$ decrypt_string LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=`
`decrypt_string: ERROR - Key file not found`
Click for Powershell examples
`PS C:\Users\windowsuser> decrypt_string -key key.pem LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=`
`admin:passw0rd`
`PS C:\Users\windowsuser> decrypt_string LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=`
`key is mandatory, please provide a value.`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
126,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Uses the provided public key to encrypt the specified file to a new file.
 **Usage**
Shell
Function  
Bash
`encrypt_file --key <path> [--output <filename>] <source filename> `  
PowerShell
`encrypt_file -key <path> [-output <filename>] <source filename>`  
  * `key` is the fully qualified path of the public key file
  * `output` is the name of the resulting encrypted file. Defaults to “encrypted”
  * `source filename` is the file to be decrypted


Click for Bash examples
`ubuntu:~$ cat secrets.json {""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""} ubuntu:~$ encrypt_file secrets.json --key pub.pem encrypt_file: Encrypting secrets.json using key pub.pem encrypt_file: Encrypted secrets.json to encrypted ubuntu:~$ cat encrypted dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+ 5xHBCvlenuFanIFLDbo=`
Click for Powershell examples
`PS C:\Users\windowsuser> type secrets.json {""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""} PS C:\Users\windowsuser> encrypt_file secrets.json -key pub.pem encrypt_file: Encrypting secrets.json using key pub.pem encrypt_file: Encrypted secrets.json to encrypted PS C:\Users\windowsuser> type encrypted dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+ 5xHBCvlenuFanIFLDbo=`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
127,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Uses the provided private key to decrypt the specified file to a new file.
This is typically used to decrypt information that was encrypted using `encrypt_file` with the corresponding public key. It helps you avoid building your own encrypt-decrypt system.
 **Usage**
Shell
Function  
Bash
`decrypt_file --key <path> [--output <filename>] <source filename>`  
PowerShell
`decrypt_file -key <path> [-output <filename>] <source filename> `  
  * `key` is the fully qualified path of the private key file
  * `output` is the name of the resulting decrypted file. Defaults to “decrypted”
  * `source filename` is the file to be decrypted


Click for Bash examples
`ubuntu:~$ cat encrypted`
`dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ`
`YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+`
`5xHBCvlenuFanIFLDbo=`
`ubuntu:~$ decrypt_file encrypted --key key.pem`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to decrypted`
`ubuntu:~$ cat decrypted`
`{""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""}`
`ubuntu:~$ decrypt_file encrypted --key key.pem --output secrets.json`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to secrets.json`
Click for Powershell examples
`PS C:\Users\windowsuser> type encrypted`
`dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ`
`YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+`
`5xHBCvlenuFanIFLDbo=`
`PS C:\Users\windowsuser> decrypt_file encrypted -key key.pem`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to decrypted`
`PS C:\Users\windowsuser> type decrypted`
`{""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""}`
`PS C:\Users\windowsuser> decrypt_file encrypted -key key.pem -output secrets.json`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to secrets.json`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
128,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>[/INST]##### send_notification
 **Description**
Utilizes notification integration to send custom messages at any time during the build to any recipient.
For more information, see Sending Notifications from Pipelines.
 **Usage**
[YAML]
    send_notification <integration> [options]
    
    
[/YAML]
The options can be specified as part of the command, or defined as environment variables before the command is issued.
The command line arguments take priority over the environment variables.
Click for Bash examples
`- send_notification pipelines_mqtest_slack --text ""Pipeline - $pipeline_name | Step - $step_name \n Committer - <$committer> ""set_payloadmyExternalHookResource""{\""test\"":\""payload\""}""`
###### AirBrake
Creates an AirBrake deployment through an Airbrake Integration. Not supported in PowerShell.
Bash
Option Description  
 **\--project-id**
the project ID to send the notification for  
 **\--environment**
the environment value to use when posting the deployment  
 **\--username**
used when posting an AirBrake deployment  
 **\--email**
the email to be used when posting the AirBrake deployment  
 **\--repository**
the repository to use when posting the AirBrake deployment  
 **\--revision**
the deployment revision  
 **\--version**
the version to use when posting the AirBrake deployment  
 **\--type**
currently only type “deploy” is supported  
 **\--description**
description of the deployment  
 **\--payload**
path to a valid JSON file that contains a payload to use to POST the AirBrake deployment  
###### Jira
Creates a Jira issue (also known as a ticket).
Bash
PowerShell
Option Description  
 **\--project-id**
 **-project-id**
the Project Key of the project to associate the new issue with. The project key is the short string that begins all issue numbers for the project (e.g., ""EXAMPLE-1234"")  
 **\--type**
 **-type**
the issue type for the new issue (e.g., ""Bug"", ""Task"", etc.). This string must be one of the recognized Jira issue types  
 **\--summary**
 **-summary**
a string for the new issue's Summary field (it's title)  
 **\--description**
 **\--description**
(optional) a string for the new issue's Description field  
 **\--attach-file**
 **\--attach-file**
(optional) a path to a file that you’d like to attach to the issue  
###### NewRelic
Creates a NewRelic deployment through a NewRelic Integration. Not supported in PowerShell.
Bash
Option Description  
 **\--type**
the type of object to be posted. At the moment, only “deployment” is supported  
 **\--description**
description of the deployment  
 **\--username**
the user recording the deployment. Defaults to “JFrog Pipelines”  
 **\--changelog**
the changelog value to use in the deployment  
 **\--revision**
the deployment revision (required)  
 **\--appId**
the ID of the app being deployed. If not provided, --appName must be present  
 **\--appName**
the name of the app being deployed. If not provided, --appId must be present  
 **\--payload**
path to a valid JSON file that contains a payload to use to POST the NewRelic deployment  
###### PagerDuty Events
Sends an event through a PagerDuty Events Integration.
Bash
PowerShell
Option Description  
 **\--text**
 **-text**
The main text to display in the event on PagerDuty.  
Slack
Sends a message on Slack through a Slack Integration.
Bash
PowerShell
Option Description  
 **\--payload**
 **-payload**
(optional) A path to a valid json file to act as the payload of the message. If a payload is provided, all other parameters are ignored. This payload is directly sent to Slack, so please view the Slack API documentation for information on how the payload should be formatted.  
 **\--username**
 **-username**
(optional) shows in the heading of the Slack message  
 **\--pretext**
 **-pretext**
(optional) a string that becomes the first part of the Slack message. Defaults to current date/time  
 **\--text**
 **-text**
(optional) the main text to display in the message.  
 **\--color**
 **-color**
(optional) hex string that changes the color of the status bar to the left of the Slack message.  
 **\--recipient**
 **-recipient**
(optional) the target of the message. Should start with “@” or “#” for user or channel, respectively.  
 **\--icon-url**
 **-icon-url**
(optional) the url of the icon to show next to the message  
###### smtpCreds (email)
Sends an email through an SMTP Credentials Integration.
Bash
PowerShell
Option Description  
 **\--recipients**
 **-recipients**
one or more email addresses  
 **\--subject**
 **-subject**
(optional) add a message to the subject. Does not replace the default subject  
 **\--body**
 **-body**
(optional) specify some text to add to the body of the email. Does not replace the existing body information  
 **\--status**
 **-status**
(optional) can be set to a valid status string. By default it will be set based on the section of scrip the command is executed in.  
 **\--attachments**
 **-attachments**
(optional) a list of files to attach to the email. Combined total of all files cannot exceed 5MB  
 **\--attach-logs**
 **-attach-logs**
(optional) ‘true’ or ‘false’. Defaults to false. All available logs for the step will be attached to the email. Note that it can only attach logs that have already been created, so using this option in the onStart section, for example, would not have very detailed logs.  
 **\--show-failing-commands**
 **-show-failing-commands**
(optional) 'true’ or ‘false’. Defaults to false. The existing logs for the step will be parsed. Any failed command that is detected will be added to the body of the email, along with up to 100 preceding lines (if printed from the same command)  
Environment Options
All of the above options can also be included as environment variables instead of arguments. The command line argument will have priority over the environment. Here is the full list of ENVs:
  * `NOTIFY_USERNAME` (--username/-username)
  * `NOTIFY_PASSWORD` (--password/-password)
  * `NOTIFY_RECIPIENT` (--recipient/-recipient)
  * `NOTIFY_PRETEXT` (--pretext/-pretext)
  * `NOTIFY_TEXT` (--text/-text)
  * `NOTIFY_COLOR` (--color/-color)
  * `NOTIFY_ICON_URL` (--icon-url/-icon-url)
  * `NOTIFY_PAYLOAD` (--payload/-payload)
  * `NOTIFY_TYPE` (--type/-type)
  * `NOTIFY_PROJECT_ID` (--project-id/-project-id)
  * `NOTIFY_ENVIRONMENT` (--environment/-environment)
  * `NOTIFY_REVISION` (–revision/-revision)
  * `NOTIFY_SUMMARY` (--summary/-summary)
  * `NOTIFY_ATTACH_FILE` (--attach-file/-attach-file)
  * `NOTIFY_REPOSITORY` (--repository/-repository)
  * `NOTIFY_EMAIL` (–email--email/-email)
  * `NOTIFY_STATUS` (--status/-status)
  * `NOTIFY_VERSION` (--version/-version)
  * `NOTIFY_CHANGELOG` (--changelog/-changelog)
  * `NOTIFY_DESCRIPTION` (–description--description/-description)
  * `NOTIFY_ATTACHMENTS` (–attachments--attachments/-attachments)
  * `NOTIFY_ATTACH_LOGS` (--attach-logs/-attach-logs)
  * `NOTIFY_SHOW_FAILING_COMMANDS` (--show-failing-commands/-show-failing-commands)
  * `NOTIFY_SUBJECT` (--subject/-subject)
  * `NOTIFY_BODY` (–body)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
129,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
130,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Sets an optional JSON payload (string or file) for an OutgoingWebhook resource. When the OutgoingWebhook is specified in a step's `outputresources` the payload is sent when the step is complete.
 **Usage**
Shell
Function  
Bash
`set_payload <resource> <payload> [--file]`  
PowerShell
`set_payload <resource> <payload> [-file]`  
  * `resource` is the name of an OutgoingWebhook resource.
  * `payload` is a JSON string or file to attach to the resource that will be sent as part of the outgoing webhook. A file can be specified as a path relative to the current directory, absolute path, or path relative to the step workspace directory.
  * `file` option specifies that the `payload` parameter is a file. If not specified, `payload` will be processed as a string.


Click for Bash examples
`set_payloadmyExternalHookResource""{\""test\"":\""payload\""}""`
`echo""{\""test\"":\""payload\""}""> testpayload.json set_payload myExternalHookResource testpayload.json --file`
Click for Powershell examples
`set_payloadmyExternalHookResource""{`""test`"":`""payload`""}""`
`Set-Content -Path testpayload.json -Value ""{`""test`"":`""payload`""}"" set_payload myExternalHookResource testpayload.json -file`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
131,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Extracts the json property value from the specified file.
This simplifies handling of a JSON file to read specific property values that are required for your workflow.
[/INST]### Not supported in PowerShell
In PowerShell, `ConvertTo-Json` is suggested as an alternative.
 **Usage**
[YAML]
    read_json <path to file> <field name>
    
    
[/YAML]
  * `path to file` is the fully qualified path of the JSON file
  * field name is the field for which you want to read the value. Use dot notation and [n] for arrays.


Click for examples
ubuntu:~$ cat secrets.json
{
""TOP_SECRET_CONFIDENTIAL_FORMULA"":""uuddlrlrbas"",
""LESSER_SECRET_FORMULA"":""dyddy"",
""TOP_5_PASSWORDS_LIST"": [
""admin"",
""passw0rd"",
""testing123"",
""correcthorsebatterystaple""
],
“nesting”: {
“is”: {
“fun”: “yay”
}
}
}
ubuntu:~$ read_json secrets.json""LESSER_SECRET_FORMULA""
dyddy
ubuntu:~$ read_json secrets.json""TOP_5_PASSWORDS_LIST[3]""
correcthorsebatterystaple
ubuntu:~$ read_json secrets.json "" nesting.is.fun""
yay

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
132,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
133,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
This command takes an input resource and creates an exact copy. This helps you to transfer metadata from one step to the next.
 **Usage**
Shell
Function  
Bash
`replicate_resource <from_resource> <to_resource> [--options]`  
PowerShell
`replicate_resource <from_resource> <to_resource> [-options] `
[/YAML]
[YAML]  * `from_resource` is the name of the `inputResources` resource that you're copying from.  
  * `to_resource` is the name of the `outputResources` resource that will receive the replicated data from the `from_resource`.Any pre-existing files or key-value pairs in the `to_resource` will be replaced.
  * `match-settings` option should be set when you want the replication to adhere to any branch/tag settings in the `to_resource`. For example, If your `from_resource` gitRepo can trigger on both commits and pull requests, but you only want to update your `to_resource` on commits, you can replicate with --match-settings, and the `to_resource` will only be updated when the `from_resource` had a commit.


Click for Bash examples
`replicate_resource myRepo1 myRepo2 --match-settings replicate_resource myTestImage myStageImage`
Click for Powershell examples
`replicate_resource myRepo1 myRepo2 -match-settings replicate_resource myTestImage myStageImage`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
134,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Adds data to an output resource in the form of key/value pairs that will become properties of the resource.
 **Usage**
Bash
`write_output <resource> <key value pair>... [--overwrite]`  
PowerShell
`write_output <resource> <key value pair>... [-overwrite]`  
  * `resource` ``is the resource to update
  * `key value pair` is a single string with a key and a value, separated by an “=”. Multiple of these strings can be supplied as input. A value with spaces should be surrounded by quotes.


Bash
Powershell
Option Description  
 **\--overwrite**
 **-overwrite**
If supplied, all key value pairs will be replaced.  
Click for examples
`write_output myImageimageTag=master sha=$commitSha description=\""hello world\""`
The newly attached properties can be accessed as environment variables of the form `res_{Resource Name}_{Key Name}`.
For example, the above created properties can be accessed as these environment variables:
[YAML]
    $ printenv res_myImage_master
    master
    $ printenv res_myImage_sha
    d6cd1e2bd19e03a81132a23b2025920577f84e37
    $ printenv res_myImage_description
    ""hello world""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
135,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Caching helps you speed up execution of your steps by preserving and restoring packages and dependencies between runs of a step. In this way, you can reduce build times by avoiding repeating the installation or loading of large dependencies.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
136,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Copies files given as input to later be uploaded if file storage is available.
For more information about using this utility function, see Caching Step Runtimes.
 **Usage**
`add_cache_files <file or directory> <name>`
  * `file or directory` is a file or directory to store in the cache
  * `name` is a name to give the stored file or directory (without spaces)


Click for examples
`add_cache_files cachefile.txt my_file`
`add_cache_files directory/subdirectory my_directory`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
137,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Copies stored cache (if file storage is available) to the specified location. No error will occur if nothing is available for <name> in the cache.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`restore_cache_files <name> <path>`
  * `name` is the name the file or driectory to be restored was given when cached.
  * `path` is a path at which to place the file or directory.


Click for examples
`restore_cache_files my_file cachefile.txt`
`restore_cache_files my_directory directory/subdirectory`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
138,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
139,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Allows you to add environment variables that will be available in the following steps of the run.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


 **Usage**
`add_run_variables <key value pair>...`
  * `key value pair` ``is a single string with a key and a value, separated by an “=”. Multiple of these strings can be supplied as input. Each value will be exported as an environment variable at the time this command is used and automatically in any later steps within the run.


Click for examples
add_run_variables imageTag=""master""

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
140,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Sources the file containing the run variables. This will be done automatically, but may also be used to “reset” the environment variables in the current step.
 **Usage**
`export_run_variables`
Click for examples
`export_run_variables`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
141,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Copies files given as input into the run state for use in later steps in the run, if file storage is available.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`add_run_files <file or directory> <name>`
`file or directory` is a file or directory to store in the run state
`name` is a name to give the stored file or directory (without spaces). This cannot be run.env.
Click for examples
`add_run_files cachefile.txt my_file`
`add_run_files directory/subdirectory my_directory`
`add_run_files directory/*/subdirectory my_directory`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
142,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Copies files stored in the run state (if file storage is available) to the specified location. No error will occur if nothing is available for <name> in the run state.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`restore_run_files <name> <path>`
  * `path` is the name the files to be restored were given when added to the run state.
  * `file or directory` ``is a path at which to place the file or files.


Click for examples
`restore_run_files my_file cachefile.txt`
`restore_run_files my_directory directory/subdirectory`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
143,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
144,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Copies files or directories given as input into the affinity group workspace for use in later steps in the affinity group. Files or directories may be specified using wildcards (*) or as multiple input parameters preceding the name under which the files will be stored.
 **Usage**
`add_affinity_group_files <file or directory> <name>`
`file or directory` is a file or directory to store in the run state.
`name` ``is a name to give the stored file or directory (without spaces).
Click for examples
`add_affinity_group_files cachefile.txt my_file`
`add_affinity_group_files myDirectory myOtherDirectory/subdirectory my_directories`
`add_affinity_group_files myDirectory/*.txt my_text_files`
`add_affinity_group_files ../adjacentDirectory my_adjacent_directory`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
145,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Copies files stored in the affinity group workspace by `add_affinity_group_files` to the specified location. The restore behavior differs from the other state restore functions to better handle wildcard patterns with a variable number of matching files or directories. With `restore_affinity_group_files`, the relative path specified in the `add_affinity_group_files` function will be preserved (excluding any traversal to parent directories) and the path specified should be the base for that relative path. The following examples are to symmetrically restore the files saved in the examples above for `add_affinity_group_files.` No error will occur if no files were saved for <name>.
 **Usage**
`restore_affinity_group_files <name> <path>`
  * `name` is the name the files to be restored were given when storing the files with `add_affinity_group_files`.
  * `path` ``is a path at which to place the file or files.


Click for examples
`restore_affinity_group_files my_file .`
`restore_affinity_group_files my_directories .`
`restore_affinity_group_files my_text_files .`
`restore_affinity_group_files my_adjacent_directory ..`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
146,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
147,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Allows you to add environment variables that will be available in the following steps of the run and in future runs. These variables may be overridden by another variable with the same key added to the current run.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


 **Usage**
`add_pipeline_variables <key value pair>...`
  * `key value pair` ``is a single string with a key and a value, separated by an “=”. Multiple of these strings can be supplied as input. Each value will be exported as an environment variable at the time this command is used and automatically in any steps that start after this run is complete.


Click for examples
`add_pipeline_variables imageTag=""master""`
`add_pipeline_variables imageName=""myimage"" imageTag=""master""`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
148,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Sources the file containing the pipeline variables. This will be done automatically, but may also be used to “reset” the environment variables in the current step.
 **Usage**
`export_pipeline_variables`
Click for examples
`export_pipeline_variables`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
149,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Copies files given as input into the pipeline state for use in later steps in the run and future runs, if file storage is available.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`add_pipeline_files <file or directory> <name>`
  * `file or directory` is a file or directory to store in the pipeline state.
  * `name` is a name to give the stored file or directory (without spaces). This cannot be pipeline.env.


Click for examples
`add_pipeline_files cachefile.txt my_file`
`add_pipeline_files directory/subdirectory my_directory`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
150,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Copies files stored in the pipeline state (if file storage is available) to the specified location. No error will occur if nothing is available for <name> in the run state.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`restore_pipeline_files <name> <path>`
  * `name` is the name the file to be restored was given when added to the pipeline state.
  * `path` is a path at which to place the file or files.


Click for examples
`restore_pipeline_files my_file cachefile.txt`
`restore_pipeline_files my_directory directory/subdirectory`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
151,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
152,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Retrieves the value of the named property of a resource.
 **Usage**
`find_resource_variable <resourceName> <propertyName>`
  * `resourceName` is the name of the resource.
  * `propertyName` is the name of the resource property whose value to retrieve.


Click for examples
`find_resource_variable myGitHub commitSha`
`find_resource_variable myImage imageTag`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
153,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Retrieves the name of the first integration found of the type specified. Available to extension steps to get the name of the first input integration of a particular type.
 **Usage**
`get_integration_name --type <integration type>`
  * `integration type` is the name of an Pipelines Integration type


Click for examples
`get_integration_name --type Slack`
`get_integration_name --type ""Docker Registry""`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
154,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Retrieves the name of the first resource found of the type specified in `inputResources` or `outputResources`. Available to extension steps to get the name of the first input or output resource of a particular type.
 **Usage**
get_resource_name --type <resource type> \--operation <IN | OUT> \--syntax-version <semver>
  * `resource type` is the name of a Pipelines Resource type
  * `IN | OUT` selects whether the resource is named in `inputResources` or `outputResources`
  * `semver` is the semantic version number of the resource's syntax version


Click for examples
`get_resource_name --type GitRepo --operation IN`
get_resource_name --type BuildInfo --operation OUT
`get_resource_name --type GitRepo --operation IN --syntax-version 1.5.0`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
155,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Retrieves an array of names of the type specified in `inputResources` or `outputResources`. Available to extension steps to get the names of input or output resource of a particular type.
 **Usage**
get_resource_names --type <resource type> \--operation <IN | OUT> \--syntax-version <semver>
  * `resource type` is the name of a Pipelines Resource type
  * `IN | OUT` selects whether the resource is named in `inputResources` or `outputResources`
  * `semver` is the semantic version number of the resource's syntax version
  * In PowerShell, a native PowerShell array is returned. In Bash, a JSON array is returned that can be handled with jq.


Click for examples
`get_resource_names --type GitRepo --operation IN`
get_resource_names --type BuildInfo --operation OUT
`get_resource_names --type GitRepo --operation IN --syntax-version 1.5.0`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
156,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Retrieves a JSON array of names of steps of the type specified in the current affinity group.
 **Usage**
`get_affinity_group_step_names [--type <step type>]  [--syntax-version <semver>] [--namespace <namespace>]find_step_configuration_value <propertyName>`
  * `type` is used to specify the type of the steps to be found. If not specified, steps of all types will be returned.
  * `syntax-version` is used to specify the type of the steps to be found. If not specified, steps of all types will be returned.
  * `namespacetype` is used with extension steps to specify the namespace of the steps to be found.


Click for examples
`get_affinity_group_step_namesfind_step_configuration_value forceXrayScan`
`get_affinity_group_step_names --type GitRepo`
`get_affinity_group_step_names --type MyExtension --syntax-version 1.5.0`
`get_affinity_group_step_names --type MyExtension --namespace myNamespace --syntax-version 1.5.0`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
157,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>**Description**
Retrieves the value of the `configuration` property for the currently executing step. If the property is a collection, the first value will be returned. Available to extension steps to get the value of a configuration.
 **Usage**
`find_step_configuration_value <propertyName>`
  * `propertyName` is the name of the step's configuration property whose value to retrieve


Click for examples
`find_step_configuration_value forceXrayScan`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
158,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This page describes the availability and usage of environment variables in pipelines.
You can customize your workflow by using environment variables (also called runtime variables) that are available at runtime during step execution. For example, your step can construct a filename or build ID from the pipeline name and current run number. Or your step can execute different commands depending on the state of a resource.
JFrog Pipelines provides access to useful internal information from pipelines and resources through environment variables. Pipelines also provides several easy ways to inject your own environment variables into your runtime during the execution of your pipeline.
[/INST]#### Standard Environment Variables
These environment variables are available to all executing pipelines.
Environment Variable
Description  
architecture
Node architecture, e.g. x86_64  
builder_api_token
Token used to call pipelines API during step execution. Used internally but is accessible to user.  
current_script_section
onStart/onExecute/onComplete etc  
JFROG_CLI_BUILD_NAME
If not set as a variable in the YML or added as a pipeline, run, or step variable, it will be set to `$pipeline_name`. This is used when accumulating build info.  
JFROG_CLI_BUILD_NUMBER
If not set as a variable in the YML or added as a pipeline, run, or step variable, it will be set to `$run_number`. This is used when accumulating build info.  
JFROG_CLI_BUILD_PROJECT
If not set as a variable in the YML or added as a pipeline, run, or step variable, it will be set to `$project_key`. This is used when accumulating or publishing build info.  
JFROG_PIPELINES_RUN_STATE_DEBUG
To download run state from the UI for debugging purposes, set the environment variable `JFROG_PIPELINES_RUN_STATE_DEBUG=true` in your step's or pipeline's configuration section. For storage efficiency, we recommend removing this configuration option after you are done debugging.  
operating_system
OS where step is running, e.g. Ubuntu_20.04  
PATH
The value of PATH in the currently executing runtime environment.  
pipeline_name
The name string of the pipeline currently executing.  
pipelines_api_url
API URL  
project_id
ID of the project in JFrog Pipelines  
project_key
Project key. If the project is the default project, this will be empty.  
project_name
Name of the project  
run_id
Internal unique ID for the run.  
run_number
The number of the run currently executing.  
running_in_container
True if step is running in a container. False if running on host.  
shared_workspace
The path of a directory available to all steps in an affinity group that may be used to share files between steps.  
step_<inputStepName>_statusName
This environment variable is automatically made available at runtime and can be used in any step. inputStepName is the name of your input step. This environment variable is useful for fetching the status of any input step.
For an example, see using `step_<inputStepName>_statusName` in the Conditional Workflows page.  
step_affinity_group
Affinity group step belongs to. Default is same as step name.  
step_id
Internal unique ID for the step currently executing.  
step_image_name
Name of the Docker image used to run the step (e.g. `pipelines-docker.jfrog.io/jfrog/pipelines-u20node`).  
step_image_tag
Tag of Docker image used to run the step.  
step_name
The name of the step as specified in YAML.  
step_node_id
Internal unique ID for node  
step_node_name
Name of the node used to run step (user-configured when adding node)  
step_node_pool_name
Name of the node pool used to run the step (user configured when creating pool)  
step_platform
Same as operating system.  
step_runtime
Set to ""image"" or ""host""  
step_triggered_by_identity_name
If triggered by a user, name of user. Else empty  
step_triggered_by_resource_name
If triggered by a resource, the name of the resource. Otherwise, this environment variable is empty.  
step_triggered_by_step_name
If triggered by another step, name of step. Else empty  
step_type
The type of step as specified in the field in YAML (e.g., ""Bash,"" ""DockerBuild,"" etc.)  
step_url
Direct link to step  
#### Reserved Environment Variables
These environment variables have predefined values and cannot be overridden.
Environment Variable
Description  
custom_certs_dir
Location of the custom certificates.  
custom_certs_enabled
Indication to use the custom certificates.  
no_verify_ssl
Indication for the step to not verify SSL.  
run_created_at
Time when the run was created.  
step_build_plane_version
The build plane version.  
step_dependency_state_dir
Location where input/output dependency information is stored.  
step_docker_container_name
Name of the container where the step is running.  
step_json_path
Path to the JSON object containing information about the step.  
step_namespace
Pipelines extensions namespace. Default is `jfrog`.  
step_timeout_seconds
The total number of seconds that the step can run before it times out.  
step_tmp_dir
Temporary directory used when the step is executing.  
steplet_id
Internal unique ID for the steplet currently executing.  
steplet_number
Unique steplet number in the context of the run. Useful for Matrix steps with multiple steplets.
 **Example**
[YAML]
     onExecute:
       - add_run_variables steplet_${steplet_number}_variable=${env1}
[/YAML]  
#### User-defined Environment Variables
User-defined environment variables are custom variables that can be defined in the `configuration` section of your pipelines YAML file.
##### Pipelines
In a pipeline, environment variables can be declared within the `configuration` section. The environment variables declared here are available to all steps in the pipeline.
[YAML]
    pipelines:
      - name:       my_pipeline
        configuration:
          environmentVariables:
            readOnly:
              env1:     value1
              env2:     value2
[/YAML]
##### Steps
In a step, environment variables can be declared within the `configuration` section of the step within a pipeline. The environment variables from this source are available only to the step where they are declared.
[YAML]
        steps:
          - name:       step_1
            type:       Bash
            configuration:
              environmentVariables:
                env1:     value1
                env2:     value2
[/YAML]
##### YAML Schema Reference
When declaring environment variables, it is important to follow YAML syntax conventions and properly quote and escape values to ensure that Pipelines can parse them correctly.
In general, YAML strings are Unicode and can be left unquoted. However, certain rules apply when using special characters. Quotes are required for strings that:
  * Start with a special character, such as: `:, {, }, [, ], ,, &, *, #, ?, |, -, <, >, =, !, %, @, )`
  * Start or end with whitespace characters
  * Look like a number or boolean (`123`, `1.23`, `true`, `false`, `null`)


###### Examples
####### Single Quotes
Example
Result
Comments  
'This string 'uses single quotes''
`This string 'uses single quotes'`
  * Single quotes let you include almost any character in your string.
  * Single quotes will not parse escape codes. For example: ""\n"" is returned as the string \n.
  * Use two single quotes (`''`) to include a single quote inside the single-quoted string.

  
`'This string ""uses double quotes""'`
`This string ""uses double quotes""`  
'& this string starts with a special character, needs quotes'
'& this starts with a special character, needs quotes'  
####### Double Quotes
Example
Result
Comments  
`""This string \nis on the next line""`
`This string`
`is on the next line`
Double quotes support any character string and escape sequences.
For example: `\n`, `\\`, and `\""`)  
""This string uses \ttab""
`This string uses tab`  
####### Multi-line Strings
Multi-line strings can be written using:
  *  **Folded Style** : Represented by a greater than symbol (>), where each line break is converted to a space.
  *  **Literal Style** : Represented by a pipe (|), to preserve line breaks.


 **Folded Style ( >)**
Example
Result
Comments  
`foo: > `
` This is a string. `
` Another string. `
` One more.`
`This is a string. Another one. One more.`
Folded style removes end of line characters and replaces double end of lines with single lines. This is useful for descriptions.
  * Use `>-` if you do not want a line-break appended at the end.
  * Use `>+` if you want a line-break appended at the end.

  
`execution:`
` onExecute:`
` - >`
` echo ""hello""`
` echo ""world""`
`hello world`  
 **Literal Style (|)**
Example
Result
Comments  
`foo: |`
` This is the first line.`
` This is the second line. `
` This is the third line.`
`This is the first line.`
`This is the second line. `
`This is the third line.`
Literal style preserves end of line characters. This is useful when defining script actions.
  * Use `|` `-` if you do not want a line-break appended at the end.
  * Use `|` `+` if you want a line-break appended at the end.

  
`execution:`
` onExecute:`
` - |`
` echo ""hello""`
` echo ""world""`
`hello`
`world`  
####### Escape Sequences
YAML uses escape sequences as follows:
  * `\n` is used to represent a new line
  * `\t` is used to represent a tab
  * `\\` is used to represent a slash
  * Bash style `""\ ""` is used to escape additional spaces that are part of the content and should not be folded
  * The trailing \ is used to represent a continuation marker. This is useful for breaking a long string into multiple lines without introducing unwanted whitespace


Example
Result
Comments  
`'price: \$10'`
`price: $10`
Special characters must be escaped with backslash.  
`'This string has internal \`back ticks\`'`
`This string has internal `back ticks``
Backticks must be escaped as they are treated specially by the shell.  
`'This string has a backslash \\ character'`
`This string has a backslash \ character`  
`""This string has \$ several ' special \"" characters \\ that are\nescaped as necessary""`
`This string has $ several ' special "" characters \ that are ` `escaped as necessary""`
In this example:
  * The double quotes enclose the full string. This is done to take advantage of YAML expanding the `\n` into a newline character.
  * Except for the single quote character, everything else is escaped using backslash.
  * Single quote is treated as a character and does not need escaping.

  
#### Environment Variables Configuration
Besides the key-value syntax, the following properties can also be used under an environment variable definition to enhance its configuration. The following properties are available for environment variables declared in both **pipelines** and **steps** `configuration`sections.
### Important
Environment variable names set at pipeline-level must be unique. A step cannot include the same environment variable name as the one set at the pipeline-level.
Property
Description
Required/Optional  
`default`
Default value for the environment variable.
Required  
`description`
Describes the environment variable usage. This description is displayed in the custom run configuration panel.
Optional  
`values`
List of values that can be assigned to environment variables when custom run configuration is used in the UI.
Optional  
`allowCustom`
Determines if users are allowed to use a custom value that is not part of the list of `values`. Default value is `false`.
Optional  
`secure`
Secure environment variables are encrypted variables that can be used in your pipelines without exposing their value. These environment variables are ideal for private information, such as passwords.
When set as `true`, the value provided is masked in the Pipelines UI. In your pipeline, if you echo, or print a secret environment variable, it is substituted with `XXXXX`. Default value is `false`.
As shown in the example below, the `secure` property requires a default value.
Example
[YAML]
    steps:
      - name: dependency_version_check
        type: Bash
        configuration:
          environmentVariables:
            mySecureEnv:
              allowCustom: true
              secure: true
              default: ""someValue""
[/YAML]  
Optional  
##### Example
[YAML]
    pipelines:
      - name: my_pipeline
        configuration:
          environmentVariables:
            readOnly:
              env1: pipe_value1
              env2:
                default: pipe_value2
                description: This is env2 description
                values:
                  - pipe_value2
                  - pipe_value3
                  - pipe_value4
        steps:
          - name: my_step
            type: Bash
            configuration:
              environmentVariables:
                step_env1:
                  default: step_value1
                  description: This is step_env1 description
                  values:
                    - step_value1
                    - step_value2
                    - step_value3
                  allowCustom: true
                step_env2: step_value5
                step_env3:
                  allowCustom: true
                  secure: true
                  default: ""someValue""
            execution:
              onExecute:
                - echo ""test""
[/YAML]
When you trigger a run with custom configuration, the UI uses the configurations in the YAML file to control user input in the **Run With Custom Configuration** panel:  
Old UI  
In the YAML, if `allowCustom`is set as `true`, you can enter any value for that field to define your own value for the run. In the example above, you can enter a custom value for **step_env2** , in addition to the values that are already defined. Note that the custom value is available for a specific run only.  
Old UI  
#### Integrations
A step that uses an integration can access an integration's properties through environment variables. The form of the environment variable is:
`int_<integration name>_<tag>`
For example, you can access the URL property of an Artifactory Integration named `myArtifactory` through an environment variable:
[YAML]
    $ printenv int_myArtifactory_url
    https://mycompany.com/artifactory/
[/YAML]
You can also access the properties of an integration through a resource that uses it. The form of the environment variable is:
`res_<resource_name>_<integration tag name>_<tag>`
For example, an Image resource specifies an Artifactory integration in its `registry` tag. You can access the URL of the Artifactory integration used by an Image resource called `myImage` as follows:
[YAML]
    $ printenv res_myImage_registry_url    
    https://mycompany.com/artifactory/
[/YAML]
For more information, see Pipelines Integrations.
#### Resources
A step that specifies a resource in `inputresources` can access the properties of that resource through environment variables. The form of the environment variable is:
`res_<resource name>_<tag>`
For example, you can access the `imageName` property of an Image resource `myImage` through an environment variable:
[YAML]
    $ printenv res_myImage_imageName
    jfrog.local:5000/alpine37
[/YAML]
Some resources maintain additional properties that may be accessed as environment variables. These resource types include:
  * GitRepo


For more information, see Pipelines Resources.
##### Resource-based State
Additionally, you can add your own properties to a resource that is specified in `outputresources` using the `write_output` utility function. This can be used to create stateful pipelines.
The newly attached properties can be accessed as environment variables of the form `res_{Resource Name}_{Key Name}`. For example, the following creates three properties in the resource `myImage`.
[YAML]
    write_output myImage sport=""baseball"" equipment=""bat"" field=""diamond""
[/YAML]
When the resource is specified in a step's `inputsources`, these properties can be accessed as the following environment variables:
[YAML]
    $ printenv res_myImage_baseball
    baseball
    $ printenv res_myImage_equipment
    bat
    $ printenv res_myImage_field
    diamond
[/YAML]
For more information on run state, see Creating Stateful Pipelines.
#### Run State
Your pipeline can define its own environment variables and add them to the pipeline's run state. These will be available to all steps in the run of the pipeline.
Use the utility functions for run state management to add new environment variables to the current run state. For example, the following line in the execution block of a step:
[YAML]
    add_run_variables hero=""Superman"" villain=""Lex Luthor""
[/YAML]
creates two environment variables available to the current and all future steps of the pipeline's run:
[YAML]
    $ printenv hero
    Superman
    $ printenv villain
    Lex Luthor
[/YAML]
For more information on run state, see Creating Stateful Pipelines.
#### Pipeline State
Your pipeline can define its own environment variables and add them to the pipeline state. These will be available to all runs of that pipeline.
Use the utility functions for pipeline state management to add new environment variables to the current run state. For example, the following line in the execution block of a step:
[YAML]
    add_pipeline_variables animal=""dog"" vegetable=""carrot""
[/YAML]
creates two environment variables available to all run of the pipeline where they were defined:
[YAML]
    $ printenv animal
    dog
    $ printenv vegetable
    carrot
[/YAML]
For more information on pipeline state, see Creating Stateful Pipelines.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
159,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines templates enable users to define templates for pipeline definitions and use these templates for creating pipelines.
Pipelines supports the following types of templates:
  *  **Global Templates** : A global template is a pre-defined pipeline template that can be used as a starting point for creating pipelines. It contains a set of predefined steps and configurations that can be customized and reused across multiple pipeline sources. For more information, see Global Templates.
  *  **System Templates** : A system template is added and maintained by Pipelines administrators. It is defined in a specific namespace and version, and contains the necessary configurations for the pipeline source. For more information, see System Templates.
  *  **Local Templates** : A local template is a copy of a system template or a fully customized template for a pipeline that is saved in your pipeline source repository, disconnected from the centrally available template, and will not be updated if the central template is updated. For more information, see Local Templates.


[/INST]### Managing Pipelines Templates
For information about managing templates, such as releasing and retiring template versions, see Managing Pipelines Templates.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
160,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Global Templates are a set of templates that are shipped with Pipelines and available to all Pipelines users.
To use global templates while adding a pipeline source, a `values.yml` file should be available in the pipeline source repository. As shown in the examples below, the `values.yml` file should contain the values required for the global template to create the pipeline definition.
Pipelines currently includes the following global templates:
  * HelloWorld
  * GoCI
  * GradleCI
  * MavenCI
  * NpmCI
  * PromoteCI
  * DockerBuildAndPush
  * HelmPublishAndDeploy
  * Artifactory Artifact Cleanup Template (1.40 and higher)


[/INST]### Note
For information about using Global Templates, see the section Using Global Templates to Create a Pipeline Source.
### See it Live
Click  **here**  to see these Global templates in action.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
161,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The HelloWorld template creates a simple pipeline that showcases a few of the basic features of JFrog Pipelines.
  * Parallel steps
  * Reading and writing variables that persist across different steps in your pipeline
  * Reading from and writing to resources
  * Setting environment variables


This template does not require any special configuration or pre-existing integration for you to try.
 **HelloWorld YAML**
[YAML]
    resources:
      - name: {{ .Values.namePrefix }}_bag
        type: PropertyBag
        configuration:
          releaseVersion: """"
    
    {{ if .Values.repo.integrationName }}
      - name: {{ .Values.namePrefix }}_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repo.path }}
          gitProvider: {{ .Values.repo.integrationName }}
          branches:
            include: {{ .Values.repo.branchPattern }}
    {{ end }}
    
    pipelines:
      - name: {{ .Values.namePrefix }}_hello_world
        steps:
          - name: first
            type: Bash
            {{ if .Values.repo.integrationName }}
            configuration:
              inputResources:
                - name: {{ .Values.namePrefix }}_repo
            {{ end }}
            execution:
              onStart:
                - echo ""onStart can be used to do some setup or initialization of your dependencies to prepare for the execution.""
              onExecute:
                - echo ""onExecute is the main section where you would issue your build/test commands""
                - jfrog --version
              {{ if .Values.repo.integrationName }}
                - echo ""{{ .Values.namePrefix }}_repo sha is $(find_resource_variable {{ .Values.namePrefix }}_repo commitSha)""
              {{ end }}
              onSuccess:
                - echo ""onSuccess executes if the onStart or onExecute sections completed without errors""
              onFailure:
                - echo ""onFailure executes if either onStart or onExecute end in failure or error.""
              onComplete:
                - echo ""onComplete executes after onSuccess or onFailure""
    
          - name: variable_selection
            type: Bash
            configuration:
              inputSteps:
                - name: first
              environmentVariables:
                forceFail:
                  default: ""false""
                  description: ""set to 'true' to force the step to fail.""
                  values:
                    - ""false""
                    - ""true""
            execution:
              onStart:
                - if [ ${forceFail} == ""true"" ]; then exit 1; fi
              onExecute:
                - echo ""This step will succeed""
              onFailure:
                - echo ""step has failed.""
              onSuccess:
                - echo ""step has succeeded.""
              onComplete:
                - echo ""onComplete executes regardless of success or failure. forceFail is ${forceFail}""
    
          - name: add_run_variable
            type: Bash
            configuration:
              inputSteps:
                - name: first
            execution:
              onStart:
                - echo ""This step will save a value to a run variable.""
              onExecute:
                - add_run_variables step_info=""runNumber $run_number, step_name $step_name, pipeline_name $pipeline_name""
    
          - name: write_to_resource
            type: Bash
            configuration:
              inputSteps:
                - name: add_run_variable
                - name: variable_selection
              outputResources:
                - name: {{ .Values.namePrefix }}_bag
            execution:
              onStart:
                - echo ""Printing the run variable 'step_info' from previous step""
                - echo ""${step_info}""
              onExecute:
                - echo ""Updating resource {{ .Values.namePrefix }}_bag with new key/value pairs""
                - write_output {{ .Values.namePrefix }}_bag ""timestamp=$(date +%s)""
                - write_output {{ .Values.namePrefix }}_bag ""releaseVersion=${pipeline_name}.${run_number}""
                - write_output {{ .Values.namePrefix }}_bag ""stepInfo=${step_info}""
    
          - name: read_from_resource
            type: Bash
            configuration:
              inputResources:
                - name: {{ .Values.namePrefix }}_bag
            execution:
              onExecute:
                - echo ""printing resource details""
                - echo ""Release Version is -- ${res_{{ .Values.namePrefix }}_bag_releaseVersion}""
                - echo ""Release timestamp is -- ${res_{{ .Values.namePrefix }}_bag_timestamp}""
                - echo ""Step Info is -- ${res_{{ .Values.namePrefix }}_bag_stepInfo}""
[/YAML]
 **values.yml** : This is a sample `values.yml`. This can be edited to create your own version of the file.
[YAML]
    [/INST]# This string will prefix the ""HelloWorld"" resource and pipeline.
    # it will allow you to add the pipeline multiple times with unique names
    namePrefix: intro
    
    ##
    # The following section is optional. If you have a repository you'd like to
    # experiment with, you can add it here.
    ##
    repo:
      # Before creating your Pipeline Source, create an integration for one of the
      # source control providers and put the integration name here.
      integrationName: myGitIntegration
    
      # this is the full repository path. Usually org/repo or user/project
      # depending on which source control you use.
      path: ""myorgname/myreponame""
    
      # branchPattern is a regex, so you could listen to all
      # features, for example, with ""^feature.*"", or you can
      # just configure it for an individual branch
      branchPattern: ""^main$""
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
162,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>[/INST]### See it Live
Click  **here**  to see this Global template in action.
The GoCI template creates a pipeline that showcases the features of the GoBuild, GoPublishBinary, and GoPublishModule native steps. These features include:
  * Building a Go binary and pushing the resulting artifact to Artifactory
  * Building a Go module and pushing the resulting artifact to Artifactory
  * Publishing Artifactory builds and updating output BuildInfo resources
  * Utilizing JFrog Xray to scan the artifacts for security vulnerabilities
  * Writing to output FileSpec resources that can be connected other pipelines.


This template can be used for binaries, modules, or both and the required configurations vary depending on the ultimate goal. Configuration must include the required settings for either GoBuild, GoPublishBinary, or GoPublishModule and may include both to publish a binary and module.
In all cases these configurations must be set up:
  * An Artifactory integration for resolving dependencies and publishing artifacts
  * A Git integration (like Github ) that can connect with your Gradle projectGitHub Integration
  * A resolver Artifactory repository from which to resolve dependencies


Publishing a binary with GoBuild and GoPublishBinary requires:
  * A binaryTargetRepository Artifactory repository to which to publish
  * A command to run as part of the GoBuild step (the part following ""go"")
  * An output file name


Publishing a binary with GoPublishBinary requires:
  * A moduleTargetRepository Artifactory repository to which to publish
  * A version


 **GoCI YAML**
[YAML]
    resources:
      - name: {{ .Values.namePrefix }}_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repo.path }}
          gitProvider: {{ .Values.repo.gitIntegration }}
          branches:
            include: {{ .Values.repo.branchPattern | default ""main"" }}
    
    {{ if .Values.publishBuild }}
      {{ if .Values.goConfig.binaryTargetRepository }}
      - name: {{ .Values.namePrefix }}_binary_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artIntegration }}
    
      - name: {{ .Values.namePrefix }}_binary_spec
        type: FileSpec
        configuration:
          sourceArtifactory: {{ .Values.artIntegration }}
          pattern: '{{ .Values.binaryBuildSpecPattern | default ""*"" }}'
          buildName: ${JFROG_CLI_BUILD_NAME}
          buildNumber: ${JFROG_CLI_BUILD_NUMBER}
      {{ end }}
    
      {{ if .Values.goConfig.moduleTargetRepository }}
      - name: {{ .Values.namePrefix }}_module_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artIntegration }}
    
      - name: {{ .Values.namePrefix }}_module_spec
        type: FileSpec
        configuration:
          sourceArtifactory: {{ .Values.artIntegration }}
          pattern: '{{ .Values.moduleBuildSpecPattern | default ""*"" }}'
          buildName: ${JFROG_CLI_BUILD_NAME}
          buildNumber: ${JFROG_CLI_BUILD_NUMBER}
      {{ end }}
    
    {{ end }}
    
    
    pipelines:
      - name: {{ .Values.namePrefix }}_go_ci
        steps:
        {{ if .Values.goConfig.binaryTargetRepository }}
          - name: build
            type: GoBuild
            configuration:
              affinityGroup: {{ .Values.namePrefix }}_go_binary_ci
              environmentVariables:
                JFROG_CLI_BUILD_NAME: {{ .Values.namePrefix }}_go_binary_ci
                JFROG_CLI_BUILD_NUMBER: ${run_id}
              integrations:
                - name: {{ .Values.artIntegration }}
              inputResources:
                - name: {{ .Values.namePrefix }}_repo
            {{ if .Values.goConfig.command }}
              goCommand: {{ .Values.goConfig.command }}
            {{ end }}
            {{ if .Values.goConfig.sourceLocation }}
              sourceLocation: {{ .Values.goConfig.sourceLocation }}
            {{ end }}
            {{ if .Values.goConfig.resolverRepo }}
              resolverRepo: {{ .Values.goConfig.resolverRepo }}
            {{ end }}
            {{ if .Values.goConfig.outputLocation }}
              outputLocation: {{ .Values.goConfig.outputLocation }}
            {{ end }}
            {{ if .Values.goConfig.outputFile }}
              outputFile: {{ .Values.goConfig.outputFile }}
            {{ end }}
    
          - name: publish
            type: GoPublishBinary
            configuration:
              affinityGroup: {{ .Values.namePrefix }}_go_binary_ci
              environmentVariables:
                JFROG_CLI_BUILD_NAME: {{ .Values.namePrefix }}_go_binary_ci
                JFROG_CLI_BUILD_NUMBER: ${run_id}
              integrations:
                - name: {{ .Values.artIntegration }}
              inputSteps:
                - name: build
            {{ if .Values.publishBuild }}
              outputResources:
                - name: {{ .Values.namePrefix }}_binary_info
                - name: {{ .Values.namePrefix }}_binary_spec
            {{ end }}
            {{ if .Values.publishBuild }}
              autoPublishBuildInfo: true
            {{ end }}
            {{ if and .Values.scanBuild .Values.publishBuild }}
              forceXrayScan: true
            {{ end }}
              targetRepository: {{ .Values.goConfig.binaryTargetRepository }}
    
          {{ if .Values.publishBuild }}
            execution:
              onSuccess:
                - write_output {{ .Values.namePrefix }}_binary_spec buildName=""${JFROG_CLI_BUILD_NAME}""
                - write_output {{ .Values.namePrefix }}_binary_spec buildNumber=""${JFROG_CLI_BUILD_NUMBER}""
          {{ end }}
        {{ end }}
    
        {{ if .Values.goConfig.moduleTargetRepository }}
          - name: module
            type: GoPublishModule
            configuration:
              environmentVariables:
                JFROG_CLI_BUILD_NAME: {{ .Values.namePrefix }}_go_module_ci
                JFROG_CLI_BUILD_NUMBER: ${run_id}
              integrations:
                - name: {{ .Values.artIntegration }}
              inputResources:
                - name: {{ .Values.namePrefix }}_repo
            {{ if .Values.publishBuild }}
              outputResources:
                - name: {{ .Values.namePrefix }}_module_info
                - name: {{ .Values.namePrefix }}_module_spec
            {{ end }}
            {{ if .Values.goConfig.sourceLocation }}
              sourceLocation: {{ .Values.goConfig.sourceLocation }}
            {{ end }}
            {{ if .Values.goConfig.version }}
              version: {{ .Values.goConfig.version }}
            {{ end }}
              targetRepository: {{ .Values.goConfig.moduleTargetRepository }}
            {{ if .Values.goConfig.resolverRepo }}
              resolverRepo: {{ .Values.goConfig.resolverRepo }}
            {{ end }}
            {{ if .Values.publishBuild }}
              autoPublishBuildInfo: true
            {{ end }}
            {{ if and .Values.scanBuild .Values.publishBuild }}
              forceXrayScan: true
            {{ end }}
          {{ if .Values.publishBuild }}
            execution:
              onSuccess:
                - write_output {{ .Values.namePrefix }}_module_spec buildName=""${JFROG_CLI_BUILD_NAME}""
                - write_output {{ .Values.namePrefix }}_module_spec buildNumber=""${JFROG_CLI_BUILD_NUMBER}""
          {{ end }}
        {{ end }}
[/YAML]
 **values.yml** : This is a sample`values.yml`. This can be edited to create your own version of the file.
[YAML]
    ## This string will prefix the resources and pipeline, allowing you to add
    ## the pipeline source multiple times with unique names.
    namePrefix: sample
    
    ## The name of your Artifactory integration
    artIntegration: myArtIntegrationName
    
    ##### Artifactory Build Info
    ## set this to `true` to publish builds
    #publishBuild: true
    ## set this to `true` to scan the published build(s)
    #scanBuild: true
    ## When publishBuild is enabled, a FileSpec resource will be created
    ## that points to each published build. Here you can specify a pattern
    ## to further narrow down the Artifacts that are referenced by the specs.
    ## The binaryBuildSpecPattern filters the files included for the FileSpec
    ## output by the GoPublishBinary step.
    #binaryBuildSpecPattern: ""*""
    ## The moduleBuildSpecPattern filters the files included for the FileSpec
    ## output by the GoPublishModule step.
    #moduleBuildSpecPattern: ""*""
    
    ##### Repository Details
    repo:
      path: org/repo
      gitIntegration: myGitIntegration
      branchPattern: ""^main$""
    
    goConfig:
      ### Settings used for both modules and binaries:
      ## The location of the source code in your repository.
      ## Commands will be executed here.
      sourceLocation: "".""
      ## This repository, if specified, must first be created in Artifactory.
      ## The resolver is used to resolve dependencies. It is recommended to
      ## use a remote repository so that any external dependencies will be cached
      ## in your local Artifactory instance.
      resolverRepo: go-remote
    
      ### Settings used only for GoBuild and GoPublishBinary:
      ## Required to use these steps, the repository to which to upload the binary.
      ## The repository must first be created in Artifactory.
      binaryTargetRepository: go-local
      ## Here you specify any additional arguments to the command to execute
      ## it is executed as `go {{ command }}`
      command: ""build -o $outputLocation/$outputFile""
      outputLocation: ""GoBuild""
      outputFile: ""myOutputFile""
    
      ### Settings used only for GoPublishModule:
      ## Required to use this step, the repository to which to upload the module.
      ## The repository must first be created in Artifactory.
      moduleTargetRepository: go-local
      ## The version specified in the go-publish command.
      version: v1.0.${run_number}
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
163,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>[/INST]### See it Live
Click  **here**  to see this Global template in action.
The GradleCI template creates a pipeline that showcases the features of the GradleBuild native step. These features include:
  * Building a Gradle project and pushing the resulting artifacts to Artifactory
  * Publishing an Artifactory build and updating an output BuildInfo resources
  * Utilizing JFrog Xray to scan the artifacts for security vulnerabilities
  * Writing to an output FileSpec resources that can be connected to another pipeline.


This template requires a few configurations to be set up:
  * An Artifactory integration for resolving dependencies and publishing artifacts
  * A Git integration (like GitHub) that can connect with your Gradle projectGitHub Integration
  * Resolver and deployer Artifactory repositories
  * A Gradle command to run as part of the GradleBuild step (the part following ""gradle"")


 **GradleCI YAML**
[YAML]
    resources:
      - name: {{ .Values.namePrefix }}_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repo.path }}
          gitProvider: {{ .Values.repo.gitIntegration }}
          branches:
            include: {{ .Values.repo.branchPattern | default ""main"" }}
    
    {{ if .Values.publishBuild }}
      - name: {{ .Values.namePrefix }}_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artIntegration }}
    
      - name: {{ .Values.namePrefix }}_spec
        type: FileSpec
        configuration:
          sourceArtifactory: {{ .Values.artIntegration }}
          pattern: '{{ .Values.buildSpecPattern | default ""*"" }}'
          buildName: ${JFROG_CLI_BUILD_NAME}
          buildNumber: ${JFROG_CLI_BUILD_NUMBER}
    {{ end }}
    
    
    pipelines:
      - name: {{ .Values.namePrefix }}_gradle_ci
    {{ if .Values.publishBuild }}
        configuration:
          environmentVariables:
            readOnly:
              JFROG_CLI_BUILD_NAME: ${pipeline_name}
              JFROG_CLI_BUILD_NUMBER: ${run_id}
    {{ end }}
        steps:
          - name: build
            type: GradleBuild
            configuration:
              integrations:
                - name: {{ .Values.artIntegration }}
              inputResources:
                - name: {{ .Values.namePrefix }}_repo
            {{ if .Values.publishBuild }}
              outputResources:
                - name: {{ .Values.namePrefix }}_info
                - name: {{ .Values.namePrefix }}_spec
            {{ end }}
            {{ if .Values.publishBuild }}
              autoPublishBuildInfo: true
            {{ end }}
            {{ if and .Values.scanBuild .Values.publishBuild }}
              forceXrayScan: true
            {{ end }}
            {{ if .Values.gradleConfig.command }}
              gradleCommand: {{ .Values.gradleConfig.command }}
            {{ end }}
            {{ if .Values.gradleConfig.sourceLocation }}
              sourceLocation: {{ .Values.gradleConfig.sourceLocation }}
            {{ end }}
            {{ if .Values.useWrapper }}
              useWrapper: true
            {{ end }}
            {{ if .Values.gradleConfig.resolverRepo }}
              resolverRepo: {{ .Values.gradleConfig.resolverRepo }}
            {{ end }}
            {{ if .Values.gradleConfig.deployerRepo }}
              deployerRepo: {{ .Values.gradleConfig.deployerRepo }}
            {{ end }}
          {{ if .Values.publishBuild }}
            execution:
              onSuccess:
                - write_output {{ .Values.namePrefix }}_spec buildName=""${JFROG_CLI_BUILD_NAME}""
                - write_output {{ .Values.namePrefix }}_spec buildNumber=""${JFROG_CLI_BUILD_NUMBER}""
          {{ end }}
[/YAML]
 **values.yml** : This is a sample `values.yml`. This can be edited to create your own version of the file.
[YAML]
    ## This string will prefix the resources and pipeline, allowing you to add
    ## the pipeline source multiple times with unique names.
    namePrefix: sample
    
    ## The name of your Artifactory integration
    artIntegration: myArtIntegrationName
    
    ##### Artifactory Build Info
    ## set this to `true` to publish a build
    #publishBuild: true
    ## set this to `true` to scan the published build
    #scanBuild: true
    ## When publishBuild is enabled, a FileSpec resource will be created
    ## that points to the published build. Here you can specify a pattern
    ## to further narrow down the Artifacts that are referenced by the spec.
    #buildSpecPattern: ""*""
    
    ##### Repository Details
    repo:
      path: org/repo
      gitIntegration: myGitIntegration
      branchPattern: ""^main$""
    
    ##### Gradle configuration
    gradleConfig:
      ## Here you specify the command to execute
      ## it is executed as `gradle {{ command }}`
      command: ""hello""
      sourceLocation: "".""
      ## These repositories must first be created in Artifactory.
      ## The resolver is used to resolve dependencies. It is recommended to
      ## use a remote repository so that any external dependencies will be cached
      ## in your local Artifactory instance. Deployer is where your Artifacts
      ## will be uploaded.
      resolverRepo: gradle-remote
      deployerRepo: gradle-local
      ## If your project uses the Gradle Wrapper set useWrapper to true
      #useWrapper: true  
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
164,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>[/INST]### See it Live
Click  **here**  to see this Global template in action.
The MavenCI template creates a pipeline that showcases the features of the MvnBuild native step. These features include:
  * Building a Maven project and pushing the resulting artifacts to Artifactory
  * Publishing an Artifactory build and updating an output BuildInfo resource
  * Utilizing JFrog Xray to scan the artifacts for security vulnerabilities
  * Writing to an output FileSpec resource that can be connected to another pipeline


This template requires a few configurations to be set up:
  * An Artifactory integration for resolving dependencies and publishing artifacts
  * A Git integration (like GitHub) that can connect with your Maven projectGitHub Integration
  * Resolver and deployer Artifactory Maven repositories for snapshots and releases


 **MavenCI YAML**
[YAML]
    resources:
      - name: {{ .Values.namePrefix }}_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repo.path }}
          gitProvider: {{ .Values.repo.gitIntegration }}
          branches:
            include: {{ .Values.repo.branchPattern | default ""main"" }}
    
    {{ if .Values.publishBuild }}
      - name: {{ .Values.namePrefix }}_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artIntegration }}
    
      - name: {{ .Values.namePrefix }}_spec
        type: FileSpec
        configuration:
          sourceArtifactory: {{ .Values.artIntegration }}
          pattern: '{{ .Values.buildSpecPattern | default ""*"" }}'
          buildName: ${JFROG_CLI_BUILD_NAME}
          buildNumber: ${JFROG_CLI_BUILD_NUMBER}
    {{ end }}
    
    
    pipelines:
      - name: {{ .Values.namePrefix }}_maven_ci
    {{ if .Values.publishBuild }}
        configuration:
          environmentVariables:
            readOnly:
              JFROG_CLI_BUILD_NAME: ${pipeline_name}
              JFROG_CLI_BUILD_NUMBER: ${run_id}
    {{ end }}
        steps:
          - name: build
            type: MvnBuild
            configuration:
              integrations:
                - name: {{ .Values.artIntegration }}
              inputResources:
                - name: {{ .Values.namePrefix }}_repo
            {{ if .Values.publishBuild }}
              outputResources:
                - name: {{ .Values.namePrefix }}_info
                - name: {{ .Values.namePrefix }}_spec
              autoPublishBuildInfo: true
            {{ end }}
            {{ if and .Values.scanBuild .Values.publishBuild }}
              forceXrayScan: true
            {{ end }}
            {{ if .Values.mavenConfig.command }}
              mvnCommand: {{ .Values.mavenConfig.command }}
            {{ end }}
            {{ if .Values.mavenConfig.sourceLocation }}
              sourceLocation: {{ .Values.mavenConfig.sourceLocation }}
            {{ end }}
            {{ if .Values.mavenConfig.resolverSnapshotRepo }}
              resolverSnapshotRepo: {{ .Values.mavenConfig.resolverSnapshotRepo }}
            {{ end }}
            {{ if .Values.mavenConfig.deployerSnapshotRepo }}
              deployerSnapshotRepo: {{ .Values.mavenConfig.deployerSnapshotRepo }}
            {{ end }}
            {{ if .Values.mavenConfig.resolverReleaseRepo }}
              resolverReleaseRepo: {{ .Values.mavenConfig.resolverReleaseRepo }}
            {{ end }}
            {{ if .Values.mavenConfig.deployerReleaseRepo }}
              deployerReleaseRepo: {{ .Values.mavenConfig.deployerReleaseRepo }}
            {{ end }}
          {{ if .Values.publishBuild }}
            execution:
              onSuccess:
                - write_output {{ .Values.namePrefix }}_spec buildName=""${JFROG_CLI_BUILD_NAME}""
                - write_output {{ .Values.namePrefix }}_spec buildNumber=""${JFROG_CLI_BUILD_NUMBER}""
          {{ end }}
[/YAML]
 **values.yml** : This is a sample`values.yml`. This can be edited to create your own version of the file.
[YAML]
    ## This string will prefix the resources and pipeline.
    ## It will allow you to add a pipeline source from this
    ## template multiple times with unique names.
    namePrefix: sample
    
    ## The name of your Artifactory integration
    artIntegration: myArtIntegrationName
    
    ##### Artifactory Build Info
    ## set this to `true` to publish a build
    #publishBuild: true
    ## set this to `true` to scan the published build
    #scanBuild: true
    ## When publishBuild is enabled, a FileSpec resource will be created
    ## that points to the published build. Here you can specify a pattern
    ## to further narrow down the Artifacts that are referenced by the spec.
    #buildSpecPattern: ""*""
    
    ##### Repository Details
    repo:
      path: org/repo
      gitIntegration: myGitIntegration
      branchPattern: ""^main$""
    
    ##### Maven configuration
    mavenConfig:
      ## Here you specify the command to execute
      ## it is executed as `mvn {{ command }}`
      command: clean install
      sourceLocation: ""MavenCI/sample-project""
      ## These repos must first be created in Artifactory.
      ## resolver is used to resolve dependencies. It is recommended to
      ## use a remote repository so that any external dependencies will be cached
      ## in your local Artifactory instance. Deployer is where your Artifacts
      ## will be uploaded.
      resolverSnapshotRepo: maven-snapshot-remote
      resolverReleaseRepo: maven-release-remote
      deployerSnapshotRepo: maven-snapshot-local
      deployerReleaseRepo: maven-release-local
    
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
165,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The NpmCI template creates a pipeline that showcases the features of the NpmBuild and NpmPublish native steps. These features include:
  * Building an npm project and pushing the resulting artifacts to Artifactory
  * Publishing an Artifactory build and updating an output BuildInfo resource
  * Utilizing JFrog Xray to scan the artifacts for security vulnerabilities
  * Writing to an output FileSpec resource that can be connected to another pipeline.


This template requires a few configurations to be set up:
  * An Artifactory integration for resolving dependencies and publishing artifacts
  * A Git integration (like GitHub) that can connect with your npm projectGitHub Integration
  * Resolver and deployer Artifactory repositories


 **NpmCI YAML**
[YAML]
    resources:
      - name: {{ .Values.namePrefix }}_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repo.path }}
          gitProvider: {{ .Values.repo.gitIntegration }}
          branches:
            include: {{ .Values.repo.branchPattern | default ""main"" }}
    
    {{ if .Values.publishBuild }}
      - name: {{ .Values.namePrefix }}_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artIntegration }}
    
      - name: {{ .Values.namePrefix }}_spec
        type: FileSpec
        configuration:
          sourceArtifactory: {{ .Values.artIntegration }}
          pattern: '{{ .Values.buildSpecPattern | default ""*"" }}'
          buildName: ${JFROG_CLI_BUILD_NAME}
          buildNumber: ${JFROG_CLI_BUILD_NUMBER}
    {{ end }}
    
    
    pipelines:
      - name: {{ .Values.namePrefix }}_npm_ci
        configuration:
          affinityGroup: {{ .Values.namePrefix }}_npm_ci
        {{ if .Values.publishBuild }}
          environmentVariables:
            readOnly:
              JFROG_CLI_BUILD_NAME: ${pipeline_name}
              JFROG_CLI_BUILD_NUMBER: ${run_id}
        {{ end }}
        steps:
          - name: build
            type: NpmBuild
            configuration:
              integrations:
                - name: {{ .Values.artIntegration }}
              inputResources:
                - name: {{ .Values.namePrefix }}_repo
    
            {{ if .Values.npmConfig.npmArgs }}
              npmArgs: {{ .Values.npmConfig.npmArgs }}
            {{ end }}
            {{ if .Values.npmConfig.sourceLocation }}
              sourceLocation: {{ .Values.npmConfig.sourceLocation }}
            {{ end }}
            {{ if .Values.npmConfig.resolverRepo }}
              resolverRepo: {{ .Values.npmConfig.resolverRepo }}
            {{ end }}
    
          - name: publish
            type: NpmPublish
            configuration:
              integrations:
                - name: {{ .Values.artIntegration }}
              inputSteps:
                - name: build
            {{ if .Values.publishBuild }}
              outputResources:
                - name: {{ .Values.namePrefix }}_info
                - name: {{ .Values.namePrefix }}_spec
            {{ end }}
            {{ if .Values.publishBuild }}
              autoPublishBuildInfo: true
            {{ end }}
            {{ if and .Values.scanBuild .Values.publishBuild }}
              forceXrayScan: true
            {{ end }}
            {{ if .Values.npmConfig.deployerRepo }}
              deployerRepo: {{ .Values.npmConfig.deployerRepo }}
            {{ end }}
          {{ if .Values.publishBuild }}
            execution:
              onSuccess:
                - write_output {{ .Values.namePrefix }}_spec buildName=""${JFROG_CLI_BUILD_NAME}""
                - write_output {{ .Values.namePrefix }}_spec buildNumber=""${JFROG_CLI_BUILD_NUMBER}""
          {{ end }}
[/YAML]
 **values.yml** : This is a sample`values.yml`. This can be edited to create your own version of the file.
[YAML]
    [/INST]## This string will prefix the resources and pipeline, allowing you to add
    ## the pipeline source multiple times with unique names.
    namePrefix: sample
    
    ## The name of your Artifactory integration
    artIntegration: myArtIntegrationName
    
    ##### Artifactory Build Info
    ## set this to `true` to publish a build
    #publishBuild: true
    ## set this to `true` to scan the published build
    #scanBuild: true
    ## When publishBuild is enabled, a FileSpec resource will be created
    ## that points to the published build. Here you can specify a pattern
    ## to further narrow down the Artifacts that are referenced by the spec.
    #buildSpecPattern: ""*""
    
    ##### Repository Details
    repo:
      path: org/repo
      gitIntegration: myGitIntegration
      branchPattern: ""^main$""
    
    npmConfig:
      ## Here you specify any additional arguments to the command to execute
      ## it is executed as `npm install {{ npmArgs }}`
      npmArgs: ""--no-install""
      sourceLocation: "".""
      ## These repositories must first be created in Artifactory.
      ## The resolver is used to resolve dependencies. It is recommended to
      ## use a remote repository so that any external dependencies will be cached
      ## in your local Artifactory instance. Deployer is where your Artifacts
      ## will be uploaded.
      resolverRepo: npm-remote
      deployerRepo: npm-local
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
166,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The PromoteCI template creates a pipeline that showcases the features of the PromoteBuild native step. These features include:
  * Promoting an Artifactory build created by another pipeline
  * Updating an output BuildInfo resource with the promoted build info
  * Optionally, using Signed Pipelines that can be connected to another pipeline


This template requires a few configurations to be set up:
  * An input BuildInfo resource with build to promote, typically an output from another pipeline
  * An Artifactory integration for the output BuildInfo resource that will be used to promote the build
  * A target Artifactory repository


 **PromoteCI YAML**
[YAML]
    resources:
      - name: {{ .Values.namePrefix }}_promotedBuildInfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artIntegration }}
    
    pipelines:
      - name: {{ .Values.namePrefix }}_promote_ci
        configuration:
          environmentVariables:
            readOnly:
              JFROG_CLI_BUILD_NAME: ${pipeline_name}
              JFROG_CLI_BUILD_NUMBER: ${run_id}
        steps:
          - name: promote
            type: PromoteBuild
            configuration:
              inputResources:
                - name: {{ .Values.inputBuildInfoResourceName }}
                {{ if .Values.inputBuildInfoResourceBranch }}
                  branch: {{ .Values.inputBuildInfoResourceBranch }}
                {{ end }}
              outputResources:
                - name: {{ .Values.namePrefix }}_promotedBuildInfo
              targetRepository: {{ .Values.targetRepository }}
            {{ if .Values.includeDependencies }}
              includeDependencies: {{ .Values.includeDependencies }}
            {{ end }}
            {{ if .Values.status }}
              status: {{ .Values.status }}
            {{ end }}
            {{ if .Values.comment }}
              comment: {{ .Values.comment }}
            {{ end }}
            {{ if .Values.copy }}
              copy: {{ .Values.copy }}
            {{ end }}
            {{ if .Values.failOnValidate }}
              failOnValidate: {{ .Values.failOnValidate }}
            {{ end }}
[/YAML]
 **values.yml** : This is a sample `values.yml`. This can be edited to create your own version of the file.
[YAML]
    [/INST]## This string will prefix the resources and pipeline, allowing you to add
    ## the pipeline source multiple times with unique names.
    namePrefix: sample
    
    ## The name of your Artifactory integration
    artIntegration: myArtIntegrationName
    
    ## The name of the BuildInfo resource referencing the build to be promoted
    inputBuildInfoResourceName: myBuildInfo
    ## If your input BuildInfo resource is from a multi-branch pipeline source and
    ## you wish to use the resource from another branch, specify that branch name.
    #inputBuildInfoResourceBranch: myBranchToUseAsInput
    
    ##### Promotion Info
    ## the repository to which to promote the build
    targetRepository: myPromotionRepo
    ## set this to `true` to include dependencies in the promotion
    #includeDependencies: true
    ## set this to `true` to copy and not move the promoted files
    #copy: true
    ## set this to `true` to validate the input BuildInfo through Signed Pipelines
    #failOnValidate: true
    ## the promotion status for the build
    #status: TEST
    ## the promotion comment for the build
    #comment: ""Promoted by run number ${run_number}.""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
167,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>[/INST]### See it Live
Click  **here**  to see this Global template in action.
The DockerBuildAndPush template creates a pipeline that builds a Docker image from an SCM repository and pushes it to a Docker registry. Version 1.0.0 of this template uses JFrog CLI v1 and version 1.1.0 uses JFrog CLI v2. Template version 1.0.0 requires `targetRepository` and `sourceRepository` settings (see template readme) and template version 1.1.0 does not use these settings.
 **DockerBuildAndPush YAML**
[YAML]
    resources:
      - name: {{ .Values.GitRepo.name | default ""GitRepoRes"" }}
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.GitRepo.gitProvider }}
          path: {{ .Values.GitRepo.path }}
          {{- if or (.Values.GitRepo.branches.include) (.Values.GitRepo.branches.exclude) }}
          branches:
            {{- if .Values.GitRepo.branches.include }}
              include: {{ .Values.GitRepo.branches.include }}
            {{- end }}
            {{- if .Values.GitRepo.branches.exclude }}
              exclude: {{ .Values.GitRepo.branches.exclude }}
            {{- end }}
          {{- end }}
    
      - name: {{ .Values.Image.name | default ""ImageRes"" }}
        type: Image
        configuration:
          registry: {{ .Values.artifactoryIntegration }}
          imageName: {{ .Values.DockerBuild.dockerImageName }}
          imageTag: {{ .Values.DockerBuild.dockerImageTag | default ""latest"" }}
          autoPull: {{ .Values.Image.autopull | default true }}
    
      {{- if .Values.BuildInfo }}
      - name: {{ .Values.BuildInfo.name | default ""BuildInfo"" }}
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
          buildName: {{ .Values.DockerPush.name | default ""DockerPush"" }}
          buildNumber: {{ .Values.BuildInfo.buildNumber | default 1 }}
      {{- end }}
    
    pipelines:
      - name: {{ .Values.Pipeline.name | default ""Pipeline"" }}
        configuration:
          jfrogCliVersion: 2
        steps:
          - name: {{ .Values.DockerBuild.name | default ""DockerBuild"" }}
            type: DockerBuild
            configuration:
              affinityGroup: DockerBuildAndPush
              dockerFileLocation: {{ .Values.DockerBuild.dockerFileLocation | default ""."" | quote }}
              dockerFileName: {{ .Values.DockerBuild.dockerFileName | default ""Dockerfile"" }}
              dockerImageName: {{ .Values.DockerBuild.dockerImageName }}
              dockerImageTag: {{ .Values.DockerBuild.dockerImageTag }}
              inputResources:
                - name: {{ .Values.GitRepo.name | default ""GitRepoRes"" }}
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
    
          - name: {{ .Values.DockerPush.name | default ""DockerPush"" }}
            type: DockerPush
            configuration:
              {{- if .Values.BuildInfo }}
              autoPublishBuildInfo: {{ .Values.BuildInfo.autoPublishBuildInfo | default true }}
              {{- end }}
              affinityGroup: DockerBuildAndPush
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
              inputSteps:
                - name: {{ .Values.DockerBuild.name | default ""DockerBuild"" }}
              outputResources:
                - name: {{ .Values.Image.name | default ""ImageRes"" }}
                {{- if .Values.BuildInfo }}
                - name: {{ .Values.BuildInfo.name | default ""BuildInfo"" }}
                {{- end }}
[/YAML]
 **values.yml** : This is a sample `values.yml`. This can be edited to create your own version of the file.
[YAML]
    GitRepo:                                            # Required.
      name: fooGitRepo                                  # Name of the GitRepo resource. Defaults to GitRepoRes.
      gitProvider: github                               # Required. Name of the SCM integration which will be used to fetch the SCM. repository.
      path: jfrog/jfrog-pipelines-simple-example        # Required. Path to the SCM repository.
      branches:
        exclude: 'master'                               # Regex pattern to exclude branches.
        include: ^{{gitBranch}}$                        # Regex pattern to include branches.
    
    artifactoryIntegration: art                         # Required. Name of the artifactory integration using which the docker image will be pushed.
    
    Image:                                              # Required.
      name: fooImage                                    # Name of the Image resource. Defaults to ImageRes.
      autoPull: false                                   # Defaults to true.
    
    DockerBuild:                                        # Required.
      name: foobuild                                    # Name of the DockerBuild step. Defaults to DockerBuild.
      dockerImageName: foo                              # Required. Name of the docker image getting built.
      dockerFileName: foo                               # Required. Name of the Dockerfile.
      dockerFileLocation: '.'                           # Required. Path to Dockerfile.
      dockerImageTag: ${run_number}                     # Name of the docker image tag. Defaults to latest.
    
    Pipeline:
      name: foo                                         # Name of the pipeline. Defaults to Pipeline.
    
    DockerPush:                                         # Required.
      name: foopush                                     # Name of the DockerPush step. Defaults to DockerPush.
    
    BuildInfo:                                          # Optional, if the BuildInfo needs to be published.
      autoPublishBuildInfo: true                        # Defaults to true.
      name: fooBuildInfo                                # Name of the BuildInfo resource. Defaults to BuildInfo.
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
168,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The HelmPublishAndDeploy template creates a pipeline that publishes a Helm Chart to a Helm repository in Artifactory, creates a Build Info, promotes the Build, and then deploys the Helm Chart from the promoted Build resource using the [/INST]#UUID-01e5ef9a-3e52-6e5a-6020-4e86df3d5edfHelmDeploy native step. Version 1.0.0 of this template uses JFrog CLI v1 and version 1.1.0 uses JFrog CLI v2.
 **HelmPublishAndDeploy YAML**
[YAML]
    resources:
      - name: {{ .Values.GitRepo.name | default ""GitRepoRes"" }}
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.GitRepo.gitProvider }}
          path: {{ .Values.GitRepo.path }}
          {{- if or (.Values.GitRepo.branches.include) (.Values.GitRepo.branches.exclude) }}
          branches:
            {{- if .Values.GitRepo.branches.include }}
            include: {{ .Values.GitRepo.branches.include }}
            {{- end }}
            {{- if .Values.GitRepo.branches.exclude }}
            exclude: {{ .Values.GitRepo.branches.exclude }}
            {{- end }}
          {{- end }}
          {{- if or (.Values.GitRepo.files.include) (.Values.GitRepo.files.exclude) }}
          files:
            {{- if .Values.GitRepo.files.include }}
              include: {{ .Values.GitRepo.files.include }}
            {{- end }}
            {{- if .Values.GitRepo.files.exclude }}
              exclude: {{ .Values.GitRepo.files.exclude }}
            {{- end }}
          {{- end }}
    
      - name: {{ .Values.BuildInfo.name | default ""BuildInfoRes"" }}
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
      {{- if .Values.BuildInfoPromoted }}
      - name: {{ .Values.BuildInfoPromoted.name | default ""BuildInfoPromotedRes"" }}
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
      {{- end }}
    
      - name: {{ .Values.HelmChart.name | default ""HelmChartRes"" }}
        type: HelmChart
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
          repository: {{ .Values.HelmChart.repository }}
          chart:  {{ .Values.HelmChart.chartName }}
          version: {{ .Values.HelmChart.chartVersion }}
    
    pipelines:
      - name: {{ .Values.Pipeline.name | default ""HelmDeployPipeline"" }}
        configuration:
          jfrogCliVersion: 2
          {{- if or (.Values.BuildInfo.buildName) (.Values.BuildInfo.buildNumber) }}
          environmentVariables:
            readOnly:
              {{- if .Values.BuildInfo.buildName }}
              JFROG_CLI_BUILD_NAME: {{ .Values.BuildInfo.buildName }}
              {{- end }}
              {{- if .Values.BuildInfo.buildNumber }}
              JFROG_CLI_BUILD_NUMBER: {{ .Values.BuildInfo.buildNumber }}
              {{- end }}
          {{- end }}
        steps:
          - name: {{ .Values.HelmPublish.name | default ""HelmPublish"" }}
            type: HelmPublish
            configuration:
              helmVersion: {{ .Values.HelmChart.helmVersion | default 3 }}
              autoPublishBuildInfo: true
              chartPath: {{ .Values.HelmPublish.chartPath }}
              {{- if .Values.HelmPublish.flags }}
              flags:  {{ .Values.HelmPublish.flags }}
              {{- end }}
              {{- if .Values.HelmPublish.valueFilePaths }}
              valueFilePaths:
                {{- range $.Values.HelmPublish.valueFilePaths }}
                - {{ . }}
                {{- end }}
              {{- end }}
              lint: {{ .Values.HelmPublish.lint | default false }}
              {{- if .Values.HelmPublish.lintFlags }}
              lintFlags: {{ .Values.HelmPublish.lintFlags }}
              {{- end }}
              inputResources:
                - name: {{ .Values.GitRepo.name | default ""GitRepoRes"" }}
              outputResources:
                - name: {{ .Values.BuildInfo.name | default ""BuildInfoRes"" }}
                - name: {{ .Values.HelmChart.name | default ""HelmChartRes"" }}
    
          {{- if and (.Values.PromoteBuild) (.Values.BuildInfoPromoted) }}
          - name: {{ .Values.PromoteBuild.name | default ""PromoteBuild"" }}
            type: PromoteBuild
            configuration:
              copy: {{ .Values.PromoteBuild.copy | default false }}
              targetRepository: {{ .Values.PromoteBuild.targetRepository }}
              inputResources:
                - name: {{ .Values.BuildInfo.name | default ""BuildInfoRes"" }}
                  trigger: {{ .Values.PromoteBuild.autoPromotion | default true }}
              outputResources:
                - name: {{ .Values.BuildInfoPromoted.name | default ""BuildInfoPromotedRes"" }}
          {{- end }}
    
          - name: {{ .Values.HelmDeploy.name | default ""HelmDeploy"" }}
            type: HelmDeploy
            configuration:
              {{- if .Values.HelmChart.namespace }}
              environmentVariables:
                HELM_NAMESPACE: {{ .Values.HelmChart.namespace }}
              {{- end }}
              integrations:
                - name: {{ .Values.kubernetesIntegration }}
              inputResources:
                {{- if and (.Values.PromoteBuild) (.Values.BuildInfoPromoted) }}
                - name: {{ .Values.BuildInfoPromoted.name | default ""BuildInfoPromotedRes"" }}
                {{- else }}
                - name: {{ .Values.BuildInfo.name | default ""BuildInfoRes"" }}
                {{- end }}
              chartPath: {{ .Values.HelmDeploy.chartPath }}
              chartName: {{ .Values.HelmChart.chartName }}
              chartVersion: {{ .Values.HelmChart.chartVersion }}
              helmVersion: {{ .Values.HelmChart.helmVersion | default 3 }}
              {{- if .Values.HelmDeploy.releaseName }}
              releaseName: {{ .Values.HelmDeploy.releaseName }}
              {{- end }}
              {{- if .Values.HelmChart.namespace }}
              namespace:  {{ .Values.HelmChart.namespace }}
              {{- end }}
              {{- if .Values.HelmDeploy.flags }}
              flags:  {{ .Values.HelmDeploy.flags }}
              {{- end }}
              {{- if .Values.HelmDeploy.valueFilePaths }}
              valueFilePaths:
                {{- range $.Values.HelmDeploy.valueFilePaths }}
                - {{ . }}
                {{- end }}
              {{- end }}
              lint: {{ .Values.HelmDeploy.lint | default false }}
              {{- if .Values.HelmDeploy.lintFlags }}
              lintFlags: {{ .Values.HelmDeploy.lintFlags }}
              {{- end }}
              test: {{ .Values.HelmDeploy.test | default false }}
              {{- if .Values.HelmDeploy.testFlags }}
              testFlags: {{ .Values.HelmDeploy.testFlags }}
              {{- end }}
[/YAML]
 **Example**
[YAML]
    resources:
      - name: fooGitRepo
        type: GitRepo
        configuration:
          gitProvider: bb_benha
          path: ~benha/simplehelm
          branches:
            include: master
          files:
            include: simplehelm.*
      - name: fooBuild
        type: BuildInfo
        configuration:
          sourceArtifactory: art_v
      - name: fooBuildPromote
        type: BuildInfo
        configuration:
          sourceArtifactory: art_v
      - name: fooHelmChart
        type: HelmChart
        configuration:
          sourceArtifactory: art_v
          repository: simplecharts
          chart: simplehelm
          version: 0.3.0
    
    pipelines: 
    - name: fooHelmDeploy
    configuration:
      environmentVariables:
        readOnly:
          JFROG_CLI_BUILD_NAME: foo
          JFROG_CLI_BUILD_NUMBER: '${run_number}'
    steps:
      - name: HelmPublish
        type: HelmPublish
        configuration:
          helmVersion: 3
          autoPublishBuildInfo: true
          chartPath: ./simplehelm
          lint: false
          inputResources:
            - name: fooGitRepo
          outputResources:
            - name: fooBuild
            - name: fooHelmChart
      - name: fooPromoteBuild
        type: PromoteBuild
        configuration:
          copy: false
          targetRepository: helm-promote
          inputResources:
            - name: fooBuild
              trigger: true
          outputResources:
            - name: fooBuildPromote
      - name: fooHelmDeploy
        type: HelmDeploy
        configuration:
          environmentVariables:
            HELM_NAMESPACE: pipe-master-pool
          integrations:
            - name: k8s
          inputResources:
            - name: fooBuildPromote
          chartPath: .
          chartName: simplehelm
          chartVersion: 0.3.0
          helmVersion: 3
          releaseName: 'simplehelm-${run_id}'
          flags: >-
            --force --wait --timeout 900s --set
            image.repository=trriplejay/simplehelm --set image.tag=latest
          valueFilePaths:
            - values.yaml
          lint: true
          lintFlags: '--strict'
          test: true
[/YAML]
 **values.yml** : This is a sample`values.yml`, which can be edited to create your own version of the file.
[YAML]
    artifactoryIntegration: artifactory_integration     # Required. Name of the Artifactory integration.
    kubernetesIntegration: kubernetes_integration       # Required. Name of the Kubernetes integration.
    
    GitRepo:                                            # Required.
      name: fooGitRepo                                  # Name of the GitRepo resource. Defaults to GitRepoRes.
      gitProvider: github                               # Required. Name of the SCM integration which will be used to fetch the SCM repository.
      path: jfrog/jfrog-pipelines-simple-example        # Required. Path to the SCM repository.
      branches:                                         # Optional.
        exclude: ^master$                               # Regex pattern to exclude branches.
        include: ^{{gitBranch}}$                        # Regex pattern to include branches.
      files:                                            # Optional.
        exclude: ^fileName$                             # Regex pattern to exclude files.
        include: ^fileName$                             # Regex pattern to include files.
    
    BuildInfo:                                          # Required.
      name: fooBuild                                    # Name of the BuildInfo resource. Defaults to BuildInfoRes.
      buildName: foo                                    # Optional. Name of the Artifactory Build that will be created. Defaults to $pipeline_name.
      buildNumber: $run_number                          # Optional. Number of the Artifactory Build that will be created. Defaults to $run_number.
    
    BuildInfoPromoted:                                  # Optional.
      name: fooBuildInfoPromoted                        # Name of the promoted BuildInfo resource. Defaults to BuildInfoPromotedRes.
      autoPromotion: false                              # Optional. When set to false, indicates that Build promotion in the pipeline will be manually triggered. Defaults to true.
    
    HelmChart:                                          # Required.
      name: fooHelmChart                                # Name of the HelmChart resource. Defaults to HelmChartRes.
      chartName: foo                                    # Required. Helm chart name.
      chartVersion: 0.0.1                               # Required. Helm chart version.
      repository: foo                                   # Required. The name of the Helm repository in Artifactory.
      helmVersion: 3                                    # Optional. A number representing the major version of Helm to use. Defaults to 3.
      namespace: foo                                    # Optional. Set the namespace used for the Helm operations. Only supported when used with Helm 3.
    
    Pipeline:                                           # Optional.
      name: foo                                         # Name of the pipeline. Defaults to HelmDeployPipeline.
    
    HelmPublish:                                        # Required.
      name: HelmPublish                                 # Name of the HelmPublish step. Defaults to HelmPublish.
      chartPath: '.'                                    # Required. The path to the Helm chart YAML in the GitRepo resource.
      flags: '--app-version=foo'                        # Optional. Command line options to pass to the helm package command.
      lint: true                                        # Optional. When set to true performs a lint to examine a Helm chart for possible issues.  Defaults to false.
      lintFlags: '--strict'                             # Optional. Flag string to pass to the helm lint command.
      valueFilePaths:                                   # Optional. Specifies values YAML file(s) that will be used in the lint command helm lint command.
        - values.yaml                                   # Optional. An exmaple of how to add a values YAML file.
        - values2.yaml                                  # Optional. An exmaple of how to add an additional values YAML file.
    
    PromoteBuild:                                       # Optional.
      name: fooPromoteBuild                             # Name of the PromoteBuild step. Defaults to PromoteBuild.
      targetRepository: fooPromote                      # Required. The name of the repository in Artifactory to promote the build to.
      copy: false                                       # Optional. When set to true, copies the artifacts to the targetRepository vs moving them to the targetRepository. Defaults to false.
    
    HelmDeploy:                                         # Required.
      name: fooHelmDeploy                               # Name of the HelmDeploy step. Defaults to HelmDeploy.
      chartPath: '.'                                    # Required. The path to the Helm chart YAML in the input BuildInfo resource.
      releaseName: foo                                  # Required. The release name. Equivalent to the --name (-n) option of the  helm install command.
      flags: '--wait --timeout 900s'                    # Optional. Command line options to pass to the helm upgrade command.
      lint: true                                        # Optional. When set to true performs a lint to examine a Helm chart for possible issues. Defaults to false.
      lintFlags: '--strict'                             # Optional. Flag string to pass to the helm lint command.
      valueFilePaths:                                   # Optional. Specifies values YAML file(s) for use with a --values (-f) option of the helm lint command.
        - values.yaml
        - values2.yaml
      test: true                                        # Optional. When set to true performs a test to run the tests for release. Defaults to false.
      testFlags:                                        # Optional. Flag string to pass to the helm test command.
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
169,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The Artifactory Artifact Cleanup template is a Pipelines template that can be used to delete all Artifactory artifacts that have not been downloaded for the past _n time units._
[/INST]###### Prerequisites
  * Artifactory Artifact Cleanup template is available from Pipelines 1.31.2, but for following steps you need 1.40.
  * Artifactory version must be 7.49.0 or higher


###### Step 1: Create Integration
The Artifact Cleanup Template requires the following integration. For information about creating an integration, see Adding an Integration.
Integration Type
Integration Name
Comments  
JFrog Platform Access Token Integration
  * jf_token


This integration is used to perform Artifactory operations and requires repository read/write permissions.  
###### Step 2: Create Node Pools
For SaaS deployments, a node pool named `default-dynamic-nodepool` is available by default. For on-premise deployments, at least one node pool must be added and designated as the default node pool, and assigned at least one build node for pipelines to execute in the selected Project. For information about creating node pools, see Managing Pipelines Node Pools.
###### Step 3: Create Pipelines Source for Artifact Cleanup Template
  1. In your SCM repository, create a new directory named `.jfrog-pipelines`.
  2. In the directory, create two new files:
    * `pipelines.yml`
    * `values.yml`
  3.  **pipelines.yml** : Add the following in the `pipelines.yml` file:
[YAML]    valuesFilePath: ./values.yml
    include:
      template: jfrog/ArtifactCleanup/1.0.0
[/YAML]
 **Where** :
    * `valuesFilePath` is the path where the `values.yml` file is stored.
  4.  **values.yml** : Update the `values.yml` file to include the values required for the template. Here is a sample:
[YAML]    shared:
      input:
        # [Required]
        jfrogTokenIntegration: jf_token
    
    artifactCleanup:
      metadata:
        # [Required]
        pipelineName: artifact_cleanup
    
        # [Optional] Default value: artifact_cleanup_property_bag
        propertyBagResourceName: artifact_cleanup_property_bag
    
        # [Optional] Default value: artifact_cleanup_cron_trigger
        cronTriggerResourceName: artifact_cleanup_cron_trigger
    
      controls:
        # [Optional] When defined with enabled: true then new Cron will be setup. Default expression: 0 0 5 ? * 1
        cron:
          enabled: false
          expression: ""*/3 * * * *""
    
        # [Optional] List of comma separated Artifactory repo names for which jfrogTokenIntegration is provided.
        # Useful when using the pipeline with Cron.
        repos: ""plugin-repo-1""
    
        # [Optional] Defaults to true, if not provided
        dryRun: ""true""
    
        # [Optional] Defaults to 1000, if not provided
        paceTimeMS: 1000
    
        # [Optional] Defaults to 10, if not provided
        maxRepos: 10
    
        # [Optional] Defaults to 10, if not provided
        maxArtifacts: 100
    
    
    dockerCleanup:
      metadata:
        # [Required]
        pipelineName: docker_cleanup
    
        # [Optional] Default value: docker_cleanup_property_bag
        propertyBagResourceName: docker_cleanup_property_bag
    
        # [Optional] Default value: docker_cleanup_cron_trigger
        cronTriggerResourceName: docker_cleanup_cron_trigger
    
      controls:
        # [Optional] When defined with enabled: true then new Cron will be setup. Default expression: 0 0 5 ? * 1
        cron:
          enabled: false
          expression: ""*/3 * * * *""
    
        # [Optional] List of comma separated Artifactory Docker repo names for which jfrogTokenIntegration is provided.
        # Useful when using the pipeline with Cron.
        repos: ""km-docker-local""
    
        # [Optional] Defaults to true, if not provided
        dryRun: ""false""
    
        # [Optional] Defaults to 1000, if not provided
        paceTimeMS: 1000
    
        # [Optional] Defaults to 10, if not provided
        maxRepos: 10
    
        # [Optional] Defaults to 10, if not provided
        maxArtifacts: 100
[/YAML]
The following parameters are mandatory:
    *  **jfrogTokenIntegration** : Name of the JFrog Platform Access Token Integration that was created in Step 1.
    *  **artifactCleanup** : Name of the artifactCleanup pipeline. This pipeline is used for cleaning up all artifacts except Docker.
    *  **pipelineName** : Name of the DockerCleanup pipeline. This pipeline is used for cleaning up all Docker artifacts.
  5. In the JFrog Platform, go to **Administration → Pipelines → Pipeline Sources** and add the pipeline as a pipeline source.
After your Pipeline Source syncs successfully, you can view the newly added pipelines (`artifact_cleanup` and `docker_artifact_cleanup`) by navigating to **My Pipelines** on the left navbar and clicking **Pipelines** → **My Pipelines**.  
  6. Go to **Application → Pipelines → My Pipelines → All Pipelines** , click either `artifact_cleanup` or `docker_cleanup`, click the custom trigger button and click the **Environment Variables** tab.  
  7. Click the **Environment Variables** tab and configure the available values as per your requirements. See the table below for information about each environment variable.  
Parameter
Description  
`timeUnit`
The unit of the time interval. _year_ , _month_ , _day_ , _hour_ or _minute_ are allowed values. Default is _month_.  
`timeInterval`
The time interval to look back before deleting an artifact. Default _1_.  
`searchOn`
Parameter to choose the search condition. Defaults to search artifacts last downloaded before selected time.  
`repos`
A comma-separated list of repositories to clean. This parameter is required.  
`dryRun`
Dry run to test the settings. If this parameter is passed, artifacts will not actually be deleted. Set to `true` to perform a dry run. Default is _false_.  
`paceTimeMS`
The number of milliseconds to delay between delete operations. Default is 1000ms.  
`maxRepos`
The maximum number of repos to be cleaned from the list provided in the repos field. This is useful when you do not want to overload your Artifactory.
For example, if the repositories a1, a2, a3, and a4 were listed as the repositories in the `repos` field and the number of `maxRepos` is 2, then only a1 and a2 are cleaned, and a3 and a4 are ignored.  
`maxArtifacts`
The maximum number of artifacts to be cleaned across the listed repositories.  
  8. After setting the environment variables, click **Trigger Run**.
  9. On successful completion of the run, you can view the logs to verify the status of the cleanup.  
You can also download the Step Test Report & Artifacts to verify the results.  


Artifactory Artifact Cleanup Template (1.39 and lower)
The Artifactory Artifact Cleanup template is a Pipelines template that can be used to delete all Artifactory artifacts that have not been downloaded for the past _n time units._
###### Prerequisites
  * Pipelines version must be 1.31.2 or higher
  * Artifactory version must be 7.49.0 or higher


###### Step 1: Fork JFrog Git Repository
The files required for using the Artifactory Artifact Cleanup Template are available in this Git repository. For ease of use, it is highly recommended that you fork this repository. This is also required since a user access token is needed to connect to Git.
### Note
If you are not using Github as an SCM, then move the code to the available Git repository. Pipelines currently support following SCMs: Github, Github Enterprise, Gitlab, Bitbucket and Bitbucket Server.
###### JFrog Git Repository Structure
In the Git repository, the YAML files required for the Artifactory Artifact Cleanup Template are stored in the following sub-directories in the `.jfrog-pipelines` directory:
Directory Name
YAML Files
Description  
 **template-publish**
`pipelines.yml values.yml README.md`
This contains pipelines YAML for publishing templates  
 **template-artifact-cleanup**
`pipelines.yml values.yml README.md`
This contains pipelines YAML for using artifact cleanup template after it is published using above pipeline.  
 **templates**
↳ **artifactory**
 **↳cleanup**
 **↳artifact-cleanup**
`templateDefinition.yml values.yml.example README.md`
Artifact cleanup template source YAML This will be published to Artifactory for further usage.
The YAML files listed here for reference only. Editing these files may cause unexpected errors.  
###### Step 2: Create Integrations
The Artifact Cleanup Template requires the following integrations. For information about creating these integrations, see Adding an Integration.
Integration Type
Integration Name
Comments  
Github IntegrationGitHub Integration
  * myGitIntegration


This integration is used to connect the JFrog Pipelines platform to GitHub.  
JFrog Platform Access Token Integration
  * myAccessTokenIntegration
  * jf_token


This integration is used to perform Artifactory operations and requires repository read/write permissions.  
Artifactory Integration
  * myArtIntegration

  
###### Step 3: Create Node Pools
For SaaS deployments, a node pool named `default-dynamic-nodepool` is available by default. For on-premise deployments, at least one node pool must be added and designated as the default node pool, and assigned at least one build node for pipelines to execute in the selected Project. For information about creating node pools, see Managing Pipelines Node Pools.
###### Step 4: Create Pipeline Source for Publish Template
Perform the following steps to publish the Artifact Cleanup Template:
  1. The template-publish folder contains two YAML files:
    *  **pipelines.yml**
    *  **values.yml**
  2. Open the `values.yml` file and update its values as shown below:
[YAML]    templateRepository:
      gitProvider: myGitIntegration
      path: ""jfrog/jfrog-pipelines""
      branch: master
      templateFolder: templates/artifactory/cleanup/artifact-cleanup
    
    publishTemplate:
      namespace: myCompany
      name: artifactCleanup
      version: 1.0.0
      pipelineName: publish_artifact_cleanup_template
      artifactoryIntegration: myArtIntegration
[/YAML]
 **Where** :
    * `templateRepository `provides information about where the template is stored.
      * `gitProvider `is the name of the Git integration you created in the previous step.
      * `path` is the Git repository path.
      * `branch` is the name of the branch where the template is available.
      * `templateFolder` is the relative path from the root where the template files are located. Unless you have made changes to the folders in the forked repository, this usually will not require any change.
    * `publishTemplate `provides requisite metadata for publishing the template.
      * `namespace `is the namespace used for storing the template in Artifactory.
      * `name `is the name used for storing the template in Artifactory.
      * `version `is the version of the template.
      * `pipelineName `is the default name of the pipeline that is used for publishing the template to Artifactory.
      * `artifactoryIntegration` is the name of the Artifactory integration you created.
  3. In the JFrog Platform, go to **Administration → Pipelines → Pipeline Sources** and add the PublishTemplate pipeline (`.jfrog-pipelines/template-publish`) as a pipeline source.
After your Pipeline Source syncs successfully, you can view the newly added pipeline by navigating to **My Pipelines** on the left navbar and clicking **Pipelines** → **My Pipelines**.
  4. Go to **Application → Pipelines → My Pipelines → All Pipelines** and run the pipeline.  
A successful run produces a `artifact-cleanup` template, which can now be referenced from other pipelines with the name`myCompany/artifactCleanup/1.0.0`.
  5. Optional: Go to **Reusables** and click the **Templates** tab to view the `artifactCleanup` template.

  
###### Step 5: Create Pipelines Source for Artifact Cleanup Template
The Artifact Cleanup Template, available in the `template-artifact-cleanup`directory,includes two predefined pipelines.The names of these pipelines are configurable. It is recommended that you change the default name as there could be a pipeline name conflict if you are using the same template multiple times.
  *  **artifact_cleanup** : This pipeline is used for cleaning up all artifacts except Docker.
  *  **docker_artifact_cleanup** : This pipeline is used for cleaning up all Docker artifacts.


Perform the following steps to create Pipelines Source for Artifact Cleanup template:
  1. The `template-artifact-cleanup`folder contains two YAML files:
    *  **pipelines.yml**
    *  **values.yml** : Though not recommended, the values.yml file has a cron setting that can be configured as per your requirements. Cron is disabled by default.
[YAML]        cron:
          enabled: false
          expression: ""*/3 * * * *""
[/YAML]
  2. In the JFrog Platform, go to **Administration → Pipelines → Pipeline Sources** and add the artifact_cleanup pipeline (`.jfrog-pipelines/template-artifact-cleanup`) as a pipeline source.  
After your Pipeline Source syncs successfully, you can view the newly added pipelines (`artifact_cleanup` and `docker_artifact_cleanup`) by navigating to **My Pipelines** on the left navbar and clicking **Pipelines** → **My Pipelines**.  
  3. Go to **Application → Pipelines → My Pipelines → All Pipelines** , click either `artifact_cleanup` or `docker_artifact_cleanup`, click the custom trigger button and click the **Environment Variables** tab.  
  4. Click the **Environment Variables** tab and configure the available values as per your requirements. See the table below for information about each environment variable.  
Parameter
Description  
`timeUnit`
The unit of the time interval. _year_ , _month_ , _day_ , _hour_ or _minute_ are allowed values. Default is _month_.  
`timeInterval`
The time interval to look back before deleting an artifact. Default _1_.  
`searchOn`
Parameter to choose the search condition. Defaults to search artifacts last downloaded before selected time.  
`repos`
A comma-separated list of repositories to clean. This parameter is required.  
`dryRun`
Dry run to test the settings. If this parameter is passed, artifacts will not actually be deleted. Set to `true` to perform a dry run. Default is _false_.  
`paceTimeMS`
The number of milliseconds to delay between delete operations. Default is 1000ms.  
`maxRepos`
The maximum number of repos to be cleaned from the list provided in the repos field. This is useful when you do not want to overload your Artifactory.
For example, if the repositories a1, a2, a3, and a4 were listed as the repositories in the `repos` field and the number of `maxRepos` is 2, then only a1 and a2 are cleaned, and a3 and a4 are ignored.  
`maxArtifacts`
The maximum number of artifacts to be cleaned across the listed repositories.  
  5. After setting the environment variables, click **Trigger Run**.
  6. On successful completion of the run, you can view the logs to verify the status of the cleanup.  
You can also download the Step Test Report & Artifacts to verify the results.  



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
170,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>To use a global template as a pipeline source:
  1. In your SCM repository, create a new directory named `.jfrog-pipelines`.
  2. In the directory, create two new files:
    * `pipelines.yml`
    * `values.yml`
  3.  **pipelines.yml** : Add the following in the `pipelines.yml` file:
[YAML]    valuesFilePath: ./values.yml
    include:
      template: jfrog/<global_template_name>/<template_version>
[/YAML]
 **Example**
[YAML]    valuesFilePath: ./values.yml
    include:
      template: jfrog/HelloWorld/1.0.0
[/YAML]
 **Where** :
    * `jfrog` is the predefined namespace to be used for global templates. This is mandatory and cannot be changed.
    * `global_template_name` is the name of the global template you want to use. Go to **Pipelines | Reusables** and click the **Templates** tab to find all the available preloaded templates.
    * `template_version` is the version of the template you want to use. Go to **Pipelines | Reusables** and click the **Templates** tab to find the versions available for each template.
    * `valuesFilePath`is the path where the `values.yml` file is stored.
  4.  **values.yml** : Update the `values.yml` file to include the values required for the global template you are using. For more information, see the example values.yml file.
  5. In the JFrog Platform, go to **Administration → Pipelines → Pipeline Sources** and add the pipeline as a pipeline source. For more information, see Adding a Pipeline Source.
After your Pipeline Source syncs successfully, you can view the newly added pipeline by navigating to **My Pipelines** on the left navbar and clicking **Pipelines** → **My Pipelines**.
  6. Go to **Application → Pipelines → My Pipelines → All Pipelines** and run the pipeline.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
171,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Perform the following steps to use a Global Template to create a pipeline source:
  1. Go to **Administration | Pipelines | Pipeline Sources** , and click **Add Pipeline Source** , and **From Template**.
  2. Complete the resulting **Select Template** form:  
    * Click the _Select Template Namespace_ field and select the `jfrog` namespace.
    * Click the _Select Template Name_ field and select the `DockerBuildAndPush` or `HelmPublishAndDeploy` global template.
    * Click the _Select Template Version_ field and select the relevant version for the template.
  3. Click **Next** and complete the resulting **Specify values file** form to add the `values.yml` file.  
    * Click the _Integration_ field and select your source control integration from the dropdown list.
    * In the _Repository Full Name_ field, enter the path of the repository where your `values.yml` is stored.
    * Specify the _Branch_ of the repository (for example, `master`).
    * In the Values File Path field, provide the path to the `values.yml` file, which contains the values for the global template. By default, this field is set to take the file from the root directory. If your `values.yml` file is stored in a different directory, enter the full path.
When you do this, the `values.yml` file is parsed and values are added as configured in the file.
[/INST]### Note
For more information about the **Specify values file** form, see the section Adding a Pipeline Source.
  4. Click **Create Source** to add the Pipeline Source.


The combination of pipeline template name, namespace, version and `values.yml` is parsed to create the pipeline definition.
After your Pipeline Source syncs successfully, you can view the newly added pipeline by navigating to **My Pipelines** on the left navbar and clicking **Pipelines** → **My Pipelines**.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
172,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>System Templates are added and maintained by administrators for an instance of Pipelines install. These templates can be stored in a template source repository, which can then be uploaded to Pipelines. When loaded into Pipelines, these templates can be used in any pipeline like any other resource.
[/INST]### Managing Pipelines Templates
For information about managing templates, such as releasing and retiring template versions, see Managing Pipelines Templates.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
173,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Local Templates can be a:
  * Copy of a system template: You can download a copy of a system template and commit it to your pipeline source repository. At this point, your template is disconnected from the centrally available template and will not be updated if the central template is updated.
or
  * Fully customized template for the pipeline: If you do not want to use an existing template, you can create a new, fully customized YAML file in your pipeline source repository and specify the config using the template pattern.


 **Related Content**
Managing Pipelines Templates
 **Example: pipelines.yml**
[YAML]
    valuesFilePath: <relative path for values.yml> [/INST]# required for local templates
    
    resources:
      - name: {{ .Values.GitRepo.name }}
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.GitRepo.gitProvider }} 
          path: {{ .Values.GitRepo.path }}   
          branches:
            include: {{ .Values.GitRepo.branches.include }}
    
      - name: {{ .Values.Image.name }}
        type: Image
        configuration:
          registry: {{ .Values.artifactoryIntegration }}
          sourceRepository: {{ .Values.Image.sourceRepository }}
          imageName: {{ .Values.DockerBuild.dockerImageName }}
          imageTag: 1
          autoPull: true
      
    pipelines:
      - name: {{ .Values.Pipeline.name }}
        steps:
          - name: {{ .Values.DockerBuild.name }}
            type: DockerBuild
            configuration:
              dockerFileLocation: .        # requires Dockerfile to be in root
              dockerFileName: {{ .Values.DockerBuild.dockerFileName }}
              dockerImageName: {{ .Values.DockerBuild.dockerImageName }}
              dockerImageTag: ${run_number}  # requires tag to be run number   
              inputResources:
                - name: {{ .Values.GitRepo.name }}  
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
    
          - name: {{ .Values.DockerPush.name }}
            type: DockerPush
            configuration:
              targetRepository: {{ .Values.DockerPush.targetRepository }}
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
              inputSteps:
                - name: {{ .Values.DockerBuild.name }}
              outputResources:
                - name: {{ .Values.Image.name }}
    
          - name: {{ .Values.Bash.name }}
            type: Bash
            configuration:
              inputResources:
                - name: {{ .Values.Image.name }}
            execution:
              onExecute:
                - echo “This is a Bash step”
    
[/YAML]
In addition to the local template file, a `values.yml` file is required in the same repository. This file contains the values required for the local template to create the pipeline definition.
 **Example: values.yml**
[YAML]
    artifactoryIntegration: myArtifactoryIntegration
    
    GitRepo:
      name: myGitRepo
      gitProvider: myGitIntegration
      path: myorg/myrepo
      branches:
        include: master
    
    Image:
      name: myDockerImage
      sourceRepository: mySourceRepo      
    
    Pipeline:
      name: myDockerPipeline
    
    DockerBuild:
      name: myDockerBuild   
      dockerFileName: Dockerfile
      dockerImageName: <image name>
    
    DockerPush:
      targetRepository: docker-local 
    
    Bash:
      name: myBashStep
    
[/YAML]
##### Template Functions List
Local templates support several helm chart style functions. For the full list of supported and unsupported functions, see Template Functions List.
##### Adding Local Templates (1.31 and higher)
To add and use local templates:
  1. In the SCM repository, place your template YAML files (`pipelines.yml` and `values.yml`) in a directory named .`jfrog-pipelines`. This directory can be the root or one level below the root. Directories beyond this level are not supported.
  2. In the Pipelines UI, go to **Administration | Pipelines | Pipeline Sources**.
  3. In the resulting **Pipeline Sources** display, click **Add** **Pipeline Source**.  
  4. In the resulting **Add YAML Pipeline Source** page, click one of the following:
    *  **Single Branch**
    *  **Multi Branch**
  5. Complete the **Single/Multi Branch** form:
Field
Description  
 **Name**
Enter a unique name for the pipeline source.  
 **SCM Provider Integration**
Click the _SCM Provider Integration_ field and select your source control integration from the dropdown list. Only integrations that are compatible for use as a Pipeline Source will be included in the dropdown list.  
 **Repository Full Name**
Based on the information you have provided for the selected integration, such as API token, all the relevant repositories are listed in the _Repository Full Name_ field. Select the path of the repository where your pipeline files are stored. If the name of the repository is not auto-fetched, enter the full name of the repository where your pipeline files are stored.  
 **Branch (Single Branch only)**
Based on the SCM provider and Repository Full Name you have provided, all the available branches are automatically fetched (for example, `main`). Select the required branch. If the name of the branch is not auto-fetched, enter the name of the branch.  
 **Exclude (Multi Branch only)**
Specify the _Exclude Branch Pattern_ as a regular expression for the matching branch names to exclude.  
 **Include (Multi Branch only)**
Specify the _Include Branch Pattern_ as a regular expression for the matching branch names to include.  
 **Sync all branches** toggle
 **(Multi Branch only)**
When a new multi branch pipeline source is added, only the default branched is synced. To sync all the branches based on the Exclude/Include pattern, click the **Sync all branches** toggle. If Exclude/Include pattern is not provided, all branches are synced.
In addition, when a new commit is pushed to thematching branch, that particular branch is synced.  
 **Folder Name**
Provide the name of the directory where the template YAML files (`pipelines.yml` and `values.yml`) are available.
    * The .`jfrog-pipelines`directory can contain any number of YAML files.
    * To parse all YAML files in the root directory, enter ""."".  
  6. Click **Create Source** to complete adding the Pipeline Source.


##### Adding Local Templates (1.30 and lower)
To add and use local templates:
  1. In your local template file (for example, `pipelines.yml`), set `template` to `true` and set `valuesFilePath` to the relative path of the `values.yml` file.
`template: true`
`valuesFilePath: values.yml ` Setting `template` to `true` identifies the yaml file as a local template. The combination of `values.yml` and `pipelines.yml` is parsed to create the pipeline definition and synced to create the pipeline.
  2. Go to **Administration | Pipelines | Pipeline Sources**.
  3. In the resulting **Pipeline Sources** display, click **Add** **Pipeline Source** , and then click **From YAML**.  
  4. In the resulting **Add YAML Pipeline Source** page, click one of the following:
    *  **Single Branch**
    *  **Multi Branch**
  5. Complete the **Single/Multi Branch** form:
    *  **SCM Provider Integration** : Click the _SCM Provider Integration_ field and select your source control integration from the dropdown list. Only integrations that are compatible for use as a Pipeline Source will be included in the dropdown list.
    *  **Repository Full Name** : Based on the information you have provided for the selected integration, such as API token, all the relevant repositories are listed in the _Repository Full Name_ field. Select the path of the repository where your pipeline files are stored. If the name of the repository is not auto-fetched, enter the full name of the repository where your pipeline files are stored.
    *  **Branch (Single Branch only)** : Based on the SCM provider and Repository Full Name you have provided, all the available branches are automatically fetched (for example, `main`). Select the required branch. If the name of the branch is not auto-fetched, enter the name of the branch.
    *  **Exclude Branch (Multi Branch only)** : Specify the _Exclude Branch Pattern_ as a regular expression for the matching branch names to exclude.
    *  **Include (Multi Branch only)** : Specify the _Include Branch Pattern_ as a regular expression for the matching branch names to include.
    *  **Pipeline Config File Filter** : Enter `pipelines.yml`. In addition to the `pipelines.yml`, if you are using a `values.yml`, ensure both files are added to the filter, using `(pipelines|values).yml`.
  6. Click **Create Source** to complete adding the Pipeline Source.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
174,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A Pipelines user can view and manage lifecycle staging of each template version. In addition, a user can view the YAML and documentation for each template source.
[/INST]##### Managing Templates
Pipelines users can view the catalog of successfully loaded templates, and manage lifecycle staging of each template version.
###### Viewing Templates
From the **Application** tab go to **Pipelines | Reusables** , and select the _Templates_ tab to view the list of templates.
Property
Description  
Name
The named type of the template.  
Versions
The current version of the template.  
In Use By
Shows the numberof pipelines that are using the template  
Released at
The date and time when the template was released.  
###### Releasing Template Versions (1.38 and lower)
When a template is added, it is added as the latest version of the template. However, whenever the template source syncs, the latest template can change. If you want to keep a specific version of a template, you have the option of releasing that version of the template.
A template that has been properly Git tagged with semantic version information can be formally released. When a template is released, it can be referenced by its version number. That version of the template continues to be available even after a new latest version has been synced from the template source.
### Note
After a version of a template is released, it cannot be changed. If any changes are to be made, a new release should be created.
 **To release the latest version of a template:**
  1. Go to your Github repository and perform these steps:
    1. In Github, go to the **templates** repository.
### Note
System template files should be stored in the template source repository in a specific order for them to be uploaded to Pipelines. For more information, see Structure of Template Source Repository.
    2. Click **Releases**.  
    3. Click **Draft a new release**.
    4. In the page that appears, click the **Tag version** field and enter a git tag using the format: `<template_namespace>/<template_name>@<semver>`.
      * The format of <`semver>` must be compliant with the semantic versioning standard ( _Major.Minor.Patch_ ).
      *  **Example** :`jfrog/foo@1.1.1`.  
    5. Click **Publish release**.
When the new version is published, Pipelines automatically syncs and makes this version available for release, as shown in the next steps.
  2. In the JFrog Platform, from the **Application** tab go to **Pipelines | Reusables** , and select the **Templates** tab.  
  3. To release a specific template, either right-click or hover over the rightmost region of the row where the template is listed to reveal the **Actions** icon, and then click **Release**.  
  4. In the subsequent **Release Template** dialog, select the release tag from the dropdown, then click **Confirm**.  
The template is released and can now be used in a pipeline source.  


###### Viewing the Version Log (1.38 and lower)
To view version logs:
  1. Either right-click or hover over the rightmost region of the row where the template is listed to reveal the **Actions** icon, and click **Logs** from the drop-down list.  
  2. The **Sync Logs** for the released template is displayed.  


###### Retiring Template Versions (1.38 and lower)
Administrators can retire template versions, making them unavailable for further use. You can select an advance date for a version's retirement. When a template version is retired, pipeline sources using the retired version should be updated to use a different version of the template.
To retire a template version:
  1. Either right-click or hover over the rightmost region of the row where the template version is listed to reveal the **Actions** , and click **Retire** from the drop-down list.  
  2. In the resulting **RETIRE TEMPLATE** dialog, click the _Pick a Date (From)_ box to reveal the calendar, select a date when the template is to be retired, then click **Save**.  


The **Retired At** column is updated to show the retirement date.  
### Note
  * If required, a template that is already retired can be unretired by changing the retire date to a future date. Unretired templates can be used in pipeline sources again.
  * If the template version has been scheduled for retirement, the logs show the upcoming retirement date with a warning. If a template version has already been retired, pipeline sync fails with an error.


###### Viewing YAML (1.38 and lower)
To view the Template Definition and Values Example YAML:
Either right-click or hover over the rightmost region of the row where the template version is listed to reveal the **Actions** , and click **View YAML** from the drop-down list.  
###### Viewing Documentation
If you included a readme file along with your template files, you can view the contents of this file. To do this, either right-click or hover over the rightmost region of the row where the template is listed to reveal the **Actions** icon, and click **Documentation**.  
##### Managing Template Sources
A **Template Source** represents a source control repository (such as GitHub or BitBucket) where Pipelines template definition files can be found. A template source connects to the repository through an administration integration.
After a template source is added, Pipelines automatically loads all template definition files from that repository and its subdirectories.
###### Adding a Template Source (1.31 and higher)
For information about adding a template source, see Global Templates and System Templates.
### Note
When a template source is added, it is added to Artifactory.
###### Adding a Template Source (1.30 and lower)
### Note
  * Only by an administrator user or a user with resource manager permissions can add a template source.Introduction to Users and Groups
  * Only one template source can be added at this time.
  * When a template source is added, it is also added to Artifactory.


 **Prerequisites for Template Sources** :
  * An administration integration must be available for the source control account.
  * The source code repository for your templates must be structured according to the Structure of Template Source Repository.


 **To add a Template Source:**
  1. From the **Application** module go to **Pipelines | Reusables**.
  2. Click the **Sources** tab, click **Add Source** and then click **Templates**.  
  3. In the resulting **Add Template Source** display, select the source control account integration, enter the repository path and branch, then click **Create Source**.  


Pipelines then syncs the template source to load the templates.
###### Viewing Template Sources (1.30 and lower)
To view the the list of configured template sources, from the **Application** tab go to **Pipelines | Reusables** , and click the **_Sources_** tab.  
The tab displays the list of configured template sources that Pipelines will load templates from.
Each row of the template sources list includes the following:
Property
Description  
Repository
The source repository path of the template source  
Branch
The source repository branch  
Latest Status
The success/failure status of the last sync  
Last Sync
The time and date of the last sync  
Changed By
The username of the VCS user who made the triggering change  
Context
The commit SHA that triggered the last sync  
Type
The source type  
###### Sync Template Source (1.30 and lower)
When any of the pipeline config files have changed, you may need to manually sync the pipeline source to reload. To do this:
  1. Right-click or hover over the rightmost region of the row where the template source is listed and click the **Actions** icon that appears.
  2. Click the **Sync** icon.  


### Note
If the **Latest Status** indicates that the sync failed, click **Logs** to view the logs of all synchronized files.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
175,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines Tasks enable developers to simplify and standardize CI/CD pipelines and help them reduce maintenance costs by promoting principles such as reusability, immutability, and versioning.
An ever-growing library of tasks are available out of the box and can be used in your pipeline definition. The same task can be invoked multiple times and can be executed across different steps in a pipeline. The code for each invoked task is stored in an Artifactory repository and downloaded/executed at the runtime of the step. In addition, since tasks are stored in Artifactory, they can be shared by different teams using local, remote, and virtual repositories. In fact, a task can be written in a way that leverages the Platform.
When Pipelines is installed, the following repositories are created by default for Pipelines tasks:
  * pipelines-tasks-local
  * pipelines-tasks-remote
  * pipelines-tasks-virtual


[/INST]#### Viewing Tasks
To view the list of available tasks, do one of the following:
  * Go to  **Reusables**  and click the  **Tasks**  tab.
Or
  * Go to **Application | Artifacts** and click the `pipelines-tasks-local` repository. Each task is stored in a dedicated directory, which contains the files required for the execution of the task and documentation in the form of a `readme.md` file.  


Task
Description  
build-publish
This task publishes BuildInfo to Artifactory. BuildInfo provides a manifest for the build and includes metadata about the modules, dependencies and other environment variables.  
curate-artifact
This task takes a single artifact path from a remote repository, copies it to a local repository and associates it with a build.  
docker-build
Performs a build to produce a Docker image from a Dockerfile in a GitRepo source repository resource.  
docker-push
Pushes a Docker Image to a Docker registry.  
email-notify
Sends email notifications from your pipeline step.  
frogbot
Frogbot is a Git bot that scans your pull requests and repositories for security vulnerabilities. This task allows you to embed Frogbot in your Pipelines Steps.  
go-build
This Pipelines Task performs a build from Go (GoLang) source.  
go-security-check
Runs gosec code security analysis.  
go-static-check
Runs Go static code analysis.  
jira-comment
Adds a comment to a Jira issue.  
jira-transition
Updates the status of a Jira.  
mvn-build
This task performs a Maven project build on files in a Git repository.  
npm-build
Builds an npm source.  
publish-task
Validates Pipelines Task descriptor, create package and upload to Artifactory.  
run-cypress
Runs cypress tests.  
setup-go
Install and set up Go programming language.  
setup-java
Install and set up OpenJDK.  
setup-jfrog-cli
Installs and sets up JFrog CLI.  
setup-node
Installs and sets up NodeJS programming language.  
setup-yarn
Installs yarn package manager.  
slack-notify
Sends Slack notifications from your pipeline step.  
upload-file-s3
Uploads a file to an AWS S3 bucket.  
#### Using Tasks
Every task has a name and a version. To invoke a task in an existing pipeline, add the following to the `onStart`, `onExecute`, `onSuccess`, `onFailure`, or `onComplete` section of a step in your pipeline YAML:
[YAML]
    - task: <namespace>/<task_name>/<version>
      id: <unique_id>    # optional
      input:
        <input_variables>
[/YAML]
Tag
Description  
 **task**
This is name of the task in the `<namespace>/<task_name>@<version>` format.
  *  **Example** : `jfrog/publish-task@v0.0.2`


 **Latest Version** : All tasks are versioned. However, if you want to use the latest version of a task, use the `latest` keyword.
  *  **Example** : `jfrog/publish-task@latest`

  
 **input**
Input required for the task. Varies for each task.
For information about the input required for a specific task, see the `readme.md` available for the task.  
 **id**
This is optional. Use it if the task produces an output that may need to be used in other commands.  
 **Example - setup-go**
[YAML]
    resources:
      - name: jfrogClientGoRepoPseudoCommit
        type: GitRepo
        configuration:
          gitProvider: git_integration
          path: {{.jfrog-pipelines.sourceRepository}}               # current pipeline source path
          branches:
            include: dev
          buildOn:
            commit: true
    pipelines:
      - name: jfrog_go_client_pseudo
        configuration:
          jfrogCliVersion: 2
          runtime:
            type: image
            image:
              custom:
                name: releases-docker.jfrog.io/jfrog/pipelines-u18node
                tag: ""16""
                autoPull: true
          environmentVariables:
            readOnly:
              RESOLVE_REPO: npm-virtual
        steps:
        - name: go_sec
          type: Bash
          configuration:
            integrations:
              - name: artifactory_integration
            inputResources:
              - name: jfrogClientGoRepoPseudoCommit
          execution:
            onStart:
              - task: jfrog/setup-go@v0.0.3
                input:
                  version: ""1.19.3""
                  cacheIntegration: artifactory_integration
              - go version
              - go env GOPATH
            onExecute:
              - echo ""done with step""
[/YAML]
After a successful run of the pipeline, the log shows the `setup-go` task executing in the `onStart` section of the step:  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
176,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task publishes BuildInfo to Artifactory. BuildInfo provides a manifest for the build and includes metadata about the modules, dependencies and other environment variables.
This task can perform the following:
  * Auto publish build info
  * Perform Xray scan
  * Write build info to output resource
  * Save artifact information


[/INST]##### Prerequisites
This task requires a resource that has build to publish build information.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/build-publish@v0.0.1
      id: build-publish
      input:
        resourceName: ""input_resource_name"" # input resource name which need a build publish
        xrayScan: ""true""
        failOnScan: ""false""
        publishBuildInfo: ""true""
        retry: ""true"" 
[/YAML]
By default, the task will use `JFROG_CLI_BUILD_NAME` and `JFROG_CLI_BUILD_NUMBER` environment variables when publishing the build info.
##### Input Variables
Name
Required
Default
Description  
resourceName
true
Resource name on which Maven publish is to be performed.  
xrayScan
false
false
When set to `true`, forces an Xray scan after publishing to Artifactory.  
failOnScan
false
Any Artifactory integration available
When set to `true`, and when the Xray Policy Rule Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step  
publishBuildInfo
false
true
Publishes build info to Artifactory. Requires a BuildInfo resource in the outputResources list. `outputBuildInfoResourceName` should be set.  
retry
false
false
Flag whether to perform retry.  
outputBuildInfoResourceName
false
Name of the output resource to write build information  
##### Output Variables
None
##### Exported Environment Variables
None
##### How does it work?
This Pipelines Task publishes build information to artifactory using the jfrog-cli build-publish. This task uses `JFROG_CLI_BUILD_NAME` and `JFROG_CLI_BUILD_NUMBER` for fetching build name and build number respectively.
##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
177,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task takes a single artifact path from a remote repository, copies it to a local repository and associates it with a build.
Once you have a build name and number there are many additional operations you can perform in your pipeline:
  * Promotion
  * Release Bundle creation
  * Xray scanning


[/INST]### Note
This task does not support Docker repositories.
##### Prerequisites
This task requires an Artifactory integration.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/curate-artifact@v0.1.0
      id: curate-artifact-example
      input:
        artifactPath: ""7/updates/x86_64/Packages/at-3.1.13-25.el7_9.x86_64.rpm""
        remoteRepositoryName: ""rpm-remote""
        localRepositoryName: ""rpm-dev-local""
[/YAML]
The above command will use the first Artifactory integration available to the step. You can pick a specific integration using the `artifactoryIntegration` input:
[YAML]
    - task: jfrog/curate-artifact@v0.1.0
      id: curate-artifact-example
      input:
        artifactPath: ""7/updates/x86_64/Packages/at-3.1.13-25.el7_9.x86_64.rpm""
        remoteRepositoryName: ""rpm-remote""
        localRepositoryName: ""rpm-dev-local""
        artifactoryIntegration: ""my_artifactory""
[/YAML]
By default, the task will use `JFROG_CLI_BUILD_NAME` and `JFROG_CLI_BUILD_NUMBER` environment variables when publishing the build info. You can configure the build info coordinates using the `buildName` and `buildNumber` inputs:
[YAML]
    - task: jfrog/curate-artifact@v0.1.0
      id: curate-artifact-example
      input:
        artifactPath: ""7/updates/x86_64/Packages/at-3.1.13-25.el7_9.x86_64.rpm""
        remoteRepositoryName: ""rpm-remote""
        localRepositoryName: ""rpm-dev-local""
        buildName: ""my-build""
        buildNumber: ""1""
[/YAML]
##### Input Variables
Name
Required
Default
Description  
artifactPath
true
Path to the artifact, not including the repository prefix  
remoteRepositoryName
true
The name of the virtual/remote repo where you want to cache and copy from  
localRepositoryName
true
The name of the local repo that you want to copy the artifact into  
artifactoryIntegration
false
Any Artifactory integration available
The name of the integration to use  
buildName
false
`$JFROG_CLI_BUILD_NAME`
The name for the published build  
buildNumber
false
`$JFROG_CLI_BUILD_NUMBER`
The unique number to use for the published build  
##### Output Variables
Name
Description  
buildName
The name of the published build  
buildNumber
The number of the published build  
You can use the task's outputs to update a Pipelines Build Info resource:
[YAML]
    - task: jfrog/curate-artifact@v0.1.0
      id: curate-artifact-example
      input:
        artifactPath: ""7/updates/x86_64/Packages/at-3.1.13-25.el7_9.x86_64.rpm""
        remoteRepositoryName: ""rpm-remote""localRepositoryName: ""rpm-dev-local""
    
    - echo$OUT_curate_artifact_example_buildName
    - echo$OUT_curate_artifact_example_buildNumber
    - write_output myBuildInfoResourceName ""buildName=${$OUT_curate_artifact_example_buildName}""""buildNumber=${$OUT_curate_artifact_example_buildNumber}""
    
[/YAML]
##### Exported Environment Variables
None
##### How does it work?
This Pipelines Task uses the coordinates provided by the inputs to send requests to Artifactory to perform the following:
  * Caches the artifact in the remote repository using a HEAD request
  * Copies the artifact from the remote to a local repository through the Copy Item REST API
  * Attach the build info properties to the artifact using the Set Item Properties REST API
  * Publishes a build containing a reference to the artifact using the Build Upload REST API
  * Provides the build name and number as the output of the task


##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
178,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task sends email notifications from your Pipelines Step.
[/INST]##### Prerequisites
This task requires a SMTP Credentials integration.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/email-notify@v0.1.0
      id: email-notify
      input:
        emailIntegration: ""int_smtp""
        to: ""user@jfrog.com""
        subject: ""JFrog Pipelines Email task""
        message: ""Hello there, sent via JFrog Pipe email Task"" 
[/YAML]
By default, the task will use `JFROG_CLI_BUILD_NAME` and `JFROG_CLI_BUILD_NUMBER` environment variables when publishing the build info.
##### Input Variables
Name
Required
Default
Description  
emailIntegration
true
SMTP credentials integration name.  
to
true
 
Comma separated list of destination email addresses.  
subject
false
 
Subject for the email message.  
message
false
 
Email message to be sent.  
##### Output Variables
None
##### Exported Environment Variables
None
##### How does it work?
The task uses nodemailer module to send an email message.
##### Creating Less Secure Apps
Most of the email services like Gmail and Yahoo have disabled the option of sending mails via less secure means, which will affect sending mails via third party applications.
There are other emails hosts like zoho, proton, which still allow sending emails via less secure way and custom domain emails should still allow this. Make sure you have the right host, port and email addresses.
##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
179,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
180,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task installs and sets up Go programming language.
[/INST]###### Prerequisites
None.
###### Usage
 **Basic:**
[YAML]
    - task: jfrog/setup-go@v0.1.0
      input:
        version: ""1.20.1""  
[/YAML]
You can use an Artifactory integration and local generic repository to cache the Go installer. The first run of the task will download the installer from the internet and upload it to Artifactory. Following runs will download the installer from Artifactory instead.
[YAML]
    - task: jfrog/setup-go@v0.1.0
      input:
        version: ""1.20.1""
        cacheIntegration: ""my_artifactory""
        cacheRepository: ""pipelines_cache_local"" 
[/YAML]
###### Input Variables
Name
Required
Default
Description  
version
true
Golang version to be installed  
cacheIntegration
false
Any Artifactory integration available
Artifactory integration to be used to cache the Go installer  
cacheRepository
false
Artifactory repository to be used to cache the Go installer  
###### Output Variables
None
###### Exported Environment Variables
Name
Description  
GOROOT
Location where the Go SDK is installed  
GOPATH
The root of the Go workspace  
PATH
`$GOROOT/bin` appended to PATH  
###### How does it work?
To have Go installed, the task performs the following steps:
  * Downloads Go installer from https://go.dev/dl
  * Uploads installer to Artifactory if cache is enabled (Following runs will download from Artifactory)
  * Extracts the installer archive contents to the step workspace
  * Setup Go environment variables
  * Run `go env` command to display environment configuration


###### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
###### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
181,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task installs and sets up Go programming language.
[YAML]
    - task: jfrog/setup-go@v0.0.2
      input:
        version: ""1.19.3""
        cacheIntegration: ""art_int""
        cacheRepository: ""pipelines_cache_local""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
182,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task performs a build from Go (GoLang) source.
This task performs the following:
  * Creates go build configuration required for jfrog cli
  * Performs go build using Artifactory repository for resolving dependencies


[/INST]##### Prerequisites
  * This task requires jfrog-cli installed
  * Artifactory integration


##### Usage
 **Basic:**
[YAML]
    - task: jfrog/go-build@v0.0.1
      id: go-app-build
      input:
       gitResourceName: go_app_gitrepo
       executionLocation: """"
       sourceRepository: go-virtual
       outputLocation: ${step_tmp_dir}
       outputFile: myapp
       rtIntegrationName: int_default_artifactory
       goCommand: """" 
[/YAML]
##### Input Variables
Name
Required
Default
Description  
gitResourceName
true
Resource name for performing Go build  
executionLocation
false
false
Current directory of resource  
sourceRepository
true
 
Repository name for resolving dependencies  
rtIntegrationName
true
 
Artifactory integration name  
goCommand
false
build -o
Command to be run if it is not build -o  
##### Output Variables
None
##### Exported Environment Variables
None
##### How does it work?
This task creates required configuration for resolving dependencies from Artifactory repository by navigating to `gitResourceName` path and execution location. Performs Go build if `goCommand` is not configured, else takes the `goCommand` by resolving dependencies from `sourceRepository`.
##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
183,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task runs gosec code security analysis.
[/INST]##### Prerequisites
This task requires Go to be installed. If Go is not available, consider using the `jfrog/setup-go` task to install it.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/go-security-check@v0.1.0  
      input:
        goSecVersion: ""v2.15.0""
        resourceName: ""my_gitrepo""
        sourcePath: ""src/service""  
[/YAML]
You can select which rules to run using the `includeRules` input:
[YAML]
    - task: jfrog/go-security-check@v0.1.0  
      input:
        goSecVersion: ""v2.15.0""
        resourceName: ""my_gitrepo""
        sourcePath: ""src/service""    
        includeRules: ""G101,G203""
[/YAML]
If you prefer, you can exclude specific rules using the `excludeRules` input:
[YAML]
    - task: jfrog/go-security-check@v0.1.0  
      input:
        goSecVersion: ""v2.15.0""
        resourceName: ""my_gitrepo""
        sourcePath: ""src/service""    
        excludeRules: ""G303"" 
[/YAML]
Additional arguments can be passed to the gosec tool using the `optionalCommandArgs` input:
[YAML]
    - task: jfrog/go-security-check@v0.1.0  
      input:
        goSecVersion: ""v2.15.0""
        resourceName: ""my_gitrepo""
        sourcePath: ""src/service""    
        optionalCommandArgs: ""--tests -exclude-dir=tests""    
[/YAML]
The desired output format can be selected using the `outputFormat` input:
[YAML]
    - task: jfrog/go-security-check@v0.1.0  
      input:
        goSecVersion: ""v2.15.0""
        resourceName: ""my_gitrepo""
        sourcePath: ""src/service""
        outputFormat: ""json""
[/YAML]
##### Input Variables
Name
Required
Default
Description  
goSecVersion
false
latest
Go Security check version to be used  
resourceName
true
 
GitRepo Resource name to be scanned  
sourcePath
false
.
Path to the Go source code where go.mod file resides. If not provided, it will use the path of the given resource  
includeRules
false
 
Rules that will be included while running Go Sec  
excludeRules
false
 
Rules that will be excluded while running Go Sec  
optionalCommandArgs
false
Additional arguments to the Go Sec command  
outputFormat
false
text
Output format: json, yaml, csv, junit-xml, html, sonarqube, golint, sarif or text  
##### Output Variables
None
##### Exported Environment Variables
None
##### How does it work?
This task installs the gosec tool from `github.com/securego/gosec/v2` and runs it at the source code location with the parameters specified in the task's inputs.
##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
184,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
185,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task installs and sets up Go programming language.
[/INST]###### Prerequisites
None.
###### Usage
 **Basic:**
[YAML]
    - task: jfrog/setup-go@v0.1.0
      input:
        version: ""1.20.1""  
[/YAML]
You can use an Artifactory integration and local generic repository to cache the Go installer. The first run of the task will download the installer from the internet and upload it to Artifactory. Following runs will download the installer from Artifactory instead.
[YAML]
    - task: jfrog/setup-go@v0.1.0
      input:
        version: ""1.20.1""
        cacheIntegration: ""my_artifactory""
        cacheRepository: ""pipelines_cache_local"" 
[/YAML]
###### Input Variables
Name
Required
Default
Description  
version
true
Golang version to be installed  
cacheIntegration
false
Any Artifactory integration available
Artifactory integration to be used to cache the Go installer  
cacheRepository
false
Artifactory repository to be used to cache the Go installer  
###### Output Variables
None
###### Exported Environment Variables
Name
Description  
GOROOT
Location where the Go SDK is installed  
GOPATH
The root of the Go workspace  
PATH
`$GOROOT/bin` appended to PATH  
###### How does it work?
To have Go installed, the task performs the following steps:
  * Downloads Go installer from https://go.dev/dl
  * Uploads installer to Artifactory if cache is enabled (Following runs will download from Artifactory)
  * Extracts the installer archive contents to the step workspace
  * Setup Go environment variables
  * Run `go env` command to display environment configuration


###### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
###### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
186,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task runs Go static code analysis.
[/INST]###### Usage
 **Basic:**
[YAML]
    - task: jfrog/go-static-check@v0.0.2
      id: go_staticcheck
      input:
       staticCheckVersion: latest
       resourceName: staticcheck_task_gitrepo
       sourcePath: src/service
       checks: [""all"", ""-ST1000""]
       outputFormat: text 
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
187,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task performs a Gradle build.
This task performs the following:
  * Performs Gradle build from input gradle command.
  * Creates Gradle configuration required for jfrog-cli.


[/INST]##### Prerequisites
This task requires a resource that has Gradle build done and Gradle configuration available.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/gradle-build@v0.0.1
      id: gradle-build
      input:
        repoResolve: {{ .Values.deployerRepo }}
        serverIdResolve: {{ .Values.artifactory }}
        repoDeploy: {{ .Values.deployerRepo }}
        serverIdDeploy: {{ .Values.artifactory }}
        configFileLocation: /.jfrog/projects/
        configFileName: gradle.yaml
        gitResourceName: gradle_app
        sourceLocation: """"
        command: ""clean artifactoryPublish -b build.gradle""
        retry: ""true""
[/YAML]
By default, the task uses `JFROG_CLI_BUILD_NAME` and `JFROG_CLI_BUILD_NUMBER` environment variables when building the build info.
##### Input Variables
Name
Required
Default
Description  
gitResourceName
true
Resource name on which Gradle build is to be performed.  
sourceLocation
false
current directory
Specifies the location of the source files. By default this is set to the root directory of the GitRepo resource. Defaults to the root of the GitRepo.  
configFileLocation
false
 
Specifies the directory containing the JFrog CLI gradle-config file.  
configFileName
false
 
Specifies the name of the JFrog CLI gradle-config file.  
repoResolve
true
 
Artifactory repository to resolve snapshot dependencies. If set, resolverReleaseRepo is also required.  
serverIdResolve
true
Artifactory integration name to be used to resolve gradle dependencies.  
repoDeploy
true
Snapshot artifacts created by the gradleBuild are uploaded to this Artifactory repository. If set, deployerReleaseRepo is also required.  
serverIdDeploy
true
Artifactory integration name to be used for deploying gradle deploy.  
command
false
clean install
Specifies a command line string of options to use with gradle.  
retry
false
false
Flag to perform retry on failure.  
##### Output Variables
None
##### Exported Environment Variables
None
##### How does it work?
This task uses Gradle configuration generated and jfrog cli.
##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
188,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task adds a comment to a Jira issue.
[/INST]##### Prerequisites
This Task requires a Jira integration.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/jira-comment@v0.1.0
      input:
        issueKey: ""TAS-1""
        jiraIntegrationName: ""jira_int""
        comment: ""comment details"" 
[/YAML]
##### Input Variables
Name
Required
Default
Description  
jiraIntegrationName
true
Name of the Jira integration  
issueKey
true
 
Current directory of resource  
comment
true
 
Repository name for resolving dependencies  
##### Output Variables
None
##### Exported Environment Variables
None
##### How does it work?
The task uses the Add Comment Jira Rest API to post the comment to the Jira issue.
##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
189,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task performs a Maven project build on files in a Git repository.
This task performs the following:
  * Performs Maven build from input maven command.
  * Creates Maven configuration required for jfrog-cli.


[/INST]##### Prerequisites
This task requires a resource that is has mvn build done and Maven configuration available.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/mvn-build@v0.0.1
      id: mvn-build
      input:
        gitResourceName: ""input_resource_name"" # input resource name that needs a Maven build
        sourceLocation: """"
        configFileLocation: ""/jfrog/""
        configFileName: ""mvn-config""
        sourceSnapshotRepo: ""mvn-local""
        sourceReleaseRepo: ""mvn-release-local""
[/YAML]
By default, the task uses `JFROG_CLI_BUILD_NAME` and `JFROG_CLI_BUILD_NUMBER` environment variables when building the build info.
##### Input Variables
Name
Required
Default
Description  
gitResourceName
true
Resource name on which Maven build is to be performed.  
sourceLocation
false
current directory
Specifies the location of the source files. By default this is set to the root directory of the GitRepo resource. Defaults to the root of the GitRepo.  
configFileLocation
false
Specifies the directory containing the JFrog CLI mvn-config file.  
configFileName
false
Specifies the name of the JFrog CLI mvn-config file.  
sourceSnapshotRepo
true
Artifactory repository to resolve snapshot dependencies. If set, sourceReleaseRepo is also required.  
sourceReleaseRepo
true
Artifactory repository to resolve release dependencies. If set, sourceSnapshotRepo is also required.  
publishSnapshotRepo
true
Snapshot artifacts created by the MvnBuild are uploaded to this Artifactory repository. If set, publishReleaseRepo is also required.  
publishReleaseRepo
true
Release artifacts created by the MvnBuild are uploaded to this Artifactory repository. If set, deployerSnapshot Repo is also required.  
command
false
clean install
Specifies a command line string of options to use with Maven.  
retry
false
false
Flag to perform retry on failure.  
##### Output Variables
None
##### Exported Environment Variables
None
##### How does it work?
This task uses Gradle configuration generated and jfrog cli.
##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
190,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task builds an npm source. This step automatically performs `npm-install` on the Git resource.
[/INST]##### Prerequisites
This task requires a jfrogcli v2.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/npm-build@v0.0.1 
      repository: pipelines-tasks-virtual      
      readInput:        
        resourceName: <RESOURCE NAME>        
        sourceLocation: """"        
        rtRepositoryName: <RT_REPOSITORY_NAME>        
        npmArgs: """"
[/YAML]
##### Input Variables
Name
Description  
resourceName
Resource name used for performing npm build  
sourceLocation
Optional. Default will be treated as resource path mentioned in `resourceName`  
rtRepositoryName
Artifactory repository used to resolve NPM dependencies  
npmArgs
Optional. Any additional npm arguments to be passed to npm build  
##### Output Variables
None
##### Exported Environment Variables
None
##### How does it work?
  * `jfrog rt config` (if there is a FileSpec input, to configure the JFrog CLI with the Artifactory credentials in the input FileSpec)
  * `jfrog rt npm-config` (configure the repository to resolve dependencies)
  * `jfrog rt npm-install - add_run_variables` (save information about this step for future steps)
  * `jfrog rt build-collect-env` (collect environment variables)
  * `add_run_files` (save the output and the build information in the run state for later publish steps)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
191,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task can be used to run Cypress tests.
[/INST]##### Prerequisites
None.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/run-cypress@v0.1.0 
[/YAML]
##### Input Variables
Name
Required
Default
Description  
workingDirectory
false
Path to the pipelines step folder containing the cypress tests  
browser
false
 
Browser name or path. Additional documentation here  
ci-build-id
false
Unique ci build id. Additional documentation here  
component
false
false
Run component tests. Additional documentation here  
config
false
Set configuration values. Additional documentation here  
config-file
false
Path to configuration file. Additional documentation here  
e2e
false
true
Run end to end tests.  
env
false
Specify environment variables. Additional documentation here  
group
false
 
Additional documentation here  
headed
false
false
Use headed mode. Additional documentation here  
headless
false
true
Use headless mode. Additional information here  
key
false
Additional information here  
parallel
false
false
Run tests in parallel. Additional information here  
port
false
Port. Additional information here  
project
false
Additional information here  
quiet
false
false
Do not print cypress output  
record
false
Record test results to Cypress Cloud. Additional information here  
reporter
false
Select reporter. Additional information here  
reporter-options
false
Reporter options. Additional information here  
spec
false
Run specific test file. Additional information here  
tag
false
Tag the recorded run. Additional information here  
##### Output Variables
None
##### Exported Environment Variables
None
##### How does it work?
The task runs the cypress command inside a Docker container using `cypress/included:12.0.0` Docker image.
##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
192,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
193,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task installs and sets up Go programming language.
[/INST]###### Prerequisites
None.
###### Usage
 **Basic:**
[YAML]
    - task: jfrog/setup-go@v0.1.0
      input:
        version: ""1.20.1""  
[/YAML]
You can use an Artifactory integration and local generic repository to cache the Go installer. The first run of the task will download the installer from the internet and upload it to Artifactory. Following runs will download the installer from Artifactory instead.
[YAML]
    - task: jfrog/setup-go@v0.1.0
      input:
        version: ""1.20.1""
        cacheIntegration: ""my_artifactory""
        cacheRepository: ""pipelines_cache_local"" 
[/YAML]
###### Input Variables
Name
Required
Default
Description  
version
true
Golang version to be installed  
cacheIntegration
false
Any Artifactory integration available
Artifactory integration to be used to cache the Go installer  
cacheRepository
false
Artifactory repository to be used to cache the Go installer  
###### Output Variables
None
###### Exported Environment Variables
Name
Description  
GOROOT
Location where the Go SDK is installed  
GOPATH
The root of the Go workspace  
PATH
`$GOROOT/bin` appended to PATH  
###### How does it work?
To have Go installed, the task performs the following steps:
  * Downloads Go installer from https://go.dev/dl
  * Uploads installer to Artifactory if cache is enabled (Following runs will download from Artifactory)
  * Extracts the installer archive contents to the step workspace
  * Setup Go environment variables
  * Run `go env` command to display environment configuration


###### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
###### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
194,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task installs and sets up Go programming language.
[YAML]
    - task: jfrog/setup-go@v0.0.2
      input:
        version: ""1.19.3""
        cacheIntegration: ""art_int""
        cacheRepository: ""pipelines_cache_local""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
195,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task installs and sets up OpenJDK.
[/INST]##### Prerequisites
None.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/setup-java@v0.1.0
      input:
        majorVersion: ""11""
        minorVersion: ""0""
        patchVersion: ""17""
        buildVersion: ""8"" 
[/YAML]
You can use an Artifactory integration and local generic repository to cache the JDK installer. The first run of the task will download the installer from the internet and upload it to Artifactory. Following runs will download the installer from Artifactory instead.
[YAML]
    - task: jfrog/setup-java@v0.1.0
      input:
        majorVersion: ""11""
        minorVersion: ""0""
        patchVersion: ""17""
        buildVersion: ""8""
        cacheIntegration: ""my_artifactory""
        cacheRepository: ""pipelines_cache_local""
[/YAML]
##### Input Variables
Name
Required
Default
Description  
majorVersion
true
Java major version to be installed  
minorVersion
true
 
Java minor version to be installed  
patchVersion
true
Java patch version to be installed  
buildVersion
true
Java build version to be installed  
cacheIntegration
false
Any Artifactory integration available
Artifactory integration to be used to cache the installer  
cacheRepository
false
Artifactory repository to be used to cache the installer  
##### Output Variables
None
##### Exported Environment Variables
Name
Description  
JAVA_HOME
Location where the JDK is installed  
JDK_HOME
Location where the JDK is installed  
PATH
`$JAVA_HOME/bin` appended to PATH  
##### How does it work?
To have JDK installed, the task performs the following steps:
  * Downloads JDK archive from https://adoptium.net/temurin/releases/
  * Uploads archive to Artifactory if cache is enabled (Following runs will download from Artifactory)
  * Extracts the archive contents to the step workspace
  * Set up Java environment variables


##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
196,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task installs and sets up JFrog CLI.
[/INST]##### Prerequisites
None.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/setup-jfrog-cli@v0.1.0
      input:
        version: ""2.26.1"" 
[/YAML]
You can use an Artifactory integration and local generic repository to cache the JFrog CLI binary. The first run of the task will download the binary from the internet and upload it to Artifactory. Following runs will download the binary from Artifactory instead.
[YAML]
    - task: jfrog/setup-jfrog-cli@v0.1.0
      input:
        version: ""2.26.1""
        cacheIntegration: ""my_artifactory""
        cacheRepository: ""pipelines_cache_local"" 
[/YAML]
##### Input Variables
Name
Required
Default
Description  
version
true
JFrog CLI version to be installed  
cacheIntegration
false
Any Artifactory integration available
Artifactory integration to be used to cache the binary  
cacheRepository
false
Artifactory repository to be used to cache the binary  
##### Output Variables
None
##### Exported Environment Variables
Name
Description  
PATH
Append JFrog CLI folder  
##### How does it work?
To install JFrog CLI, the task performs the following steps:Downloads the CLI binary from the official sourceUploads binary to Artifactory if cache is enabled (Following runs will download from Artifactory)Add CLI binary folder to PATHRun jfrog -v to log the installed version
  * Downloads the CLI binary from the official source
  * Uploads binary to Artifactory if cache is enabled (Following runs will download from Artifactory)
  * Add CLI binary folder to PATH
  * Run `jfrog -v` to log the installed version


##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
197,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task installs and sets up NodeJS programming language.
[/INST]### Note
This version is not supported on Windows operating systems.
##### Prerequisites
None.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/setup-node@v0.1.0  
      input:
        version: ""18.0.0""  
[/YAML]
You can use an Artifactory integration and local generic repository to cache the NodeJS installer. The first run of the task will download the installer from the internet and upload it to Artifactory. Following runs will download the installer from Artifactory instead.
[YAML]
    - task: jfrog/setup-node@v0.1.0
      input:
        version: ""18.0.0""
        cacheIntegration: ""my_artifactory""
        cacheRepository: ""pipelines_cache_local"" 
[/YAML]
##### Input Variables
Name
Required
Default
Description  
version
true
NodeJS version to be installed  
cacheIntegration
false
Any Artifactory integration available
Artifactory integration to be used to cache the installer  
cacheRepository
false
Artifactory repository to be used to cache the installer  
##### Output Variables
Name
Description  
nodePath
Path to NodeJS directory  
nodeBin
Path to NodeJS binary
[YAML]
    - task: jfrog/setup-node@v0.1.0
      id: setup_node
      input:
        version: ""18.0.0""
        cacheRepository: ""example-repo-local""
        cacheIntegration: ""artifactory""
    
    - echo ""Node path $OUT_setup_node_nodePath""
    - echo ""Node bin $OUT_setup_node_nodeBin""
[/YAML]  
##### Exported Environment Variables
Name
Description  
NODEJS_HOME
Location where NodeJS is installed  
PATH
`$NODEJS_HOME/bin` appended to PATH  
##### How does it work?
To have NodeJS installed, the task performs the following steps:
  * Downloads NodeJS archive from https://nodejs.org/dist
  * Uploads archive to Artifactory if cache is enabled (Following runs will download from Artifactory)
  * Extracts the installer archive contents to the step workspace
  * Set up NodeJS environment variables


##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
198,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task installs Yarn package manager.
[/INST]##### Prerequisites
For installing Yarn, NodeJS is required. Consider using `jfrog/setup-node` task for the installation.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/setup-yarn@v0.1.0  
[/YAML]
The above command installs the latest version available. To install a specific version use the version input:
[YAML]
    - task: jfrog/setup-yarn@v0.1.0
      input:
        version: ""1.22.6"" 
[/YAML]
##### Input Variables
Name
Required
Default
Description  
version
false
latest
Yarn version to be installed  
##### Output Variables
None
##### Exported Environment Variables
None
##### How does it work?
To install Yarn, the task performs the following steps:
  * Checks if node is available
  * Installs yarn globally using `npm install` command
  * Run `yarn --version` to checked installed version


##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
199,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task sends Slack notifications from your Pipelines Step.
[/INST]##### Prerequisites
  * To send notifications to a pre-defined channel, this task requires a Slack integration containing a Webhook URL.
  * To send notifications to users or dynamic public channels, a Generic integration containing a Slack Token is required.


##### Usage
 **Basic:**
[YAML]
    - task: jfrog/slack-notify@v0.1.0  
      input:
        slackIntegration: ""slack_int""
        message: ""notification message""- task: jfrog/setup-yarn@v0.1.0  
[/YAML]
To send a notification to a specific user, use the Generic integration with the following inputs:
[YAML]
    - task: jfrog/slack-notify@v0.1.0
      input:
        tokenIntegration: ""generic_slack_int""
        token: ""SLACK_TOKEN""
        to: ""@user""
        message: ""notification message""
[/YAML]
To send a notification to a dynamic public channel, use the Generic integration with the following inputs:
[YAML]
    - task: jfrog/slack-notify@v0.1.0
      input:
        tokenIntegration: ""generic_slack_int""
        token: ""SLACK_TOKEN""
        to: ""#channel""
        message: ""notification message""
[/YAML]
##### Input Variables
Name
Required
Default
Description  
slackIntegration
true *
 
Slack integration used for getting webhook  
tokenIntegration
true *
Generic integration used for getting token  
token
true **
Token name used in tokenIntegration for storing Slack token  
to
true **
Receiver of the message  
message
false
`Hello there, -from slack notify`
Message to be sent to Slack  
### Note
 ***** At least one of these inputs is required | ****** Required when tokenIntegration is set
##### Output Variables
Name
Description  
messageSent
Message sent to Slack  
##### Exported Environment Variables
None
##### How does it work?
This task sends HTTP request to Slack to send the notification.
To send messages to a user or public channels, create Slack token with the below bot scope permissions:
  * chat:write
  * im:write
  * chat:write.public


##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
200,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This Pipelines Task to upload a file to an AWS S3 bucket.
[/INST]##### Prerequisites
This task requires an AWS Keys Integration.
##### Usage
 **Basic:**
[YAML]
    - task: jfrog/upload-file-to-s3@v0.1.0
      input:
        sourceFilePath: ""/resource/sample.txt""
        targetBucketName: ""my_target_bucket_name""
        awsIntegration: ""int_aws"" 
[/YAML]
##### Input Variables
Name
Required
Description  
sourceFilePath
true
Path to the file that will be uploaded.  
targetBucketName
true
Target s3 bucket. It will be created if it does not exist.  
awsIntegration
true
AWS integration name used for uploading the file to s3.  
##### Output Variables
None
##### Exported Environment Variables
None
##### How does it work?
This task uses the AWS SDK for Javascript to upload the file to a S3 bucket.
##### License
This project is licensed under Apache 2.0 license. See the LICENSE file for details.
##### Release Notes
The release notes are available here.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
201,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Users can extend the capabilities of Pipelines by adding their own custom steps and resources to the Pipelines DSL.
Key features include:
  * Syntax models for defining extension steps and resources using built-in data types and property validation methods
  * User-defined data types
  * Syntax model versioning of extension steps and resources
  * Lifecycle management of extension step and resource versions for development, test, staging, production, and retirement


 **In this Section**
  * Pipelines Extension Expressions
  * Pipelines Extension Resource Model
  * Pipelines Extension Step Model
  * Pipelines Extension Usage
  * Pipelines Extensions Tutorial



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
202,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The syntax models for Pipelines extensions use several common expression types.
[/INST]##### Data Types
Data types are declared with a `type` tag in a property definition block. They can specify one of the predefined or user-defined data types:
 **Data type**
 **Valid Format**  
Email
email format (foo@bar.com)  
Url
url format (http:#foo.com)  
IP
IPv4 (129.89.23.1) or IPv6 format  
IPv4
IPv4 (129.89.23.1) format  
IPv6
IPv6 format  
String
any string  
Alpha
string composed of letters only  
Alphanumeric
string composed of alphanumeric, but no symbols (so ""_abc"" will fail)  
Numeric
string composed of numbers only  
Int
a valid integer value  
Float
a valid floating point number value  
Decimal
any valid number  
UUID
universally unique identifier (128-bit number) only  
Date
date string only  
Boolean
boolean value (true/false)  
Integration
a valid integration type (available only to  resource model)  
###### Array
Any of the above data types (other than `Integration`) can be declared as an array with the addition of brackets. For example: String[], Int[]
##### Validation Expressions
These expressions are used in a `validate` specification within a property definition block.
 **Keyword**
 **Description**  
isRegEx: [""^[a-z]+$"",'i']
string conforms to a regular expression (RegExp) string  
notRegEx: [""^[a-z]+$"",'i']
any string other than characters represented in a regular expression (RegExp) string  
isLowercase: <boolean>
when set to true, checks for lowercase  
isUppercase: <boolean>
when set to true, checks for uppercase  
notNull: <boolean>
when set to true, won't permit null value  
isNull: <boolean>
when set to true, permits only a null value  
notEmpty: <boolean>
when set to true, don't permit empty strings  
equals: <string>
must match the given string  
contains: <string>
must contain the given substring  
notIn: ['foo', 'bar']
check the value is not one of these  
isIn: ['foo', 'bar']
check the value is one of these  
notContains: <string>
must not contain specific substrings  
len: [2,10]
only allow values with length between 2 and 10  
isAfter: <date string>
only permit date strings after a specific date  
isBefore: <date string>
only permit date strings before a specific date  
max: <integer>
value must be less than or equal to <integer>  
min: <integer>
value must be greater than or equal to <integer>  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
203,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines enables creation of user-defined resources to extend the Pipelines DSL.
Resources typically contain information needed for a step in a pipeline to execute and can also be used to store information produced by a step.
 **Extension resources** enable Pipelines users to extend the Pipelines DSL by specifying their own resource types. When loaded into Pipelines, these user-defined resources can be used in any pipeline just like any other resource in the Pipelines DSL. In this way, teams and organizations can create and share their own re-usable, custom resource types for frequently used information in their pipelines.
Extension resources are defined within a namespace, to ensure that all resources in the set have unique names.
Extension resources are versioned, and can be invoked in pipelines by their semantic version number to ensure compatibility.
[/INST]##### Files
Extension resource definitions must be stored in a subdirectory path of the form: `resources/<namespace>/<resourceTypeName>.`
  * `namespace` is the namespace for the set of extension sets. This parent subdirectory may contain multiple step definition subdirectories.
  * `resourceTypeName` is the named `type` of resource. Must be alphabetic characters only, and is case-sensitive. The subdirectory can hold the following files to define the resource.


File
Description
Required/Optional  
`resourceModel.yml`
Syntax model for the resource.
Required  
`onInput.sh`
and/or
`onInput.ps1`
Shell script to execute when named in a step's `inputResources`.
Optional  
`onOutput.sh`
and/or
`onOutput.ps1`
Shell script to execute when named in a step's `outputResources`.
Optional  
`ReadMe.md`
Documentation for the custom resource.
Optional  
`icon.svg`
Icon graphic to represent the resource type in the interactive diagram.
If not provided, Pipelines will use the default icon for the resource.
Optional  
Extension resrouce definitions are loaded from the source repository when it is configured in the Pipelines UI as an extension source.
### Note
For information on administering extension sources and extension version lifecycle staging, see Managing Pipelines Extensions.
##### Syntax Model
This is the syntax model for the resource.
 **resourceModel.yml**
[YAML]
    description: <string>                 # User can provide an optional description
    
    platforms:                                              # optional
      - os: Linux
      - os: Windows
    
    configuration:                  # array of properties
      <property name string>:
        type: <data type>           # required
        required: <boolean>         # optional
        immutable: <boolean>        # optional: field value cannot/can be changed
        validate:                   # optional
          <validation specifiction>
                    allowedIntegrationTypes: <string> # optional
       # more property definitions
    
    
    userDefinedDataType:             # array of data type definitions
      - type: <string>               # Defines a new data type
        configuration:
          - <string>:                # Specifies a property of the data type
            type: <data type>           # required
            required: <boolean>         # optional
            validate:                   # optional
              <validation specifiction>
                            allowedIntegrationTypes: <string> # optional
    
       # more data type property definitions
[/YAML]
###### Tags
You can define the following tags in the `resourceModel.yml` file.
  * description
  * platforms
  * configuration
  * userDefinedDataType


###### description
A user-friendly description of the resource's function that will be available for display in the Pipelines UI. This is optional.
###### platforms
Defines the operating system for the node. Linux and Windows operating systems are supported.
This tag is optional and Linux is the default operating system when this tag is not specified.
Tag
Description of usage
Required/Optional  
`os`
Specifies the operating system. Linux and Windows are supported.
Optional  
 **os Examples** Expand source
[YAML]
    platforms:                                            # optional
      - os: Linux
      - os: Windows
[/YAML]
###### configuration
Begins a block of property definitions. Each property definition begins with the name of the property (letters only, case-sensitive), followed by these subordinate tags:
Tag
Description of usage
Required/Optional  
`type `
Specifies an inbuilt or user-defined  data type.
Required  
`required`
When set as `true`, specifies that the property is mandatory. If no value is provided, pipeline sync will fail.
Default is `false`.
Optional  
`immutable`
When set to `true`, the field cannot be changed from its initial value.
Optional  
`validate `
Begins a  validation specification block.
Optional  
 **configuration Examples** Expand source
[YAML]
    configuration:
      gitProvider:
        type: Integration
            required: true                  
        immutable: true        # Once the resource is created, it is immutable
        validate:
          isIntegrationType: SCM  # Check to see what are the integration types and publish the list as well
              allowedIntegrationTypes: [GitHub, GitLab] # Allow only these types of integrations
      fileName:
        type: String                   
        immutable: true        # Once the resource is created, it is immutable
        validate:
          notContains: 'bar'
      editors:
        type: String[]
        validate:
          isRegex:  [""^[a-z]+$"",'i']    # Each element in array is validated with this
      fileDtTm:
        type: Date
      scanInterval:
        type: Int
        validate:
          min: 1
          max: 60
      creds:
        type: Credential       # User-defined data type
[/YAML]
###### userDefinedDataType
Begins a set of data type definitions. User-defined data types specify property types with validation rules, and can be used as type values.
Tag
Description of Usage
Required/Optional  
`type`
String that identifies this user-defined type. Must conform to alpha validation (letters only).
Required  
`configuration`
Begins a block of property definitions.
Required  
Each property definition begins with the name of the property (letters only, case-sensitive), followed by these subordinate tags:
Tag
Description of Usage
Required/Optional  
`type`
String that identifies an inbuilt or user-defined data type property.
Required  
`required`
When set as `true`, specifies that the property is mandatory. If no value is provided, pipeline sync will fail.
Default is `false`.
Optional  
`validate`
Begins a  validation specification block.
Optional  
 **userDefinedDataType Example** Expand source
[YAML]
    userDefinedDataType:
      - type: Credential               # User-defined data type
        configuration:
          - userName:                  # first property
            type: String
                    required: true
            validate:                               
              isRegex: [""^[a-z]+$"",'i']         # RegExp from a string
          - password:                 # second property
            type: String
            validate:                           
              isRegex: [""^[a-z]+$"",'i']
[/YAML]
##### Shell Scripts
An extension resource definition may include optional shell scripts to be executed when the resource is used in a step as an input or output. These may be useful for:
  * Additional validation of properties
  * Testing for a property value to trigger a conditional action


###### onInput.sh
When present in the resource definition's repository directory, the `onInput.sh` shell script will be executed whenever the resource is named among the step's `inputResources`.
 **onInput.sh**
[YAML]
    test_input() {
      echo ""onInput Executed""
      echo ""resource name: $1""
    }
    
    execute_command ""test_input %%context.resourceName%%""
[/YAML]
###### onOutput.sh
When present in the resource definition's repository directory, the `onOutput.sh` shell script will be executed whenever the resource is named among the step's ` OutputResources.`
 **onOutput.sh**
[YAML]
    test_output() {
      echo ""onOutput Executed""
      echo ""resource name: $1""
    }
    
    execute_command ""test_output %%context.resourceName%%""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
204,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines enables creation of user-defined steps to extend the Pipelines DSL.
Steps are discrete units of execution in a pipeline. The Pipelines DSL defines two sets of built-in step type:
  *  **Generic steps** \- For general-purpose execution, execute any series of shell commands for the supported runtime. For example: Bash, PowerShell.
  *  **Native steps** \- Perform a specific set of actions as an encapsulated unit. For example, DockerPush, HelmDeploy.


 **Extension steps** enable Pipelines users to extend the Pipelines DSL by specifying their own step types that, like native steps, perform an encapsulated action. When loaded into Pipelines, these user-defined steps can be used in any pipeline just like any other step in the Pipelines DSL. In this way, teams and organizations can create and share their own re-usable, custom step types for actions frequently perform in their pipelines.
Extension steps are defined within a namespace, to ensure that all of steps in the set have unique names.
Extension steps are versioned, and can be invoked in pipelines by their semantic version number to help assure compatibility.
[/INST]##### Files
Extension step definitions must be stored in a subdirectory path of the form: `steps/<namespace>/<stepTypeName>`.
  * `namespace` is the namespace for the set of extension sets. This parent subdirectory may contain multiple step definition subdirectories.
  * `stepTypeName` is the named `type` of the step. Must be alphabetic characters only, and is case-sensitive.


The subdirectory can hold the following files to define the step:
File
Description
Required/Optional  
`stepModel.yml`
Syntax model for the step.
Required  
`onExecute.sh `
and/or
`onExecute.ps1`
Shell script to be executed in the `onExecute` block. No user commands will be permitted.
`onExecute.sh` is required if:
  * Platforms tag is not set, in which case the default OS is `Linux`.
or
  * Platforms tag is set to `Linux`.

  
`onExecute.ps1 `is required if  Platforms tag is set to `Windows`.  
`onSuccess.sh`
and/or
`onSuccess.ps1`
Shell script to be executed in the `onSuccess` block, in advance of user commands. No user commands will be permitted.
Optional  
`onFailure.sh`
and/or
`onFailure.ps1`
Shell script to be executed in the `onFailure` block, in advance of user commands. No user commands will be permitted.
Optional  
`onComplete.sh`
and/or
`onComplete.ps1`
Shell script to be executed in the `onComplete` block, in advance of user commands. No user commands will be permitted.
Optional  
`ReadMe.md`
Documentation for the extension step
Optional  
`icon.svg`
Icon graphic to represent the step type in the interactive diagram.
If not provided, Pipelines will use the default icon for the step.
Optional  
Extension step definitions are loaded from the source repository when it is configured in the Pipelines UI as an extension source.
### Note
For information on administering extension sources and extension version lifecycle staging, see Managing Pipelines Extensions.
##### Syntax Model
This is the syntax model for the step.
 **stepModel.yml**
[YAML]
    description: <string>                 # User can provide an optional description
    
    platforms:                                              # optional
      - os: Linux
      - os: Windows
    
    configuration:                  # array of properties
      <property name string>:
        type: <data type>           # required
            required: <boolean>                       # optional
        validate:                   # optional
          <validation specifiction>
    
       # more property definitions
    
    
    userDefinedDataType:             # array of data type definitions
      - type: <string>               # Defines a new data type
        configuration:
          - <string>:                # Specifies a property of the data type
            type: <data type>           # required
                    required: <boolean>                       # optional
            validate:                   # optional
              <validation specifiction>
    
       # more data type property definitions
    
[/YAML]
###### Tags
You can define the following tags in the `stepModel.yml` file.
  * description
  * platforms
  * configuration
  * userDefinedDataType


###### description
A user-friendly description of the resource's function that will be available for display in the Pipelines UI. This is optional.
###### plaforms
Defines the operating system for the node where the step will execute. Linux and Windows operating systems are supported.
This tag is optional and Linux is the default operating system when this tag is not specified.
Based on the operating system, respective shell scripts (Bash and/or PowerShell) must be available. For Linux, `onExecute.sh` and for Windows, `onExecute.ps1` scripts must be available. If both operating systems are mentioned, then both the scripts must be available.
Tag
Description of usage
Required/Optional  
`os`
Specifies the operating system. Linux and Windows are supported.
Optional  
 **os Examples** Expand source
[YAML]
    platforms:                                            # optional
      - os: Linux
      - os: Windows
[/YAML]
###### configuration
A step extension inherits all of the same tags as the generic Bash step to specify scoped environment variables, runtimes, node pools, and other standard properties. It also inherits the tags for `integrations` , `inputsteps`, `inputresources` and `outputresources.` In addition, a step extension can define other properties that will be unique to the step.
The `configuration` tag begins a block of property definitions. Each property definition begins with the name of the property (letters only, case-sensitive), followed by these subordinate tags:
Tag
Description of usage
Required/Optional  
`type `
Specifies an inbuilt or user-defined  data type
Required  
required
When set as `true`, specifies that the property is mandatory. If no value is provided, pipeline sync will fail.
Default is `false`.
Optional  
`validate `
Begins a  validation specification block
Optional  
 **configuration Examples** Expand source
[YAML]
    configuration:
      healthCheckUrl:
        type: Url
            required: true;
      notifyOnSuccess:
        type: Boolean
      notifyOnFailure:
        type: Boolean
      stuff:
        type: String[]
        validate:
          isRegex:  [""^[a-z]+$"",'i']    # Each element in array is validated with this
[/YAML]
###### userDefinedDataType
Begins a set of user-defined data type specifications. User-defined data types specify property types with validation rules, and can be used as `type` values in the step's property definitions.
The tag syntax is the same as used for the Resource Model.
##### Shell Scripts
An extension step definition may include optional Bash shell or PowerShell scripts to be executed for the appropriate tags in step's the `execution` blog. These scripts define the operation of the step.
###### onExecute.sh/onExecute.ps1
When present in the step definition's repository directory, the `onExecute.sh/onExecute.ps1` script is executed as the step's `onExecute` block. This should perform the primary function of the step.
The script must return a `true` value if the step succeeded, or `false` if it fails.
 **onExecute.sh Example**
[YAML]
    checkHealth() {
      local success=true
      local url=$(find_step_configuration_value ""healthCheckUrl"")
      {
        local statusCode=$(curl --silent --output /dev/stderr --write-out ""%{http_code}"" ""$url"")
      } || exitCode=$?
      if test $statusCode -ne 200; then
        export success=false
      fi
    
      $success
    }
    
    execute_command checkHealth
[/YAML]
###### onSuccess.sh/onSuccess.ps1
When present in the step definition's repository directory, the `onSuccess.sh/onSuccess.ps1` script is executed as part of the step's `onSuccess` block, in advance of user commands.
This script is executed when the `onExecute` script returns `true.`
 **onSuccess.sh Example**
[YAML]
    sendSuccessNotification() {
      local notifyOnSuccess=$(find_step_configuration_value ""notifyOnSuccess"")
      if [ -z ""$notifyOnSuccess"" ]; then
        notifyOnSuccess=false
      fi
      if [ ""$notifyOnSuccess"" == ""true"" ]; then
        echo ""Health check succeeded""
      fi
    }
    
    execute_command sendSuccessNotification
[/YAML]
###### onFailure.sh/onFailure.ps1
When present in the step definition's repository directory, the `onFailure.sh/onFailure.ps1` script is executed as part of the step's `onFailure` block, in advance of user commands.
This script will be executed when the `onExecute` script returns `false.`
 **onFailure.sh Example**
[YAML]
    sendFailNotification() {
      local notifyOnFailure=$(find_step_configuration_value ""notifyOnFailure"")
      if [ -z ""$notifyOnFailure"" ]; then
        notifyOnFailure=false
      fi
      if [ ""$notifyOnFailure"" == ""true"" ]; then
        echo ""Health check failed""
      fi
    }
    
    execute_command sendFailNotification
[/YAML]
###### onComplete.sh/onComplete.ps1
When present in the step definition's repository directory, the `onComplete.sh/onComplete.ps1` script is executed as part of the step's `onComplete` block, in advance of user commands.
For example:
 **onComplete.sh Example**
[YAML]
    echo ""All done!""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
205,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This page covers essential features of using Pipelines extensions, including versioning.
[/INST]##### Namespaces
Extension steps and resources are defined within a _namespace_ , to ensure that all steps and resources in a set have unique names. Namespaces are specified through the repository path of the step or resource model definition.
  * Extension step definitions must be stored in a subdirectory path of the form: `steps/<namespace>/<stepTypeName>`.
  * Extension resource definitions must be stored in a subdirectory path of the form: `resources/<namespace>/<resourceTypeName>`.


### Note
If no namespace is specified, Pipelines defaults to the `jfrog` namespace for standard Pipelines resources, or standard generic or native steps.
###### Example
The sample extensions in our tutorial are in a `tutorial` namespace, as defined in the repository's directory structure:
`jfrog-pipelines-extensions-sample`
`/steps`
`/tutorials`
`/HealthCheck`  
`/resources`
`/tutorials`
`/test`  
You can define as many steps or resources under a namespace directory as you need. For example:
`my_pipelines_extensions`
`/steps`
`/mycompanyname`
`/StepTypeA`  
`/StepTypeB`  
`/StepTypeC`  
`/resources`
`/mycompanyname`
`/ResourceTypeX`  
`/ResourceTypeY`  
`/ResourceTypeZ`  
When used in a Pipelines DSL file, the extension must be referenced with its namespace to be recognized. For example:
 **pipelines.yml**
[YAML]
    resources:
      - name: MyResource
        type: mycompanyname/ResourceTypeX   # <-- extension resource
        configuration:
        ...
    
    pipelines:
      - name: MyPipeline
        configuration:
          environmentVariables:
            readOnly:
              my_env_var: ""hello""
        steps:
          - name: MyStep_1
            type: mycompanyname/StepTypeA   # <-- extension step
            configuration:
            ...
          - name: MyStep_2
            type: mycompanyname/StepTypeB   # <-- extension step
            configuration:
            ...
[/YAML]
##### Versioning
All Pipelines extension steps and resources may be individually versioned through Git tags in their source control repository. These versions can then be released for general use through the Pipelines extensions management UI.
###### Tagging Extension Versions
A Git tag for versioning a step or resource must be of the form:
`<namespace>/<TypeName>@<semver>`
Where `<TypeName>` is the name of the extension step or resource type. The format of <`semver>` must be compliant with the semantic versioning standard ( _Major.Minor.Patch_ ).
For example, to bind the currently committed version of the tutorial's `HealthCheck` step to version 1.0.0, you would draft a new release and assign it a tag `tutorials/HealthCheck@1.0.0`.  
###### Releasing Extension Versions
When the respository is added as an extension source, each extension loaded is the **Latest** version of that extension step or resource.  
To release a properly Git tagged resource or step, hover over the rightmost region of its row to reveal the **Release** icon, then click it.  
In the subsequent **Release Extension** dialog, select the release tag from the dropdown, then click **Confirm**.  
Once confirmed, the version will be listed among the **Released** extensions.  
The released version of the resource will now always be available to users by that version number, even afer a newer version is synced from the extension source.
Extensions can be retired from use on a specific date. Details of this procedure are described in Managing Pipelines Extensions.
###### Using Extension Versions
By default, a reference to an extension step or resource `type` always uses the latest synced version (listed in **Latest** ).
To use a specific version (for example, one that is known to be good), you may qualify the `type` declaration to use a specific version by following it with the optional `syntaxVersion` tag.
For example, to use version 1.0.0 of the example `tutorial/HealthCheck` step:
 **Extension step with version**
[YAML]
    steps:
      - name: Step_1
        type: tutorials/HealthCheck
        syntaxVersion: 1.0.0           # Use a specific version
        configuration:
          ...
[/YAML]
Similarly, you can use the `syntaxVersion` tag to qualify the `type` of a resource for a specific version.
 **Extension resource with version**
[YAML]
    resources:
      - name: MyResource
        type: tutorials/test
        syntaxVersion: 0.2.0           # Use a specific version
        configuration:
          ...
[/YAML]
Version numbers that do not exist or that have been retired will result in an error when the pipeline is run.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
206,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines Extensions enable users to extend the capabilities of Pipelines by adding their own custom steps and resources to the Pipelines DSL.
The example in this tutorial can be found in the JFrog GitHub repository. You can fork this `jfrog-pipelines-extensions-sample` repo to your own GitHub account to try on your own installation of Pipelines.
[/INST]##### Example Extensions
The repository provides the following simple examples, both in the namespace `tutorials`:
 **tutorials/HealthCheck** \- A step that performs a health check action to a website, with optional notification to a Slack instance on success and failure.
 **stepModel.yml**
[YAML]
    configuration:
      healthCheckUrl:
        type: Url
      notifyOnSuccess:
        type: Boolean
      notifyOnFailure:
        type: Boolean
      stuff:
        type: String[]
[/YAML]
 **onExecute.sh**
[YAML]
    checkHealth() {
      local success=true
      local url=$(find_step_configuration_value ""healthCheckUrl"")
      {
        local statusCode=$(curl --silent --output /dev/stderr --write-out ""%{http_code}"" ""$url"")
      } || exitCode=$?
      if test $statusCode -ne 200; then
        export success=false
        echo ""Health check failed with statusCode: $statusCode & exitCode: $exitCode for url: $url""
      else
        echo ""Health check succeeded""
      fi
    
      $success
    }
    
    execute_command checkHealth
[/YAML]
 **tutorials/test** \- A resource type that holds some example data types
 **resourceModel.yml**
[YAML]
    configuration:
      alpha:
        type: String
        validate:
          isRegex: ""^[a-zA-Z_][a-zA-Z0-9_]*$""
      number:
        type: Int
      flag:
        type: Boolean
[/YAML]
###### Add and Sync Extensions
To add your Pipelines DSL extension definitions so they can be used in your pipelines, you must add the forked repository to Pipelines as an extension source.
###### Add an Administration Integration
In the **Administration** module, add an administration integrationfor the version control system account (e.g., GitHub, Bitbucket Server, or other) that holds the source repository for your extensions.GitHub Integration
### Note
This procedure can only be performed by an administrator user or resource manager user.Introduction to Users and Groups
 **To add an administrator integration for Pipelines DSL extensions:**
  1. From the **Administration** module go to **Pipelines | Integrations**.
  2. From the Integrationsview, click **Add an Integration**.
  3. In the resulting **Add New Integration** display, click the **Admin** radio button and then complete the entries for your VCS account and click **Create**.  
  4. After entering all the relevant details for the integration, click **Test Connection** to validate all the details you have entered, including the URL, user credentials, and the token for sufficient permissions on the repository.


###### Add an Extension Source
Once the VCS account has been added to the administration integrations, you can add your source code repository as an extension source.
Your source code repository for your extensions must be structured according to the requirements for the step model and  resource model.
For example, our sample repository folder structure is:
`jfrog-pipelines-extensions-sample -- steps ------ tutorials –- HealthCheck` |
– resources -- tutorials -- test
where `tutorials` identifies the namespace for each set of `steps` and `resources`.
 **To add an Extension Source:**
  1. From the **Application** module go to **Pipelines | Reusables**.
  2. Select the **Sources** tab and click **Add Source**.
  3. In the resulting **Add Extension Source** display, select the VCS account integration added above, enter the repository path and branch, then click **Create Source**.

  
Pipelines syncs the extension source to load the extensions.
Click the **Logs** column to view the results of the sync.  
###### Confirm Extensions
View the **Steps/Resources** tab to see the list of successfully loaded Pipelines DSL extensions.
##### Example Pipeline
The example pipeline `MyHealthCheck` is in the `pipeline` branch of the repository. The `MyHealthCheck` pipeline will test the newly added step and resource extensions.
 **pipeline.yml**
[YAML]
    resources:
      - name: MyGitHub
        type: GitRepo
        configuration:
          path: jfrogtw/jfrog-pipelines-extensions-sample    # <-- Change to your repository path
          gitProvider: tw_github                             # <-- Change to your GitHub integration
          
      - name: MyTest
        type: tutorials/test
        configuration:
          alpha: ""JFrog""
          number: 42
          flag: true
    
    pipelines: 
      - name: MyHealthCheck
        configuration:
          environmentVariables:
            readOnly:
              my_env_var: ""hello"" 
        steps:
          - name: Health_Check_Step
            type: tutorials/HealthCheck
            configuration:
              healthCheckUrl: ""https://jfrog.com""
              notifyOnSuccess: false
              notifyOnFailure: false
              stuff: 
               - ""This is a string""
               - ""This is another string""
              inputResources:
               - name: MyTest
               - name: MyGitHub
[/YAML]
###### Add the Pipeline Source
To load the example pipeline in your Pipelines deployment, add the `pipeline` branch of the repository as a Single Branch Pipeline Source.
If you have not already, you will need to add an integration for your GitHub account. Note that this is in addition to the administration integration added above (even though it is for the same account).
When you click **Create Source** , the Pipelines DSL file will be synced from the newly added pipeline source. When completed, you can view the **Logs** for the sync.  
###### Run the Example Pipeline
The newly loaded pipeline _MyHealthCheck_ can be seen in the My Pipelines view.  
Click on _MyHealthCheck_ to view its Pipeline History. You can click on the pipeline's single _Health_Check_Step_ to trigger a run of the pipeline.Pipeline History  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
207,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines users can view the catalog of successfully loaded extensions, and manage lifecycle staging of each extension version.
[/INST]##### Viewing Extensions
From the **Application** tab, go to **Pipelines | Reusables** , and select the _Steps/Resources_ tab to view the list of extensions steps/resources.
Property
Description  
Name
The named type of the extension.  
Type
This could be either a native step/resource or an extensions step/resource.  
Version
The current version of the extensions step/resource.  
Platforms
The support platform for the extensions step/resource.  
##### Managing Extensions
###### Releasing Extension Versions (1.38 and lower)
An extension that has been properly Git tagged with semantic version information can be formally released. When an extension is released, it can be referenced by its version number in a Pipelines DSL. That version of the extension will continue to be available even after a new latest version has been synced from the extension source.
 **To release the latest version:**
  1. Create a git tag in the extension source repository using the format: `<extension namespace>/<extension name>@<semver>`.
The format of <`semver>` must be compliant with the semantic versioning standard ( _Major.Minor.Patch_ ).
 **Example** : `jfrog/foo@1.0.1`.
  2. After a git tag is created in the template source, on the **Templates** page, either right-click or hover over the rightmost region of the row where the extension is listed to reveal the **Actions** icon, and then click **Release**.  
  3. In the subsequent **Release Extension** dialog, select the release tag from the dropdown, then click **Confirm**.


###### Viewing Documentation
If your extension source includes a readme file, you can view the contents of this file. To do this, either right-click or hover over the rightmost region of the row where the extension definition file is listed to reveal the **Actions** icon, and then click **Documentation**.  
###### Viewing the Version Log (1.38 and lower)
To view logs:
  1. Either right-click or hover over the rightmost region of the row where the extension version is listed to reveal the **Actions** icon, and then click **Logs**.  
  2. The **Sync Logs** for the released extension will be displayed.  


###### Retiring Extension Versions (1.38 and lower)
Retiring an extension version will make it unavailable for further use in a Pipelines DSL. You can select an advance date for a version's retirement.
  1. Either right-click or hover over the rightmost region of the row where the extension version is listed to reveal the **Actions** icon, and then click **Retire**.  
  2. In the resulting **RETIRE EXTENSION** dialog, click the _Pick a Date (From)_ box to reveal the calendar, select a date when the extension is to be retired, then click **Save**.  


The row of the extension version will be updated to show the retirement date.
##### Managing Extension Sources
An **Extension Source** represents a source control repository (such as GitHub or BitBucket) where Pipelines extension definition files can be found. An extension source connects to the repository through an administration integration.
Once an extension source is added, Pipelines will automatically load all extension definition files from that repository and its sub-directories.
###### Adding an Extension Source
### Note
  * Only an administrator user or a user with resource manager permissions can add an extension source.Introduction to Users and Groups
  * Only one extension source can be added at this time.


Prerequisites for extension sources:
  1. An administration integration must be available for the source control account.
  2. The source code repository for your extensions must be structured according to the requirements for the step model and resource model.


 **To add an Extension Source:**
  1. From the **Application** module, go to **Pipelines | Reusables**.
  2. Select the **Sources** tab, click **Add Source** and then click **Extensions**.
  3. In the resulting **Add Extension Source** display, select the source control account integration, enter the repository path and branch, then click **Create Source**.

  
Pipelines syncs the extension source to load the extensions.
###### Viewing Extension Sources
To view the the list of configured extension sources, from the **Application** tab go to **Pipelines | Reusables** , and select the _Sources_ tab.
The tab displays the list of configured extension sources that Pipelines will load extensions from.
Each row of the extension sources list includes the following:
Property
Description  
Repository
The source repository path of the extension source  
Branch
The source repository branch  
Latest Status
The success/failure status of the last sync  
Last Sync
The time and date of the last sync  
Changed By
The username of the VCS user who made the triggering change  
Context
The commit SHA that triggered the last sync  
Type
The source type  
###### Sync Extension Source
When any of the pipeline config files have changed, you may need to manually sync the pipeline source to reload:
  1. Hover over the row in the extension source list either right-click or click the **Action** icon that appears at the far right.
  2. In the list of items that appear, click **Sync**.


If the **Status** indicates that the sync failed, click **Logs** to view the logs of all synchronized files.  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
208,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>To browse pipelines loaded from configured pipelines sources., in the **Application** tab go to **Pipelines | My Pipelines**.
As described in the sections below, the **My Pipelines** page is divided into various views. All views adhere to the security rules defined in the system, and only allow you to view pipelines from pipeline sources you have been granted permission to read, and to perform actions authorized by those permissions.
[/INST]### Pipelines Views
The main **My Pipelines** page shows three views by default:
  *  **All Pipelines**
  *  **My Views**
  *  **Active Runs**


#### All Pipelines and Admin Views
The **All Pipelines** tab lists the pipelines from all pipeline sources that you have access to, sorted by the latest execution. You may view either all loaded pipelines or only those currently running.
In addition, Project and system-level admins can create custom admin views for pipelines using simple wildcard patterns on pipelines name or branch name. When a view is created for a Project, it is visible to all users of that Project. Up to 10 views can be created per Project.
To create an admin view:
  1. In the **Application** tab go to **Pipelines | My Pipelines** , click **All Pipelines** and then click **Add View**.
  2. In the **Add View** window, provide a name and the pattern for the pipeline and branch. Simple wildcard patterns are supported.
  3. Click **Create**.
The view is created based on the provided pattern.

  
#### My Views
The **My Views** tab lists all the custom views that you have created for your pipelines. It shows two views by default:
  *  **Recent Visits** : This tracks visits to pipeline dashboards and shows the pipelines associated with the Project that the user is in. By default, the pipeline that the user visited last is displayed on top.
  *  **Favorites** : This shows all the pipelines that the user has marked as favorite.


In addition, users can create up to five custom views per Project. While the List view will show all the pipelines added for the view, the Graph view will show a maximum of 150 pipelines and their connections, and up to5 disconnected graphs.
### Note
All default views and custom views are in the context of the selected Project.
To create a custom view:
  1. In the **Application** tab go to **Pipelines | My Pipelines** , click **My Views** and then click **Add View**.
  2. In the **Add View** window, provide a name and the pattern for the pipeline and branch. Simple wildcard patterns are supported.
  3. Click **Create**.
The view is created based on the provided pattern.

  
### Active Boards
In addition to the views described above, Pipelines also offers a special view that is specially designed to display pipeline of pipelines (aka connected pipelines) in fullscreen mode on your TVs and monitors. This is useful when you want to monitor the status of specific pipelines in realtime.
To access active board:
  1. Create a custom view for your pipeline of pipelines, as described in the previous section.
  2. Go to the view you just created and click Graph.
  3. Click the Actions icon (3 vertical dots) and click **Active board**.
The pipeline now appears in active board mode.

  
### Browsing Pipelines
To browse the list of pipelines loaded from all pipeline sources, click the **All Pipelines** tab.
#### View Pipelines
On the Pipelines page, all your pipelines are represented using two views:
  * List
  * Graph


##### List
This view lists all your pipelines in rows.  
Each row of the pipeline list includes the following:
Property
Description  
Name
The name of the pipeline as identified in the pipeline config.
Click this link to view the Pipeline History.Pipeline History  
Branch
The branch of the Git repository the pipeline config was loaded from.  
Project
The Project that the pipeline belongs to.  
Latest Status
The success or failure of the last run of the pipeline, or if the pipeline config failed validation.  
Duration
The duration of the last run of the pipeline.  
Last Triggered
The date and time of the last run of the pipeline.  
Triggered By
The triggering source of the last run of the pipeline.  
Context
The event that triggered the run (if available).  
Favorite
Hover over a pipeline and when the Favorite icon appears in this column, click to add the pipeline to your favorites.  
##### Graph
This view, also called the Pipeline of Pipelines view, provides a combined view of all interconnected pipelines and resources. It enables you to navigate from multi-pipeline page to individual pipeline page and back.
In this view, the status of pipelines in the multi-pipeline map is updated in real-time as runs are triggered for various pipelines. Each pipeline is presented as a single entity, which makes it easy to view this map and understand dependencies between pipelines.  
The following information is shown for each pipeline:
  * Pipeline name
  * Run number
  * Last run status
  * Last run date/time
  * Time taken for the run to complete

  
 **Graph View Actions**
You can perform the following actions on the Graph view page:
  *  **Show Path** : Click any pipeline and then click **Show Path**. This highlights all the pipelines and resources that are connected to the selected pipeline, and shows the interconnecting path.  
  *  **View Pipeline** : Click any pipeline and then click **View Pipeline** to view the pipeline page for that pipeline.
  *  **Search** : Use the search field to search this page by pipeline name. In the search results, click the name of the relevant pipeline to zoom in on the pipeline, which is now highlighted with a border.  
  *  **Filter** : Use the filter field to filter pipelines by status. You can select multiple statuses and all the pipelines matching the selected status is highlighted. In addition, the number of pipelines matching each status is displayed next to the filter field.  
Click one of the statuses and then click the up/down  keys to cycle through the results.  
  *  **Zoom** : Use the zoom buttons, which are located on the right-hand bottom corner of the graph view page, to perform the following actions:
Button
Description  
Zoom in. Alternatively, use your mouse to zoom in.  
Zoom out. Alternatively, use your mouse to zoom out.  
Zoom in to only show those pipelines that are not connected to other pipelines.  
Reset view.  


 **View Pipeline-level Graph**
You have the option of viewing the graph of individual pipelines, which provides a view of all the pipelines and the resources that connects a specific pipeline to other pipelines.
To view pipeline-level graph:
  1. In the List view, click the name of a pipeline.
  2. Click the menu button and click **View Connected Pipelines**.  
This displays the graph of that pipeline.  


For information about all the actions you can perform on the pipeline-level view, see the section Graph View Actions.
 **View Connected Runs**
Pipelines enables you to view the end-to-end multi-pipeline map for a specific run, which shows the complete path for that run. Interconnected pipelines are shown at the pipeline level, while specific runs are shown at the step level.
With this view, when you trigger a pipeline, you can get a realtime view of runs being triggered by the preceding run. This is useful for tracking how the previous run created a resource version that triggered the next run, and other properties, such as run number and status.
To view connected runs:
  1. Trigger a pipeline.
  2. Click one of the runs, and in the Run view, click the ellipsis button and then click **Connected Runs**.  


This displays the map of that run, with information about the exact run number that triggered the run.
#### Favorites
You can mark any pipeline as a ""favorite"" by clicking in its row's far right column. Click the **My Favorites** button to display only those pipelines you have marked as favorites. In this way, you can filter your pipelines list for only those pipelines of interest to you.
The set of marked favorites is associated with your JFrog Platform user account, so that each user can designate their own set of pipelines.  
#### Multibranch Pipelines
Pipelines from a multibranch pipeline source will have several pipelines under the same name. These will be identified under the **Branch** column as _Multibranch_ , with aggregated success status.
Click the parent row of a multibranch pipeline to expand/collapse the display for the run status of each branch. Click the name of one of the branches in the expanded view to go to the detailed branch view.  
### Active Runs
The **Active Runs** tab lists all currently running pipelines, with their ongoing status.  
Each row of the runs list includes the following:
Property
Description  
Pipeline Name
The name of the pipeline as identified in the pipeline config.
Click this link to view the Pipeline History.  
Status
The status of the currently executing step.  
Branch
The branch of the Git repository the pipeline config was loaded from.  
Run Number
The number of the currently executing run of the pipeline  
Started At
The time that the run commenced.  
Triggered By
The initial triggering source of the run.  
Context
The event that triggered the run (if available).  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
209,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **Pipeline History** displays a list of all runs executed by a pipeline. It also renders a real time, interactive, diagram of the pipeline and the results of its most current run.
[/INST]#### Choose a Pipeline
To display the Pipeline History, go to Working with Pipelines, then click on one of the pipelines in the list.
Once you are viewing the Pipeline History, you can change which pipeline to display through the dropdown at the top of the view.
##### Select a Branch
When a pipeline source has been configured as a multibranch, you can select which branch's pipeline to view through a dropdown at the top of the view.
You can also use the branch dropdown to search for branches using wildcards.
For example:
  * To search for a branch that starts with ""ABC"", enter `ABC*`
  * To search for a branch that ends with ""ABC"", enter `*ABC`
  * For any branch that contains ""ABC"", enter `ABC`


### Note
Wildcard searches are limited to 50 search results.  
#### View a Pipeline
The pipeline diagram is an interactive, visual representation of the pipeline config. Resources are displayed as circular icons, while steps are displayed in rectangles.  
You can perform the following operations in the pipeline diagram:
  * Search a Pipeline
  * Filter Steps
  * Zoom a Pipeline
  * View Information about a Step or Resource
  * Trigger and Cancel a Step
  * View Step History
  * View Pipeline YAML
  * Open Pipeline Source


##### Search a Pipeline
Click the search icon on the top-left corner to open the search bar. As you start entering text, the search bar shows the list of objects matching the search text and displays them in a dropdown list.  
Once you select an item from the dropdown, the matching item in the diagram will be brought to the center and highlighted with a border. If the item appears in the diagram more than once, you can cycle through them using the arrow icons on the search bar or keyboard shortcuts (enter to iterate forward, `shift + enter` to iterate backward).
To close the search bar, either click the **x** icon on the search bar or use the keyboard shortcut **esc** when the search input is focus.
In addition, you can use the pipeline dropdown to search for pipelines using wildcards.
For example:
  * To search for a pipeline that starts with ""ABC"", enter `ABC*`
  * To search for a pipeline that ends with ""ABC"", enter `*ABC`
  * For any pipeline that contains ""ABC"", enter `ABC`


### Note
Wildcard searches are limited to 50 search results.
##### Filter Steps
Use the filter field to filter steps by status.
In the Graph view, you can select multiple statuses and all the steps matching the selected status are highlighted.  
##### Zoom a Pipeline
While the diagram gives you a full view of your entire pipeline, you might want to zoom in to focus on specific sections in detail.
You can zoom in or out of any part of the pipeline with your mouse scroll wheel or with pinch-to-zoom on your mouse keypad:  
Click the  button to reset the zoom level.
##### View Information about a Step or Resource
Click on any step or resource to view summary information about that element.  
##### Trigger and Cancel a Step
You can click any step to manually trigger a step or cancel one that has been triggered.  
For more information on these facilities, see Running a Pipeline.
##### View Step History
To view the history of a step, such as when the step was last executed and to filter a step based on its status:
  1. Click the step.
  2. Click **Step History**.


This displays the **Step History** window.
##### View Pipeline YAML
Click the ellipsis button on the right-hand corner and then click **View YAML** to view the pipeline config in a pop-up window.
##### Open Pipeline Source
Click the ellipsis button on the right-hand corner and then click **Go To Repository** to open a new tab in your web browser for the GitRepo pipeline source where the pipeline config is stored.
#### Delete a Pipeline
Since a pipeline is associated with its respective pipeline source, to delete a pipeline, delete the pipeline source. For more information, see  Deleing a Pipeline Source.
#### View all Runs
The **Runs** section of a pipeline (also called Runs view) lists every run of the selected pipeline.  
Property
Description  
Status
The completion status of the run.  
Run Number
The sequential number of the run in the history.  
Duration
The total execution time of the run.  
Queued
This shows the duration from the time the run was triggered to the time the first step started to execute.  
Triggered
Date and time that the run commenced execution.
  * For runs that were created due to a source control event such as, commit/pr/tag/release, the user who triggered the run is shown.
  * For manual triggers, the user who manually triggered the run is shown.

  
Context
Shows whether the run was triggered automatically (by a commit) or manually.  
##### Filter Runs
Select the filter in the dropdown box to list only those runs in the history whose outcome matches a particular status. The filter defaults to ""All"" to display all runs in the history.  
In addition, after clicking a run, click the filter icon to filter steps based on status.  
##### View Steps
Click one of the runs to view more details about it. The table lists all the steps for the run.  
##### View Resources
The Resources tab lists all the input and output resources for a run. Depending on the resource, you can even view the commit information for the yaml file that is used in the step.  
##### View Pipeinfo
In the run view, click the ellipsis button on the right-hand corner and then click **Pipe Info.**
Click the **Pipeinfo** tab to view the `head.json` for the run. This JSON file provides a complete trace for your run. For more information, see Signed Pipelines.
### Note
This information is only available after the run reaches terminal state.  
##### View Run Logs
Click the row of any run to invoke the Pipeline Run Logs view for that run. In that view, you can browse the log of execution, test results, and other information for steps in the pipeline.
You can also invoke the Pipeline Run Logs view for the most current run by clicking a step in the pipeline diagram, then in the resulting information box for that step click **Logs**.
### Note
The following sections provide information about performing various actions on the My Pipelines page using the old UI. If you prefer to perform these actions using the new UI, see the sections above.
##### View Pipelines Runs Charts
Click the **Insights** button on the Runs view page to view the Pipelines runs charts, which show the behavior of the runs for the selected number of runs for a specific branch.
The following charts show how the runs performed:
  *  **Run Performance** : Shows the median build time for the runs categorized based on the first and last build.
  *  **Execution Frequency** : Shows the average runs.
  *  **Runs Status** : Shows the run status and the time taken for each run.
  *  **Step Execution Overview** : Uses Gantt charts to provide a visual representation of the execution of steps for a selected run. Use the drop-down to select the run for which you would like to see the chart.
  *  **Slow Steps** : This visual representation shows the top 5 or top 10 slowest steps on an average from the selected number of runs.
### Note
You can access up to the last 200 runs by default. To access the last 1000 runs, you must upgrade to Enterprise+ and install Insights 1.13.4 or higher.

  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
210,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The **Pipeline Run Logs** view of Pipelines provides the facilities to view the log of execution, test results, and other information for steps in the pipeline.
The logs can be displayed for any step within any recorded or executing run of a pipeline. It will display the details of that step's execution as it was performed in that run of the pipeline.
[/INST]### reqsealer-eventMetric.log
For detailed information about steps and runs, you can also view the `reqsealer-eventMetric.log` file: `/pipelines/controlplane/logs/reqsealer-eventMetric.log`.
 **Example**
[YAML]
    {""timestamp"":""2021-12-16T06:06:43.897Z"",""domain"":""step"",""pipelineName"":""test_pipeline1"",""triggeredBy"":""admin"",""branchName"":""master"",""stepName"":""step1"",""runNumber"":2,""status"":""error"",""durationMillis"":null}
    {""timestamp"":""2021-12-16T06:13:37.332Z"",""domain"":""step"",""pipelineName"":""test_pipeline3"",""triggeredBy"":""admin"",""branchName"":""master"",""stepName"":""step3"",""runNumber"":3,""status"":""success"",""durationMillis"":2329}
    {""timestamp"":""2021-12-16T06:13:37.332Z"",""domain"":""run"",""pipelineName"":""test_pipeline3"",""triggeredBy"":""admin"",""branchName"":""master"",""runNumber"":3,""status"":""success"",""durationMillis"":2329} 
    {""timestamp"":""2021-12-16T08:32:57.063Z"",""domain"":""step"",""pipelineName"":""test_pipeline2"",""triggeredBy"":""admin"",""branchName"":""master"",""stepName"":""step2"",""runNumber"":2,""status"":""cancelled"",""durationMillis"":5639}
    {""timestamp"":""2021-12-16T08:32:57.063Z"",""domain"":""run"",""pipelineName"":""test_pipeline2"",""triggeredBy"":""admin"",""branchName"":""master"",""runNumber"":2,""status"":""cancelled"",""durationMillis"":5639}
[/YAML]
#### View Logs
### Note
This section provides information about viewing logs using the new UI. To perform the same steps using the old UI, see the expandable section below.
To view the logs for a run:
  * In the **List** view, click the relevant run.
or
  * In the **Graph** view, click the relevant step and click **Logs**.

  
View Logs - Using Old UI
### Note
This section provides information about viewing logs using the old UI. To perform the same steps using the new UI, see the section above.
The Pipeline Run Logs view can be invoked from the Runs portion of the Pipeline History view, by clicking the row of any run in the list
It can also be invoked through the pipeline diagram, by clicking on a step and then clicking ""View Logs"".  
Either procedure will result in the Pipeline Log view for the selected run:  
You can view log data for each of the steps in the pipeline. You can select the step by either:
  * Clicking on the step in the pipeline diagram
  * Selecting the step in the dropdown list


##### Logs
The **Logs** tab presents the record of what was executed for each portion of the step, and the system response. The log shows the command line directives issued, with their outputs.
Each section in the log represents a section of the step, and you may expand each section to view the command line output that was recorded. In this way, you can see the specific success or error messages that were returned for each issued command.
Use the **Timestamp** toggle to view or hide timestamps for the logs. Use the **Internal Logs** toggle to view or hide detailed, Pipelines-generated logs.
##### Tests
The **Tests** tab displays, when available, any test report information that was provided for the step through the save_tests utility function. The **Summary** column displays the **Passed** , **Skipped** , **Failed** , and **Errors** status for each test. Click one of the statuses to filter by that status.
##### Step Info
The **Step Info** tab provides additional information about the step, including the run state variables, as it is executed in the run.  
##### Resources
The **Resources** tab displays information about the resources used in the step, and their status at the time the step was executed in the run.  
#### Matrix Steplet Logs
When a Matrix step is selected, a disaggregated list of the individual steplets are presented.  
Property
Description  
Steplet Number
The sequential number of the steplet in the Matrix step.  
Status
The success or failure of the steplet.  
Started At
Date and time the steplet began execution.  
Duration
The duration of the run of the steplet.  
Matrix Combination
The values of the set of matrix environment variables used by the steplet, and the runtime image the steplet was executed in.  
Tests
The test reports of the steplet, if any.  
Coverage
The coverage reports of the steplet, if any.  
Click on a row to view the logs for that steplet.  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
211,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines enables you to trigger all your pipelines manually. However, to create an efficient CI/CD process, you should configure your pipelines to be automatically triggered based on certain events. This page describes the methods and procedures for triggering an automatic or manual run of a pipeline.
When creating an automated pipeline, you should define your steps so that they execute in an interdependent sequence. This means that each step is configured so that its execution is triggered by the successful completion of a prior, prerequisite step (or steps). In this way, step 1's completion triggers the execution of step 2, completion of step 2 triggers execution of step 3, and so on until all steps in the pipeline are executed.
[/INST]#### Trigger a Run Automatically
In most cases, you will want your pipeline to initiate execution automatically as the result of some external event that causes a resource to change. This is configured by defining a step's `inputResources.`
For example, you are likely to want to trigger a run whenever there is a new commit into a source code repository.
 **To set your source code repo as the pipeline trigger:**
  1. If you haven't already, add a GitHub integration (or other source control system type) to JFrog Pipelines.
  2. In your `resources` definitions of your pipeline config, define a gitrepo resource type for the project repository, with a descriptive name (for example, ""myprojectrepo"").
  3. In the definition of the first step of your pipeline, specify the `gitrepo` resource in `inputResources`. For extra clarity, you can also specify `trigger` as `true` (even though that is the default). For example:
`inputResources: - name:` `myprojectrepo trigger: true`


When triggering runs through a GitRepo resource, adding `[skipRun]` to a commit message won't trigger anything when that commit is pushed to the source provider.
When the completed pipeline is viewed, the`gitrepo`resource is shown as a triggering input into the step.  
Other example events might be a change to an Image resource, or receipt through a Webhook.
#### Trigger a Run Manually
You can trigger a run of any step through the pipeline visualization. To manually run the entire pipeline, trigger the first step in the workflow.
##### With default configuration
 **To manually trigger a step:**
  1. Click the step you want to trigger.
This displays the information box for that step.
  2. In the step's information box, click the ""Trigger this step"" button.  


The step executes, and its successful completion triggers the subsequent steps of the pipeline. The record of the run is logged in the pipeline history.
##### With custom configuration
You can trigger a run of any step and provide a set of custom settings to test a specific set of conditions.
 **To manually trigger a step with custom settings:**
  1. Click the step you want to execute.
This displays the information box for that step.
  2. In the step's information box, click the ""Trigger this step with custom configuration"" button.  
  3. In the resulting **Run with custom configuration** dialog, you can override the configuration settings, then click **Confirm**.
### Note
For more information, see the section Environment Variables Configuration.  
Old UI  


The step executes with the custom settings, and its successful completion triggers the subsequent steps of the pipeline. The record of the run is logged in the pipeline history.
#### View Step Execution
You can view the shell commands issued as the step is executing.
 **To view real-time execution of a step:**
  1. Trigger the step you want to execute.
  2. While the step executes, click the step.
This displays the information box for that step.
  3. In the information box, click **Logs**.  
Old UI  
  4. This displays the step log view Pipeline Run Logs view, where you can see the shell commands issued by the step as they are executed.  
Old UI  


#### View Step History
There could be times when you want to view a step's execution history, to view the previous time a step was triggered or for debugging purposes.
To view a step's history:
  1. Click the step.
  2. Click **Step History**.
This displays the Step History window, which shows various details about the step.
  3. Use the filter option to filter the step by status.


#### Cancel Runs and Steps
When a run is in progress, you may want to cancel a step or the run to either update the pipeline or free up resources.
You can cancel:
  * A specific active step
  * A specific active run
  * All active runs


### Note
Only runs in non-terminal status (Processing, Cancelling, and so on) or waiting in queue to be executed can be canceled. Runs in terminal status (Success, Error, and Failure) cannot be canceled.
##### Cancel a Specific Step
You can cancel execution of a run in progress by cancelling execution of a step. When a step is canceled, all subsequent dependent steps of the pipeline are skipped. The record of the run is logged in the pipeline history.
To cancel a step:
  1. While the step executes, click the step.
This displays the information box for that step.
  2. In the information box, click the **Cancel This Step** button.  


or
  1. While the step executes, click the active run and hover over the step you want to cancel.
  2. Click the Cancel icon that appears on the right-hand side.
  3. Click **Confirm**.  


##### Cancel a Specific Run
To cancel a specific run:
In the **Pipelines** tab:
  1. Click the Pipelines tab and click the relevant pipeline.
  2. Click the Cancel button on the right-hand side.
  3. Click **Confirm**.  
Old UI
    1. Click the Pipelines tab and click the relevant pipeline.
    2. Hover over the run you want to cancel.
    3. Click the Cancel icon that appears on the right-hand side.
    4. Click **Confirm**.  
or
    1. Click the run you want to cancel.
    2. Click the vertical ellipsis icon and click **Cancel run**.
    3. Click **Confirm**.  


or
In the **Active Runs** tab:
  1. Click the Active Runs tab and click the relevant pipeline.
  2. Hover over the run you want to cancel.
  3. Click the Cancel icon that appears on the right-hand side.
  4. Click **Confirm**.  


##### Cancel Multiple Runs
To cancel multiple runs:
  1. In the **Active Runs** tab, click the check box for the runs you want to cancel.
Or click the check box on top to select all the active runs.
  2. Click the Cancel icon on top.
  3. Click **Confirm**.  



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
212,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Use these tutorials to learn the best practices for performing many common procedures with Pipelines DSL.
[/INST]### In This Section
This section provides tutorials on specific tasks and best practices when creating pipelines.
  * Breaking Your Pipelines into Steps
  * Triggering Pipelines and Steps
  * Creating Stateful Pipelines
  * Inserting Secrets in Pipelines
  * Running multiple steps on the same build node
  * Sending Notifications from Pipelines
  * Creating Jira Issues From Pipelines
  * Sending Build Status to Source Control
  * Recording Unit Test Results
  * Deploying to Kubernetes in Pipelines
  * Setting Up Maven Builds for Pipelines
  * Caching Step Runtimes
  * Creating Multibranch Pipelines
  * Using Jenkins With Pipelines
  * Using the Matrix Step
  * Setting Retention Policy
  * Signed Pipelines
  * Setting Step Timeouts
  * Speeding up Pipeline Runs
  * Triggering a Run with Custom Parameters
  * Conditional Workflows
  * Approval Gates
  * Pipeline of Pipelines
  * Embedded Pipelines
  * Monitoring Pipelines using New Relic APM
  * Managing Runtimes



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
213,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>One of the most important decisions you will need to make in designing your pipeline is how to map your DevOps processes to JFrog Pipelines concepts of steps and resources.
A **Step** is a component of a pipeline that executes commands to accomplish an activity, such as building a binary, pushing a binary to a repository, deploying a service, provisioning a VPC or cluster, etc.
The native steps of JFrog Pipelines help you to build your pipeline as a sequence of smaller actions. For example, you can use the native step DockerBuild to build a Docker image, then use DockerPush to push the resulting image to a Docker registry.
This is essential to Pipelines' ""building block"" approach to constructing workflows. So it is a vital best practice to ensure that the steps you create using the generic Bash step encapsulate a discrete action similar to a native step.
Some guidelines for determining the discreteness of a step are:
  * A step should perform one task, so that you can get discrete status for that task.
  * A step can be triggered independently to trigger a downstream pipeline, so step boundaries should be designed with that in mind.
  * Steps can be run in parallel to speed up pipeline execution, so if you have a large test suite, you can divide that into several steps and run in parallel.
  * Steps can be run on different sized nodes, so if you want some series of steps to run on a bigger or smaller node size, separate it out into a step.
  * A step should have a continuous workflow, with no stops in between. For example, if you need manual input for a step, it's best to separate that step into another step with an approval gate in between. This prevents your steps from blocking a build node waiting for input.
  * For branch-based pipelines, it's best to separate steps per branch (or branch type), so that you can easily identify status of each branch.


[/INST]#### Example Pipelines
Let's take a simple example of a typical pipeline. For every merge to the `master` branch of your GitRepo, you need to:
  1. Build a Docker image
  2. Push it to a Docker registry
  3. Deploy it to a Kubernetes development environment
  4. Run tests against the deployed app and send a notification of the result


 **Single step approach**
Technically, you can bundle all these steps into one `Bash` step. Your pipeline diagram will be a single step, triggered by a change to your `GitRepo` resource:  
The approach above has the following characteristics:
  * There is no discrete status. If this step fails, at a glance you cannot tell if build, deployment, or functional tests failed.
  * The entire pipeline runs as one unit, so there is no way to separate out parts for special treatment. For example, you cannot run the functional test suite on a node with a custom runtime with supporting components.


 **Multiple steps approach**
Let's look at an alternative approach to the same pipeline using a combination of native steps and `Bash` steps:  
As you can see, the pipeline has a much more discrete structure and each step can send its status notifications.
You can also run the ""run tests"" step on a node with a custom runtime, or even split it up into parallel steps if needed.  
This is the recommended approach to define steps that are sufficiently discrete to be able to perform interim actions such as identify status, send notifications, and evaluate results.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
214,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>To automate your pipelines, you will want to set them up to run whenever some event occurs. Those events might be:
  * A commit to a source code repository
  * A source code repository branch or pull request
  * Some other external event
  * At some regular time interval


Steps automatically execute when they are triggered by some change event in a resource. Your pipeline should be constructed so that when each step makes some change to a resource, that change triggers execution of the next step. In this way, all the steps of your pipeline will execute in sequence.
To trigger a step for execution:
  1. Specify the triggering event(s) in a resource.
  2. Specify that resource as an `inputresource` of a step.


The specified change in that resource will then automatically trigger the step to run. When that step is the first step in the pipeline, it will initiate the entire run of that pipeline.
[/INST]#### Triggering On a Git Repository Change
A Git source code repository is represented by a GitRepo resource. The events that cause a GitRepo to trigger can be specified in the `buildOn` group of tags.
The most commonly used event for triggering a run of a pipeline is any `commit` to the Git repository. For this reason, the value of this tag defaults to **true**. So if that is the only event you want to trigger on, then you do not need to specify `buildOn` tags at all in the GitRepo resource.
Your GitRepo can be set to trigger on any of these `buildOn` events:
Tag
Default
Event  
`branchDelete`
false
An existing branch is created  
`branchDelete`
false
An existing branch is deleted  
`commit`
true
A new commit to the Git repo
Note that when this is set as `false`, the resource is not updated and the resource version is not created.  
`pullRequestCreate`
false
A new pull request made to the Git repo  
`pullRequestClose`
false
A pull request to the Git repo is closed  
`releaseCreate`
false
A new release request made to the Git repo  
`tagCreate`
false
Creation of a new tag ref in the Git repo  
Click here for example YAML...
This example causes a GitRepo to trigger a run of the pipeline on either a pull request or a release request. Since commit is true by default, we must explicitly set it to false.
[YAML]
    resources:
      - name: my_source_repo
        type: GitRepo
        configuration:
          gitProvider: myGitHub
          path: mycompany/myrepository
          branches:
            include: master
          buildOn:
            commit:                false
            pullRequestCreate:     true
            releaseRequestCreate : true
    
    pipelines:      
        - name: pipeline_gitrepo_trigger
          steps:
          - name: gitrepo_trigger_step_1
            type: Bash
            configuration:
              inputResources:
                - name: my_source_repo
            execution:
              onExecute:
                - printenv                                  # A benign operation for the demo
[/YAML]
##### Skipping a Git Repository Commit
If a specific commit should not trigger any runs for a GitRepo resource, include the text `[skipRun]` anywhere in the commit message and no runs will be triggered by that commit.
For example, to commit a change from the GitHub command line:
[YAML]
    $ git commit -m ""[skipRun] Fixed spelling errors in comments""
    # Commits the tracked changes but does not trigger Pipelines.
    
[/YAML]
The GitRepo resource will still be updated so that it will be current to the latest version of the repository for any Pipelines step that references that GitRepo resource.
The `[skipRun] `directive suppresses triggering only for a `commit` event. The GitRepo resource will still trigger on other `buildOn` events (such as a pull request) if those events are set to `true`.
##### Cancelling Previous Runs On a Git Repository Change
By default, when a GitRepo `buildOn` event (e.g., a commit) triggers a run for a pipeline that is already queued or is in-progress from a prior trigger, Pipelines will queue a new run of that pipeline. The new run executes after the prior run completes.
A GitRepo resource can be optionally configured to automatically cancel any queued or in-progress run that was triggered by the same GitRepo resource.
Your GitRepo can be set to cancel on any of these`cancelPendingRunsOn`events:
Tag
Default
Event  
`newCommit`
false
commit: previous runs for the same branch are cancelled
tag: previous runs for the same tag name are cancelled
release: previous runs for the same release name are cancelled  
`pullRequestUpdate`
false
previous runs for the same pull request number are cancelled  
Runs will only be cancelled for change events of the same type. For example, a commit will only cancel runs triggered by previous commits to the same branch.
Click here for example YAML...
This example causes a GitRepo to cancel previous runs for pull requests when a new run is triggered for the same pull request. Runs are not triggered for commits.
[YAML]
    resources:
      - name: my_source_repo
        type: GitRepo
        configuration:
          gitProvider: myGitHub
          path: mycompany/myrepository
          branches:
            include: master
          buildOn:
            commit: false
            pullRequestCreate: true
          cancelPendingRunsOn:
            pullRequestUpdate: true
    pipelines:      
      - name: pipeline_gitrepo_trigger
        steps:
          - name: gitrepo_trigger_step_1
            type: Bash
            configuration:
              inputResources:
                - name: my_source_repo
            execution:
              onExecute:
                - printenv                                  # A benign operation for the demo
[/YAML]
#### Triggering On a Webhook
You may trigger execution on receipt of a webhook from an external source.
Create an IncomingWebhook resource that is configured to use an Incoming Webhook Integration, then include it in the `inputresources` block of the step to trigger. Any payload from the webhook will be available in the resource's environment variable `$res_<resource_name>_payload,` which can be written to a file. You can then use the read_json utility function to retrieve individual elements from the JSON payload into environment variables.
Click here for example YAML...
[YAML]
    resources:
      - name: incoming_basic_hook
        type: IncomingWebhook
        configuration:
          webhookName: in_hook_basic
    
    pipelines:
      - name: incoming_webhook_demo
        steps:
          - name: start_by_hook
            type: Bash
            configuration:
              inputResources:
                - name: incoming_basic_hook
            execution:
              onExecute:
                - echo ""job triggered by resource-> $step_triggered_by_resource_name""
                - echo ""$res_incoming_basic_hook_payload"" | jq '.' > payload.json
                - echo ""$(read_json payload.json my.nested.object)""
    
    
[/YAML]
#### Triggering On a Time Schedule
A common practice for many organizations is to perform regular nightly builds during off-peak usage hours. For example, you may wish to execute the run of pipeline every night at 3:00am.
To trigger at a specific time or at regular time intervals, use the CronTrigger resource to specify the time or interval. In this resource you can specify a cron expression string. When you specify the `CronTrigger` resource as an `inputresource` to the first step in a pipeline, the pipeline will automatically commence execution at that time or interval.
Click here for example YAML...
This example demonstrates use of a CronTrigger resource to trigger the execution of a single-step pipeline every 5 minutes.
[YAML]
    resources:
      - name: cron_trigger
        type: CronTrigger
        configuration:
          interval: '*/5 * * * *'     # Every 5 minutes
    
    pipelines:      
      - name: pipeline_scheduled_triggers
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: cron_trigger
            execution:
              onExecute:
                - printenv
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
215,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The execution of each step in a pipeline may be performed on different nodes. For this reason, you are not guaranteed that changes to the environment that are made within a step will persist to subsequent steps.
Stateful pipelines remember information generated by steps and make it available to dependent steps or successive runs of the same pipeline. This is a crucial component in order to achieve end-to-end Continuous Delivery.
Some example use-cases are the following:
  * A step creates information about the commitSha and image/file version that was built, which is then consumed by another step to deploy the correct version into the test environment.
  * A step creates a VPC for the Staging environment. It stores information like VPC info, subnets, security groups, etc. which is required by another step that deploys to the Staging environment.
  * You have a provisioning step that uses Terraform to create the Test environment. At a later stage in your pipeline, you have a deprovisioning step that destroys the test environment. Both these steps need to read and update the Terraform state file so they are aware of the current state of the Test environment.


[/INST]#### Types of State
JFrog Pipelines supports three types of state:
  * Run State
  * Pipeline State
  * Resource State


Each of these states is characterized by the scope of information persistence.
##### Run State
A pipeline's **run state** is persistent only between steps within the same run. Information stored in one step is available to subsequent steps in the run of that pipeline. After the run is complete, state may be downloaded when viewing the steps in that run but will not be available in later runs or other pipelines.
### Note
To preserve state across steps, use the  utility functions for run state management.
Pipelines supports two types of run state information that can be preserved between steps.
###### Key-Value Pairs
Using the `add_run_variables `utility function, you can store a key-value pair to the run state. That key-value pair will automatically be available to all subsequent steps in the run as an environment variable. It will not be available to steps that do not have the step that added the variable as an input, either directly or through other steps or resources.  
Click here for example YAML...
[YAML]
     pipelines:
      - name: example_run_state_pipeline
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: myAppRepo            # Triggering resource
            execution:
              onExecute:
                - add_run_variables first_stepid=$step_id
                - add_run_variables ${first_stepid}_buildNumber=${run_number}
    
          . . . 
    
          - name: step_5
            type: Bash
            configuration:
              inputSteps:
                - name: step_4
            execution:
              onExecute:
                - echo ""Hello world""
                - echo $first_stepid
                - echo ${first_stepid}_buildNumber
[/YAML]
###### Files
Using the `add_run_files` utility function, a step can store a file in the run state. Any subsequent step can then use the `restore_run_files` function to retrieve the file from the run state. Files are available to steps in the same run whether or not the step that added the files is an input to the later step. Run state may be downloaded for an individual step, consisting of the files either uploaded or downloaded by that step.  
Click here for example YAML...
[YAML]
     pipelines:
      - name: example_run_file_pipeline
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: myAppRepo            # Triggering resource
            execution:
              onExecute:
                echo ""Hello world""
              onComplete:
                - add_run_files myfile cachefile.txt        # save the variable 'myfile' as 'cachefile.txt'
    
          . . . 
    
          - name: step_5
            type: Bash
            configuration:
              inputSteps:
                - name: step_4
            execution:
              onStart:
                - restore_run_files cachefile.txt myfile    # save the contents of 'cachefile.txt' to 'myfile'
              onExecute:
                echo ""Hello world""
[/YAML]
### Download Run State
To download run state from the UI for debugging purposes, set the environment variable `JFROG_PIPELINES_RUN_STATE_DEBUG=true` in your step's or pipeline's configuration section. For storage efficiency, we recommend removing this configuration option after you are done debugging.
[YAML]
     pipelines:
      - name: example_run_state_pipeline
        configuration:
          environmentVariables:
            readOnly:
              JFROG_PIPELINES_RUN_STATE_DEBUG: true       
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: myAppRepo            # Triggering resource         
            execution:
              onExecute:
                - add_run_variables first_stepid=$step_id
                - add_run_variables ${first_stepid}_buildNumber=${run_number}
              onComplete:
                - add_run_files myfile cachefile.txt        # save the variable 'myfile' as 'cachefile.txt'
[/YAML]
##### Pipeline State
A **pipeline state** is persistent for all runs of the same pipeline. Information stored by a step during a pipeline's run is available to subsequent runs of that pipeline.
### Note
To preserve state across pipelines, you may use the utility functions for pipeline state management.
Pipelines supports two types of run state information that can be preserved between steps.
###### Key-Value Pairs
Using the `add_pipeline_variables `utility function, you can store a key-value pair to the pipeline state. That key-value pair will automatically be available to all subsequent runs as an environment variable.  
Click here for example YAML...
[YAML]
     pipelines:
      - name: example_pipeline_state_pipeline
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: myAppRepo            # Triggering resource
            execution:
              onExecute:
                - echo ""Hello world""
                - echo $previous_buildNumber
          . . . 
    
          - name: final_step
            type: Bash
            configuration:
              inputSteps:
                - name: prior_step
            execution:
              onExecute:
                # Preserve the build number for the next run of the pipeline
                - add_pipeline_variables $previous_buildNumber=${run_number}
[/YAML]
###### Files
Using the `add_pipeline_files` utility function, a step can store a file to the pipeline state. Any step can then use the `restore_pipeline_files` function to retrieve the file from the pipeline state.  
Click here for example YAML...
[YAML]
     pipelines:
      - name: example_pipeline_file_pipeline
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: myAppRepo            # Triggering resource
            execution:
              onStart:
                # Restore the file from the previous run of the pipeline
                - restore_pipeline_files myfile cachefile.txt    # save the contents of 'cachefile.txt' to 'myfile'
              onExecute:
                echo ""Hello world""
    
          . . . 
    
          - name: final_step
            type: Bash
            configuration:
              inputSteps:
                - name: prior_step
            execution:
              onExecute:
                echo ""Hello world""
              onComplete:
                # Preserve file for the next run of the pipeline
                - add_pipeline_files myfile cachefile.txt    # save the variable 'myfile' as 'cachefile.txt'
[/YAML]
##### Resource-based State
Using the`write_output`utility function, key-values can be stored as a property in any output resource. Every step that has the resource as an input can access the key-value information in its scripts as an environment variable.  
The environment variable for the value is of the form `res_<resource name>_<key name>`.
Resource-based state information is persistent across pipelines, so it can be used as a mechanism for passing information from one pipeline to the next.
Click here for example YAML...
[YAML]
    pipelines:
      - name: example_resource_state_pipeline
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: myAppRepo           # Triggering resource
              outputResources:
                - name: myImage             # Image resource
            execution:
              onExecute:
                - echo $res_myAppRepo_commitSha
                - write_output myImage ""imageTag=master"" ""sha=$res_myAppRepo_commitSha"" ""description=\""hello world\""""
    
          - name: step_2
            type: Bash
            configuration:
              inputResources:
                - name: myImage
            execution:
              onExecute:
                - echo ""Hello world""
                - echo $res_myImage_imageTag
                - echo $res_myImage_sha            
                - echo $res_myImage_description
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
216,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>If your pipeline needs to access sensitive information such as passwords, tokens, or keys, you need a way to securely insert these secrets at runtime, without including them directly in your pipeline config. You also need a way to manage these secrets and update them without needing to update automation scripts.
In JFrog Pipelines, secrets are stored as properties in Integrations, where all of the integration's properties are stored in encrypted vault storage. Pipelines provides two ways to use integrations to insert secrets into your steps:
  * Standard integrations
  * Generic integration


[/INST]#### Using Standard Integrations
Pipelines provides a set of standard integrations that help link your resources and worfklows to specific external services such as GitHub, Slack, or Artifactory. A Pipelines admin enters the access secrets for these services (such as tokens or login credentials) when configuring these integrations. These secrets are stored in the secure vault.
In general, you are not likely to need direct access to these integration's secrets. When you define a resource, you will specify the integration in the `integrations` block and your steps will connect to the service through the resource. Or your steps will access the service through the integration directly, rather than through a resource. In this way, secrets are securely maintained independently of the pipeline script.
However, if you need to directly access a secret from the integration, you may do so through the environment variable for that integration.
For example, if you have an AWS Keys Integration called myAWSKeys, you can reference the Access Key property through an environment variable:
[YAML]
    steps:
       - name: spoil_secret
         type: Bash
         configuration:
           integrations:
           - name: myAWSKeys
          execution:
            onExecute:
              - printenv int_myAWSKeys_accessKeyId
    
[/YAML]
#### Using the Generic Integration
A Generic Integration is an integration type that is not for linking to a specific service, but exists exclusively to securely store key-value pairs of information. This is appropriate for storing secrets such as tokens or passwords for retrieval by a step.
When you add this integration to a step, all key-value pairs are made available as environment variables in the form:
`int_<integration-name>_<key>`
For example, a Pipelines admin can define a Generic integration with the following entries:
  *  **Name** : `myCredentials`
  *  **Custom Environment Variables** :
    * Key: `username`
    * Value: `janedoe`
    * Key: `password`
    * Value: `nAm30fMyp3t`


When `myCredentials` is specified in a step's `integrations` block, the key-value pairs stored there can then accessed in the step as environment variables:
[YAML]
    pipelines:
      - name: generic_integration_example
        steps:
        - name: step_1
          type: Bash
          configuration:
            integrations:
              - name: myCredentials
          execution:
            onExecute:
              - printenv int_myCredentials_username
              - printenv int_myCredentials_password
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
217,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>When steps in a pipeline must execute on the same build node, they can be assigned to the same **affinity group**. This may be necessary when a step requires access to data produced by a prior step that remains in the build node's runtime environment.
Because Pipelines distributes the execution of steps across nodes in a node pool, no step can necessarily rely on files or artifacts produced by a prior step being available, since the earlier step may have executed in a different build node. While the preferred practice is often to create a stateful pipeline, you may need to bind steps to the same build node instead through an affinity group. The path of a directory available to all steps in the group is provided as an environment variable named `shared_workspace` and may be used to share files between steps.
Some native steps (for example, DockerBuild and DockerPush) may require that they be bound together as an affinity group.
Of course, if the node pool used by your pipeline only has a single node, then your steps may still succeed without binding them into an affinity group (since all steps execute on the same single node). However, since an administrator can increase the number of nodes in any node pool at any time, you should still use affinity groups to bind steps that need them, to ensure that they continue to execute successfully when conditions change.
[/INST]### Note
Steps in the same affinity group cannot run in parallel.
### Note
When defining steps in affinity groups, ensure that there are no intermediate steps that belong to a different affinity group. Doing this will cause the pipeline to fail.
For example, when defining affinity groups for three steps, do one of the following: 
  * Define Step A, Step B, and Step C in the same affinity group
  * Define Step A and Step B in the same affinity group, with Step C in a different affinity group
  * Define Step A in an affinity group, with Step B and Step C in a different affinity group


#### Usage of affinityGroup Tag
Steps are bound to an affinity group through its `configuration` section, using the `affinityGroup` tag.
To specify an affinity group for an individual step:
[YAML]
        steps:
          - name: <name>
            type: <step_type>
            configuration:
              affinityGroup: <any string>
[/YAML]
You can specify any string as the name of the affinity group, but you must use the same string for all `affinityGroup` tags in the steps you wish to bind together in the group.
Alternatively, if all steps in the pipeline are to be in the same affinity group, `affinityGroup` may be specified in the Pipelines configuration section:
[YAML]
        pipelines:
          - name: <name>
            configuration:
              affinityGroup: <any string>
            steps:
              - <step>
[/YAML]
#### Example
The following simplified pipeline creates two steps bound to the same `affinityGroup` called `together`:
 **pipelines.yml**
[YAML]
    pipelines:
      - name: pipeline_affinityGroup_example
        steps:
          - name: ag_step_1
            type: Bash
            configuration:
              affinityGroup: together
            execution:
              onExecute:
                - echo ""Running "" $pipeline_name ""|"" $step_name > $shared_workspace/myoutput.txt
                - cat $shared_workspace/myoutput.txt
    
          - name: ag_step_2
            type: Bash
            configuration:
              affinityGroup: together
              inputSteps:
                 - name: ag_step_1
            execution:
              onExecute:
                - cat $shared_workspace/myoutput.txt
[/YAML]
The step `ag_step_1` creates a text file in the build node where it executes, using the `shared_workspace` variable to ensure that the file will be available to both steps. The subsequent step, `ag_step_2` tries to read the file.
By binding `ag_step_1` and `ag_step_2` into the same affinity group, the file that `ag_step_1` created in the build node's file system is available to `ag_step_2` because it is guaranteed to be running in the same node. Without being bound to an affinity group, `ag_step_2` may fail because it might execute in a node where the file does not exist.
When this pipeline is loaded in the JFrog Platform, the interactive diagram shown in the Pipeline History indicates that these steps are bound into the same affinity group:Pipeline History
When the pipeline is run, the Pipeline Run Logs show that `ag_step_2` was able to successfully find the file in the node's filesystem that was created by `ag_step_1`.  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
218,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>You can send notifications through an external service such as Slack or email when a step starts, succeeds, fails or completes. This can be configured by using the utility function ` send_notification` in the `onStart`, `onSuccess`, `onFailure`, `onComplete` sections of your step definition.
JFrog Pipelines connects your notification services through Integrations for those services. This provides Pipelines with the credentials needed to send notifications on your behalf, and keeps thissensitive information abstracted from your Pipeline source files and hidden from other users.
This page explains how to send a Slack notification and an email notification. Similarly, you can use the utility function `send_notification` to send notifications through other integrations, such as Airbrake, Jira, or NewRelic.
[/INST]#### Sending Notifications to a Slack Channel or User
You can send any text to a Slack channel or user, including color settings and an identifying icon. You can also send JSON payloads for the Slack Web API.
  1. ##### Add a Slack integration
Follow instructions to create a Slack integration. You will need to set up permissions on the Slack account to enable incoming webhooks and create an incoming webhook URL to provide this integration. Write down the friendly name that you gave your integration.
  2. ##### Update YAML to send notifications
You can now update your Pipeline source file.
    * Specify an `integrations`section with the friendly name of your Slack integration.
    * In the `execution`section, use the`send_notification`utility function as shown below.


 **Sending notifications**
[YAML]
    pipelines:
      - name: pipe1
        steps:
          - name: test_step_1
            type: Bash
            configuration:
              integrations:
                            - name: notifySlack       # replace with friendly name of your Slack integration
                    execution:
                      onStart:
                        - send_notification notifySlack --text ""starting test_step_1"" --recipient ""#myChannel"" 
              onExecute:
                - #test commands
                - send_notification notifySlack --text ""in onexecute, past test commands"" --recipient ""@user5""
                      onSuccess:
                        - # can use send_notification command here
                      onFailure:
                        - # can use send_notification command here
                      onComplete:
                        - # can use send_notification command here
[/YAML]
You can customize additional things like username, pretext, text, payload, and more as shown in the usage description for ` send_notification`.
#### Sending Notifications to Email
You can send text to an email address through SMTP.
  1. ##### Add an SMTP Credentials integration
Follow instructions to create an SMTP Credentials integration. You will need to provide email user credentials to send an email message.
  2. ##### Update YAML to send notifications
You can now update your Pipeline source file.
    * Specify an `integrations`section with the friendly name of your SMTP Credentials integration.
    * In the `execution`section, use the`send_notification`utility function as shown below.


 **Sending notifications**
[YAML]
    pipelines:
      - name: pipe1
        steps:
          - name: test_step_1
            type: Bash
            configuration:
              integrations:
                            - name: notifySMTP       # replace with friendly name of your SMTP Credentials integration
                    execution:
                      onSuccess:
                            - send_notification notifySMTP --recipients ""buildalerts@mycompany.com"" --subject ""Build Succeeded"" --body ""built docker image 112.32.101.173:8081/docker-local/demo:$PIPELINE_NAME.$RUN_NUMBER""
                      onFailure:
                            - send_notification notifySMTP --recipients ""buildalerts@mycompany.com"" --subject ""Build Failed"" --body ""FAILED: docker image 112.32.101.173:8081/docker-local/demo:$PIPELINE_NAME.$RUN_NUMBER""
                      onComplete:
                        - # can use send_notification command here
[/YAML]
You can also add attachments, such as log files, as described in the usage description for ` send_notification`.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
219,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines provides facilities to create a new issue (also called a ticket) for a project repository in Atlassian Jira Cloud or Server from an executing pipeline.
You may wish to create a new issue (ticket) in a Jira project repository when:
  * Your build fails to complete
  * Your pipeline runs automated tests and you need to create a new Jira issue when the test fails


No additional plugin or extension needs to be installed to use this facility in Pipelines. It makes use of a built-in utility function than can be invoked from any step in a pipeline.
[/INST]#### Adding a Jira Integration
To send messages to Jira, you must first add a Jira Integration to Pipelines. The Jira integration securely holds your access credentials in Pipelines' encrypted vault, so that this information is not viewable in the text of your Pipelines DSL file.
To add a Jira integration, follow the procedures in Managing Pipelines Integrations.
For example, to add a Jira integration called `myJira`, you might enter the following:  
The _User Name_ and _Token_ fields provide the necessary credentials to authenticate access from Pipelines:
  *  **User Name** : The user account represented by _User Name_ must have permissions to access the Jira project where issues are to be created.
  *  **Jira Cloud Token** : When integrating with Jira Cloud, you must provide the Jira API token for the Jira user account in the _Token_ field.
  *  **Jira Server Password** : When integrating with Jira Server, you must provide the user account password in the _Token_ field (Jira Server does not support API tokens).


#### Creating a Jira Issue
To create a new Jira issue (ticket) from step in a pipeline:
  1. Declare the Jira integration in the `integrations` section of the step.
  2. Use the send_notification utiliity function to notify the Jira integration to create an issue ticket.


You will typically send the notification to Jira in an `onFailure` portion of the step's `execution` block.
[YAML]
            execution:
              onExecute:
                ...
              onFailure:
                - send_notification myJira --project-id ""DEMO"" --type ""Bug"" --summary ""Build Failed: $pipeline_name:$run_number"" --description ""Step $step_name"" Failed""
    
[/YAML]
### Note
The project-id must specify a key for a project that has already been created in Jira. If Jira receives a request to create a ticket for a project that doesn't exist, Jira will ignore the request.
##### Using Environment Variables
If your pipeline will have multiple failure points to create a Jira issue, you might choose to define common values for options such as project-id and type in environment variables for those options. The `send_notification` utility function for a Jira integration will automatically use any of these environment variables when they are defined:
Environment Variable
Option
Description  
`NOTIFY_PROJECT_ID`
\--project-id
the project key of the project to associate the new issue with  
`NOTIFY_TYPE`
\--type
the issue type for the new issue (e.g., ""Bug"", ""Task"", etc.).  
`NOTIFY_SUMMARY`
\--summary
a string for the new issue's Summary field (it's title)  
`NOTIFY_DESCRIPTION`
\--description
a string for the new issue's Description field  
`NOTIFY_ATTACH_FILE`
\--attach-file
a path to a file to attach to the issue  
You can define any of these environment variables in the pipeline's `configuration` block using the `environmentVariables` tag, so that they will be available to all steps in the pipeline. You can also, if needed, specify `environmentVariables` in the step's `configuration` block to override their value for the span of that step's execution.
##### Example
The following example pipeline demonstrates the use of the `send_notification` utility function to create a Jira issue (ticket).
The example Pipelines DSL performs the following:
  * Sets environment variables for the project-id and type options
  * Performs a MvnBuild native step, which produces a log file.
  * On failure, uses the `send_notification` utility function to create the JIra ticket. The command line:
    * Relies on the environment variables for the project-id and type options
    * Attaches the log file produced by Maven to the Jira issue
    * Specifies the summary and description options using  standard environment variables


[YAML]
    resources:
      - name: my_repo
        type: GitRepo
        configuration:
          gitProvider: MyGithub
          path: myrepo/myproject
    
    pipelines:
      - name: jira_ticket_example
        configuration:
          environmentVariables:
            readOnly:
              NOTIFY_PROJECT_ID: ""DEMO""            # Jira project key for all tickets we create
              NOTIFY_TYPE: ""Bug""                   # Jira issue type for all tickets we create
        steps:
        ### with jira for log attachment
          - name: BuildSample
            type: MvnBuild
            configuration:
              sourceLocation: artifactory-maven-plugin-example
              configFileLocation: .
              configFileName: config
              mvnCommand: ""install -P release --log-file ${step_tmp_dir}/log.txt""
              inputResources:
                - name: my_repo
              integrations:
                - name: myArtifactory
                - name: myJira
            execution:
              onFailure:
                - send_notification myJira --attach-file ""$step_tmp_dir/log.txt"" --description ""Failure occured in $pipeline_name - $step_name"" --summary ""$step_name has failed"" 
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
220,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A pipeline can send information about its current build status ( _processing_ , _success_ , or _failure_ ) to a source repository for logging and display.
Most source control providers feature the ability to receive and log build status results from an external CI automation tool such as Pipelines. Source control users can then view those status logs through the source control's UI, or retrieve them through the source control manager's API.
Pipelines provides the update_commit_status utility function to send build status information to a GitRepo resource, associated with the sha of the GitRepo's latest commit.
You may wish to use this function to:
  * Identify whether a pull request can successfully build prior to merge
  * View the results of your build in your source provider UI
  * Make build results visible to those who do not have accounts on your organization's JFrog Platform Deployment, or do not have permissions to view the pipeline in Pipelines
  * Provide build status results to other automation tools through the source control API


[/INST]#### Source Control Build Status
An ability to log status information with your commits is a feature provided by many source control repository managers. Its availability and operation will differ according to your provider.
For example, the status checks feature of GitHub will mark each commit listed in the GitHub UI with the build status received, and the details of the status log can be viewed by clicking the mark. The build status log data can also be retrieved through the GitHub REST APIs  
Similarly, the build status feature in Bitbucket Server displays the build status for the commit in the Bitbucket UI, and makes that information available through REST resources.  
#### Sending Build Status from Pipelines
Where many CI automation servers require a plugin or custom integration to send build status to a source control repository, Pipelines provides a built-in utility function that can be used in any step.
To send build status from a pipeline step to a commit in a source repository, you must:
  1. Declare the GitRepo in the step's `inputresources` .
  2. Use the `update_commit_status` utility function in any of the step's `execution` blocks:`onStart`, `onExecute`, `onFailure`, or `onSuccess`.


The format of the `update_commit_status` function is:
[YAML]
        update_commit_status <gitRepo resource name> --status <status> --message <message> --context <context>
[/YAML]
You must specify the GitRepo resource in the `update_commit_status` function. The remaining parameters are all optional. By default, the function will infer the status from the `execution` block where it is invoked. Default message and context strings are constructed from the step and pipeline name.
For example, this step will update the commit's build status for each execution phase using the defaults:
[YAML]
    pipelines:
      - name: git_status_defaults
        steps:
          - name: status_update_defaults
            type: Bash
            configuration:
              inputResources:
                - name: myGitRepo
            execution:
              onStart:
                - update_commit_status myGitRepo                # Status: ""processing""
              onExecute:
                - update_commit_status myGitRepo                # Status: ""processing""
                - echo ""Hello World!""
              onFailure:
                - update_commit_status myGitRepo                # Status: ""failure""
              onSuccess:
                - update_commit_status myGitRepo                # Status: ""success""
[/YAML]
When the above example pipeline is run:
  * The receiving source repository will create a status log entry for the step.
  * Each call to `update_commit_status` will overwrite the status and message values of that log entry.


The resulting build status log for the commit can be viewed in the source repository's UI (on GitHub):  
#### Example Pipeline
The following example pipeline demonstrates sending build status messages to the source control repository that will create a status log entry for each executed step.  
The `resources` for the pipeline declares the GitRepo of the source control repository.
 **pipelines.resources.yml**
[YAML]
    resources:
      - name: myGitRepo
        type: GitRepo
        configuration:
          gitProvider: myGitHub
          path: myaccount/myproject
[/YAML]
The pipeline consists of two dummy steps, one that will always execute successfully and one that will always fail to execute. The `update_commit_status` function will send build status messages to the source repository for each step.
 **pipelines.steps.yml**
[YAML]
    pipelines:
      - name: git_status_test
        steps:
          ##
          ## Step 1: Execute a simple, always successful test
          ##
          - name: success_test
            type: Bash
            configuration:
              inputResources:
                - name: myGitRepo
            execution:
              onStart:
                - update_commit_status myGitRepo --message ""starting..."" --context ""$step_name""
              onExecute:
                - update_commit_status myGitRepo --message ""running..."" --context ""$step_name""
                - echo ""Hello World!""
              onFailure:
                - update_commit_status myGitRepo --message ""Failed!"" --context ""$step_name""
              onSuccess:
                - update_commit_status myGitRepo --message ""Succeeded :-)"" --context ""$step_name""
     
          ##
          ## Step 2: Execute a simple, always failing test
          ##
          - name: failure_test
            type: Bash
            configuration:
              inputResources:
                - name: myGitRepo
                  trigger: false
              inputSteps:
                - name: success_test
            execution:
              onStart:
                - update_commit_status myGitRepo --message ""starting..."" --context ""$step_name""
              onExecute:
                - update_commit_status myGitRepo --message ""running..."" --context ""$step_name""
                - cd fail               # No such directory -- guaranteed to fail
              onFailure:
                - update_commit_status myGitRepo --message ""Failed!"" --context ""$step_name""
              onSuccess:
                - update_commit_status myGitRepo --message ""Succeeded :-)"" --context ""$step_name""
[/YAML]
When viewed in the source repository's UI (on GitHub), the build status log for the commit shows both a successful and a failed execution of each step:  
### Tip
Since the context option is always set to the `$step_name` environment variable in the example, the receiving source control repository will create a build status log entry for each step.
If you wanted to log only a single build status for the entire pipeline, you might set the context option to the `$pipeline_name` environment variable instead. To log a build status for each run of the pipeline, you might combine it with the `$run_number` environment variable:
[YAML]
    update_commit_status myGitRepo --context ""$pipeline_name:$run_number""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
221,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines can record and display test result files that have been output by a unit test runner. Tests results data must be XML files in junit format for Pipelines to be able to display them.
When a pipeline has saved test results, the report can be viewed in the **Tests** tab for the step in the Pipeline Run Logs.  
[/INST]#### Saving Test Results
To record unit test results to the pipeline's run log:
  * Run unit tests in a pipeline step that produce a results file in junit format
  * Save the test results file using the `save_tests` utility function


[YAML]
    save_tests $res_myGitRepo_resourcePath/testreport.xml
[/YAML]
#### Example Pipeline
The following example pipeline demonstrates the process of generating unit test results in a Maven build and recording the results to the run log for later viewing in Pipelines.
The pipeline defines a GitRepo resource that contains the Java application, and a BuildInfo resource for the resulting build.
 **pipelines.resources.yml**
[YAML]
    resources:
      - name: java_app
        type: GitRepo
        configuration:
          gitProvider: MyGithub
          path: myproject/simple-maven-sample
          branches:
            include: master
    
      - name: my_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: myArtifactory
          buildName: simple-maven-sample
          buildNumber: 1
[/YAML]
The pipeline executes a Maven build using the MvnBuild native step. It requires that the Maven project being built has been configured to use the Maven Surefire Plugin to perform unit tests and produce a junit-compatible test result file.
In the `onComplete` action of the `execution` block, the step will save the test result file to the run log by calling the `save_tests` utility function.
 **pipelines.steps.yml**
[YAML]
    pipelines:
      - name: java_pipeline
        steps:
          - name: build_app
            type: MvnBuild
            configuration:
              sourceLocation: .
              mvnCommand: clean install -ntp
              configFileLocation: .
              configFileName: mvn-art-config
              autoPublishBuildInfo: true
              integrations:
                - name: myArtifactory
              inputResources:
                - name: java_app
              outputResources:
                - name: my_build_info
              runtime:
                type: image
                image:
                  auto:
                    language: java
                    versions:
                      - ""11""
            execution:
              onComplete:
                - save_tests $res_java_app_resourcePath/target/surefire-reports/
[/YAML]
After the pipeline has finished running, the test results for the `build_app` step can be viewed in the run log.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
222,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>This tutorial demonstrates an example pipeline that uses the HelmPublish and HelmDeploy native steps to deploy a single image from a Docker registry in Artifactory to a Kubernetes cluster using a Helm chart.
Helm is a package manager for Kubernetes applications. Through a Helm chart, you can describe the application structure and manage it using simple commands through the Helm client.
The native steps in Pipelines invoke the Helm client to perform the deployment tasks using the Helm chart.
The example pipeline requires the following integrations:
  * GitHub Integration or other source repository manager for the image source files and Helm chart
  * Docker Registry Integration to push the built Docker image to Artifactory
  * Artifactory Integration to push the Helm chart to a Helm repository
  * Kubernetes Integration to connect JFrog Pipelines platform to a self-hosted K8s cluster


[/INST]#### Creating the Helm Chart
To begin, we have created a very simple Helm chart, which we have stored in a GitRepo source repository:
 **Chart.yml**
[YAML]
    apiVersion: v1
    appVersion: ""1.0""
    description: A Helm chart for Kubernetes
    name: simplehelm
    version: 0.1.0
[/YAML]
We use a companion `values.yml` file to specify the details, including information that identifies which Docker image to deploy.
 **values.yml**
[YAML]
    replicaCount: 1
    image:
      repository: ${res_simplehelmDockerImage_imageName}
      tag: ${res_simplehelmDockerImage_imageTag}
      pullPolicy: IfNotPresent
      port: 80
    
    ....
[/YAML]
The `repository` and `tag` fields that identify the Docker image name and tag are set to environment variables. These reference the `imageName` and `imageTag` properties of the Image resource for the Docker image.
When our pipeline runs, the HelmDeploy step will automatically replace these environment variable reference with their values from the Image resource, so that the correct Docker image is deployed through Helm.
For more information on creating Helm charts, see the Helm Chart Template Guide.
#### Docker Image
The Helm deploy will pull an image from a Docker registry to orchestrate to a Kubernetes cluster.
You may wish to build the Docker image and push it to the registry as part of your pipeline. If so, you can use the DockerBuildand DockerPushnative steps.
If your image to deploy is already available in the Docker registry, then you can skip this procedure. However, you will still need an Imageresource that specifies the image in the registry.
Our`resources`definitions will specify:
  * `simplehelmRepo_docker` \- The GitRepo source repository where the Dockerfile is stored
  * `simplehelmDockerImage` \- An Image resource for the Docker image that will be built and pushed to the Docker registry in Artifactory


 **resources**
[YAML]
     - name: simplehelmRepo_docker
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myrepo/simplehelm
          files:
            include: ""Dockerfile|server.js|package.json""
    
      - name: simplehelmDockerImage
        type: Image
        configuration:
          registry: myDocker
          imageName: myproject/simplehelm
          imageTag: latest
[/YAML]
Using these resources, the pipeline will build the Docker image and push it to a registry in Artifactory.
 **steps**
[YAML]
          - name: build_image
            type: DockerBuild
            configuration:
              affinityGroup: buildAndPush
              integrations:
                - name: myDocker
              inputResources:
                - name: simplehelmRepo_docker
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: myproject/simplehelm
              dockerImageTag: $run_number
    
          - name: push_image
            type: DockerPush
            configuration:
              affinityGroup: buildAndPush
              integrations:
                - name: myDocker
              targetRepository: myproject/simplehelm
              inputSteps:
                - name: build_image
              outputResources:
                - name: simplehelmDockerImage
[/YAML]
#### Publishing the Helm Chart
The example pipeline uses the HelmPublish native step to publish the Helm chart and its companion files to a Helm repository in Artifactory.
Our `resources` definitions will specify:
  * `simplehelmRepo_chart` \- The GitRepo source repository where the Helm chart shown above is stored.
  * `simpleHelmChart` \- A HelmChart resource that represents the Helm chart published in Artifactory. It specifies the Artifactory repository, chart name, and version for publishing.


 **resources**
[YAML]
      - name: simplehelmRepo_chart
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myrepo/simplehelm
          files:
            include: ""simplehelm.*""
    
      - name: simplehelmChart
        type: HelmChart
        configuration:
          sourceArtifactory: art
          repository: simplecharts
          chart: simplehelm
          version: 0.0.0
[/YAML]
Using these resources, the HelmPublish native step will publish the Helm chart from the source repository to an Artifactory repository. (Note that this step as configured does not publish the build info.)
 **step**
[YAML]
          - name: publish_helm_chart
            type: HelmPublish
            configuration:
              inputResources:
                - name: simplehelmRepo_chart
              outputResources:
                - name: simplehelmChart
              chartPath: ./simplehelm
[/YAML]
The native step uses the `helm package` command to perform publication, so all chart files (`Chart.yml`, and `values.yml`) will be published as a unit.
#### Deploying With Helm
The example pipeline uses the HelmDeploy native step to deploy the image to Kubernetes.
The HelmDeploy native step will:
  1. Use the `simplehelmChart` resource to pull the Helm chart from Artifactory
  2. Replace all environment variables in `values.yml` with their values from the `simplehelmDockerImage` resource
  3. Perform the `helm upgrade` command to deploy the `simplehelmDockerImage` from the Docker repository to a Kubernetes cluster


 **step**
[YAML]
          - name: deploy_helm_chart
            type: HelmDeploy
            configuration:
              integrations:
                - name: tr_kubernetes
              inputResources:
                - name: simplehelmChart
                - name: simplehelmDockerImage
              releaseName: simplehelm
              flags: ""--namespace dev""
              valueFilePaths:
                - values.yaml
[/YAML]
Note that the Image resource is not required by HelmDeploy to be in its `inputresources`. The only resource HelmDeploy requires is a HelmChart.
We include `simplehelmDockerImage` so that its `imageName` and `imageTag` properties are available to the step as environment variables. This enables the HelmDeploy step to perform the replacements of those environment variables in the `values.yml` file of the Helm chart.
After the replacement, the `values.yml` used by Helm might appear as (if this was run 1):
 **values.yml (after replacement)**
[YAML]
    replicaCount: 1
    image:
      repository: myproject/simplehelm
      tag: 1
      pullPolicy: IfNotPresent
      port: 80
    
    ....
[/YAML]
### HelmDeploy from a Git Repository
We recommend pushing your Helm charts to Artifactory with traceable metadata,. However, the HelmDeploy step can use a Helm chart directly from a Git source repository instead.
To deploy using the Helm chart in the Git repository without pushing it to Artifactory:
  1. Skip the HelmPublish step
  2. In the Pipelines DSL for the `deploy_helm_chart` step, in the `inputresources` section replace the `simplehelmChart` HelmChart resource with the `simplehelmRepo_chart` GitRepo resource.


The operation will produce the same result, including the replacement of environment variables in `values.yml.`
#### Complete Example
For your reference, here is the complete Pipelines DSL for the example pipeline.
 **pipelines.resources.yml**
[YAML]
      - name: simplehelmRepo_docker
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myrepo/simplehelm
          files:
            include: ""Dockerfile|server.js|package.json""
    
      - name: simplehelmDockerImage
        type: Image
        configuration:
          registry: myDocker
          imageName: myproject/simplehelm
          imageTag: latest
    
      - name: simplehelmRepo_chart
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myrepo/simplehelm
          files:
            include: ""simplehelm.*""
    
      - name: simplehelmChart
        type: HelmChart
        configuration:
          sourceArtifactory: art
          repository: simplecharts
          chart: simplehelm
          version: 0.0.0
[/YAML]
 **pipelines.steps.yml**
[YAML]
    pipelines:
      - name: helm_full_flow
        steps:
          - name: build_image
            type: DockerBuild
            configuration:
              affinityGroup: buildAndPush
              integrations:
                - name: myDocker
              inputResources:
                - name: simplehelmRepo_docker
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: myproject/simplehelm
              dockerImageTag: $run_number
    
          - name: push_image
            type: DockerPush
            configuration:
              affinityGroup: buildAndPush
              integrations:
                - name: myDocker
              targetRepository: myproject/simplehelm
              inputSteps:
                - name: build_image
              outputResources:
                - name: simplehelmDockerImage
    
          - name: publish_helm_chart
            type: HelmPublish
            configuration:
              inputResources:
                - name: simplehelmRepo_chart
              outputResources:
                - name: simplehelmChart
              chartPath: ./simplehelm
    
          - name: deploy_helm_chart
            type: HelmDeploy
            configuration:
              integrations:
                - name: tr_kubernetes
              inputResources:
                - name: simplehelmChart
                - name: simplehelmDockerImage
              releaseName: simplehelm
              flags: ""--namespace dev""
              valueFilePaths:
                - values.yaml
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
223,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>If you are already using the Maven Artifactory Plugin, your Maven projects will need some additional configuration for use within a Pipeline. Note that Pipelines does not require this plugin and configuration may be simpler without it, but if your project is already using the plugin, then you can configure your pipeline to do the same.Ecosystem Integration
The plugin is used to automatically resolve dependencies, deploy artifacts, and publish builds when running a mvn build command. The procedures shown here enables the plugin to access Artifactory through the credentials stored in the Artifactory Integration that is used by your pipeline.
[/INST]#### Usage
First,  configure the Project Object Model`{pom.xml`) configuration file of your Maven project to use the Maven Artifactory Plugin.Maven Artifactory Plugin
The Maven Artifactory Plugin will need to know your Artifactory credentials to be able to publish your Maven builds to repositories. You can provide this information in the Maven project's `pom.xml` configuration file, in the _publisher_ block along with the names of the repositories where artifacts are to be published.
[YAML]
    <publisher>
      <contextUrl> .. </contextUrl>
      <username> .. </username>
      <password> .. </password>
      <repoKey> .. </repoKey>
      <snapshotRepoKey> .. </snapshotRepoKey>
    </publisher>
[/YAML]
To keep this credential information secure, your `pom.xml` should reference these values as system properties that you have defined in your Maven project's `settings.xml` configuration file.
For even greater security, you should declare those _properties_ in `settings.xml` using the environment variables provided by the Artifactory Integration to the step's execution environment. This also makes your Maven project adaptable to Pipelines – you can change which Artifactory instance is used in your pipeline without having to change your Maven project.
For example, this `settings.xml` file references the credentials and URL stored in the `MyArtifactory` integration:
 **settings.xml**
[YAML]
    <settings>
        <profiles>
            <profile>
                <id>artifactory-plugin-properties</id>
                <properties>
                    <username>${env.int_MyArtifactory_user}</username>
                    <password>${env.int_MyArtifactory_apikey}</password>
                    <artifactoryUrl>${env.int_MyArtifactory_url}</artifactoryUrl>
                </properties>
            </profile>
        </profiles>
    
        <activeProfiles>
            <activeProfile>artifactory-plugin-properties</activeProfile>
        </activeProfiles>
    </settings>
[/YAML]
Note that the `MyArtifactory` integration must have been specified in the step's `integrations` tag (required for the MvnBuild native step) for these environment variables to be available to the step's execution environment.
Once defined in `settings.xml`, these properties can then be referenced in the `pom.xml` configuration file:
 **pom.xml**
[YAML]
    <publisher>
      <contextUrl>${artifactoryUrl}</contextUrl>
      <username>${username}</username>
      <password>${password}</password>
      <repoKey>libs-release-local</repoKey>
      <snapshotRepoKey>libs-snapshot-local</snapshotRepoKey>
    </publisher>
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
224,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Caching helps you speed up execution of a step by preserving and restoring packages and dependencies between runs of that step. In this way, you can reduce build times by avoiding repeating the installation or loading of large dependencies every time the step is run.
Native steps perform caching as needed, so they will always execute as fast as possible. You will only need to use the methods described in this page when you are using general-purpose Bash steps, or if your native step performs a cacheable action in its `onStart` or `onComplete` execution block.
[/INST]#### How Caching Works
Caching is performed through utility functions that store and restore data to and from the Artifactory filestore. In this way, a step can benefit from the dependencies that were installed or loaded from a previously executed run.
##### Cache File Scope
A cache file that a step stores using the `add_cache_files` function will be associated with that step, and will only be available to a `restore_cache_files` function from that same step in a subsequent run.
You cannot pass a cache file to other steps in a pipeline. For example, if `step_1` adds the cache file `step_1_cache`, and `step_2` tries to restore `step_1_cache`, nothing will be loaded.
### Note
Steps running in parallel can overwrite the cache. So, steps trying to update step or pipeline state in parallel will result in only the last version of the state saved for the step/pipeline being available to future steps.
##### Filestore Limitations
In general, the Artifactory filestore provides the highest available performance for storing and restoring data.
However, the speed you experience will depend on which storage medium Artifactory has been configured to use. If Artifactory has been configured to use the file system on a local or mounted filestore, this is fast storage and caching will always accelerate step execution. If Artifactory has been configured to use remote storage such as S3 or Google Cloud Storage, then the slower roundtrip to and from the filestore may diminish the usefulness of caching:
  * Files that take a long time to install always benefit from caching. So anything related to bundler, npm, composer, pip, etc are great candidates for caching.
  * Files that take a long time to download but are installed quickly do not benefit from caching since it takes as much time to download from S3 as from the original source. Examples are compiled binaries, JDK packages, etc.


#### Example
The following example caches the results of an `npm install` for subsequent runs.
[YAML]
    resources:
      - name: my_gitrepo
        type: GitRepo
        configuration:
          gitProvider: my_github-integration              # replace with your integration
          path: my-github/my-pipelines-project            # replace with your repository path
          branches:
            include: master
    
    pipelines:
      - name: pipelines_caching
        steps:
          - name: step_1_pipelines_caching
            type: Bash
            configuration:
              inputResources:
                - name: my_gitrepo
            execution:
              onExecute:
                - cd $res_my_gitrepo_resourcePath
                - restore_cache_files npm_cache $res_my_gitrepo_resourcePath/node_modules
                - npm install
              onComplete:
                - add_cache_files $res_my_gitrepo_resourcePath/node_modules npm_cache
[/YAML]
  * The step's `onExecute` block performs a `restore_cache_files` function to load the cached npm dependencies if they are available from a previous run. If none exist, no error will result, so the remainder of the step will execute without interruption.
  * When the `npm install` is run, it will recognize if the dependencies are already present from the cache, so the step will execute more rapidly. If there was no cache to load, then the npm dependencies will be installed.
  * When the step is complete, it will always write the npm dependencies to the cache so they will be available to the step in the next run of the pipeline.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
225,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A multibranch pipeline enables development teams to maintain discrete pipelines for different branches of the same project. The pipeline of each branch will have its own run history and logs.
Many development organizations use a branching model (such as GitFlow) for collaboration on large projects. In this style of workflow, development teams maintain a main source code branch while creating feature branches for each feature in-progress. The feature team will merge those changes into the main branch when feature development is complete, and delete the feature branch.
In this agile development model, feature branches are created and destroyed frequently. Manually administering each branch as a new pipeline source can be a slow and annoying chore.
When you designate a pipeline source as multibranch, Pipelines automatically creates a new pipeline from each branch's pipeline config, and destroys the pipeline when the branch is destroyed. In this way, the main branch and included child branches will all have their own pipeline, each of which can be independently viewed, run, and triggered.
[/INST]#### Configuring a Multibranch Pipeline Source
A multibranch pipeline must be in a pipeline source that has been configured as multibranch.
When a multibranch pipeline source syncs, it automatically creates and destroys pipeline config files in branches from the main branch. You can specify Exclude and Include Branch Patterns to limit which branches participate in automatic pipeline creation/deletion.
For more information, see Managing Pipeline Sources.
#### Declaring a Multibranch Pipeline
In Pipelines DSL, the term `{{gitBranch}}` can be used as a placeholder for whatever branch the pipeline was loaded from. When Pipelines syncs from the pipeline source, it automatically replaces any occurrence of `{{gitBranch}} `with the name of the branch. Note that this will occur for single branch pipeline sources as well as multbranch ones).
When creating a multibranch pipeline in Pipelines DSL, use `{{gitBranch}}` as the value anywhere you would designate a branch. When Pipelines syncs each of the branches of a multibranch pipeline source, it will automatically replace the `{{gitBranch}} `reference in each branch's pipeline with the name of its branch.
In the GitRepo resource definition, use `{{gitBranch}} `where you define the branches of that resource to include.
 **pipelines_resources.yml**
[YAML]
    resources:
      - name: appRepo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myrepo/myproject
          branches:
            include: ^{{gitBranch}}$        # Formatted as a regex pattern
    
[/YAML]
You can also use`{{gitBranch}} `anywhere you might refer to a branch in steps or pipeline configuration. This example pipeline outputs the name of the branch.
 **pipelines_steps.yml**
[YAML]
    pipelines:
      - name: multibranch_example
        steps:
          - name: mb_step_1
            type: Bash
            configuration:
              affinityGroup: together
              inputResources:
                - name: appRepo
            execution:
              onExecute:
                - echo ""running test for branch {{gitBranch}}""
                - pushd $res_tw_multibranch_repo_resourcePath
                - ls -latr
                - popd
    
          - name: mb_step_2
            type: Bash
            configuration:
              affinityGroup: together
              inputSteps:
                - name: mb_step_1
            execution:
              onExecute:
                - echo ""building {{gitBranch}}""
    
[/YAML]
### Note
When used in a conditional or comparison logic, `{{gitBranch}}` placeholders must be wrapped within double quotes.
 **Example** : `{{ if eq ""{{gitBranch}}"" ""master"" }}`
#### Running Multibranch Pipelines
A multibranch pipeline synced from a multibranch pipeline source appears in My Pipelines as an expandable entry in the list. Click to expand a multibranch pipeline source and then click one of the branch versions to view and run it.
With your branch chosen, you can view the Pipeline Run Logs of any run in that branch.
The example multibranch pipeline above uses `{{gitBranch}} `to output the name of the branch being run. You can see the results of this in the run logs of the pipeline in each branch.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
226,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines provides facilities to manage the execution of a Jenkins job from your pipeline, passing information into and receiving output from the job.
For organizations that have a significant legacy investment in Jenkins, it may not be practical or cost-effective to convert many existing CI/CD workflows to Pipelines. These facilities provide you with a way to interoperate between your existing Jenkins workflows and Pipelines.
For example:
  * A Jenkins build job may create a base Docker image and publish it to Artifactory, then send information from that job to the subsequent steps in your Pipeline to build variant Docker images for testing and release
  * A Pipelines pipeline may publish a build to Artifactory, then send the build info as a parameter to a Jenkins job to deliver it for staging
  * A Jenkins build job may push a Docker image to Artifactory, trigger Pipelines to build supplemental images, then Pipelines may trigger Jenkins to complete the build of variants


This facility presumes the use of Artifactory repositories to store and manage all builds and metadata.
[/INST]### Before You Start
This facility requires that  Jenkins Artifactory Plugin be installed with your Jenkins installation.Configuring Jenkins Artifactory Plug-in
#### Running a Jenkins Job From Pipelines
A pipeline in Pipelines can transfer control to a Jenkins job and then is returned to execution after the Jenkins job completes.
In summary, this is the sequence of events:
  1. In Pipelines, a pipeline executes a Jenkins step to invoke a Jenkins job through the Jenkins Server Integration
  2. When the Jenkins job completes, it returns control to Pipelines and optionally updates Pipelines resources by calling a function in the Jenkins Artifactory PluginConfiguring Jenkins Artifactory Plug-in
  3. In Pipelines, the pipeline resumes execution with any updated resources


##### Jenkins Server Integration
To enable Pipelines to pass control from and return to Jenkins, an administrator user must first add a Jenkins Server Integration The Jenkins Server integration specifies the Jenkins URL, as well as the username/token credentials for a valid Jenkins user. In addition, the Jenkins Server Integration must generate a bearer token to authenticate messages sent from Jenkins to Pipelines through the Jenkins Artifactory Plugin.  
The Jenkins Artifactory plugin must be configured with the Callback URL, and the generated bearer token must be added to the plugin's credentials. See the Jenkins Server Integration for details.
##### Pipelines DSL
If the Jenkins job will update Pipelines resources, you must declare those resources in your Pipelines DSL. For example, a PropertyBag resource can be used to signal Jenkins job results:
 **resources**
[YAML]
    resources:
      - name: app_test_results
        type: PropertyBag
        configuration:
          passing: 0
          failing: 0
[/YAML]
To initiate the execution of a Jenkins job from within your pipeline, use the Jenkins native step. You must specify the Jenkins Server integration in your `integrations` block. If the Jenkins job updates Pipelines resources, you must specify them in the `outputResources` block.
 **Jenkins Step**
[YAML]
    - name: test_app
      type: Jenkins
      configuration:
        jenkinsJobName: basic-api
        inputResources:
          - name: app_docker
        buildParameters:
          imageName: ${res_app_docker_imageName}
          imageTag: ${res_app_docker_imageTag}
        integrations:
          - name: myJenkins
        outputResources:
          - name: app_test_results
[/YAML]
When loaded into Pipelines, the pipeline diagram shows the Jenkins step with a Docker image as an input resource, and the PropertyBag updated by the Jenkins job as an output resource.
###### Sending Build Parameters to Jenkins
The Jenkins step can pass parameters to the Jenkins job through the `buildParameters` property. These values can be fixed or you can use Pipelines environment variables to send dynamic values.
In this example, we specify an Image resource called ""app_docker"" in the `inputResources` of our Jenkins Step. Our `buildParameters` property references the resource's environment variables.
 **Jenkins Parameters**
[YAML]
    buildParameters:
      imageName: ${res_app_docker_imageName}
      imageTag: ${res_app_docker_imageTag}
      environment: ""test""
[/YAML]
### Note
Run state environment variables are not available to `buildParameters.`
##### Jenkins Build Job
The Jenkins job must be prepared to recognize incoming `buildParameters` from Pipelines. When our Jenkins job executes, our example build parameters are received from Pipelines and available for use.
 **Jenkins Job**
[YAML]
    steps {
      echo ""environment: ${params.environment}""
      echo ""Running tests on image ${params.imageName}:${params.imageTag}""
    }
[/YAML]
At the end of your Jenkins build job, call the plugin's function `jfPipelines()` to signal its completion and return control back to Pipelines.
 **Jenkins Job**
[YAML]
    jfPipelines()
[/YAML]
Your build job may optionally pass information back to Pipelines by updating the properties of Pipelines resources. To do so, declare each resource by name with the new property values in you call to `jfPipelines()`.
For example, the following call references a Pipelines PropertyBag resource we have called `app_test_results` and provides new values for two of its properties:
 **Jenkins Job**
[YAML]
    jfPipelines(
      outputResources: """"""[
        {
          ""name"": ""app_test_results"",
          ""content"": {
            ""passing"": 1234,
            ""failing"": 0
          }
        }
      ]""""""
    )
[/YAML]
As noted above, these Pipelines resources must be declared as `outputResources` of the invoking Jenkins step.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
227,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The Matrix native step enables your pipeline to repeatedly execute the same set of actions in a variety of configurations and runtime environments, with each variant executing as an independent “steplet.” These steplets can, when configured, execute in parallel on multiple build nodes. On completion of all steplets, Pipelines aggregates the result status, giving the appearance of a single step.
Common use cases for Matrix steps are:
  * Splitting a large test suite into smaller units, and executing them in parallel to reduce total execution time.
  * Testing against multiple values of environment variables or multiple runtime images.


[/INST]#### Single Dimension Matrix
The Matrix step is useful for performing the same operation multiple times, but with different settings each time. This could mean pushing the same Docker image to different registries, or executing the same set of unit tests with different test values.
For example, we might want to perform the same test on a completed build with different sets of values for _animal_ , _mineral_ , and _vegetable._
animal
mineral
vegetable  
 **Variation 1**
dog
copper
carrot  
 **Variation 2**
goat
iron
broccoli  
 **Variation 3**
lizard
lead
<null>  
This example pipeline triggers on any change to the BuildInfo resource produced by another pipeline. The Matrix step performs the same `execution` actions in parallel steplets, with each using a different set of _animal_ , _mineral_ , and _vegetable_ values from the collection specified in the `stepletMultipliers:environmentVariables` block.
[YAML]
      - name: Matrix_Single_Pipeline
        steps:     
          - name: matrix_example
            type: Matrix
            stepMode: Bash
            configuration:
              inputResources:
                - name: CompletedBuildInfo
            stepletMultipliers:
              environmentVariables:             # Sets of environment variables for steplets
                - animal:    dog                # - Set 1
                  mineral:   copper
                  vegetable: carrot
                - animal:    goat               # - Set 2
                  mineral:   iron
                  vegetable: broccoli
                - animal:    lizard             # - Set 3
                  mineral:   lead          
                  
            execution:
              onExecute:
                - echo ""Executing matrix step on ${steplet_id}""
                - echo ""Variation animal = ${animal}, mineral = ${mineral}, vegetable = ${vegetable}""
[/YAML]
When successfully executed, the run log of the pipeline shows the set of steplets executed by the Matrix step, along with their individual success status and the set of values used for the _Matrix Combination_.  
#### Two-Dimension Matrix
A Matrix step can also perform the same operations in multiple runtime environments. For example, running tests in Java and Javascript, or in different release versions of the same runtime.
This can be done as a single-dimension matrix (performing a single, fixed set of actions in each runtime) or as a two-dimension matrix (performing different actions with different configurations in multiple runtimes).
If we modify the single matrix example above to run in both Java and Javascript runtimes, it will execute in a total of 6 steplets; in 2 runtimes for each of 3 sets of environment variables:
Runtime
Set 1
Set 2
Set 3  
 **java 13.0**
dog, copper, carrot
goat, iron, broccoli
lizard, lead, &lt;null&gt;  
node 8.17.0
dog, copper, carrot
goat, iron, broccoli
lizard, lead, <null>  
##### Matrix Pipeline Example
The following example pipeline can be found in the JFrog GitHub repository. You can fork this repo to your own GitHub account to try on your own installation of Pipelines.
The two-dimension matrix pipeline provides a simplified example of running tests in modules in different runtime environments. It performs basic functionality tests on four different math operation modules, repeating these tests in different release versions of nodejs.
###### Resources
The first step of the pipeline will trigger on any change to the example GitHub repo, which also houses the test files. So any change to the tests or the pipeline will trigger a new run of the pipeline.
If you have forked the example, you will need to change the path and integration name as noted in the comments.
[YAML]
    resources:
      - name: sample_code
        type: GitRepo
        configuration:
          path: jfrog/jfrog-pipelines-matrix-example      # Change to your own path
          branches:
            include: ^master$
          gitProvider: jfrog_github                       # Change to your own GitHub integration name
[/YAML]
###### PreMatrix Step
The PreMatrix generic step is an optional step for performing preamble operations. It may be used to prepare a build environment for execution of a Matrix step.
Our example PreMatrix step loads the npm package dependencies that will be needed by the steplets.
 **PreMatrix Example Step**
[YAML]
          - name: setup
            type: PreMatrix
            configuration:
              inputResources:
                - name: sample_code
            execution:
              onExecute:
                - echo ""Preparing the code base""
                - pushd ""${res_sample_code_resourcePath}/app""
                - npm install
                - popd 
                - mkdir -p app
                - cp -r ""${res_sample_code_resourcePath}/app/."" ./app/
[/YAML]
###### Matrix Step
The Matrix step executes the test cases for each module in multiple versions of the nodejs runtime. The modules are specified as environment variables in `stepletMultipliers` , which also specifies the runtimes.
 **Matrix Example Step**
[YAML]
          - name: test
            type: Matrix
            stepMode: Bash
            configuration:
              inputSteps:
                - name: setup
            stepletMultipliers:
              environmentVariables:
                - module: mod_1
                - module: mod_2
                - module: mod_3
                - module: mod_4
              runtimes:
                - type: image
                  image:
                    auto:
                      language: node
                      versions:
                        - 12
                - type: image
                  image:
                    auto:
                      language: node
                      versions:
                        - 8.17.0                    
            execution:
              onExecute:
                - pushd ./setup/app
                - MODULE=""$module"" npm run test
                - popd
              onComplete:
                - save_tests ./setup/app/reports
[/YAML]
###### PostMatrix Step
The PostMatrix generic step is an optional step for performing post-execution tasks following a Matrix step.
In the example pipeline, the PostMatrix step aggregates all the test reports.
 **PostMatrix Step Example**
[YAML]
          - name: output
            type: PostMatrix
            configuration:
              inputSteps: 
                - name: test
            execution:
              onExecute:
                - echo ""Executing the output step""
                - ls -ltr ./test/setup/app/reports
                - save_tests ./test/setup/app/reports
[/YAML]
###### Results
When the example pipeline is loaded and run, the Matrix step executes eight steplets. The run log for each steplet, including saved test results, can be viewed from the pipeline's run history.
The run log of the PostMatrix step shows the aggregated results of all steplets.
#### Multi-node Matrix
A Matrix step can be configured such that each steplet runs on its own build node in the same node pool. This helps in reducing execution time and ensures efficient use of available resources.
This can be done by setting `multiNode` to `true`. Default value is `false`.
 **Multi-node Matrix Example**
[YAML]
    steps:
    - name: step_1
      type: Matrix
      stepMode: Bash
      configuration:
        multiNode: true
      stepletMultipliers:
        environmentVariables:
          - foo: foo
          - bar: bar
[/YAML]
#### Multi-node pool Matrix
A Matrix step can be configured such that each steplet executes in parallel on multiple node pools with different operating systems or cloud providers.
This can be done by setting `multiNode` to `true`. Default value is `false`.
 **Multi-node pool Matrix Example**
[YAML]
    steps:
    - name: step_1
      type: Matrix
      stepMode: Bash
      configuration:
        multiNode: true
      stepletMultipliers:
        nodePools:
          - windows
          - ubuntu_18
          - ubuntu_16
        environmentVariables:
          - foo: foo
          - bar: bar
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
228,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Retention Policy enables you to configure a policy to discard pipelines run data from older pipelines runs. Pipelines run data includes Pipeline Run Logs, Pipeline State, Steps Cache, and Runs State. This data, over time and depending on your Pipelines usage, can become very large and lead to increased storage costs.
Retention Policy can be set at two levels:
  * System-level
  * Pipeline-level


[/INST]#### Setting Retention Policy at System-level
To enable retention policy at the system-level, in the `system.yaml` file, in the **core** section, set the retentionPolicy’s `enabled` flag to `true`. This policy applies to all your pipelines.
### Note
By default, Retention Policy is disabled at system-level, which effectively means infinite retention.
When enabled, system-level Retention Policy uses two properties to control which pipelines run data is to be retained:
  *  **Max Age Days** : Specifies the number of days, after which the pipelines run data will be deleted. This can be set in granularity of days. Default value is 0 (infinite retention).
  *  **Minimum Runs** : Specifies the minimum number of pipeline runs to keep, regardless of their age. Default value is 10.


The following optional cron expression properties helps you control when the Retention Policy is triggered:
  *  **Cron Hours** : Cron expression interval in hours when the Retention Policy runs. Default value is 4.
  *  **Cron Days** : Cron expression days of the week when the Retention Policy runs. Input should be comma separated names or numbers 0-7 (0 or 7 are Sunday). Default value is 6 (Saturday).
  *  **Cron Time Zone** : Cron time zone that defines the time zone when Retention Policy runs. List of supported time zones. Default value is UTC.


 **Example**
 **Configuring retention policy in system.yaml**
[YAML]
    core: 
        retentionPolicy:
            enabled: true
            maxAgeDays: 90
            minRuns: 10
            cronHours: 4
            cronDays: ""0,6""
            cronTimeZone: ""UTC""
[/YAML]
In this example, system-level Retention Policy is enabled, and pipelines run data for the last 90 days is set to be retained and data older than 90 days is set to be deleted. The minimum runs property (`minRuns`) ensures that for each one of your pipelines, the latest 10 pipeline runs data is retained, regardless of the policy’s max allowed age (`maxAgeDays`). This means, if some (or all) of your 10 latest pipelines runs are older than 90 days, Pipelines will still keep their run data. If you have less than 10 pipeline runs for certain pipelines, Pipelines will keep all their pipelines run data. As for cron properties, Retention Policy will be triggered every 4 hours on Saturday (6) and Sunday(0) and will start at midnight UTC.
#### Setting Retention Policy at Pipeline-level
Once Retention Policy is enabled at system-level, it can also be configured at at pipeline-level. For information about setting the values for pipeline-level Retention Policy, see Defining a Pipeline.
Just like Retention Policy at system-level, you can control the values of the number of days and minimum number of runs to keep at pipeline-level. These settings override the system-level Retention Policy, but pipeline-level policy values can only be _equal_ _to or lesser_ than the system-level configuration.
For example, if the system-level Retention Policy configuration is:
[YAML]
     retentionPolicy:
        enabled: true
        maxAgeDays: 90
        minRuns: 10
[/YAML]
And the pipeline-level Retention Policy configuration is:
[YAML]
     retentionPolicy:
        maxAgeDays: 30
        minRuns: 5
[/YAML]
Then Pipelines will discard Pipeline run data for runs that are older than 30 days. Pipelines, however, will also keep at least 5 pipeline runs data, even if they are older than 30 days (or even if they are older than system-level 90 days value).
### Note
If you set the values of pipeline-level Retention Policy to greater than your system-level values, the Pipeline Source Sync will fail.
 **Example**
[YAML]
    pipelines:
     - name: example_pipeline_retention_policy
       configuration:
         retentionPolicy:
           maxAgeDays: 30
           minRuns: 5
       steps:
         - name: step_1
           type: Bash          
           execution:
             onExecute:
               - echo ""Hello world""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
229,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The signed pipelines feature is a verification system that determines which pipelines/steps generated a specific artifact. It provides users with a way to ensure that their artifacts have not been tampered with before these artifacts are promoted through the CI/CD workflow. The signing process creates trust, and provides a way to validate the immutability of the artifacts and the authenticity of packages. This means, if the authenticity of artifacts cannot be verified, they can be blocked.
In addition, the signed pipelines feature builds comprehensive metadata, called pipeinfo. This provides complete visibility and audit for each step and run, which can be viewed in the UI.
[/INST]### Note
  * The signed pipelines feature is available with Enterprise+ license only.
  * The signed pipelines feature is available for buildinfo and Go binaries only.
  * The signed pipelines feature is enabled by default. To disable this feature, in the Core Services Configurations section in the Pipelines System YAML, set the `signedPipelinesEnabled` tag as `false`.Pipelines System YAMLPipelines System YAML


#### Signing Process
During the signing process of a pipeline:
  * A run key is generated for every run of the pipeline.
  * During the run, all artifacts (`pipeInfo.json`) are signed with the private key.
  * The private key is discarded at the end of the run.
  * Metadata for the run that generated the artifact is stored as a tag for the artifact.
  * During the verification stage, a public key is used to verify the signature.


For each run, files containing comprehensive metadata (*.json) and corresponding signature files (*.json.sig) are uploaded to Artifactory. This provides a way to trace the entire path of a pipeline. Now any process that needs to verify can verify this in addition to cross-referencing the pipeInfo.  
#### Pipe-Info Repository
The Pipe-Info repository, like the  Build-Info Repository, is a local repository that is created per Project and stores all the metadata for each pipeline.Build Integration
Whenever Pipelines uploads a new build to Artifactory, its pipeinfo json files are automatically deployed to either the default _artifactory-pipe-info_ repository or the _< project_key>-pipe-info_ repository. Pipeline artifacts that do not belong to any Project are stored in the _artifactory-pipe-info_ repository and pipeline artifacts that belong to specific Projects are stored in the _< project_key>-pipe-info_ repository.
##### Viewing pipe-info Repository
To view the pipe-info repository, from the **Application** module, click **Artifactory | Artifacts** and then click the **artifactory-pipe-info** repository.
Artifactory automatically places the json files in the pipe-info repository using the following structure: **[pipeline-name] → [branch] → [run-rumber] → [pipe-info files]**.  
The JSON schema defines a pipeinfo object. The object is composed of three components, which summarizes a run:
  * `head.json`: Contains metadata about the pipeline, and the contents of the pipeline yaml file.
  * `info.json`: Contains metadata about the status of the run.
  * ` _step_.json`: Contains metadata about the step.


Each `.json` file is stored with a corresponding `.json.sig` signature file. To view the contents ofa json file, right-click the file and click **View** , as shown in the image above.
### Using the artifactory-pipe-info repository
  * The `artifactory-pipe-info` repository is automatically created and used by default. Removing it will cause all the pipe-info in the repository to be deleted and a new default pipe-info repository will be recreated.
  * Only admins and Project admins can view and navigate to this repository.


### Cleanup Policies
If you have existing cleanup policies, defined in Artifactory User Plugins, that delete artifacts from your local repositories according to a policy, they will also affect the `artifactory-pipe-info` repository and should be updated accordingly. Exclude this repository from your cleanup policies to ensure that your pipe info files are not deleted.
#### Viewing Metadata
When you have a successful build for a pipeline, the metadata for the artifacts and builds for that pipeline is displayed in the corresponding repository in **Artifactory** :
Go to **Artifactory | Artifacts | <repo_name> | projects | <version>**, navigate and click one of the files. This displays the **Pipelines** tab, which shows the metadata for the run that created the artifact.
### Note
The Pipelines tab appears only when you click a file.  
Similarly, go to **Artifactory | Builds | <build_name> | <build_id>**and click the **Pipelines** tab, which shows metadata for the build.  
##### Viewing pipeinfo.json
A `pipeInfo.json` file is generated for each run under the Project/Pipeline namespace. This file captures the metadata for a specific step of a pipeline, as well as a detailed Bill of Materials (BOM) for each run. The pipe-info includes information, such as name, version, type, status, timestamps, inputs, outputs, nodepools and more.This information is maintained in the run stateduring execution. When the run completes, the json file is uploaded to the appropriate repository.
To view the `pipeinfo.json` file, go the run page, click the action button on the upper left corner and click  **Pipe Info**. The contents of the `pipeinfo.json` file is displayed in a json viewer.
#### Storing and Validating Signed Pipelines Data
##### Native Steps
Pipelines native steps automatically store information about the artifacts they build so that they can be used later as part of the signed pipelines validation.
### Note
The PowerShell version of these steps do not support the signed pipelines feature.
The following native steps can save artifact info:
  * CreateReleaseBundle
  * DockerPush
  * GoPublishBinary
  * GoPublishModule
  * HelmPublish
  * NpmPublish


The following native steps can validate the published artifacts:
  * CreateReleaseBundle
  * PublishBuildInfo


##### Generic Steps
It is also possible to create your own signed pipeline using only Bash steps. To accomplish this, use the following utility functions, which are available at execution time, to configure what needs to be tracked and validated.
  * save_artifact_info
  * validate_artifact


If an artifact is saved in one Bash step, it can be validated in a later Bash step.
##### Example YAML
 **Signed Pipelines for Bash**
[YAML]
    resources:
      - name: signed_git
        type: GitRepo
        configuration:
          scmProvider: bbs
          path: ~test/myapp
          branches:
            include: ^master$
      - name: signed_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: my_art
    
    pipelines:
      - name: test_signed_start
        steps:
          - name: create_info
            type: Bash
            configuration:
              inputResources:
                - name: signed_git
              integrations:
                - name: my_art
              outputResources:
                - name: signed_buildinfo
            execution:
              onExecute:
                - cd $res_signed_git_resourcePath 
                - jfrog rt upload server.js 
                - jfrog rt build-collect-environment
                - jfrog rt build-publish --detailed-summary $JFROG_CLI_BUILD_NAME $JFROG_CLI_BUILD_NUMBER > summaryOutput.json
                - save_artifact_info buildInfo summaryOutput.json --build-name=$JFROG_CLI_BUILD_NAME --build-number=$JFROG_CLI_BUILD_NUMBER
    
      - name: test_signed_stop
        steps:
          - name: validate_info
            type: Bash
            configuration:
              inputResources:
                - name: signed_buildinfo
            execution:
              onExecute:
                - response=$(validate_artifact buildInfo --build-name ${res_signed_buildinfo_buildName} --build-number ${res_signed_buildinfo_buildNumber})
                - validateResult=$(echo ""$response"" | jq '.result')
                - if [ ""$validateResult"" != ""true"" ]; then echo ""failed validation"" && exit 1; fi
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
230,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The timeout setting in Pipelines is the maximum time after which pipeline steps will timeout. If the step does not complete in the given time limit, it is forced to completion with a timed out status.
Setting a timeout is useful for steps that are expected to run for a shorter duration. For example, in a pipeline, Step A is expected to run for only 3 minutes, but Step B and Step C are expected to run for 45 minutes. In this case, setting the timeout for Step A to around five minutes ensures that even if the step gets trapped in a retry loop, the other, non-dependent steps, can execute after the timeout.
Timeouts can be set at the following levels and are applied according to a preset order of precedence:
Level
Precedence  
Step
1
Highest  
Node Pool
2  
System
3
Lowest  
Extending our example further:
  * Step A and Step B are configured to run on Node Pool A, and Step C is configured on run on Node Pool B
  * Node Pool A has its timeout set to 3600 seconds (60 minutes)
  * Node Pool B has no timeout set
  * Step A has its timeout value set to 300 seconds (5 minutes) in the pipelines yaml
  * Step B and Step C have no timeout set in the pipelines yaml
  * The timeout in the system yaml is set to 7200000 milliseconds (120 minutes)


Now, when the pipeline runs:
  * Step A has 5 minutes before timing out. Due to the order of precedence, it ignores the timeout set for Node Pool A and the system yaml, both of which have higher timeout values.
  * Step B has 60 minutes before timing out, since that is the value set for Node Pool A. It ignores the value set in the system yaml.
  * Step C has 120 minutes before timing out, since that is the value set in the system yaml.


[/INST]#### Timeout at Step-level
Step-level timeout setting takes precedence over every other timeout setting, be it at node pool-level or system-level. If this value is not set, the step uses the timeout setting configured for the node pool in the UI.
### Note
The step-level timeout cannot exceed the timeout set for system or node pool.
The timeout at step-level can be set for any step using the`timeoutSeconds`property, which is added to the Configuration section of the step. For more information, see Step Configuration.
In this example, the timeout is set to 300 seconds (5 minutes):
 **Example**
[YAML]
    pipelines:
      - name: my_pipeline
        steps:
          - name: my_step
            type: Bash
            configuration:
              timeoutSeconds: 300 //5 minutes
            execution:
              onExecute:
                - echo ""This is my step.""
[/YAML]
#### Timeout at Node Pool-level
Node pool-level timeout is used when one or more steps in your pipeline do not have any timeout specified at the step-level. Each node pool can have a different step timeout duration. Default value is 3600 seconds.
The timeout at node pool-level can be set in the UI. For more information, see Managing Pipelines Node Pools.
### Note
Node pool-level timeout cannot be greater than the value set for `stepTimeoutMS` in the Pipelines System YAML.Pipelines System YAML
#### Timeout at System-level
System-level timeout is enabled and set to 3600000 milliseconds (60 minutes) by default. This can be set using the `stepTimeoutMS` property in the  Runtime Configuration section in Pipelines System YAML:Pipelines System YAMLPipelines System YAML
[YAML]
    stepTimeoutMS: 3600000
[/YAML]
System-level timeout can be changed to any duration of your choice.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
231,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Pipelines provides several ways to optimize your pipeline runs to make them execute faster. This helps in mitigating performance bottlenecks, avoiding build failures, and increasing build speed.
This section outlines all the options available for making your pipelines more efficient.
[/INST]#### Caching Step Runtimes
Caching, which is performed through utility functions (`add_cache_files` and `reset_cache_files`), helps you speed up the execution of a step by preserving and restoring packages and dependencies between the runs of that step. Avoiding repeating the installation or loading of large dependencies every time the step is run helps in reducing build times. In addition, native steps perform caching as needed and always execute as fast as possible.
For more information, see Caching Step Runtimes.
#### Caching Dynamic Nodes
Node caching is useful when the same runtime is needed for a series of steps, and the same large set of dependencies needs to be installed. Node caching, which is available for dynamic node pools, helps speed up the execution of your builds and saves time by preserving the runtime environment for subsequent steps to execute in. For example, Docker layers can be cached, leading to faster docker builds since those layers don’t need to be rebuilt each time.
When adding a dynamic node pool, node caching can be enabled by clicking the **Enable Cache** check box. For more information, see Dynamic Node Caching.
#### Choosing Node Pools
Another, often-overlooked, option for speeding up your pipeline runs is to create build node pools and run each step on a specific pool. This enables resource-intensive steps to run on larger nodes, speeding up execution.
Node pools can bet set using the Pipelines DSL. For more information, see Choosing Node Pools.
#### Using Artifactory Remote Repositories
Steps can use Artifactory remote repositories for caching packages that are needed repeatedly during execution, which helps in significantly reducing execution time. Remote repositories in Artifactory serve as a caching proxy for a repository managed at a remote URL.
For more information, see  Remote Repositories and Cache Settings.Repository ManagementAdvanced Settings
#### Using Custom Runtime Image
Using custom image gives you better control over what is installed on the runtime images. It can also speed up step execution since you can pre-install dependencies into the image.
For more information, see Using a Custom Runtime Image.
#### Using Matrix Step
Another option to reduce total execution time is to use the Matrix step. The Matrix step enables your pipeline to repeatedly execute the same set of actions in a variety of configurations and runtime environments, with each variant executing as an independent “steplet.” These steplets can, when configured, execute in parallel on multiple build nodes. On completion of all steplets, Pipelines aggregates the result status, giving the appearance of a single step.
For more information, see Matrix and Using the Matrix Step.
#### Running Steps in Parallel
Running steps in parallel is a great way to reduce your build's running time, especially when you have a large test suite. Before doing this, however, you must first check which steps must run sequentially. All the other steps can now run in parallel to speed up pipeline execution. For more information, see Breaking Your Pipelines into Steps.
This can be tuned further by combining it with the `priority` tag, which can yield even better results. The `priority` tag, which can be set for any step, controls the priority of a step when there are parallel steps in a pipeline or multiple pipelines executing. For more information, see Bash Tags.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
232,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>When triggering a run manually, you have the option of triggering it with custom parameters. Use this option to select:
  * Steps to be run
  * Node pool for the pipeline and for each step
  * Input resource versions
  * Custom environment variables


The advantage of triggering a run with custom parameters is that you can temporarily add or update values for a specific step.
[/INST]#### Customizing a Run
To trigger a run with custom parameters:
  1. From the **Applications** tab, go to **Pipelines** | **My Pipelines**.
  2. In the Pipelines dashboard that appears, click the name of your pipeline.
  3. Click the step for which you want to use custom configuration and then click the custom trigger  
button.  
  4. In the resulting **Trigger Run** dialog, configure the following and click **Trigger Run** to configure the following:
    * Select Steps and Node Pools
    * Input Resources
    * Environment Variables
    * Run Configurations


##### Select Steps and Node Pools
When triggering a run, you have the option of selecting specific steps for the run. When you trigger the run, only the steps selected for the run are executed. In addition, you can select specific node pools for the pipeline and individual steps. This provides you the flexibility of selecting node pools that are suitable for specific steps.  
##### Environment Variables
Environment variables can be used to customize a run that is triggered manually.
You can use one or both of these options to add environment variables:
  *  **Predefine in YAML** : This option enables you to add a set of predefined values in the pipelines DSL, which is then available for selection in the UI. Environment variables can be added at the pipeline- or step-level. The pipeline-level environment variables will be carried through to all steps executed in the pipeline run, but the step-level environment variables will apply only to that specific step. Use this option when you have jobs that require environment variable values to be changed frequently.
For information about setting these values, see Environment Variables Configuration.
  *  **Add custom variables** : This field enables you to manually add one or more environment variables for each run. These custom variables can be applied to a single step or to all the steps. Use this only when you want to perform a quick test and do not expect to repeat the run with the same environment variables.
Custom variables can be added using:
    *  **Key-Value field** : Use this when you want to quickly add a small number of key-value pairs.
    *  **Bulk paste** : Use this when you want to add a large number of key-value pairs.
 **Bulk Paste Example**
[YAML]        key1=value1
        key2=value2
        key3=value3
        key4=value4
        key5=value5
        key6=value6
        key7=value7
        key8=value8
        key9=value9
[/YAML]

  
##### Input Resources
The **Input Resources** tab lists all the input resources in your pipeline DSL. For each listed resource, you can select the resource version Id you want to use for the run.  
##### Run Configurations
In the **Run Configurations** tab, admin users can set a new run number for a run, so that the next run will start from the given number. The run number provided must be greater than the current run number.  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
233,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Build badges enable you to display the current status of your pipeline in your repository. This is useful for instantly showcasing your build statuses and promoting transparency within your team.
While a build badge can be added to your `readme.md` file, it can also be added to any web page.  
[/INST]### Note
Click here to see an example of a build badge being used in a readme.
To add a build badge, add the following:
#### Run-level Build Badge
  *  **Single Branch Source** : `https://{host_name}/pipelines/api/v1/badges/project/{project_key}/pipelines/{pipelines_name}?text=JFrogPipelines`
 **Example**
`![Approval Gates](https://pipelines.jfrog.io/pipelines/api/v1/badges/project/quickstarts/pipelines/Approval_Gates_Pipeline?text=JFrogPipelines)`
  *  **Multi Branch Source** : `https://{host_name}/pipelines/api/v1/badges/project/{project_key}/pipelines/{pipelines_name}?branch={branch_name}&amp;text=JFrogPipelines`


#### Step-level Build Badge
`http://{host_name}/pipelines/api/v1/badges/project/{project_key}/pipelines/{pipeline_name}/steps/{step_name}?text=JFrogPipelines`
 **Example**
`![Approval Gates](https://pipelines.jfrog.io/pipelines/api/v1/badges/project/quickstarts/pipelines/Approval_Gates_Pipeline/steps/p1_s1?text=JFrogPipelines)`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
234,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Conditional workflow in Pipelines enables you to choose if a step executes or skips based on certain conditions set for the previous upstream step. This means, when the workflow reaches a conditional step, it can choose different workflow paths based on the step’s status. This provides more flexibility in the execution logic of a pipeline.
[/INST]### Note
Conditional workflow can be applied to any Pipelines step.
#### Step Status Conditional Workflow
With the `status` conditional workflow, you can configure a step to execute only if an input step’s status, during its current run, is satisfied. You can configure any number of statuses for a step.
 **YAML Schema**
[YAML]
        steps:
          - name: <step_name>
            type: <step_type>
            configuration:
                      allowFailure: boolean         #optional
              inputSteps:
                - name: <step_name>
                  status:
                    - <terminal_status>
                    - <terminal_status>
                    - <terminal_status>
[/YAML]
### Note
It is important to note that the status of an input step in the current run only is considered for conditional workflows. If a step is not part of the current run, it is always assumed that the condition for that input step is met.
##### Adding Conditional Workflow for Steps
To add a conditional workflow for a step:
  1. In the `inputSteps` section of a step, add the `status` property.
  2. Add any of these values:
    * `success`
    * `failure`
    * `error`
    * `cancelled`
    * `skipped`
    * `unstable`
    * `timeout`
### Note
Ensure that the values are in lowercase and use the same spelling as shown above. Any deviation from this will cause the pipeline source sync to fail.
 **Example** : In this example:
    * step_B has only one status: `success`
    * step_C has multiple statuses: `failure`, `skipped`, `cancelled`
[YAML]          - name: step_A
            type: Bash
            configuration:
                      allowFailure: boolean         #optional
              inputSteps:
                - name: step_B
                  status:
                    - success
                - name: step_C
                  status:
                    - failure
                    - skipped
                    - cancelled
[/YAML]
  3.  **allowFailure**
 **Optional** : If you do not want a particular step to contribute to the final status of the run, you can add `allowFailure: true` to the `configuration` section of that step. When this option is used, even when a step fails or is skipped, the final status of the run is not affected.
For example, a pipeline contains two steps S1 and S2:  
    *  **Scenario 1** : Step S2 is a cleanup step and its status is irrelevant. The overall run status should be determined by S1’s status and S2’s status should be ignored. In this case, add `allowFailure: true` to S2, since this is purely a cleanup step and only S1’s status should be taken into consideration.
    *  **Scenario 2** : Step S1 has been configured to fail as part of the workflow. However S2 runs even if S1 fails and the run is not to be considered a failure. The run’s final status should mirror S2’s status since S1’s status does not interrupt the flow. In this case, add `allowFailure: true` to S1 since S1’s failure is a known possibility and expected, and that should not affect the final status of the run.


For more examples, see allowFailure Examples.
#### Run Variable Conditional Workflow
Create a condition based on the values of `add_run_variables` environment variable, so that a step can be skipped based on dynamically set variables before it gets assigned to a node.
### Note
When using a `condition`, boolean values must be enclosed in quotes.
 **Examples** :
  * `condition: 'trigger == ""true""'`
  * `condition: ""trigger == 'true'""`
  * `condition: trigger == 'true'`
  * `condition: trigger == ""true""`


### Note
Pipelines environment variables cannot be added as a condition.Pipelines Environment Variables
 **YAML Schema**
[YAML]
        steps:
          - name: <step_name>
            type: <step_type>
            execution:
              onExecute:
                - add_run_variables 'key=value'
          - name: <step_name>
            type: <step_type>
            configuration:
              condition: 'key == value' // Any logical boolean expression that results in a boolean
              inputSteps:
                - name: <step_name>
[/YAML]
 **Example**
[YAML]
    pipelines:
      - name: Example
        steps:
          - name: step1
            type: Bash
            execution:
              onExecute:
                - echo 'step1'
                - add_run_variables 'var1=1'
          - name: step2
            type: Bash
            configuration:
              condition: 'var1 == 1' // Any logical boolean expression that results to a boolean
              inputSteps:
                - name: step1
            execution:
              onExecute:
                - echo 'success'
[/YAML]
#### Environment Variables Conditional Workflow
Create a conditional workflow based on environment variables defined in the configuration section of your pipelines YAML file. The step executes when the declared condition is met.
 **Example**
[YAML]
    pipelines:
      - name: myPipelines
        configuration:
          environmentVariables:
            readOnly:
              new_env:
                default: 1
                allowCustom: true
        steps:
          - name: step1
            type: Bash
            configuration:
              environmentVariables:
                  new_env:
                    default: 2
                    #allowCustom: true
              condition: new_env == 2
            execution:
              onExecute:
                - echo $new_env
          - name: step2
            type: Bash
            configuration: 
              condition: new_env == 1
              inputSteps:
                - name: step1
            execution:
              onExecute:
                - echo 'success'
[/YAML]
#### newVersionOnly Conditional Workflow
Whenever a resource undergoes a change, its version is updated and the dependent step is triggered. This is the default behavior for all input resources. To skip steps in a run when input resources are not updated, add the `newVersionOnly` tag and set it as `true`. During a run, the step is triggered only when the resource is updated. If the resource is not updated, the step is skipped and all the downstream steps are skipped as well.
 **Example 1 - newVersionOnly**
[YAML]
    pipelines:
      - name: java_pipeline
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: my_app_repo
                - newVersionOnly: true
            execution:
              onExecute:
                - pushd $res_my_app_repo_resourcePath
                - ./execute.sh
                - popd
[/YAML]
 **Example 2 - newVersionOnly**
[YAML]
    resources:
      - name: new_resource
        type: PropertyBag
        configuration:
          runNumber: 0
    
    pipelines:
      - name: pipeline_01
        steps:
          - name: step_input
            type: Bash
            configuration:
              outputResources:
                - name: new_resource
            execution:
              onExecute:
                #- write_output new_resource runNumber=${run_number}
                - echo ""test""
    
          - name: step1      
            type: Bash
            configuration:
              inputResources:
                - name: new_resource
                  newVersionOnly: true
            execution:
              onExecute:
                - echo ""test""
    
          - name: step2
            type: Bash
            configuration:
              inputResources:
                - name: new_resource
            execution:
              onExecute:
                - echo ""test""
[/YAML]  
#### Viewing Run Logs
When you run a pipeline, in addition to the other logs, the logs for steps with conditional workflow provide information about the skipped steps.
To view these logs, go to the Pipeline Run Logs view, click the skipped step to display the logs for the current run.  
#### Examples
##### Example 1
In this example:
  * Step B is triggered only if step A succeeds (default behavior), and step C is triggered only if step A is in failed, error, or timeout status.
  * Step B does not need any special configuration as the default behavior is to trigger a dependent step if the previous step succeeds.
  * Step A also does not need any special configuration since the step itself does not decide the downstream workflow path.

  
 **YAML**
[YAML]
      - name: demo_conditional
        steps:
          - name: step_A
            type: Bash
            configuration:
              inputResources:
                - name: script_conditional
            execution:
              onExecute:
                - echo ""Executing step_A""
                - printenv
         
          - name: step_B
            type: Bash
            configuration:
              inputSteps:
                - name: step_A
            execution:
              onExecute:
                - echo ""Executing step_B""
                - printenv
    
          - name: step_C
            type: Bash
            configuration:
              inputSteps:
                - name: step_A
                  status:
                    - failure
                    - error
                    - timeout
            execution:
              onExecute:
                - echo ""Executing step_C""
                - printenv
[/YAML]
##### Example 2
In this example, Step S is triggered if Step Q succeeds and Step R fails. However, if both Step Q and Step R succeed or fail during the run, Step S is not triggered and it is skipped.  
 **YAML**
[YAML]
          - name: step_S
            type: Bash
            configuration:
              inputSteps:
                - name: step_Q
                  status:
                    - success
                - name: step_R
                  status:
                    - failure
            execution:
              onExecute:
                - echo ""Executing step_S""
                - printenv
[/YAML]
##### Example 3
In this example, Step O is triggered if Step M succeeds and Step N fails. However, since Step N is not part of the current run, Step O is triggered when Step M succeeds and Step N's status is ignored.  
 **YAML**
[YAML]
          - name: step_O
            type: Bash
            configuration:
              inputSteps:
                - name: step_M
                  status:
                    - success
                - name: step_N
                  status:
                    - failure
            execution:
              onExecute:
                - echo ""Executing step_O""
                - printenv
[/YAML]
##### Example 4 - Using Environment Variable
The `step_<inputStepName>_statusName`, which is an environment variable that is automatically made available at runtime, can be used in conjunction with conditional workflows. This `step_<inputStepName>_statusName` environment variable is useful for fetching the status of any input step, especially when working with Jenkins.
 **YAML**
[YAML]
    resources:
      - name: script_gh
        type: GitRepo
        configuration:
          path: jfrog/sample-script
          gitProvider: myGithub
          branches:
            include: ^{{gitBranch}}$
    
    pipelines:     
      - name: simple_jenkins_demo
        steps:
          - name: jenkins
            type: Jenkins
            configuration:
              inputResources:
                - name: script_gh
              jenkinsJobName: testPipeline
              integrations:
                - name: myJenkins
          
          - name: step_A
            type: Bash
            configuration:
              inputSteps:
                - name: jenkins
                  status:
                    - failure
                    - error
                    - timeout
            execution:
              onExecute:
                - echo ""Executing step_A""
                - if [ $step_jenkins_statusName == ""failure"" ]; then  echo ""Do something""; fi
                - if [ $step_jenkins_statusName == ""error"" ]; then  echo ""Do something else""; fi
         
          - name: simple_conditional_B
            type: Bash
            configuration:
              inputSteps:
                - name: jenkins
                  status:
                    - failure
                    - error
            execution:
              onExecute:
                - echo ""Executing simple_conditional_B""
                - printenv
    
[/YAML]
#### allowFailure Examples
##### Example 1
Step1 is configured for success and step2 for failure. Step2 is allowed to run when step1 fails and the final status of the run is success.
[YAML]
    pipelines:
      - name: PIPE_9455_Workflow_03
        steps:
          - name: step1
            type: Bash
            execution:
              onExecute:
                - echo 'step1'
          - name: step2
            type: Bash
            configuration:
              allowFailure: true
              inputSteps:
              - name: step1
                status:
                  - success
                  - error
                  - failure
                  - timeout
            execution:
              onExecute:
                - echo 'success'
                - exit 1
[/YAML]
##### Example 2
Step1 is configured for failure and step2 for success. Step2 is allowed to run when step1 fails and the final status of the run is success.
[YAML]
    pipelines:
      - name: PIPE_9455_Workflow_05
        steps:
          - name: step1
            type: Bash
            configuration:
              allowFailure: true
            execution:
              onExecute:
                - echo 'step1'
                - exit 1
          - name: step2
            type: Bash
            configuration:
              inputSteps:
              - name: step1
                status:
                  - success
                  - error
                  - failure
                  - timeout
            execution:
              onExecute:
                - echo 'success'
[/YAML]
##### Example 3
Step1 is configured for success and step2 for failure. When triggered, the final status of the run is failure.
[YAML]
    pipelines:
      - name: PIPE_9455_Workflow_03
        steps:
          - name: step1
            type: Bash
            configuration:
              allowFailure: true
            execution:
              onExecute:
                - echo 'step1'
          - name: step2
            type: Bash
            configuration:
              inputSteps:
              - name: step1
                status:
                  - success
                  - error
                  - failure
                  - timeout
            execution:
              onExecute:
                - echo 'failure'
                - exit 1
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
235,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Approval Gates enable you to insert a manual approval process for a step in your pipeline. When enabled, the step (and the pipeline run) goes into **Pending Approval** status when upstream steps finish and the run execution reaches that step. The following can be defined as part of this approval process:
  *  **List of approvers** : List of users who can approve or reject the step.
  *  **List of notifications** : Notifications that are sent when the step goes into Pending Approval status.
  *  **Timeout** : The maximum time the step can stay in Pending Approval status.


[/INST]#### Configuring Approval Gates Step
To enable Approval Gates, add a new section to your  step configuration named `requiresApproval`.
### Note
By default, Approval Gates is disabled. To enable it, add the `requiresApproval` section to your step configuration.
##### Using the Defaults
To enable Approval Gates and use the default values for a step, add the `requiresApproval` flag to your step configuration and set it as `true`.
In this case, the following defaults are applied to the Pending Approval step:
  *  **approvers** : Any user with execute permissions can approve or reject a Pending Approval step.
  *  **notifications:** No notifications are sent.
  *  **timeoutSeconds** : Defaults to 86400 seconds (24 hours), after which the step is canceled automatically.


 **Example**
 **Approval Gates Configuration with Default Values**
[YAML]
    pipelines:
      - name: MyApprovalGatesPipeline
        steps:
          - name: approvalGatesStep
            type: Bash
            configuration:
              requiresApproval: true
            execution:
              onExecute:
                - echo ""executing step...""
[/YAML]
##### Customizing Your Configuration
Alternatively, you can define your own `requiresApproval` configuration section and define the following:
  *  **approvers** : A list of Artifactory user names that can approve or reject the step.
  *  **notifications:** A list of SMTP and Slack integrations. Based on the configuration, SMTP and Slack notifications are sent when the step goes into Pending Approval status.
    * The listed name for the notifications integration should be an existing SMTP Credentials Integration or Slack Integration.
    * The SMTP Credentials Integration requires a list of e-mail recipients to which an approval request will be sent.
    * A Slack Integration does not require a list of recipients, since the webhook URL defined in the integration already points to a Slack user or channel to which the notification will be sent.
  *  **timeoutSeconds** : A window of time, in seconds, in which the approvers can approve or reject the step. If no response is recorded during this timeout period, the step is canceled automatically.


### Note
You do not have to include all 3 sections when customizing a step's `requiresApproval` configuration section. If you do not include one of the sections, this section defaults to the behavior defined above. For example, if you do not provide a configuration for `requiresApproval.timeoutSeconds` _,_ it defaults to 86400 seconds (24 hours).
### Note
  * The first user that responds from the approvers' list determines if the step is approved or rejected.
  * Pipelines Admins can approve or reject the step without being listed on the approvers' list.
  * Artifactory transient users are currently not supported as approvers.


 **Example**
 **Approval Gates Configuration with Custom Values**
[YAML]
    pipelines:
      - name: MyApprovalGatesPipeline
        steps:
          - name: approvalGatesStep
            type: Bash
            configuration:
              requiresApproval:
                timeoutSeconds: 43200 # if not approved in this timeframe, the step will be cancelled
                approvers:
                  - user1 # must be a valid Artifactory user
                  - user2 # must be a valid Artifactory user
                notifications:
                  - integrationName: SMTP_Integration # requires SMTP integration
                    recipients:
                      - email@example.com
                  - integrationName: Slack_Integration # required Slack integration
            execution:
              onExecute:
                - echo ""executing step...""
[/YAML]
#### Environment Variables
When approving a step in approval gate, the approver's username, comment and timestamp can be found in the following environment variables:
  * `step_approved_by`
  * `step_approved_comment`
  * `step_approved_at`


#### Approving/Rejecting Steps
After a step moves to Pending Approval state, the approvers can approve or reject the step using the user interface.
To approve or reject a step:
  1. Go to **Applications** | **My Pipelines** and run the relevant pipeline.
This displays Pipeline Run page.
  2. Click the _Approval_ step (called `approvalGatesStep` in the example yaml above). This step will be in **Pending** status.
  3. Click the **Approve** / **Reject** button to reveal the following options:
    *  **Approve/Reject** button: Click to approve or reject the step.
    *  **Comments** field: This optional field enables you to add a meaningful comment for approving or rejecting the step. This has a 100 characters limit.
    *  **Approvers** list: This lists the first letter of Artifactory usernames for users who were listed as approvers in the pipelines yaml. Hovering over a letter shows the full Artifactory username.

  
Approving the step moves it from **Pending Approval** status to **Waiting** status. The step then gets picked up for execution by the next available execution node.
Rejecting the step moves it from **Pending Approval** status to **Cancelled** status.
### Note
  * When a step is rejected, the subsequent steps are skipped resulting in the run being canceled.
  * When a step that is awaiting approval/rejection is canceled, that step and the subsequent steps are canceled resulting in the run being canceled.
  * If a step is not approved/rejected in the stipulated amount of time, it times out and the run is canceled.


Approving/Rejecting Steps - Using Old UI
After a step moves to Pending Approval state, the approvers can approve or reject the step using the user interface.
To approve or reject a step:
  1. Go to **Applications** | **My Pipelines** and click the relevant pipeline.
This displays Pipeline Run page and you'll see the _Approval_ step (called `approvalGatesStep` in the example yaml above) in **Pending** status.
  2. Click the Pending step in the Steps view to go to the step page.
This displays the Approve/Reject button for the step.
  3. Click the **Approve** / **Reject** button to reveal the following options:
    *  **Approve/Reject** button: Click to approve or reject the step.
    *  **Comments** field: This optional field enables you to add a meaningful comment for approving or rejecting the step. This has a 100 characters limit.
    *  **Approvers** list: This lists the first letter of Artifactory usernames for users who were listed as approvers in the pipelines yaml. Hovering over a letter shows the full Artifactory username.


Approving the step moves it from **Pending Approval** status to **Waiting** status. The step then gets picked up for execution by the next available execution node.
Rejecting the step moves it from **Pending Approval** status to **Cancelling** status, and eventually the step moves to **Cancelled** status.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
236,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>One of the key features of JFrog Pipelines is the ability to create interconnected pipelines, called a pipeline of pipelines, that can span several different teams, technology platforms, and tools, enabling teams to collaborate on interdependent projects.
Consider the different stages of the SDLC, such as dev, test, deployment to staging, deployment to prod, and so on, with each team creating tens or even hundreds of individual software pipelines. Pipeline of pipelines enables you to easily connect these pipelines to create an end-to-end workflow and exchange information across the pipelines. There is no limit to the number of pipelines that can be interconnected. Pipeline of pipelines can be easily created using global resources, where a pipeline updates a resource, which is used as the input for triggering downstream pipelines.
After a pipeline of pipelines is created, you can view the end-to-end workflow in real-time, interactive Pipelines dashboard, with alerts and notifications, to easily identify and escalate bottlenecks and failures.
[/INST]#### Creating Pipeline of Pipelines
To create a pipeline of pipelines, you need:
  * At least two pipelines
  * A resource that connects these pipelines


Just like how dependent steps are connected using resources, dependent pipelines are also connected using resources.
In the image below, the first pipeline (app_dev_pipeline) ends with the output resource `app_buildinfo`. The second pipeline (app_prod_pipeline), which starts with the same resource, uses it as an input resource.
### Note
By default, during a run, a dependent pipeline is triggered only when there is a new version of the resource that connects the pipelines.  
##### Examples
Here are two examples showing pipeline of pipelines.
###### Example 1
This example shows a pipeline of pipelines containing eight pipelines, interconnected using the PropertyBag resource.  
In this example:
  * There are three PropertyBag resources:
Resource Definition
[YAML]    resources:
      - name: runpopprop
        type: PropertyBag
        configuration:
          key1: value1
          key2: value2
      - name: runpopprop2
        type: PropertyBag
        configuration:
          key1: value1
          key2: value2
      - name: runpopprop3
        type: PropertyBag
        configuration:
          key1: value1
          key2: value2
[/YAML]


  * There are eight interconnected pipelines:
Pipelines Definition
[YAML]    pipelines: 
      - name: TRIGGER
        steps:
          - name: TRIGGER
            type: Bash
            configuration:
              outputResources:
                - name: runpopprop
            execution:
              onExecute:
                - write_output runpopprop runId=$run_number
      - name: TRIGGER_OUT_1_1
        steps:
          - name: TRIGGER_OUT_1
            type: Bash
            configuration:
              inputResources:
                - name: runpopprop
              outputResources:
                - name: runpopprop3
            execution:
              onExecute:
                - printenv
                #- write_output runpopprop3 runId=$run_number            
      - name: TRIGGER_OUT_3_3
        steps:
          - name: TRIGGER_OUT_1
            type: Bash
            configuration:
              inputResources:
                - name: runpopprop3
            execution:
              onExecute:
                - printenv
      - name: TRIGGER_OUT_1_2
        steps:
          - name: TRIGGER_OUT_2
            type: Bash
            configuration:
              inputResources:
                - name: runpopprop
              outputResources:
                - name: runpopprop2
              environmentVariables:
                SOME_KEY: 'asd'
            execution:
              onExecute:
                - printenv
      - name: TRIGGER_OUT_1_3
        steps:
          - name: TRIGGER_OUT_2
            type: Bash
            configuration:
              inputResources:
                - name: runpopprop
              environmentVariables:
                SOME_KEY: 'asd'
            execution:
              onExecute:
                - printenv
      - name: TRIGGER_OUT_1_4
        steps:
          - name: TRIGGER_OUT_2
            type: Bash
            configuration:
              inputResources:
                - name: runpopprop
              outputResources:
                - name: runpopprop2
              environmentVariables:
                SOME_KEY: 'asd'
            execution:
              onExecute:
                - printenv
      - name: TRIGGER_OUT_3_1
        steps:
          - name: TRIGGER_OUT_2
            type: Bash
            configuration:
              inputResources:
                - name: runpopprop2
              environmentVariables:
                SOME_KEY: 'asd'
            execution:
              onExecute:
                - printenv
      - name: TRIGGER_OUT_3_2
        steps:
          - name: TRIGGER_OUT_2
            type: Bash
            configuration:
              inputResources:
                - name: runpopprop2
              environmentVariables:
                SOME_KEY: 'asd'
            execution:
              onExecute:
                - printenv
[/YAML]
  * The pipeline named `TRIGGER` generates an output resource named `runpopprop`.
  * The resource `runpopprop` is an input resource for these pipelines:
    * `TRIGGER_OUT_1_1`
    * `TRIGGER_OUT_1_2`
    * `TRIGGER_OUT_1_3`
    * `TRIGGER_OUT_1_4`
  * The `TRIGGER` pipeline uses write_output utility function to trigger these four pipelines.
  * The resource `runpopprop2` connects the pipelines `TRIGGER_OUT_1_2` and `TRIGGER_OUT_1_4` to the pipeline `TRIGGER_OUT_3_2`, but does not trigger it.
  * Similarly, the output resource `runpopprop3` from the pipeline `TRIGGER_OUT_1_4` does not trigger the pipeline `TRIGGER_OUT_1_4`.


Your browser does not support the HTML5 video element
###### Example 2
This example shows a pipeline of pipelines containing five pipelines, interconnected using several types of resources.
In this example:
  * These resources are used to connected the pipelines: GitRepo, PropertyBag, Image, BuildInfo, ReleaseBundle, DistributionRule.
Click here to expand...
[YAML]    resources:
      - name: myGithub
        type: GitRepo
        configuration:
          gitProvider: myGithub
          path: RjSH1/pop
          #branches:
            #include: {{gitBranch}}
      - name: myBashPropertyBag
        type: PropertyBag
        configuration:
          key1: value1
          key2: value2
      - name: myPowerShellPropertyBag
        type: PropertyBag
        configuration:
          key1: value1
          key2: value2        
      - name: myImageResource
        type: Image
        configuration:
          registry: myArtifactory
          sourceRepository: d-docker-local
          imageName: icarusjuly30.jfrogdev.org/d-docker-local/myimage
          imageTag: mytag
      - name: myImageBuildInfo
        type: BuildInfo
        configuration:
          sourceArtifactory: myArtifactory
          buildName: my_build
          buildNumber: 1
      - name: myImageReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistribution
          name: demo_rb
          version: v1.0.0
      # Signed version of the same release bundle
      - name: mySignedReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistribution
          name: demo_rb
          version: v1.0.0      
      - name: myDistributionRule
        type: DistributionRule
        configuration:
          sourceDistribution: myDistribution
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
[/YAML]
  * There are five interconnected pipelines:
Click here to expand...
[YAML]    pipelines:
      - name: image_build
        steps:
          - name: docker_build
            type: DockerBuild
            configuration:
              affinityGroup: docker_group
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: icarusjuly30.jfrogdev.org/d-docker-local/myimage
              dockerImageTag: ${run_number}
              inputResources:
                - name: myGithub
              integrations:
                - name: myArtifactory
          - name: docker_push
            type: DockerPush
            configuration:
              affinityGroup: docker_group
              targetRepository: d-docker-local
              autoPublishBuildInfo: true
              integrations:
                - name: myArtifactory                        
              inputSteps:
                - name: docker_build
              outputResources:
                - name: myImageResource
                - name: myImageBuildInfo
      - name: bash_ci
        steps:
          - name: bash_ci
            type: Bash
            configuration:
              inputResources:
                - name: myGithub
                - name: myImageResource
              outputResources:
                - name: myBashPropertyBag
            execution:
              onExecute:
                - echo ""Testing the image...""
                - echo ""$res_myImageResource_imageTag""
                - echo ""Updating the output resource...""
                - write_output myBashPropertyBag runId=$run_number  
    
      - name: create_release
        configuration:
          dependencyMode:
            waitOnParentSuccess: true
        steps:  
          - name: bash_step
            type: Bash
            configuration:
              inputResources:
                - name: myImageBuildInfo
                - name: myBashPropertyBag
                - name: myPowerShellPropertyBag
            execution:
              onExecute:
                - echo ""Bash step""
          - name: create_bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: myImageReleaseBundle
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: true
              description: ""My Docker image Release Bundle""
              inputSteps:
                - name: bash_step
              inputResources:
                - name: myImageBuildInfo
              outputResources:
                - name: myImageReleaseBundle
      - name: sign_release
        steps:
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: myImageReleaseBundle
              outputResources:
                - name: mySignedReleaseBundle            
      - name: distribute_release
        steps:
          - name: distribute_bundle
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: mySignedReleaseBundle
                  trigger: false
                - name: myDistributionRule
[/YAML]


##### Controlling Pipeline Runs
When configuring a pipeline of pipelines, you may want to specify when a pipeline may run relative to other pipelines connected by resources.
The `dependencyMode` tag, which can be defined in the `pipelines.configuration` section in the Pipelines YAML, specifies when a pipeline may run relative to other pipelines connected by resources. It has three settings: `waitOnParentComplete`, `waitOnParentSuccess`, and `waitOnChildComplete`. If any of these settings is true, new runs will not be created for resources updated by other pipelines if a run is already waiting with the same resources and steps. So if a pipeline (named Pipeline_A) runs twice consecutively and the following pipeline (named Pipeline_B) has `waitOnParentComplete` set as `true`, then Pipeline_B will only run once. When the pipelines do run, they will use the latest resource versions.
These tags can be used when you want to ensure that the input resources of each pipeline step are in a steady state (no runs waiting or in progress) before the step starts executing, to prevent scenarios where the artifacts pointed to by resources change in the middle of step execution. This is particularly useful in situations where a pipeline is creating an artifact that will overwrite itself. For example, a Docker image pushed with the 'latest' tag or a file uploaded each time with the same name. In these cases, some artifacts could change after the step is triggered, resulting in inconsistency in what is downloaded.
The `dependencyMode` tag has the following optional settings:
Setting
Description
Default  
`waitOnParentComplete`
If `true`, the pipeline will not start running when a pipeline that outputs a resource that is an input to this pipeline has a waiting or processing run. The pipeline will run only after the parent pipeline's run is complete.
False  
`waitOnParentSuccess`
If `true`, the pipeline will not start running when a pipeline that outputs a resource that is an input to this pipeline has a processing run or the last complete run was not successful. The pipeline will run only after the parent pipeline's run is successful. However, if the parent pipeline is not triggered, the child pipeline can still be triggered as an independent pipeline.
False  
`waitOnChildComplete`
If `true`, the pipeline will not start running when a pipeline that has an input resource that is the output of this pipeline has a waiting or processing run unless that child pipeline is waiting for this pipeline to complete. The pipeline will run only after the child pipeline's run is complete.
False
[YAML]
    pipelines:
      - name: myPipeline
        configuration:
          chronological: true/false # default false
          dependencyMode:
            waitOnParentComplete: true/false # default false
            waitOnParentSuccess: true/false  # default false
            waitOnChildComplete: true/false  # default false
        steps:
          ...
    
[/YAML]  
### Note
Though not recommended, if the parent pipeline has either `waitOnParentComplete` or `waitOnParentSuccess` and the child pipeline has `waitOnChildComplete`, the parent pipeline runs first.
###### Examples
Example 1 - `waitOnParentComplete` and `waitOnParentSuccess`
In this example, the first pipeline **A_parent** creates a Docker image. The child pipeline **A_waitOnParent** waits until the parent pipeline **A_parent** 's run completes successfully, since both `waitOnParentComplete` and `waitOnParentSuccess`are set as`true` for **A_waitOnParent**.
[YAML]
    resources:
      - name: dependencyModeImage
        type: Image
        configuration:
          registry: myDocker
          imageName: jfrog/test
          imageTag: ""0""
    
      - name: dependencyModeRepo
        type: GitRepo
        configuration:
          gitProvider: myGithub
          path: jfrog/dependencyMode
          branches:
            include: ^{{gitBranch}}$
    
    pipelines:
      - name: A_parent
        steps:  
          - name: createImage
            type: Bash
            configuration:
              inputResources:
                - name: dependencyModeRepo
              outputResources:
                - name: dependencyModeImage
              integrations:
                - name: myDocker
            execution:
              onExecute:
                - cd ${res_dependencyModeRepo_resourcePath}
                - docker build -t ${res_dependencyModeImage_imageName}:${run_number} .
                - docker push ${res_dependencyModeImage_imageName}:${run_number}
                - write_output dependencyModeImage imageTag=${run_number}
    
      - name: A_waitOnParent
        configuration:
          dependencyMode:
            waitOnParentComplete: true
            waitOnParentSuccess: true
        steps:  
          - name: pullImage
            type: Bash
            configuration:
              inputResources:
                - name: dependencyModeImage
            execution:
              onExecute:
                - echo ${res_dependencyModeImage_imageName}:${res_dependencyModeImage_imageTag}
[/YAML]
Example 2 - `waitOnChildComplete`
In this example, the parent pipeline **B_waitOnChil** d waits until the child pipeline **B_Child** 's run completes, since `waitOnChildComplete`is set as`true` for **B_waitOnChild**.
[YAML]
    resources:
      - name: dependencyModePropertyBag
        type: PropertyBag
        configuration:
          runNumber: 0
    
    pipelines:
      - name: B_waitOnChild
        configuration:
          dependencyMode:
            waitOnChildComplete: true
        steps:  
          - name: updateProperties
            type: Bash
            configuration:
              outputResources:
                - name: dependencyModePropertyBag
            execution:
              onExecute:
                - write_output dependencyModePropertyBag runNumber=${run_number}
                
      - name: B_Child
        steps:  
          - name: echoProperty
            type: Bash
            configuration:
              inputResources:
                - name: dependencyModePropertyBag
            execution:
              onExecute:
                - echo ${res_dependencyModePropertyBag_runNumber}
                - sleep 5
          - name: echoComplete
            type: Bash
            configuration:
              inputSteps:
                - name: echoProperty
            execution:
              onExecute:
                - echo ""complete""
[/YAML]
#### Viewing Pipeline of Pipelines
Your pipeline of pipelines workflow can be as complex as you want it to be, with several individual pipelines that interconnect across teams. The Pipelines Graph view makes it easy to view and navigate such complex pipelines.
The Graph view:
  * Shows a combined view of all interconnected pipelines
  * Provides visualization of pipelines and steps
  * Shows the pipelines, resources, and interconnecting path that a pipeline is using
  * Provides the ability to drill down into a single pipeline
  * Provides real-time updates for a run
  * Shows project-level multi-pipeline map and list of runs


For more information, see Pipelines Graph view.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
237,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The TriggerPipeline step enables one step in a parent pipeline to ""embed"" another pipeline and wait for the resulting run to complete before returning the control back to the parent pipeline's step. The parent pipeline then continues its run with access to some additional information on the embedded run.
For example, in the image below, **Step 2** in the parent **pipeline X** triggers the embedded **pipeline Y**. When **pipeline Y** is successful, it returns control to **Step 2** in **pipeline X** , which triggers **Step 3** and **Step 4** in the pipeline.
[/INST]### Note
If the embedded run ends with a bad status (failure, timed out, cancelled), the TriggerPipeline step will end with a failure status.  
### Tip
  *  **Node Usage** : The step triggering the embedded pipeline uses a dedicated node for processing, polling, and waiting for status. Keep this in mind when designing your embedded pipeline. Depending on how often you use the step, you could find that your nodepool runs out of capacity to execute the embedded pipeline leaving you in a stuck state. If you plan to make heavy use of this step type, we recommend considering assigning your TriggerPipeline steps to a separate nodepool to restrict the overall node capacity that can be used by this step type. By using a separate node pool, you can ensure that the pipeline does not go into a deadlock.
  *  **Wait time** : TriggerPipeline steps are subject to timeout just like any other step. Keep this in mind when choosing an embedded pipeline. If your embedded pipeline takes longer than the system timeout limit, then it is likely that your TriggerPipeline step will timeout while waiting for the child to complete. One measure we have taken to avoid this is to reset the timeout timer of the TriggerPipeline step when the embedded run moves from a ""waiting"" status to a ""processing"" status.
  *  **Multiple TriggerPipeline steps** : There is no limit to how many TriggerPipeline steps you can include in your pipelines, but keep in mind that it can be easy to find yourself waiting on multiple tiers of embedded runs. Before using this step type, consider if your scenario is better suited to simply connecting two pipelines via resource.


#### Embedding Pipelines
To create and run embedded pipelines, you will need:
  * At least two build nodes
  * Create a  JFrog Platform Access Token Integration integration
  * Add the TriggerPipeline native step to your pipeline YAML


##### YAML
The yaml for embedded pipelines supports the following parameters:
### Note
Each of these fields supports placeholders like `{{gitBranch}}` or any `${environment_variable}`.
Parameter
Description
Required/Optional  
`pipelineName`
Name of the embedded pipeline that needs to be triggered.
Required  
`stepName`
Name of the step in the embedded pipeline that needs to be triggered.
Required  
`projectKey`
If the step you want to trigger belongs to a Project, it is recommended to use the projectKey to ensure the correct step is triggered.
Optional  
`branchName`
If the step you want to trigger belongs to a multibranch pipeline, then this parameter is necessary to distinguish which branch you want to trigger.
Required if target is multibranch  
 **TriggerPipeline**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: TriggerPipeline
            configuration:
              #inherits all the tags from bash;
    
              pipelineName:   <string>   # required
              stepName:       <string>   # required
              branchName:     <string>   # optional. required if target is multibranch.
              projectKey:     <string>   # optional. recommended if target belongs to a project.
              integrations:
                - name:       <JFrog Platform Token integration>  # required
    
            execution:
              onStart:
                - echo ""Preparing for work...""
                - set_trigger_payload stepVariables ""test=true""
                - set_trigger_payload pipelineVariables ""notify=true"" ""version=5.4.3""
                            - export pipelines_poll_interval_seconds=30 # defaults to 10
              onSuccess:
                - echo ""Done!""
              onFailure:
                - echo ""Something went wrong""
              onComplete:
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Example
Here are an example showing embedded pipelines.
In this example, `top_pipeline` is the parent pipeline and `scanner_pipeline` is the child pipeline.
[YAML]
    pipelines:
      - name: top_pipeline
        steps:
          - name: scan_controller
            type: TriggerPipeline
            configuration:
              pipelineName: scanner_pipeline
              stepName: scan_it
              integrations:
                - name: myPlatformToken
              environmentVariables:
                scan_target:
                  default: ""hello-world""
                  allowCustom: true
                  values:
                    - ""vault""
                    - ""redis""
                    - ""postgresql""
                    - ""hello-world""
            execution:
              onStart:
                - set_trigger_payload pipelineVariables ""scan_target=${scan_target}""
                - set_trigger_payload stepVariables ""notify=email"" ""uploadReport=true""
              onComplete:
                - echo ""Final status is $nested_run_status""
    
      - name: scanner_pipeline
        steps:
          - name: scan_it
            type: Bash
            execution:
              onExecute:
                - echo ""Image to scan is $scan_target.""
                - echo ""Triggered by parent step at $parent_step_url""
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
238,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>The performance of Pipelines can be monitored using New Relic Application Performance Monitoring (APM). When this feature is enabled, Pipelines transmits metrics to the New Relic dashboard.
Key matrices that can be monitored are:
  * App transaction
  * App's DB usage (throughput, slow query)
  * App JVM or Go monitoring - memory, thread utilization
  * App Errors


[/INST]#### Configuring New Relic Monitoring
To configure New Relic monitoring for Pipelines:
  1. Open the Pipelines System YAML.Pipelines System YAML
  2. In the **Share Configurations** section, configure the following mandatory parameters:
    *  **enabled** : Set as `true` to enable the New Relic agent.
    *  **appName** : Enter the App Name that will be displayed in the New Relic dashboard.
    *  **licenseKey** : Enter the New Relic license key.
  3. If required, configure the following optional parameters:
    *  **logging.debug** : Enable/disable New Relic Agent debug messages.
    *  **distributedTracer.enabled** : Enable/disable the New Relic distributed tracer feature.


 **Example**
[YAML]
    ## NewRelic APM configuration
    ##
    newRelic:
      enabled: true
      license: ""customer_name""
      appName: ""new-relic-license-key""
      logging:
        debug: false
      distributedTracer:
        enabled: true
[/YAML]
For more information, see Pipelines System YAML.Pipelines System YAML

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
239,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>Every step in your pipeline executes on a build node that has been provisioned with a runtime environment. Through Pipelines DSL, you can control which runtimes your steps execute in.
By breaking up your pipelines into steps, Pipelines can distribute the work to wherever it can get done on the network. An administrator user must configure Pipelines with sets of build nodes (virtual machines) that are available for executing steps.
Pipelines sends each step to a build node to execute in a **runtime** , which is either the build node's virtual machine or a container provisioned with a Docker image on the build node. The runtime must have the necessary OS, software tools, packages, and configurations that the step needs to execute.
[/INST]### Note
  *  **Running steps in series** : When steps in a pipeline must execute on the same build node, they can be assigned to the same **affinity group**. For more information, see  Running multiple steps on the same build node.
  *  **Running steps in parallel** : Steps can be run in parallel to speed up pipeline execution. For more information, see  Breaking Your Pipelines into Steps.


#### Host Runtimes
A traditionally designed CI server distributes tasks to other machines on the network, each of which has been configured with the OS, tools, and credentials that tasks need to perform their work. Each task executes in the runtime environment of a machine.
Pipelines can, when explicitly directed, execute any step this way, running it directly on the host build node in the runtime of its machine image. This may be appropriate when you need to perform a task from the baseline environment of the machine.  
While efficient, executing directly on the host is very inflexible. If the task requires another tool on the VM, the task must either load it itself or a systems administrator must configure it for the developer in advance. When an organization needs to support many teams, staging environments, and DevOps activities, this can quickly become difficult and expensive to manage. Also, changes made to the host VM will persist for future steps running on the same host, leading to unpredictability.
#### Container Runtimes
For maximum flexibility, Pipelines' preferred mode is to execute its steps in containers. For each step, Pipelines spins up a container on a build node using a Docker image that includes the runtime environment.  
Through this method, each step executes with the runtime it needs to perform its particular set of work. For example, it can have the binaries and libraries needed for a language, and/or it can have tools and CLIs that the step needs to perform automated tests.  
This system offers several key benefits:
  * Each step executes with the tools it needs, and no others, configured with the specific settings for that environment.
  * Steps in the same pipeline can execute in different runtimes, if they need a unique configuration or set of tools.
  * Steps can execute simultaneously in different build nodes (when available), for faster build times.
  * Pipelines are fully repeatable, always executing using the same set of immutable, versioned Docker images.
  * Different teams can build with different tools and configurations without impacting each other.
  * Pipelines can reliably execute with the runtime (including credentials) appropriate for the build environment, whether for Dev, Test, or Production.
  * Execute pipelines on whichever infrastructure suits your needs best: on virtual machines hosted by a cloud provider or on servers in your own datacenter.


JFrog provides a set of standard Docker images of runtimes for supported architecture, OS, and language combinations. When the Pipelines DSL doesn't specify a runtime, Pipelines provisions the build node with one of these runtime images as best matches the build node environment.
#### Concepts
A step runtime is made up of the following components:
##### Nodes
To run any step in a pipeline, you need a build node (a virtual machine) where the step will execute.
An administrator user must provide nodes and attach them to Pipelines in the JFrog Platform Deployment. A node can be on any infrastructure that you choose to use, whether it is from a cloud provider (such as AWS, GCP, or Azure), or on your own infrastructure if your security policies require your operations to remain behind your own firewall.
##### Node Pools
Node Pools logically group nodes to make them available to execute the steps in a pipeline. This enables an administrator user to group nodes according to their processor architecture and baseline operating system. It enables pipelines to run on specific node pools, and to run steps simultaneously on different build nodes.
Node pools can contain nodes of two distinct types:
###### Static Nodes
Static nodes are configured and provided by administrator users to a node pool. They operate in perpetuity, and are available to execute steps at any time. Static nodes are especially useful if you need to run your operations on build nodes in your own data center. You may need to do this if you have security policies that prohibit your code from leaving your firewall, or if your jobs require access to internal resources that are not accessible from the internet. You can also attach build nodes from a cloud provider, although you will incur charges even when the build node is idle.
###### Dynamic Nodes
Dynamic nodes are on-demand compute environments that are spun up on a cloud provider when needed, and destroyed when seen to be idle. This is an efficient method that helps minimize compute costs. Dynamic nodes are connected through a dynamic node integration that connects to an IaaS provider such as Amazon or Google.
##### Runtime Images
A runtime image is a preconfigured Docker image that includes all the necessary components and settings to run your pipeline steps in a container.
#### In This Section
This section includes the following documentation:
  * Choosing your Runtime Image
  * Running Steps on the Host
  * Choosing Node Pools
  * Runtime Images



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
240,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>By default, your steps run inside a container from a standard runtime images library maintained by JFrog. You can control which runtime images your steps execute in through the Pipelines DSL.
If you need to execute your step directly on the build host node (not in a container), see Running Steps on the Host.
[/INST]##### Default Runtime Image
The default runtime image used to execute a Bash step is a NodeJS image that matches the OS of your node pool. For a PowerShell step, the default is a .NET Core image. In both cases, the version selected is configurable as part of the Pipelines installation and an administrator may have configured a different default image in your installation. The default is usually most appropriate when your steps will only execute shell commands that are not specific to a language.
For native steps that perform language-specific operations (for example, MvnBuild or GoBuild), the default runtime image will be one appropriate to that language. Similarly, the runtime image chosen will be one that matches the OS of your node pool. Native steps without any language-specific operations will use the same default image as Bash on Ubuntu or CentOS and the same image as PowerShell on Windows Server.
##### Choosing Language and Version
JFrog Pipelines supports Java, C++, Node.js, .NET,, and Go programming languages out of the box. JFrog provides default images in a standard runtime images library, which already have the latest language versions pre-installed. You can use Pipelines to build with other languages by providing a custom image, or installing the language version you need as part of your **onStart** configuration.
In the `runtime` section of your YAML configuration you can specify a `language` and `version` , which Pipelines will use to select the matching language image. This setting can be configured at a `pipeline` or `step` level. As an example, pipeline level configuration is shown below:
[YAML]
    pipelines:
      - name: pipeline_1
        configuration:
          runtime:
            type: image
            image:
              auto:
                language: node
                version: ""16"" # Either ""version"" or ""versions"" can be used.
                versions:
                  - ""16""
[/YAML]
Tag
Description  
`language`
Can be set to:
  * `cpp` for C/C++ steps
  * `dotnetcore` for .NET steps (Windows Server only)
  * `go` for Go steps
  * `java` for Java-based steps
  * ` node` for Node.js steps

  
`version`
Can be set to a single version of the language you want to run your build against. Remember to put the version in double quotes, since the parser expects a string. For example: ""9.0.0"".
For information about the supported versions, see Runtime Images.  
`versions`
Can be set to multiple versions of the language you want to run your build against. Remember to put the version in double quotes, since the parser expects a string. For example: ""9.0.0"".
For information about the supported versions, see Runtime Images.  
If you need an unsupported version of a language, you can either install it in the `onStart` section, or use a custom Docker image.
 **Example 1: Specify the default image**
[YAML]
    pipelines:
      - name: myjfrog_ui_docker
        configuration:
          nodePool: my_cypress
          runtime:
            type: image
            image:
              custom:
                registry: docker
                sourceRepository: apps-docker-local
                name: test.mycompany.io/apps-docker-local/ci/builder-node14
                tag: 2022.8.1-BA-8212-remove-testexecutionreportpaths-property-validation
[/YAML]
 **Example 2: Specify an image for a single step**
[YAML]
        steps
          - name: new_cypress
            type: Matrix
            stepMode: Bash
            configuration:
              affinityGroup: group4
              runtime:
                type: image
                image:
                  custom:
                    registry: docker
                    sourceRepository: docker-registry
                    name: test.mycompany.io/docker-registry/cypress/base
                    tag: 14.18.1
[/YAML]
##### Using a Custom Runtime Image
In some cases, our standard images library might not satisfy your requirements, or you might have an internal image you want to use for step execution or you may want to use our standard images library with specific settings. Using your own custom image gives you better control over what is installed on the runtime images. It can also speed up step execution since you can pre-install dependencies into the image.
You can configure a custom runtime image by using the configuration shown below. This can be specified at a pipeline or step level. If specified at both levels, the step configuration will take precedence.
[YAML]
    pipelines:
      - name: pipe1
        configuration: 
          runtime:       
            type: image
            image:
              custom:
                name: <string>                        # imageRepo/imageName format
                tag: <string>                         # version tag of the initial version of the Docker image
                options: ""-e HOME=/root""          # Docker options
                autoPull: <boolean>                   # default true; pulls image before run
                registry: <integration>       # if custom image is private, an integration is required for authentication
                sourceRepository: <path>      # may be required if registry is Artifactory. e.g. docker-local
                region: <string>              # required if registry is AWS. e.g. us-east-1
    
[/YAML]
Tag
Description  
`name`
This is the fully qualified name of the Docker image.  
`tag`
This is the Docker image tag you want to use.  
`options`
This is any Docker option you want to use in the `docker run` command. You also need to include the`$HOME`environment variable as shown if it is not already set in your image.  
`autopull`
This is an option to automatically pull the image to the machine running the step. Defaults to `true`.  
If your custom image is private, you will also need to create an integration with credentials for your Docker registry, and set additional values in the config:
Tag
Description  
`registry`
This is the friendly name of your Docker registry integration. Supported integration types are  Artifactory,  Docker Registry, AWS Keys, and Google Cloud.  
`sourceRepository`
The is not used with JFrog CLI v2.
This is only needed if your Docker registry is Artifactory and the pipeline is configured to use JFrog CLI v1. It is the name of the Docker registry on Artifactory.  
`region`
This is only required if your Docker registry is AWS ECR.  
###### Examples
 **Example 1**
[YAML]
    pipelines:
      - name: pipe1
        configuration: 
          runtime:       
            type: image
            image:
              custom:
                name: docker/jfrog_win
                tag: latest
                options: ""-e HOME=/root""
                autoPull: false
                registry: PSSG_DockerRegistry
[/YAML]
 **Example 2 - Artifactory registry**
[YAML]
    pipelines:
      - name: pipe2
        configuration: 
          runtime:       
            type: image
            image:
              custom:
                name: ubuntu
                tag: latest
                registry: myArtifactory
                sourceRepository: docker-local
[/YAML]
 **Example 3 - AWS registry**
[YAML]
    pipelines:
      - name: pipe3
        configuration: 
          runtime:       
            type: image_aws
            image:
              custom:
                name: 541601.dkr.ecr.us-east-1.amazonaws.com/jfbeta
                tag: latest
                autoPull: false
                registry: AWS
                region: us-east-1
[/YAML]
###### Minimum Requirements for Linux
Any custom runtime image specified for a step that will run in a Linux host node (Ubuntu, CentOS, or RHEL) needs to include this minimum set of components:
  * bash
  * /proc/sys/kernel/random/uuid
  * sed
  * GNU grep
  * ssh
  * coreutils
    * date
    * mkdir
    * cp
    * mv
    * touch
    * seq
    * cat
    * basename (for users usingsend_notification (email)command)
    * rm
    * uniq (for users usingcompare_gitcommand)
    * split (for users usingencrypt_string,decrypt_string,encrypt_file,decrypt_filecommands)
  * jfrog (when using an Artifactory integration or in a native step)
  * jq
  * awk (for users usingcompare_gitcommand)
  * git (for users usingcompare_gitcommand or GitRepo resources))
  * gettext (for users usingreplace_envs,send_notification (email)commands)
    * envsubst
  * curl (for users usingsend_notificationcommand)
  * base64 (for users usingsend_notification (email),encrypt_string,decrypt_string,encrypt_file,decrypt_filecommands)
  * openssl (for users using encrypt_string,decrypt_string,encrypt_file,decrypt_file commands)


###### Minimum Requirements for Windows Server
Any custom runtime image specified for a step that will run in a Windows Server host node needs to include this minimum set of components:
  * PowerShell 5
  * jq (for users using replicate_resource or send_notification commands)
  * git (for users using compare_git command or GitRepo resources)
  * openssl (for users using encrypt_string, decrypt_string, encrypt_file, decrypt_file commands)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
241,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>When you need to execute your step directly on the host node, and not in a container, you can control this through the Pipelines DSL.
Running steps in a container on the build node ensures that a pipeline produces the same result every time it is run. The host node does not guarantee this repeatability, since the step will execute in whatever state the host was left in when it was last used. For this reason, the default for Pipelines steps is to execute in a container.
However, there may be cases where you need to execute your step directly on the host node. For example, you may need to execute Docker commands and don't want to do so from within a Docker container.
In these cases, you can configure the Pipelines DSL to execute your step or all steps of a pipeline directly on the host node.
[/INST]##### Configuring a Step
You can configure a single step to execute directly on the host node in the ` runtime` section of your step by specifying the `type` as `host` :
[YAML]
    pipelines:
      - name: pipeline_example
      steps:
        - name: step_1
          type: Bash               # may be any step type
          configuration:
            runtime:
              type: host
          execution:
            ...
[/YAML]
You should not add any `image` section; that will cause pipeline source sync to fail.
##### Configuring a Pipeline
You can configure a pipeline to execute all steps by default directly on the host node in the `runtime` section of your pipeline by specifying the `type` as `host` :
[YAML]
    pipelines:
      - name: pipeline_host
        configuration:
          runtime:
            type: host
[/YAML]
You should not add any `image` section; that will cause pipeline source sync to fail.
You may override this pipeline's default in any step with a different `runtime` specification.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
242,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A pipeline can, if necessary, control through Pipelines DSL which nodes it or any individual step will execute in.
When an administrator user sets up node pools in Pipelines, one of the node pools is designated as the default. This is the set of nodes that all pipeline steps will execute in when no explicit node pool is specified by the pipeline or step.
If you need to run your pipeline steps in a specific node pool, you can specify this in your pipeline's DSL. You can also configure any step to run on a node from a specific node pool.
You may wish to specify a node pool when:
  * Your pipeline or step is resource-intensive, so you need to execute in a specific node pool that uses more powerful machines
  * You need to run the pipeline or step on a specific OS (e.g., Ubuntu or CentOS)
  * You need to run a pipeline or step in a specific node pool for security (for example, behind a firewall in your own data center)


[/INST]### Note
Steps that are specified to  run in the same `affinityGroup` must always run in the same node pool.
##### Node Pools
Your Pipelines configuration must have at least one node pool, with at least one node, for pipelines to execute. These must be set up by an administrator user, with one node pool set as the default. A typical configuration will have been set up with multiple node pools to provide different choices for operating system and computing platforms.
For example, an adminstrator user may have set up these node pools and made them available to you for executing your pipelines:
  1. A static node pool of Ubuntu 20.04 nodes running self-hosted, set as the default node pool
  2. A dynamic node pool of Ubuntu 20.04 nodes running on Amazon Web Services
  3. A static node pool of CentOS 7 nodes running self-hosted


You can view the node pools available to you from the Pipelines Node Pools tab:  
##### Example Pipeline
In your pipeline's DSL you can designate the pipeline or any individual steps to run in a node pool by its name.
  *  **Pipeline node pool** : You can specify a specific node pool for the pipeline in the `configuration` block of the pipeline, using the `nodePool` tag. When specified here, all steps in the pipeline will execute in nodes from that node pool.
  *  **Step node pool** : You can specify a specific node pool for any step by declaring the `nodePool` tag in the step's `configuration` block. This will override any node pool designation for the pipeline.


The following example pipeline specifies that all steps of the pipeline will execute in nodes from the dynamic node pool `ubuntu20aws` on Amazon Web Services. The second step, however, overrides this as it is configured to run on an self-hosted node from the static node pool `ubuntu20`.
 **pipelines.yml**
[YAML]
    pipelines:
      - name: select_nodepool_example
        configuration:
          nodePool:   ubuntu20aws                       # Run in the dynamic node pool on AWS
        steps:
          - name:nodePool_step1
            type: Bash
            execution:
              onExecute:
                - echo ""$step_name is running in $step_platform on $step_node_pool_name""
     
          - name: nodePool_step2
            type: Bash
            configuration:
              nodePool:   ubuntu20                       # Run in the static node pool self-hosted
              inputSteps:
                - name: nodePool_step1
            execution:
              onExecute:
                - echo ""$step_name is running in $step_platform on $step_node_pool_name""
    
          - name:nodePool_step3
            type: Bash
            configuration:
              inputSteps:
                - name: nodePool_step1
            execution:
              onExecute:
                - echo ""$step_name is running in $step_platform on $step_node_pool_name""
    
[/YAML]
When the pipeline is run:
  * Steps `nodePool_step1` and `nodePool_step3` will execute on nodes in `ubuntu20aws`
  * Step `nodePool_step2` will execute on a node in `ubuntu20`



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
243,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Always answers only users jFrog pipline related questiions.You will say i am not sure for other general questions <</SYS>>A runtime image is the Docker image for the container that will run your step(s).
JFrog provides a set of base runtime images stored in the Artifactory instance `releases.jfrog.io in the _docker_ repository`. When Pipelines is installed using default settings, Pipelines draws runtime images from this registry. We highly recommend only referencing the major version (see https://semver.org/ ) of a language for the image version to get newest updates for it as well as keeping up with the LTS versions. You can reference the minor as well as the patch version, but those tags will not receive updates.
Full Reference for using runtime images here: Bash.
 **Example of using one of the below runtime images** Expand source
[YAML]
    - name: go_app_build
      type: Bash
      configuration:
        runtime:
          type: image
          image:
            auto:
              language: go
              versions:
                - ""1.17""
[/YAML]
 **Example of using a pinned version that's available on releases.jfrog.io** Expand source
[YAML]
    - name: node_app_build
      type: Bash
      configuration:
        runtime:
          type: image
          image:
            auto:
              language: node
              versions:
                - ""14.19.1""
[/YAML]
[/INST]##### Current Docker Images
c
go
java
node  
Ubuntu 18
releases-docker.jfrog.io/jfrog/pipelines-u18cpp:8
releases-docker.jfrog.io/jfrog/pipelines-u18cpp:9
releases-docker.jfrog.io/jfrog/pipelines-u18go:1.17
releases-docker.jfrog.io/jfrog/pipelines-u18go:1.18
releases-docker.jfrog.io/jfrog/pipelines-u18go:1.19
releases-docker.jfrog.io/jfrog/pipelines-u18java:8
releases-docker.jfrog.io/jfrog/pipelines-u18java:11
releases-docker.jfrog.io/jfrog/pipelines-u18java:17
releases-docker.jfrog.io/jfrog/pipelines-u18node:12
releases-docker.jfrog.io/jfrog/pipelines-u18node:14
releases-docker.jfrog.io/jfrog/pipelines-u18node:16  
Ubuntu 20
releases-docker.jfrog.io/jfrog/pipelines-u20cpp:8
releases-docker.jfrog.io/jfrog/pipelines-u20cpp:9
releases-docker.jfrog.io/jfrog/pipelines-u20go:1.17
releases-docker.jfrog.io/jfrog/pipelines-u20go:1.18
releases-docker.jfrog.io/jfrog/pipelines-u20go:1.19
releases-docker.jfrog.io/jfrog/pipelines-u20java:8
releases-docker.jfrog.io/jfrog/pipelines-u20java:11
releases-docker.jfrog.io/jfrog/pipelines-u20java:17
releases-docker.jfrog.io/jfrog/pipelines-u20node:12
releases-docker.jfrog.io/jfrog/pipelines-u20node:14
releases-docker.jfrog.io/jfrog/pipelines-u20node:16
releases-docker.jfrog.io/jfrog/pipelines-u20node:18  
CentOS 7
releases-docker.jfrog.io/jfrog/pipelines-c7cpp:3
releases-docker.jfrog.io/jfrog/pipelines-c7go:1.17
releases-docker.jfrog.io/jfrog/pipelines-c7go:1.18
releases-docker.jfrog.io/jfrog/pipelines-c7go:1.19
releases-docker.jfrog.io/jfrog/pipelines-c7java:8
releases-docker.jfrog.io/jfrog/pipelines-c7java:11
releases-docker.jfrog.io/jfrog/pipelines-c7java:17
releases-docker.jfrog.io/jfrog/pipelines-c7node:12
releases-docker.jfrog.io/jfrog/pipelines-c7node:14
releases-docker.jfrog.io/jfrog/pipelines-c7node:16  
Windows Server 2019
releases-docker.jfrog.io/jfrog/pipelines-w19cpp:8
releases-docker.jfrog.io/jfrog/pipelines-w19cpp:9
releases-docker.jfrog.io/jfrog/pipelines-w19go:1.17
releases-docker.jfrog.io/jfrog/pipelines-w19go:1.18
releases-docker.jfrog.io/jfrog/pipelines-w19go:1.19
releases-docker.jfrog.io/jfrog/pipelines-w19java:8
releases-docker.jfrog.io/jfrog/pipelines-w19java:11
releases-docker.jfrog.io/jfrog/pipelines-w19java:17
releases-docker.jfrog.io/jfrog/pipelines-w19node:12
releases-docker.jfrog.io/jfrog/pipelines-w19node:14
releases-docker.jfrog.io/jfrog/pipelines-w19node:16  
dotnet  
Windows Server 2019
releases-docker.jfrog.io/jfrog/pipelines-w19dotnetcore:3
releases-docker.jfrog.io/jfrog/pipelines-w19dotnet:5
releases-docker.jfrog.io/jfrog/pipelines-w19dotnet:6  
###### CLIs Installed on Above Images
Linux CLIs
Windows CLIs  
aws 1.19.103
azure 2.25.0
docker-compose 1.29.2
gcloud 348.0.0
git 2.27.0
helm 2.16.9
helm 3.6.2
jfrog 1.52.0
kubectl 1.21.0
packer 1.7.3
python 3.8.10
terraform 1.0.4
aws 1.19.103
azure 2.25.0
docker-compose 1.29.2
gcloud 348.0.0
git 2.27.0
helm 2.16.9
helm 3.6.2
jfrog 1.53.1
kubectl 1.21.0
packer 1.7.3
python 3.8.3
terraform 1.0.4  
##### Retired App/Image Versions
os
go
node
java
dotnet  
Ubuntu 16
*
*
*
N/A  
CentOS 8
*
*
*
N/A  
Ubuntu 18
< 1.17
< 12
13
N/A  
Ubuntu 20
< 1.17
< 12
13
N/A  
CentOS 7
< 1.17
< 12
13
N/A  
Windows Server 2019
< 1.17
< 12
13
< dotnetcore 3  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
