,text
6,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pipeline-example-hello-world?[/INST] This example demonstrates how simple pipelines can be defined and executed with JFrog Pipelines. An example Pipelines DSL is used to show how to use integrations, resources, and steps to construct a simple, automated workflow.
This pipeline example demonstrates the following:
  * Creating a GitHub Integration.
  * Adding a Pipeline Source.
  * Creating a GitRepo trigger, which will trigger a step when the contents of the source control repository change.
  * Using inputResources and inputSteps to set up dependencies between steps and resources.
  * Using environment variables (e.g. $res_myFirstRepo_commitSha) to extract information from `inputResources`.
  * Using run state to pass information to downstream steps of a run.
  * Using pipeline state to pass information to subsequent runs.
  * Connecting dependent pipelines through resources


Successful runs of the pipeline in this quickstart look like this:  
#### Before you Begin
Before trying this quickstart, ensure that you have:
  * A GitHub account. This is required for forking the example repository.
  * A JFrog Platform Cloud account, or self-hosted JFrog Pipelines.
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools.
  * A user account in Artifactory with deploy permissions to at least one binary repository.


#### Running this pipeline
Perform the following steps to run this pipeline:
  1. ##### Fork repository
The Pipelines DSL for this example is available in the JFrog GitHub repository repository in the JFrog GitHub account.
The DSL file is a yaml file that contains the pipeline definitions. This example uses two YAML files:
    * `jfrog-pipelines-hello-world.yml`, which contains the declarations for the pipelines in this example
    * `values.yml`, which contains the values required for the `jfrog-pipelines-hello-world.yml` file.
For a full breakup of all the resources, pipelines and steps used in the yml file, see the jfrog-pipelines-hello-world.yml section below. Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as Pipeline Sources or GitRepo resources, in order to add webhooks to these repositories and listen for change events.
  2. ##### Sign in
Sign in to JFrog Platform with your Artifactory credentials.
  3. ##### Add integration
Go to **Administration** | **Pipelines** | **Integrations** to  add one integration:
 **GitHub Integration**: This integration is used to add the Pipeline source, as well as the GitRepo resource defined in  values.yml, to connect Github to Pipelines. Write down the GitHub integration name.
  4. ##### Update values.yml
The pipelines configuration is available in the values.yml file. Edit this file in your fork of this repo and replace the following:
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in the previous step.
`gitProvider: my_github`  
`path`
Provide the path to your fork of this repository.
`path: myuser/jfrog-pipelines-hello-world`  
### Note
 **All pipeline and resource names are global across your JFrog Pipelines project.** The names of your steps and resources need to be unique within the JFrog Pipelines project.
  5. ##### Add pipeline source
The Pipeline Source represents the git repository where our pipelines definition files are stored. A pipeline source connects to the repository through an integration, which we added in step 3.
    1. In your left navigation bar, go to  **Administration** |  **Pipeline** | **Pipeline Sources**. Click on Add a Pipeline Source and then choose From YAML. Follow instructions to add a Pipeline Source. This automatically adds your configuration to the platform and pipelines are created based on your YAML.
    2. After your pipeline source syncs successfully, navigate to  **Pipelines** | **My Pipelines**  in the left navbar to view the newly added pipeline. In this example, `my_first_pipeline` and  `my_second_pipeline` are the names of our pipeline.
    3. Click the name of the pipeline. This renders a real-time, interactive, diagram of the pipeline and the results of its most current run.
  6. ##### Execute the pipeline
You can now commit to the repo to trigger your pipeline, or trigger it manually through the UI. The steps in the pipeline execute in sequence.  
Once the pipeline has completed, a new run is listed.  
Successful run of the first pipeline triggers the execution of the second pipeline:  


#### jfrog-pipelines-hello-world.yml
The `jfrog-pipelines-hello-world` `.yml` file is made up of resources, pipelines and steps, as shown below:
##### Resources
This example uses the following types of resources:
  *  **GitRepo**
  *  **PropertyBag**


 **GitRepo**
A GitReporesource is used to connect JFrog Pipelines to a source control repository. Adding it creates a webhook to the repo so that future commits will automatically create a new version with the webhook payload.
 **Resources**
[YAML]
      - name: myFirstRepo
        type: GitRepo
        configuration:
          # SCM integration where the repository is located
          gitProvider: {{ Values.myRepo.gitProvider }}
          # Repository path, including org name/repo name
          path: {{ Values.myRepo.path }} #manishas-jfrog/jfrog-pipelines-hello-world    # replace with your repository name
          branches:
            # Specifies which branches will trigger dependent steps
            include: master
    
[/YAML]
Tag
 **Description**
Required/Optional  
name
`myFirstRepo` is the name of the GitRepo resource pointing to the repository containing the yaml files and other source code required to build the image.
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
`gitProvider `
The name of the GitHub Integration. Its value is retrieved from the values.yml file.
Required  
`path `
The path of the repository from the integration root. Its value is retrieved from the values.yml file.
Required  
`branches`
  * `include `\-- (optional) Regular expression to include branches from the repo
  * `exclude` \-- (optional) Regular expression to exclude branches from the repo


The `include: master` tag indicates that the GitRepo resource is listening to the master branch.
Optional  
Defining a GitReporesource acts as the trigger for the pipeline.  
 **PropertyBag**
A PropertyBag resource is used to pass information from one pipeline to another, and to provide environment variables to a step in the format of a resource.
A PropertyBag resource can have any strings as properties, which will then be available as environment variables when the key is an input to a step. When it is an output, steps can change the values of properties or add new ones.
 **Resources**
[YAML]
      - name: myPropertyBag
        type: PropertyBag
        configuration:
          commitSha: 1
          runID: 1          
    
[/YAML]
Tag
 **Description**
Required/Optional  
name
`myPropertyBag` is the name of the PropertyBag resource, which is the metadata associated with the build in Artifactory.
Required  
`<string>`
A property for the PropertyBag resource. The tag should be a valid variable name (Bash or PowerShell) for the steps where it is to be an input or output and the value a string. At least one is required, multiple properties are allowed.
Required  
##### Pipelines
This example uses two pipelines:
  * `my_first_pipeline` is the name of the first pipeline, consisting of 3 linear steps. The last step outputs a resource of type PropertyBag.
  * `my_second_pipeline` is the name of the second pipelines, which contains a single step triggered by the PropertyBag resource updated by the first pipeline


##### Steps
Both `my_first_pipeline` and `my_second_pipeline` pipelines contain the following step type:
###### Bash
Bashis a generic step type that enables executing any shell command. This general-purpose step can be used to execute any action that can be scripted, even with tools and services that haven't been integrated with JFrog Pipelines. This is the most versatile of the steps while taking full advantage of what the lifecycle offers.
In our example:
  * `p1_s1`, `p1_s2`, `p1_s3` are the names of the Bash steps in the `my_first_pipeline` pipeline.
  * `p2_s1` is the name of the Bash step in the `my_second_pipeline` pipeline.


The steps are defined so that they execute in an interdependent sequence. This means that each step's execution is triggered by the successful completion of a prior, prerequisite step (or steps). In our example, step 1's (p1_s1) completion triggers the execution of step 2 (p1_s2), completion of step 2 triggers execution of step 3 (p1_s3), and so on until all steps in the pipeline are executed.
 **Steps**
[YAML]
    pipelines:
      - name: my_first_pipeline
        steps:
          - name: p1_s1
            type: Bash
            configuration:
              inputResources:
                # Sets up step to be triggered when there are commit events to myFirstRepo
                - name: myFirstRepo
            execution:
              onExecute:
                # Data from input resources is available as env variables in the step
                - echo $res_myFirstRepo_commitSha
                # The next two commands add variables to run state, which is available to all downstream steps in this run
                
                - add_run_variables current_runid=$run_id
                - add_run_variables commitSha=$res_myFirstRepo_commitSha
                # This variable is written to pipeline state in p1_s3.
                # So this will be empty during first run and will be set to prior run number in subsequent runs
                
                - echo ""Previous run ID is $prev_runid""
    
          - name: p1_s2
            type: Bash
            configuration:
              inputSteps:
                - name: p1_s1
            execution:
              onExecute:
                # Demonstrates the availability of an env variable written to run state during p1_s1
                - echo $current_runid
    
          - name: p1_s3
            type: Bash
            configuration:
              inputSteps:
                - name: p1_s2
              outputResources:
                - name: myPropertyBag
            execution:
              onExecute:
                - echo $current_runid
                # Writes current run number to pipeline state
                - add_pipeline_variables prev_runid=$run_id
                # Uses an utility function to update the output resource with the commitSha that triggered this run
                # Dependent pipelines can be configured to trigger when this resource is updated
                
                - write_output myPropertyBag commitSha=$commitSha runID=$current_runid
    
      - name: my_second_pipeline
        steps:
          - name: p2_s1
            type: Bash
            configuration:
              inputResources:
                # Sets up step to be triggered when myPropertyBag is updated
                - name: myPropertyBag
            execution:
              onExecute:
                # Retrieves the commitSha from input resource
                - echo ""CommitSha is $res_myPropertyBag_commitSha""         
[/YAML]
**configuration**
Specifies all optional configuration selections for the step's execution environment.
Tag
 **Description of usage**
Required/Optional  
`inputResources`
A collection of named resources that will be used by a step as inputs.
In this example:
  * Step `p1_s1`, in the first pipeline, is triggered when there are commit events to `myFirstRepo`, which is the name of the GitRepo resource.
  * Step `p2_s1`, in the second pipeline, is triggered when the `myPropertyBag` resource is updated.


Optional  
`inputSteps`
A collection of named steps whose completion will trigger execution of this step.
In this example:
  * Completion of step p1_s1 triggers the execution of step p1_s2.
and
  * Completion of step p1_s2 triggers the execution of step p1_s3.


Optional  
`outputResources`
A collection of named resources that will be generated or changed by a step.
Optional  
 **execution**
Declare sets of shell command sequences to perform for different execution phases:
Tag
 **Description of usage**
Required/Optional  
`onExecute`
Main commands to execute for the step
Optional  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
7,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pipeline-example-docker-build-and-push?[/INST] This pipeline demonstrates the definition of a pipeline that builds a single Docker Image and pushes it to Artifactory, and then publishes BuildInfo. An example Pipelines DSL is used to show how to use integrations, resources, and steps to construct a simple, automated workflow.
This pipeline demonstrates the following:
  * Creating a GitHub and Artifactory Integration.
  * Adding a Pipeline Source.
  * Creating a GitRepo trigger, which triggers a step when the contents of the source control repository changes.
  * Using an Image resource to add a reference to a Docker image to your pipeline.
  * Using DockerBuild and DockerPush native steps to build and push a docker image.
  * Using the PublishBuildInfo native step to publish a BuildInfo to Artifactory, and the PromoteBuild native step to promote a BuildInfo.
  * Using inputResources and inputSteps to set up dependencies between steps and resources.
  * Using outputResources.


### See it Live!
Click here to see this quickstart in action.
A successful run of the pipeline in this quickstart looks like this:  
#### Tutorial Video
#### Before You Begin
Before trying this quickstart, ensure that you have:
  * A GitHub account. This is required for forking the example repository.
  * A JFrog Platform account, or self-hosted JFrog Pipelines.
  * A user account in Artifactory with deploy permissions to at least one binary repository.
  * Create a local Docker repository to set up Artifactory as a Docker Registry.Docker RegistryDocker Registry
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools.


#### Running This Example
Perform the steps below to build and push your Docker image:
  1.  **Fork the Repository**
The Pipelines DSL for this example is available in the jfrog-pipelines-docker-sample repository in the JFrog GitHub account.
The DSL file is a yaml file that contains the pipeline definitions. The example uses a single YAML file, `pipelines.yml`. This file contains the declarations for all resources and workflow steps. For a full breakup of all the resources, pipelines and steps used in the yml file, see the pipelines.ymlsection below.
Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as Pipeline Sources or GitRepo resources, in order to add webhooks to these repositories and listen for change events.
  2.  **Sign in to Artifactory**
Sign in to JFrog Platform with your Artifactory credentials.
  3.  **Add Integrations**
    1. Go to **Administration** **| Pipelines | Integrations** to  add two integrations:
      *  **GitHub Integration**: This integration is used to add the Pipeline source, as well as the GitRepo resource defined in  pipelines.yml.
      *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to push the Docker image to the `docker-local` repository in Artifactory.
    2. Write down the names of both GitHub and Artifactory integrations as these are required for the next step. Ensure that the names are unique and easy to remember.
  4.  **Update pipelines.yml file**
The pipelines configuration is available in the **pipelines.yml** file. Edit this file in your fork of the Git repo and replace the following:
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in the previous step.
`gitProvider: my_github`  
`path`
Provide the path to your fork of this repository.
`path: myuser/repo-name`  
`registry`
Provide the the name of the Artifactory integration you added in the previous step.
`registry: art`  
`imageName`
Provide your Docker image path and name.
`docker.artprod.mycompany.com` `/docker_local/dbp`
Where, docker.artprod.mycompany.com`/docker_local` is the image path and `dbp` is the image name.  
`sourceArtifactory`
Provide your Artifactory integration.
`sourceArtifactory: art`  
### Note
 **All pipeline definitions are global across JFrog Pipelines within a Project.** The names of your pipelines and resources need to be unique within the Project in JFrog Pipelines.
  5.  **Add Pipeline Sources**
The **Pipeline Source** represents the Git repo where our Pipelines definition files are stored. A pipeline source connects to the repository through an integration, which we added in the previous step.
Follow instructions to add a Pipeline Source and point it to the `pipelines.yml` in your fork of the repo. This automatically adds your configuration to the platform and a pipeline is created based on your YAML. The `pipelines.yml` file is parsed and resources, steps, and pipelines are added as configured.
After your pipeline source syncs successfully, navigate to **Pipelines | My Pipelines** in the left navbar to view the newly added pipeline. In this example,`pipeline_dbp` is the name of our pipeline.  
Click the name of the pipeline. Thisrenders a real-time, interactive, diagram of the pipeline and the results of its most current run.
  6.  **Execute the Pipeline**
You can trigger the pipeline by committing a change to your Git repository, or by manually triggering it through the UI. The steps in the pipeline execute in sequence. Multiple steps can execute in parallel if the node pool has multiple build nodes available.  
Once the pipeline has completed, a new run is listed.  


#### pipelines.yml
The `pipelines.yml` file is made up of resources, pipelines and steps, as shown below:
##### Resources
This example uses the following types of resources:
  *  **GitRepo**
  *  **Image**
  *  **BuildInfo**


 **GitRepo**
A GitRepo resource is used to connect JFrog Pipelines to a source control repository. Adding it creates a webhook to the repo so that future commits will automatically create a new version with the webhook payload.
 **resources**
[YAML]
      - name: dbp_repo
        type: GitRepo
        configuration:
          gitProvider: my_github                       # <-- replace with your integration
          path: jfrog/jfrog-pipelines-docker-sample    # <-- replace with your repository name
          branches:
            include: master
    
[/YAML]
Tag
 **Description**
Required/Optional  
name
`dbp_repo` is the name of the GitRepo resource pointing to the repository containing the Dockerfile and other source code required to build the image.
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
`gitProvider `
The name of the GitHub Integration.
Required  
`path `
The path of the repository from the integration root.
Required  
`branches`
  * `include `\-- (optional) Regular expression to include branches from the repo
  * `exclude`\-- (optional) Regular expression to exclude branches from the repo


The `include: master` tag indicates that the GitRepo resource is listening to the master branch.
Optional  
 **Image**
An Image resource is used to add a reference to a Docker image to your pipeline. An Image resource can be used as both input and output. Steps like DockerBuild will generate it as an output and steps like DockerPublish will use it as input to publish to a Docker registry. In our example, it is used as an outputResource for the DockerPush step.
 **resources**
[YAML]
      - name: dbp_image
        type: Image
        configuration:
          registry: art                                # <-- replace with your artifactory integration
          sourceRepository: docker_local               # <-- required if registry is Artifactory
          imageName: docker.artprod.mycompany.com/docker_local/dbp  # < -- replace with your image path and name
          imageTag: latest
          autoPull: true
    
[/YAML]
Tag
 **Description**
Required/Optional  
`name`
`dbp_image` is the name that identifies the resource.
Required  
`registry`
`art` is the name of a  Docker Registry Integration.
Required  
`sourceRepository `
`docker_local` is the name of the repository in Artifactory to which the images will be published.
Required if registry is in JFrog Artifactory  
`imageName`
The file path of the Docker image, of the form imageRepo/imageName.
Required  
`imageTag`
The version tag of the initial version of the Docker image.
Required  
`autoPull `
When **true** , the image is automatically pulled to the machine running the step.
Defaults to **false**.
Optional  
 **BuildInfo**
BuildInfo is automatically created when the DockerBuild step is used to generate packages. BuildInfo is then published to the configured Artifactory repo (`sourceArtifactory: art`) through the PublishBuildInfo step, by providing that resource in the DockerPush step.
 **resources**
[YAML]
      - name: dbp_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: art
          buildName: dbp_build
          buildNumber: 1    
    
      - name: dbp_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: art
          buildName: dbp_build
          buildNumber: 1
[/YAML]
Tag
 **Description**
Required/Optional  
name
`dbp_build_info` and `dbp_promoted_build_info` are the name of the `BuildInfo` resources, which is the metadata associated with the build in Artifactory.
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
`sourceArtifactory`
`art` is the name of the Artifactory Integration.
Required  
buildName
`dbp_build` is the name of the build to associate with the BuildInfo.
Optional  
buildNumber
The number of the build to associate with the BuildInfo.
Optional  
##### Pipelines
`pipeline_dbp`, the pipeline definition for building and pushing the Docker image.
##### Steps
The `pipeline_dbp` pipeline contains the following native steps:
  *  **DockerBuild**
  *  **DockerPush**
  *  **PublishBuildInfo**
  *  **PromoteBuild**


 **DockerBuild**
The DockerBuild native step performs a build to produce a Docker image from a Dockerfile in a Git source repository.
This step builds a Docker image from a GitRepo source repository resource. You must provide in the step configuration the name and directory of the Dockerfile that contains the command to be processed by a `docker build` command, as well as the name and tag of the resulting image. The image is built on the build node, and information about that image is stored in the run state.
 **steps**
[YAML]
          - name: docker_build
            type: DockerBuild
            configuration:
              affinityGroup: dbp_group
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/docker_local/dbp  # replace with your image path and name
              dockerImageTag: ${run_number}
              inputResources:
                - name: dbp_repo
              integrations:
                - name: art                            # <-- replace with your artifactory integration
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
name
`docker_build` is the name that identifies the step.
This is the name that is used when the step is assigned as an input to the next step, `docker_push`.
Required  
`affinityGroup`
Must specify an affinity group string that is the same as specified in a subsequent DockerPush step.
Both DockerBuild and DockerPush steps are assigned to the dbp_group affinityGroup so that they share the same state. If they are not, the output of the DockerBuild step won't be available to the DockerPush step.
Optional  
`dockerFileLocation`
Directory containing the Dockerfile or the file that has Docker build configuration
Required  
`dockerFileName`
Name of the Dockerfile
Required  
`dockerImageName`
The name of the Docker image to create. This can be set using environment variables or triggering a run using parameters.
Required  
`dockerImageTag`
The tag for the Docker image to create. This can be set using environment variables or triggering a run using parameters.
In our example, the tag for the image is set to the pipeline's run number, which is the number of the run currently executing.
Required  
`inputResources`
Must specify:
  * a GitRepo resource (that contains the DockerFile)


This step accepts `dbp_repo` as an inputResource to build the image based on the provided Dockerfile.
Optionally, may also specify:
  * an Image resource of a base Image to include in the built Image
  * a FileSpec resource that specifies what files to include in the built Image. These files are automatically copied to `dockerFileLocation.`


Required/Optional  
`integrations`
Must specify an Artifactory Integration.
Required  
 **DockerPush**
The DockerPush native step pushes the specified Docker Image to Artifactory.
 **steps**
[YAML]
          - name: docker_push
            type: DockerPush
            configuration:
              affinityGroup: dbp_group
              targetRepository: docker-local
              integrations:
                - name: art                            # <-- replace with your artifactory integration
              inputSteps:
                - name: docker_build
              outputResources:
                - name: dbp_image
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
name
`docker_push` is the name that identifies the step.
Required  
`affinityGroup`
Must specify an affinity group string that is the same as specified in the prior DockerBuild step.
Optional  
`targetRepository`
The name of the Docker repository in Artifactory.
Required  
`integrations`
Must specify an Artifactory Integration.
Required  
`inputSteps`
Must specify the named DockerBuild step in the same affinity group.
The output (result) of the previous `docker_build` step is used as an inputStep for this step.
Required  
`outputResources`
May specify an Image resource. If one is specified, the `imageTag` property of that resource will be updated with the `dockerImageTag` of the preceding DockerBuild step.
This step outputs `dbp_image` as an outputResource, which can be used to trigger downstream dependent steps or pipelines if needed.
Optional
May be required  
 **PublishBuildInfo**
The PublishBuildInfo step publishes BuildInfo attached to the image to Artifactory. BuildInfo provides a manifest for the build and includes metadata about the modules, dependencies and other environment variables.
BuildInfo can also be published by any of the language-specific publish steps, when its `autoPublishBuildInfo` tag is set to true.
 **steps**
[YAML]
          - name: publish_dbp_build
            type: PublishBuildInfo
            configuration:
              inputSteps:
                - name: docker_push
              outputResources:
                - name: dbp_build_info
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
name
`publish_dbp_build` is the name that identifies the step.
Required  
`inputSteps`
Must specify the name of the DockerPush step. The output (result) of the previous step is used as an inputStep for this step.
The step must nothave set `autoPublishBuildInfo` to true.
Required  
`outputResources`
Must specify a BuildInfo resource to publish.
Required  
 **PromoteBuild**
The PromoteBuild native step promotes the dbp_build_info BuildInfo and moves or copies the related artifacts from one Artifactory repository to another.
This step requires either a PublishBuildInfo step as previous step or a BuildInfo resource as an input to successfully execute.
 **steps**
[YAML]
          - name: promote_dbp_build
            type: PromoteBuild
            configuration:
              targetRepository: demo-pipelines
              integrations:
                - name: art
              inputResources:
                - name: dbp_build_info
              outputResources:
                - name: dbp_promoted_build_info
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
name
`promote_dbp_build` is the name that identifies the step.
Required  
targetRepository
`demo-pipelines` is the name of the repository in Artifactory to promote the build to.
Required  
`integrations`
Must specify an Artifactory Integration.
Required  
`inputResources`
Must specify a named BuildInfo resource whose `buildName` and `buildNumber` properties identify the build to promote.
Required  
`outputResources`
Must specify a named BuildInfo resource to map to the promoted build. The BuildInfo will be updated with the `buildName` and `buildNumber` of the input BuildInfo resource, and its `targetRepo` with the value of `targetRepository` .
Required  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
8,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pipeline-example-release-to-edge-node?[/INST] This pipeline demonstrates the definition of a simple pipeline that creates and signs a release bundle, and distributes it to an Artifactory edge node. An example Pipelines DSL is used to show how to use integrations, resources, and steps to construct a simple, automated workflow. The pipeline is triggered by any change in any of the BuildInfo resources used to create the release bundle.
This pipeline demonstrates the following:
  * Creating a GitHub Integration, Artifactory Integration, and Distribution Integration.GitHub Integration
  * Adding a Pipeline Source.Add a Pipeline Source
  * Signing the release bundle using the SignReleaseBundle native step.
  * Distributing the release bundle to an edge node using the DistributeReleaseBundle native step.


This pipeline starts from where the Pipeline Example: Docker Build and Push left off. While that pipeline built and pushed a Docker build, this pipeline takes the Buildinfo from that example and creates the release bundle and distributes it to an edge node.
A successful run of the pipeline in this quickstart looks like this:  
#### Before you Begin
Before trying this quickstart, ensure that you have:
  * A GitHub account. This is required for forking the example repository.
  * A JFrog Platform account.
  * Set up Artifactory as a Docker registry. For more information, see  Getting Started with Artifactory as a Docker Registry and Docker Registry.Getting Started with Artifactory as a Docker RegistryDocker Registry
  * Installed and configured JFrog Distribution.
  * Generated, uploaded, and deployed GPG Keys for JFrog Distribution.GPG Signing
  * At least one Pipelines node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools.
  * Successfully run the Docker Build and Push pipeline.


#### Running This Example  
Perform the steps below to run this pipeline:
  1.  **Sign in**
Sign in to JFrog Platform with your Artifactory credentials.
  2. Add Integrations
    1. Go to **Application | Pipelines | Integrations** to add these integrations:
      *  **GitHub Integration GitHub Integration**: This integration is used to add the Pipeline source.
`myGithub`is the name of the GitHub integration used in this example.
      *  **Distribution Integration** : This integration connects your JFrog Pipeline Automation platform to a Distribution instance.
`myDist` is the name of the Distribution integration used in this example.
      *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to to get artifacts including Docker images, and maintain build information.
`myArtifactory` is the name of Artifactory integration used in this example.
    2. Write down the names of the Artifactory and Distribution integrations as these are required for the next step. Ensure that the names are unique and easy to remember.
  3.  **Fork GitHub repository**
The Pipelines DSL for this example is available in the jfrog-pipelines-cd-sample repository in the JFrog GitHub account.
The DSL file is a yaml file that contains the pipeline definitions. The example uses two YAML files, `pipelines.yml` and `values.yml`. The pipelines.yml file contains the declarations for all resources and workflow steps. The `values.yml` file contains the values required for the `pipelines.yml` file. For a full breakup of all the resources, pipelines and steps used in the yml file, see the pipelines.ymlsection below.
Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as Pipeline Sources or GitRepo resources, which is required to add webhooks to these repositories and listen for change events.
  4.  **Update values.yml**
The pipelines configuration is available in the **values.yml** file. If required, edit this file in your fork of this repo and replace the following:
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in the previous step.
`gitProvider: myGithub`  
`sourceArtifactory`
Provide your Artifactory integration.
`sourceArtifactory: myArtifactory`  
`sourceDistribution`
Provide your Distribution integration.
` sourceDistribution:` ` myDist`  
### Note
 **All pipeline names are global across your JFrog Pipelines**. The names of your pipelines and resources need to be unique within JFrog Pipelines.
  5.  **Add Pipeline Source**
The **Pipeline Source** represents the Git repo where our Pipelines definition files are stored. A pipeline source connects to the repository through an integration, which was added in the previous step.
    1. In your left navigation bar, go to Administration | Pipelines | Pipeline Sources. Click on Add a Pipeline Source and then choose From YAML. Follow instructions to add a Pipeline Source.
    2. After your Pipeline Source syncs successfully, navigate to **Pipelines** | **My Pipelines** in the left navbar to see the newly added pipeline. In this example, `cd_` `demo_release_mgmt`is the name of our pipeline.
    3. Click the name of the pipeline. This renders a real-time, interactive diagram of the pipeline and the results of its most current run.  
  6.  **Execute the pipeline**
You can now commit to the repo to trigger your pipeline, or trigger it manually through the UI. The steps in the pipeline execute in sequence. Multiple steps can execute in parallel if the node pool has multiple build nodes available.  


#### pipelines.yml
The `pipelines.yml` file is made up of resources, pipelines and steps, as shown below:
##### Resources
This example uses the following types of resources:
  * BuildInfo
  * ReleaseBundle
  * DistributionRule


 **BuildInfo**
A `BuildInfo` resource is the metadata associated with a build in Artifactory.
JFrog Pipelines automatically creates BuildInfo when any of the Build steps like MvnBuild, NpmBuild, GradleBuild, or DockerBuild are used to generate packages. BuildInfo is published to Artifactory through the PublishBuildInfo step, by setting a flag in Build steps or by providing that resource in steps like NpmPublish, GoPublishModule, GoPublishBinary, and DockerPush.
 **resources**
[YAML]
      # Build info of build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
          buildName: dbp_build
          buildNumber: 1  
[/YAML]
Tag
 **Description**
Required/Optional  
name
`gosvc_promoted_build_info` is the name of the `BuildInfo` resource, which is the metadata associated with the build in Artifactory.
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
sourceArtifactory
`myArtifactory` is the name of the Artifactory Integration.
Required  
buildName
`dbp_build` is the name of the build to associate with the BuildInfo.
### Note
`dbp_build` is the name of the Docker build and push pipeline.
Optional  
buildNumber
The number of the build to associate with the BuildInfo.
Optional  
**ReleaseBundle**
A `ReleaseBundle`resource specifies a set of artifacts in Artifactory that are distributed to Artifactory Edge nodes as a JFrog Distribution Release Bundle.Distributing Release Bundles
This resource can be used with CreateReleaseBundle or DistributeReleaseBundle. This resource can be used to represent both signed and unsigned Release Bundle.Distributing Release Bundles
 **resources**
[YAML]
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution:  {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution:  {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
[/YAML]
Tag
 **Description**
Required/Optional  
`name`
`release_bundle` and `signed_bundle` are `ReleaseBundle` resources. While `release_bundle `is the release bundle, `signed_bundle `is the signed version of the release bundle.
Required  
`sourceDistribution`
myDist is the name of the Distribution Integration.
Required  
`version`
Version number of the release bundle.
Required  
**DistributionRule**
A DistributionRule resource is the set of Destination rules that can be applied to distributing a release bundle using JFrog Distribution.Distributing Release Bundles
 **resources**
[YAML]
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution:  {{ .Values.distributionIntegration }}
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""*""  
[/YAML]
Tag
 **Description**
Required/Optional  
`name`
`distribution_rules` is the name of the DistributionRule resource.
Required  
`sourceDistribution`
myDist is the name of the Distribution Integration.
Required  
`serviceName `
The Artifactory Edge Node service name with a wildcard pattern.
Required  
`siteName`
The Artifactory Edge Node site name with a wildcard pattern.
Required  
`cityName`
The Artifactory Edge Node city name with a wildcard pattern.
Required  
`countryCodes`
The Artifactory Edge Node country codes with a wildcard pattern.
Required  
##### Pipelines
`cd_demo_release_mgmt`, the pipeline definition for signing a release bundle and releasing it to an Artifactory edge node.
##### Steps
The `cd_demo_release_mgmt` pipeline is made up of the following native steps:
  * CreateReleaseBundle
  * SignReleaseBundle
  * DistributeReleaseBundle


 **CreateReleaseBundle**
The CreateReleaseBundlenative step produces a Release Bundle for distribution to an Artifactory Edge Node . The step can be used to create a signed or unsigned release bundle.Release Bundle RepositoriesJFrog Artifactory Edge
 **steps**
[YAML]
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: false
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
[/YAML]
Tag
 **Description of usage**
Required/Optional  
`name`
`bundle` is the name that identifies the `CreateReleaseBundle` step.
Required  
`inputResources`
Must specify eithera named BuildInforesource(s)oran Aql resource.
If BuildInfo `inputresources` are provided, the query for the release bundle is constructed using the `buildName`, `buildNumber`, and `targetRepo` of each BuildInfo input.
Required  
`outputResources`
Must specify a ReleaseBundle resource.
The `name`, `version`, and `isSigned` settings in the output ReleaseBundle are updated to the step's `releaseBundleName`, `releaseBundleVersion`, and `sign` values respectively (or any environment variable values that replaced environment variable placeholders for those values).
Required  
`releaseBundleName`
An alphanumeric name for the release bundle. `demo_rb` is the name used in this example.
Required  
`releaseBundleVersion`
Version string for the release bundle
Required  
`dryRun`
When set to `true` parse and validate only to test whether a release bundle version can be created.
Default is true.
Optional  
`sign`
Specifies whether the release bundle version is signed.
Default is false.
Optional  
`storeAtSourceArtifactory`
Controls whether to store release bundle version in the source Artifactorywhile signing.
Optional  
`description`
Description of the release bundle.
Optional  
`releaseNotes`
Describes the release notes for the release bundle version.
`syntax` specifies the format of release notes: `plain_text`, `markdown`, or `asciidoc`. Default is `plain_text`.
`content` is the release notes string in the specified syntax format. Use the `|` character to denote a string preserving newlines.
Optional  
 **SignReleaseBundle**
The SignReleaseBundle native step signs a Release Bundle in preparation for distributing it to Edge nodes.
 **steps**
[YAML]
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
[/YAML]
Tag
 **Description of usage**
Required/Optional  
`name`
`sign` is the name that identifies the `SignReleaseBundle` step.
Required  
`inputResources`
Must specify a ReleaseBundleresource whose `name` and `version` properties identify the release bundle to be signed.
This step accepts `release_bundle`, which is the ReleaseBundle resource, as the inputResource.
Required  
`outputResources`
Must specify a ReleaseBundle resource. This ReleaseBundle is updated with the `name` and `version` properties of the input ReleaseBundle (`release_bundle`), and sets the `isSigned` property to true.
This step specifies `signed_bundle`, which is the other ReleaseBundle resource, as the outputResource.
Required  
 **DistributeReleaseBundle**
The DistributeReleaseBundle native step triggers the distribution of a Release Bundles to an Artifactory Edge Node. This step requires a signed release bundle and one or more distribution rules to successfullyexecute.JFrog Artifactory Edge
 **steps**
[YAML]
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
`name`
distribute is the name that identifies the `DistributeReleaseBundle` step.
Required  
`inputResources`
Must specify a ReleaseBundle resource (signed_bundle) and one DistributionRule resource (distribution_rules) .
Required  
`dryRun`
Controls whether this should be a dry run to test if the release bundle can distribute to the Edge nodes matching the distribution rule.
The default is true.
Optional  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
9,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pipeline-example-go-build?[/INST] This pipeline demonstrates the definition of a simple pipeline that builds a Go (GoLang) application and Go module. It shows through an example of Pipelines DSL and shows how to use integrations, resources, and steps to construct a simple, automated workflow.
The pipeline performs the following sequence of tasks:
  * Build a Go ""Hello World"" application using the GoBuild native step
  * Publish the completed Go binary to Artifactory using the GoPublishBinary native step
  * Publish the build info to Artifactory using the PublishBuildInfo native step


### See it Live!
Click here to see this quickstart in action.
A successful run of the pipeline in this Quickstart looks like this:  
#### Before you Begin
Before trying this example, ensure that you have:
  * A GitHub account. This is required for forking the sample repository.
  * A JFrog Platform account, or self-hosted JFrog Pipelines.
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools. Please note that if you have a Cloud account, a node pool will already be available as part of your subscription.
  * It is recommended that you run `go mod tidy` command on your local machine to produce the `go.sum` file and then push it to your Git repo.


#### Running This Example
Please follow the steps below to build your Go binary:
  1.  **Fork the repository**
This Pipelines sample is available in the  jfrog-pipelines-go-sample repository in the JFrog GitHub account. The configuration is included in YAML files at the root of the repository:
    * `pipelines.yml`, which contains the declarations for all the resources and steps required to run the pipeline. This configuration is written in template format, so you will not need to change anything in this file.
    * `values.yml`, which contains custom values that will be populated into the template to create your pipeline
Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as in your pipelines, in order to enable us to add webhooks to these repositories and listen for change events.
### Note
The Git repository includes a `go.sum` file. However, it is recommended that you run `go mod tidy` command on your local machine to produce a new `go.sum` file and then push it to your Git repo.
  2.  **Sign in to Artifactory**
Sign in to JFrog Platform with your Artifactory credentials.
  3.  **Create the required Go repositories**
You will need to create the following repositories which will be used in your pipeline configuration:
    1. go-local: A local Go repository where your binary will be publishedGo Registry
    2. go-remote: A remote Go repository that proxies https://proxy.golang.org/Go Registry
    3. go-virtual: A virtual Go repository that aggregates local and remote repositories and is used in your pipeline definition to resolve dependencies. Please ensure that you select your local and remote repositories in the **Repositories** section while creating this virtual repo.Go Registry
  4.  **Add Integrations**
    1. Go to **Administration** **| Pipelines | Integrations** to  add two integrations:
      *  **GitHub Integration**: This integration is used to add the Pipeline source, as well as the GitRepo resource.
      *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to resolve depedencies and to publish the built binary to Artifactory.
    2. Write down the names of both GitHub and Artifactory integrations as these are required for the next step . Ensure that the names are unique and easy to remember.
  5.  **Update pipeline definitions**
Since your `pipelines.yml` config file is templatized, you can just update `values.yml` in your forked repository by following instructions below.
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in the previous step (4).
`gitProvider: myGithubIntegration`  
`repoPath`
Provide the path to your fork of this repository.
`repoPath: myuser/jfrog-pipelines-maven-sample`  
`artifactory`
Provide the name of the Artifactory integration you added in the previous step (4).
`artifactory: myArtifactoryIntegration`  
And that's it. Your configuration is ready to go!
### Note
 **All pipeline definitions are global across JFrog Pipelines within a Project.** The names of your pipelines and resources need to be unique within the Project in JFrog Pipelines.
  6.  **Add Pipeline Sources**
A Pipeline Source represents the git repository where our pipelines definition files are stored. A pipeline source connects to the repository through an  integration, which we added in step 4.
    1. In your left navigation bar, go to **Administration |** **Pipelines | Pipeline Sources**. Click on **Add a Pipeline Source** and then choose **From YAML**. Follow instructions to add a Pipeline Source. This automatically adds your configuration to the platform and pipelines are created based on your YAML.
    2. After your pipeline source syncs successfully, navigate to **Pipelines | My Pipelines** in the left navbar to view the newly added pipeline. In this example, `go_build_pipeline_example` is the names of your pipeline.
    3. Click the name of the pipeline. This renders a real-time, interactive, diagram of the pipeline and the results of its most current run.
  7.  **Execute the Pipeline**
You can trigger the pipeline by committing a change to your repository, or by manually triggering it through the UI.  
  8.  **Success!**
You have successfully executed the sample go application pipeline! You can verify the results by viewing the binary and build created by the pipeline.
Navigate to **Application | Artifactory | Artifacts** and you will find your published binary under the **go-local** repository:  
Navigate to **Application | Artifactory | Builds** to view your published `build -` `go_build_pipeline_example`.  


#### Explanation of pipeline definition
Let us now take a look at the pipeline definition files and what each section means.
The **pipelines.yml** file contains the templatized definition of your pipeline. This consists of the following:
  * Resources are entities that contain information that is consumed or generated by pipeline steps. In our example, we use the following resources:
    * A GitRepo resource pointing to the source control repository where your application code is present. You can configure this resource to trigger dependent steps on specific events.
    * A BuildInfo resource that is a pointer to the Build on Artifactory. This is automatically created by the PublishBuildInfo step.
  * Steps are executable units that form your pipeline. In our example, the pipeline consists of the following steps:
    * A GoBuild native step that builds your Go project. This step is a pre-packaged step (i.e. native step) that is available to use with simple configuration and without the need for custom scripting. Detailed information on GoBuild is available here.
    * A GoPublishBinary native step that publishes your Go binary to Artifactory. This step also does not need custom scripting. Detailed information on GoPublishBinary is available here
    * A PublishBuildInfo step is a native step that gathers build metadata and pushes it to Artifactory. Artifactory Builds provide a manifest and include metadata about included modules, dependencies and other environment variables. Detailed information on PublishBuildInfo is available here.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
10,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pipeline-example-npm-build?[/INST] This pipeline demonstrates the definition of a simple pipeline that builds and publishes an npm package. An example Pipelines DSL is used to show how to use integrations, resources, and steps to construct a simple, automated workflow.
This example shows the following:
  * Create a GitHub Integration and Artifactory Integration.GitHub Integration
  * Add a Pipeline Source.
  * Build npm components using the NpmBuild native step.
  * Publish the npm component using the NpmPublish native step.


### See it Live!
Click here to see this quickstart in action.
A successful run of the pipeline in this quickstart looks like this:  
#### Before you Begin
Before trying this quickstart, ensure that you have:
  * A GitHub account. This is required for forking the example repository.
  * A JFrog Platform account, or self-hosted JFrog Pipelines.
  * Set up Artifactory as an npm Registry. Create three repositories:npm Registry
    * A  remote npm registry. Remote Repositories defined in Artifactory serve as a caching proxy for a registry managed at a remote URL such as `https://registry.npmjs.org`. Artifacts (such as TGZ files) requested from a remote repository are cached on demand.npm RegistryRemote Repositories
    * A local npm registry.
    * A  virtual npm registry. A Virtual Repository defined in Artifactory aggregates packages from both local and remote repositories. This allows you to access both locally hosted npm packages and remote proxied npm registries from a single URL defined for the virtual repository.npm Registry
When adding the virtual npm registry, ensure that both the remote and local npm registries are selected, and the Default Deployment Repository is set to the local npm registry.  
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools.


#### Running This Example
Perform the steps below to build and push your npm image:
  1. ##### Fork the repository
The Pipelines DSL for this example is available in the npm-example repository in the JFrog GitHub account.
The DSL file is a yaml file that contains the pipeline definitions. This example uses two yaml files:
    * `pipelines.yml`, which contains the declarations for all the resources and workflow steps required to run the pipeline.
    * `values.yml`, which contains the values required for the pipelines.yml file.
For a full breakup of all the resources, pipelines and steps used in the yml file, see the pipelines.yml section below.
Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as Pipeline Sources or GitRepo resources, in order to add webhooks to these repositories and listen for change events.
  2. ##### Sign in to Artifactory
Sign in to JFrog Platform with your Artifactory credentials.
  3. ##### Add Integrations
a. Go to **Administration** **| Pipelines | Integrations** to  add two integrations:
    *  **GitHub Integration**: This integration is used to add the Pipeline source, as well as the GitRepo resource defined in  values.yml.
    *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to download npm depedencies from Artifactory, and to pack and upload the built package to Artifactory.
b. Write down the names of both GitHub and Artifactory integrations as these are required for the next step. Ensure that the names are unique and easy to remember.
  4. ##### Update values.yml and pipelines.yml
The pipelines configuration is available in the `values.yml` file. Edit this file in your fork of this repo and replace the following:
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in the previous step.
`gitProvider: my_github`  
`path`
Provide the path to your fork of this repository.
`path: myuser/project-examples`  
Edit the` pipelines.yml` file and replace the following:
Tag
Description
Example  
`sourceArtifactory`
Provide your Artifactory integration.
`sourceArtifactory: art`  
`repositoryName`
Provide the name of the npm repository in Artifactory.
`repositoryName: npm-virtual`  
### Note
 **All pipeline definitions are global across JFrog Pipelines within a Project.** The names of your pipelines and resources need to be unique within the Project in JFrog Pipelines.
  5. ##### Add Pipeline Sources
The **Pipeline Source** represents the git repository where our pipelines definition files are stored. A pipeline source connects to the repository through an integration, which we added in step 3.
In your left navigation bar, go to **Administration |** **Pipelines | Pipeline Sources**. Click on **Add a Pipeline Source** and then choose **From YAML**. Follow instructions to add a Pipeline Source. This automatically adds your configuration to the platform and pipelines are created based on your YAML.
  6. ##### Execute the Pipeline
After your Pipeline Source is synced successfully, navigate to **Pipelines | My Pipelines** in the left navbar to see the newly added pipeline. In this example, `npm_example_pipeline_jfp` is the name of our pipeline. Click the name of the pipeline. This renders a real-time, interactive, diagram of the pipeline and the results of its most current run.  
You can trigger the pipeline by committing a change to your repository, or by manually triggering it through the UI. The steps in the pipeline execute in sequence. Multiple steps can execute in parallel if the node pool has multiple build nodes available.  
Once the pipeline has completed, a new run is listed.  


#### pipelines.yml
The `pipelines.yml` file is made up of resources, pipelines and steps, as shown below:
##### Resources
This example uses the following types of resources:
  * GitRepo
  * BuildInfo


 **GitRepo**
A GitRepo resource is used to connect JFrog Pipelines to a source control repository. Adding it creates a webhook to the repo so that future commits will automatically create a new version with the webhook payload.
 **resources**
[YAML]
      - name: npm_example_repo_jfp
        type: GitRepo
        configuration:
          # SCM integration where the repository is located
          gitProvider: {{ .Values.myRepo.gitProvider }}
          # Repository path, including org name/repo name
          path: {{ .Values.myRepo.path }}
          branches:
            # Specifies which branches will trigger dependent steps
            include: master
[/YAML]
Tag
 **Description**
Required/Optional  
name
`npm_example_repo_jfp` is the name of the GitRepo resource pointing to the repository containing the yaml files and other source code required to build the image.
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
`gitProvider `
The name of the GitHub Integration. Its value is retrieved from the values.yml file.GitHub Integration
Required  
`path `
The path of the repository from the integration root. Its value is retrieved from the values.yml file.
Required  
`branches`
  * `include `\-- (optional) Regular expression to include branches from the repo
  * `exclude`\-- (optional) Regular expression to exclude branches from the repo


The `include: master` tag indicates that the GitRepo resource is listening to the master branch.
Optional  
 **BuildInfo**
BuildInfo is automatically created when the NpmBuild step is used to generate packages. BuildInfo is then published to the configured Artifactory repo (`sourceArtifactory: demoArt`) by providing that resource in the NpmPublish step.
 **resources**
[YAML]
      - name: npm_example_buildinfo_jfp
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
[/YAML]
Tag
 **Description**
Required/Optional  
name
`npm_example_buildinfo_jfp` is the name of the BuildInfo resource, which is the metadata associated with the build in Artifactory..
This name is used to refer to the resource in steps, and must be unique across all repositories in your JFrog Pipelines environment.
Required  
`sourceArtifactory`
The name of the  Artifactory Integration. Its value is retrieved from the values.yml file.
Required  
##### Pipelines
`npm_example_pipeline_jfp` is the name of the pipeline, which contains the steps for running the pipeline.
##### Steps
The `npm_example_pipeline_jfp` pipeline contains the following native steps:
  *  **NpmBuild**
  *  **NpmPublish**


 **NpmBuild**
The NpmBuild native step builds an npm source. This step automatically performs `npm-install` on the source in a Git repository.
 **steps**
[YAML]
          - name: npm_build_step
            type: NpmBuild
            configuration:
              repositoryName: npm-virtual      # required, npm repository name on artifacctory
              sourceLocation: ./npm-example      # required, location of package.json file
              integrations:
                - name:  demoArt  # required
              inputResources:
                - name: npm_example_repo_jfp         # required
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
`name`
`npm_build_step` is the `name` that identifies the step.
This is the name used when the step is assigned as an input to the next step, npm_publish_step.
Required  
`repositoryName`
`npm-virtual` is the name of the npm repository in Artifactory.
Required  
`sourceLocation`
`./npm-example` is the directory containing the `package.json` file, relative to the GitRepo `path`.
Required  
`integrations`
Specifies an Artifactory Integration where modules will be published. If a FileSpec resource is specified in `inputResources` then this is optional. Otherwise, it is required.
May be required  
`inputResources`
Must specify a GitRepo resource. The `npm-install` runs on the Git repository at `sourceLocation`.
This step accepts` npm_example_repo_jfp`, which is the GitRepo resource, as the inputResource.
Required
Optional  
`` **NpmPublish**
The NpmPublish step publishes an npm package to the registry in Artifactory following an NpmBuild step.
 **steps**
[YAML]
          - name: npm_publish_step
            type: NpmPublish
            configuration:
              # for payloadType npm:
              repositoryName: npm-virtual        # required, npm repository name on artifactory
              autoPublishBuildInfo: true       # optional
              integrations:
                - name: demoArt      # required
              inputSteps:
                - name: npm_build_step                # required
              outputResources:
                - name: npm_example_buildinfo_jfp                 # optional
    
[/YAML]
Tag
 **Description of usage**
Required/Optional  
`name`
`npm_publish_step` is the `name` that identifies the step.
Required  
`repositoryName`
`npm-virtual` is the name of the npm repository to publish in Artifactory.
Required  
`autoPublishBuildInfo`
When set to true, publishes build info to Artifactory. Default is false.
Once published, the build info can be viewed in Artifactory, in the Build Browser under Builds.
Optional  
`integrations`
Must specify an  Artifactory Integration.
Required  
`inputSteps`
Must specify a named NpmBuild or Bash step.
`npm_build_step` is the name of the NpmBuild step.
Required  
`outputResources`
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set to true.
`npm_example_buildinfo_jfp` is specified as the outputResource.
May be required  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
11,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pipeline-example-maven-build?[/INST] This quickstart demonstrates a simple pipeline that builds and publishes a Maven package. An example Pipelines DSL is used to show how to use integrations, resources, and a combination of native steps to build an application and publish it to Artifactory.
This tutorial walks you through the following steps to run this application using JFrog Pipelines:
  * Create a GitHub Integration and Artifactory Integration.GitHub Integration
  * Add a Pipeline Source.
  * Build Maven components using the MvnBuild native step.
  * Publish build info using the PublishBuildInfo native step.


### See it Live!
Click here to see this quickstart in action.
A successful run of the pipeline in this Quickstart looks like this:  
#### Before you Begin
Before trying this example, ensure that you have:
  * A GitHub account. This is required for forking the sample repository.
  * A JFrog Platform account, or self-hosted JFrog Pipelines.
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see  Managing Pipelines Node Pools.
### Note
If you have a Cloud account, a node pool will already be available as part of your subscription.


#### Running This Example
Perform the steps below to build your Maven artifact:
  1.  **Fork the repository**
This Pipelines sample is available in the  jfrog-pipelines-maven-sample repository in the JFrog GitHub account. The configuration is included in the YAML files at the root of the repository:
    * `pipelines.yml`, which contains the declarations for all the resources and steps required to run the pipeline. This configuration is written in template format, so you will not need to change anything in this file.
    * values.yml, which contains custom values that will be populated into the template to create your pipeline
For a full breakup of all the resources, pipelines, and steps used in the yml file, see the pipeline definition section below.  Fork this repository to your account or organization. This is important since you need admin access to repositories that are used as in your pipelines, to enable us to add webhooks to these repositories and listen for change events.
  2.  **Sign in to Artifactory**
Sign in to JFrog Platform with your Artifactory credentials.
  3.  **Create a local Maven repository**
Create a local Maven repository and write down the repository name, since you will need to use it in your pipeline configuration.
  4.  **Add Integrations**
    1. Go to **Administration** **| Pipelines | Integrations** to  add two integrations:
      *  **GitHub Integration**:: This integration is used to add the Pipeline source, as well as the GitRepo resource.
      *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to download Maven depedencies from Artifactory, and to pack and upload the built package to Artifactory.
    2. Write down the names of both GitHub and Artifactory integrations as these are required for the next step. Ensure that the names are unique and easy to remember.
  5.  **Update pipeline definitions**
Since your pipelines.yml config file is templatized, as shown in the table below, update the values.yml in your forked repository:
Tag
Description
Example  
`gitProvider`
Provide the name of the Github integration you added in Step 4.
`gitProvider: my_github`  
`repoPath`
Provide the path to your fork of this repository.
`repoPath: myuser/jfrog-pipelines-maven-sample`  
`artifactory`
Provide the name of the Artifactory integration you added in the previous Step 4.
`artifactory: demoArt`  
`deployerRepo`
Provide the name of the local Maven repository in Artifactory you created in Step 3.
`deployerRepo: maven-local`  
And that's it. Your configuration is ready to go!
### Note
 **All pipeline definitions are global across JFrog Pipelines within a Project.** The names of your pipelines and resources need to be unique within the Project in JFrog Pipelines.
  6.  **Add Pipeline Sources**
The Pipeline Source represents the git repository where our pipelines definition files are stored. A pipeline source connects to the repository through an integration, which we added in Step 4.
    1. In your left navigation bar, go to **Administration | Pipelines | Pipeline Sources**. Click **Add a Pipeline Source** and then choose **From YAML**. Follow instructions to add a Pipeline Source. This automatically adds your configuration to the platform and pipelines are created based on your YAML.
    2. After your pipeline source syncs successfully, navigate to **Pipelines | My Pipelines** in the left navbar to view the newly added pipeline. In this example, `demo_maven` is the names of your pipeline.
    3. Click the name of the pipeline. This renders a real-time, interactive, diagram of the pipeline and the results of its most current run.
  7.  **Execute the Pipeline**
You can trigger the pipeline by committing a change to your repository, or by manually triggering it through the UI. Multiple steps can execute in parallel if the node pool has multiple build nodes available.
Once the pipeline, a new run is listed:  


#### How the Pipeline Definition Works
Let us now take a look at the pipeline definition files and what each section means.
The **pipelines.yml** file contains the templatized definition of your pipeline. This consists of the following:
  * Resources are entities that contain information that is consumed or generated by pipeline steps. In our example, we use the following resources:
    * A GitRepo resource pointing to the source control repository where your application code is present. You can configure this resource to trigger dependent steps on specific events. For more information, see GitRepo.
    * A BuildInfo resource is a pointer to the Build on Artifactory. This is automatically created by the PublishBuildInfo step. For more information, see BuildInfo.
  * Steps are executable units that form your pipeline. In our example, the pipeline consists of the following steps:
    * A MvnBuildnative step that builds your Maven project and optionally deploys it to Artifactory. This step is a pre-packaged step (that is, native step) that is available to be used with simple configuration and without the need for custom scripting. For more information, see MvnBuild.
    * A PublishBuildInfo step is a native step that gathers build metadata and pushes it to Artifactory. Artifactory Builds provide a manifest and include metadata about included modules, dependencies and other environment variables. For more information, see PublishBuildInfo.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
12,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pipeline-example-helm-blue-green-deploy?[/INST] This pipeline demonstrates the definition of a simple pipeline that builds a Docker Image and a Helm Chart and deploys it to a Kubernetes cluster using the Blue/Green strategy. An example Pipelines DSL is used to show how to use integrations, resources, and steps to construct a simple, automated workflow.
The pipeline performs the following sequence of tasks:
  * Build and publish a Docker image using DockerBuild and DockerPush native steps
  * Publish a Helm Chart using the HelmPublish native step
  * Deploy the Helm Chart to Live using the HelmBlueGreenDeploy native step
  * Promote Helm Release to Live role using the HelmBlueGreenRoleSwitch native step
  * Uninstall previous release using the  HelmBlueGreenCleanup native step


A successful run of the pipeline in this Quickstart looks like this:  
### See it Live!
Click here to see this quickstart in action.
#### Before you Begin
Before trying this example, ensure that you have:
  * A GitHub account. This is required for forking the sample repository.
  * A JFrog Platform account or self-hosted JFrog Pipelines.
  * At least one node pool. This is the set of nodes that all pipeline steps will execute in. For more information, see Managing Pipelines Node Pools.
### Note
If you have a Cloud account, a node pool will already be available as part of your subscription.


#### Running This Example
Follow the steps below to build your Go binary:
  1.  **Fork the repository**
This Pipelines sample is available in the jfrog-pipelines-helm-blue-green-sample repository in the JFrog GitHub account. The configuration is included in YAML files at the root of the repository:
    * `pipelines.yml`, which contains the declarations for all the resources and steps required to run the pipeline. This configuration is written in template format and you do not have to change anything in this file.
    * `values.yml`, which contains custom values that will be populated into the template to create your pipeline.
Fork this repository to your account or organization. This is important since you need admin access to the repositories that are used as in your pipelines to enable Pipelines to add webhooks to these repositories and listen for change events.
  2.  **Sign in to Artifactory**
Sign in to JFrog Platform with your Artifactory credentials.
  3.  **Create the required repositories**
Create the following repositories that will be used in your pipeline configuration:
    *  **docker-local** : A local Docker repository where your image will be published.Docker Registry
    *  **docker-remote** : A remote Docker repository that proxies https://registry-1.docker.io/.Docker Registry
    *  **docker** : A virtual Docker repository that aggregates local and remote repositories and is used in your pipeline definition to resolve dependencies. Ensure that you select your local and remote repositories in the **Repositories** section while creating this virtual repo. Also, ensure that you select your local repository as the **Default Deployment Repository**.Docker Registry
    *  **helm-local** : A local Helm repository where your chart will be published.Kubernetes Helm Chart Repositories
    *  **helm** : A  virtual Helm repository that aggregates local and remote repositories and is used in your pipeline definition to resolve dependencies. Ensure that you select your local repository in the **Repositories** section while creating this virtual repo. Also, ensure that you select your local repository as the **Default Deployment Repository**.Kubernetes Helm Chart Repositories
  4.  **Add Integrations**
    1. Go to **Administration** **| Pipelines | Integrations** to  add three integrations:
      *  **GitHub Integration**: This integration is used to add the Pipeline source, as well as the GitRepo resource.
      *  **Artifactory Integration** : This integration is used to authenticate with Artifactory to resolve dependencies and to publish the built binary to Artifactory.
      *  **Kubernetes Integration** : This integration is used to authenticate with the Kubernetes cluster to deploy the chart and create role Services.
    2. Write down the names of all integrations as these are required for the next step. Ensure that the names are unique and easy to remember.
  5.  **Update pipeline definitions**
Since your `pipelines.yml` config file is templatized, you can just update `values.yml` in your forked repository by following instructions below.
Tag
Description
Example  
`gitRepo.gitProvider`
Provide the name of the Github integration you added in step 4.
`gitProvider: my_github`  
`gitRepo.path`
Provide the path to your fork of this repository.
`path: myuser/jfrog-pipelines-helm-blue-green-sample`  
`gitRepo.branch`
Provide the branch of your fork to use to resolve the source code.
`branch: main`  
`appImage.registry`
Provide the name of the Artifactory integration you added in step 4.
`registry: my_artifactory`  
`appImage.registryUrl`
Provide the url to your Artifactory environment.
`registryUrl: myartifactory.myorg.com`  
`appChart.sourceArtifactory`
Provide the name of the Artifactory integration you added in step 4.
`sourceArtifactory: my_artifactory`  
`runtime.k8s_integration`
Provide the name of the Kubernetes integration you added in step 4.
`k8s_integration: my_k8s`  
`runtime.namespace`
Provide the name of the Kubernetes namespace where the Helm chart will be deployed.
`namespace: my_namespace`  
And that's it. Your configuration is ready to go!
### Note
 **All pipeline definitions are global across JFrog Pipelines within a Project.** The names of your pipelines and resources need to be unique within the Project in JFrog Pipelines.
  6.  **Add Pipeline Sources**
A Pipeline Source represents the git repository where our pipelines definition files are stored. A pipeline source connects to the repository through an  integration, which we added in step 4.
    1. In your left navigation bar, go to **Administration |** **Pipelines | Pipeline Sources**. Click **Add a Pipeline Source** and then choose **From YAML**. Follow instructions to add a Pipeline Source. This automatically adds your configuration to the platform and pipelines are created based on your YAML.
    2. After your pipeline source syncs successfully, navigate to **Pipelines | My Pipelines** in the left navbar to view the newly added pipeline. In this example, `helm_blue_green_pipeline` is the names of your pipeline.
    3. Click the name of the pipeline. This renders a real-time, interactive, diagram of the pipeline and the results of its most current run.
  7.  **Execute the Pipeline**
You can trigger the pipeline by committing a change to your repository, or by manually triggering it through the UI.  
  8.  **Success!**
You have successfully executed the Helm Blue/Green pipeline! You can verify the results by viewing the Services deployed to your Kubernetes cluster.


#### Explanation of Pipeline Definition
Let us now take a look at the pipeline definition files and what each section means.
The **pipelines.yml** file contains the templatized definition of your pipeline. This consists of the following:
  * Resources are entities that contain information that is consumed or generated by pipeline steps. In our example, we use the following resources:
    * A GitRepo resource pointing to the source control repository where your application code is present. You can configure this resource to trigger dependent steps on specific events.
    * An Image resource that adds a reference to a Docker image to your pipeline.
    * A HelmChart resource that adds a reference to a Helm Chart to your pipeline.
  * Steps are executable units that form your pipeline. In our example, the pipeline consists of the following steps:
    * A  DockerBuild native step that builds your Docker image based on a Dockerfile present in the git repo.
    * A DockerPush native step that published the built image to Artifactory.
    * A  HelmPublish native step that packages and publishes your Helm Chart to Artifactory.
    * A HelmBlueGreenDeploy native step that deploys the Helm Chart to your Kubernetes cluster using the Blue/Green deployment strategy. The chart will be deployed using the details of the release playing the Idle role and the configured Idle role Services will be create or updated. Any final stage validation to ensure the new version is ready to be available to users could be added between this step and the next one.
    * A HelmBlueGreenRoleSwitch native step that promotes the deployed Helm release to the Live role in the Blue/Green strategy. The configured Live role Services will be created or updated. Any after release validation to ensure the new release is successful could be added after this step. A copy of this step could be added downstream to rollback the release in case the validation is not successful.
    * A HelmBlueGreenCleanup native step the uninstalls any previously deployed Helm release.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
23,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a defining-a-pipeline?[/INST] This page provides a high-level overview of the structure of a pipeline configuration file.
JFrog Pipelines uses its own declarative language based on YAML syntax, Pipelines DSL, to describe workflows. You create pipelines in text files written in Pipelines DSL, which we refer to as a **pipeline config**. You can create these files in any text editor of your choice.
You must store your pipeline config file(s) in a source code repository (for example, GitHub). When Pipelines has been configured to use this repo as a pipeline source, the config files will be automatically read and the workflows loaded into Pipelines and run.
#### Pipeline Config Structure
There are two top-level sections that can be defined in a pipeline config:
  * A `resources` section that specifies the Resources used by the automated pipeline.
  * A `pipelines` section that specifies the pipeline execution environment and the Steps to execute.


For ease of illustration, we'll describe how this looks in a single pipeline config file (e.g., `pipelines.yml`).
#### Resources Section
Resources provide the information that steps need in order to execute or store information generated by a step. For example, a resource might point to a source code repository, a Docker image, or a Helm chart. A list of all supported resources is available in Resources overview.
The basic format of each `resources` declaration is:
Tag
Description  
`name`
A globally unique friendly name for the resource.  
`type`
A predefined string that specifies the type of resource.
For more information, see Resource Types.  
`configuration`
Begins the section of settings required by the resource `type`. This typically includes the name of the integration that connects the resource to the external service.  
### Note
Resource definitions are global and can be used by all pipelines in a Project that is in at least one of the same environments. This means that resource names must be unique across all pipeline config files in a Project.
For example, here is a `resources` section that defines two resources, a GitRepo and a Docker Image:GitRepo
[YAML]
    resources:
      - name: my_Git_Repository
        type: GitRepo
        configuration:
          gitProvider: my_GitHub_Integration
          path: ~johndoe/demo
          branches:
            include: master
    
      - name: my_Docker_Image
        type: Image
        configuration:
          registry: my_Docker_Registry_Integration
          imageName: johndoe/demo_image
          imageTag: latest
[/YAML]
#### Pipelines Section
The `pipelines` section defines the workflow, consisting of steps and the dependencies between them.
The basic format of each `pipelines` declaration is:
Tag
Description  
`name`
A friendly name for the resource, unique within the project.  
`configuration`
An optional section to specify environment variables and/or a `runtime` image for the pipeline to execute in.  
preRun
This is an optional step. When configured, this step will always run at the beginning of a pipeline. This is useful when you want to run some checks at the beginning of a run.
Like Pipelines Steps, preRun must include at least one execution command, such as onExecute, onStart, onSuccess, onFailure, or onComplete. In addition, it can include input and output resources.
[YAML]
    pipelines: 
      - name: PreAndPostRuns 
        preRun: 
          execution: 
            onExecute: 
              - printenv 
              - echo ""Executing Pre Run""
[/YAML]  
postRun
This is an optional step. When configured, this step will always run at the end of a pipeline. This is useful when you want to run some checks at the end of a run.
Like Pipelines Steps, postRun must include at least one execution command, such as onExecute, onStart, onSuccess, onFailure, or onComplete. In addition, it can include input and output resources.
[YAML]
    pipelines: 
      - name: PreAndPostRuns 
        postRun: 
          execution: 
            onExecute: 
              - printenv 
              - echo ""Executing Post Run""
[/YAML]  
This is followed by a collection of` step` sections that specifies the steps to execute.
### Tip
The `name` of the pipeline will be available in the environment variable `$pipeline_name`, which can be used to construct the base name for builds.
##### Pipeline Configuration
The optional `configuration` section can specify an execution environment for all steps in the pipeline. While this configuration can be defined per step, it is sometimes more convenient to define it at a pipeline level if it's going to be the same for all steps in the pipeline.
The basic format of each `configuration` section is:
Tag
Description  
`environmentVariables`
Variables defined here are available for use in every step in the pipeline. These variables are read-only; they cannot be redefined in a step.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME` : If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER` : If set, the pipeline uses this value instead of the default run number for the build info collected.
  * ` USE_LOCAL_JFROG_CLI` : If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.
  * `JFROG_XRAY_URL` : If `jfrogCliVersion` is set to 2, this variable may be used to specify an Xray URL for use when configuring the JFrog CLI with an Artifactory integration. In most cases, the platform URL will be correct and `JFROG_XRAY_URL` is not required.

  
`nodePool`
Optionally specify a specific node pool where your steps will execute. If not specified, then the node pool set as the default will be used.
For more information, see Choosing Node Pools.  
`affinityGroup`
Optionally specify an Affinity Group name to specify that all steps in this Pipeline are part of one Affinity Group. This means that all steps will run on the same build node.
For more information, see Running multiple steps on the same build node.  
`runtime`
This section allows you to specify the default runtime environment for steps in the pipeline. The options are:
  * Run steps directly on the host machine
  * Run steps inside the node pool's default Docker container or one of its language-specific variants
  * Run steps inside a custom Docker container of your choice


For more information, see Choosing your Runtime Image.  
`chronological`
Any runs of the pipeline will not start running while another run of the same pipeline is processing if `chronological` is set to true. The default is false, allowing runs to execute in parallel if there are nodes available.  
`dependencyMode`
Specifies when the pipeline may run relative to other pipelines connected by resources. If any of these three settings are true, new runs will not be created for resources updated by other pipelines if there is already a waiting run with the same resources and steps. So if a pipeline runs twice consecutively and the following pipeline has waitOnParentComplete set to true, the following pipeline will only run once. When the pipelines do run, they will use the latest resource versions.
The optional settings are:
  * `waitOnParentComplete` : If true, the pipeline will not start running when a pipeline that outputs a resource that is an input to this pipeline has a waiting or processing run.
  * `waitOnParentSuccess` : If true, the pipeline will not start running when a pipeline that outputs a resource that is an input to this pipeline has a processing run or the last complete run was not successful.
  * `waitOnChildComplete` : If true, the pipeline will not start running when a pipeline that has an input resource that is the output of this pipeline has a waiting or processing run unless that child pipeline is waiting for this pipeline to complete.

  
`retentionPolicy`
Optionally specify if the pipeline run data should be deleted after a specific number of days. Also, provides the ability to keep a minimum number of pipeline runs data:
  * `maxAgeDays` : Specifies number of days after which the pipeline run data will be deleted (cannot exceed the system level setting). Setting this value to 0 means infinite retention.
  * `minRuns` : Specifies the minimum number of pipeline runs data to keep, regardless of their age (cannot exceed the system level setting).


For more information, see Setting Retention Policy.  
`jfrogCliVersion`
Optionally specify either 1 to use JFrog CLI v1 or 2 to use JFrog CLI v2 in the steps in the pipeline. The default is currently v1.  
`integrations `
Specifies Integrations, similar to the `integrations` section in a step, to be input to all steps in the Pipeline. Integrations listed here may not be listed in the `integrations` section of a step in this Pipeline.
For more information, see Pipelines Integrations.
### Note
Pipeline-level declarations for integrations are applicable for all steps. You cannot choose the steps that will not use those integrations.  
`inputResources`
Specifies Resources, similar to the `inputResources` section in a step, to be inputs to all steps in the Pipeline. Resources listed here may not be listed in the `inputResources` or out `putResources` sections of any step in this Pipeline. Each Resource in `inputResources` should be specified by `name` and `branch` (if from a multi-branch Pipeline Sources) and the `trigger` option may be set to control if updates to the resource should trigger steps in the Pipeline.
For more information, see Using Resources.
### Note
Pipeline-level declarations for input resources are applicable for all steps. You cannot choose the steps that will not use those input resources.  
`outputResources`
Specifies Resources, similar to the out `putResources` section in a step, to be inputs to all steps in the Pipeline. Resources listed here may not be listed in the `inputResources` or out `putResources` sections of any step in this Pipeline. Each Resource in `inputResources` should be specified by `name` and `branch` (if from a multi-branch Pipeline Sources).
For more information, see Using Resources.
### Note
Pipeline-level declarations for output resources are applicable for all steps. You cannot choose the steps that will not use those output resources.  
Any step can override the pipeline's default `runtime` configuration if needed to configure its own runtime selection.
##### Pipeline Steps
Each named pipeline declares a collection of named `step` blocks the pipeline will execute.
The basic format of each `step` declaration is:
Tag
Description  
`name`
A friendly name for the step that may be referenced in other steps. Step names must be unique within the same pipeline.  
`type`
A predefined string that specifies the type of step.
For more information, see Pipelines Steps.  
`configuration`
Begins the section of settings required by the step `type`. This may include:
  * Environment variables local to the step
  * Any runtime configuration for the step
  * Any triggering input steps or resources
  * Any resources output by the step
  * Any integrations used by the step
  * All settings required by the step type

  
`execution`
Specifies the actions to perform for each execution phase of the step.  
For example, here is a simple sequence of two steps. Each uses the generic Bash step to output text to the console:
[YAML]
        steps:
        - name: step_1
          type: Bash
          configuration:
            inputResources:
              - name: my_Git_Repository     # Trigger execution on code commit
          execution:
            onExecute:
              - echo ""Hello World!""
     
        - name: step_2
          type: Bash
          configuration:
            inputSteps:
              - name: step_1               # Execute this step after the prior step
          execution:
            onExecute:
              - echo ""Goodbye World!""
[/YAML]
#### Pipeline Config File Strategies
A pipeline config file can have one or more pipelines defined in it, but the definition of a single pipeline cannot be fragmented across multiple files. Pipeline config filenames can be any form you choose, although the convention for a single file is `pipelines.yml`
Some things to note about pipelines:
  * You can have as many pipeline config files as you want. For example, our customers manage config in the following different ways:
    * Maintain a central DevOps repository and keep all pipeline config files for all projects in that repository.
    * Keep pipeline config files that build each microservice with the source code for that microservice.
    * Separating out pipeline steps and resources into separate config files (for example, `pipelines.steps.yml `and `pipelines.resources.yml` respectively).



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
24,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pipelines-integrations?[/INST] An **Integration** connects Pipelines to an external service/tool. Each integration type defines the endpoint, credentials and any other configuration detail required for Pipelines to exchange information with the service. All credential information is encrypted and held in secure storage, in conformance with best security practices.
#### Using Integrations
Integrations are most commonly used in resources to connect inputs and outputs to the external facility. They can also be referenced directly in many utility functions. Integrations can also be directly used as an input to a step.
Integrations are added through the JFrog Platform. For more information, see Managing Pipelines Integrations.
### Note
All passwords and keys used in integrations are secure and they are masked when printed.
##### Environment Variables
A step that uses an integration can access the integration and its properties through environment variables. The form of the environment variable is:
`int_<integration name>_<tag>`
For example, if an Artifactory Integration is named `myArtifactory`, then the environment variable `int_myArtifactory_url` might be used to send a notification:
[YAML]
    steps:
      . . .
    
      - name: publish_go_binary
      type: GoPublishBinary
      configuration:
        inputSteps:
        - name: build_go
        targetRepository: my-go
        integrations:
        - name: myArtifactory
       execution:
         onSuccess:
           - send_notification notifySlack --text ""Published Go binary to $int_myArtifactory_url""
    
[/YAML]
For more information about using environment variables, see Pipelines Environment Variables.
#### Integration Types
The available integrations for JFrog Pipelines are:
  * Airbrake Integration
  * Artifactory Integration
  * AWS Keys Integration
  * Azure Keys Integration
  * Bitbucket Integration
  * Bitbucket Server Integration
  * Digital Ocean Integration
  * Distribution Integration
  * Docker Registry Integration
  * File Server Integration
  * Generic Integration
  * GitHub Enterprise Integration
  * GitHub IntegrationGitHub Integration
  * GitLab Integration
  * Google Cloud Integration
  * Incoming Webhook Integration
  * Jenkins Integration
  * Jenkins Server Integration
  * JFrog Platform Access Token Integration
  * Jira Integration
  * Kubernetes Integration
  * NewRelic Integration
  * Outgoing Webhook Integration
  * PagerDuty Events Integration
  * PEM Key Integration
  * Slack Integration
  * SMTP Credentials Integration
  * SSH Key Integration



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
25,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a airbrake-integration?[/INST] The Airbrake integration is used to connect Jfrog Pipelines to Airbrake for notifying deployments performed through Pipelines.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create an Airbrake integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **Url** \-- Airbrake API endpoint. This is already hard-coded to https://airbrake.io/api/v4
  *  **Token** \-- Airbrake API Token


##### Usage
The Airbrake integration can be used in the send_notification utility function.
###### Default Environment Variables
When you create a step that uses the integration in an `inputIntegrations` entity, a set of environment variables is automatically made available that you can use in your scripts.
Environment variable
Description  
`int_<integration_name>_url`
The API Endpoint used to connect to Airbrake  
`int_<integration_name>_token`
The Token used to connect to Airbrake  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
26,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a artifactory-integration?[/INST] The Artifactory Integration connects your JFrog Pipeline Automation platform to an installation of Artifactory to push artifacts including Docker images, and maintain build information.
##### Creating an Integration
Add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- Choose a friendly name for the integration
  *  **Artifactory URL** \-- The Artifactory URL is automatically added for the user adding the integration. If required, you can manually enter the HTTP Endpoint (URL) for your JFrog Artifactory.
  *  **User** \-- The Artifactory username is automatically added for the user adding the integration. If required, you can manually enter the username for your JFrog Artifactory.
  *  **API key** \-- Your Artifactory API Key.Introduction to the User Profile
Click **Get API Key** to automatically fetch API key for the user adding the integration. If the API key does not exist, a new one is generated. If required, you can manually enter the API key for your JFrog Artifactory.


##### Usage
 **With native steps** : An Artifactory integration is mandatory in the `integrations` section of the following native steps:
  * DockerBuild
  * DockerPush
  * GradleBuild
  * MvnBuild
  * NpmBuild
  * NpmPublish
  * PromoteBuild
  * PublishBuildInfo
  * GoBuild
  * GoPublishBinary
  * GoPublishModule


 **In resources** : An Artifactory integration must/may be used in the following resources:
 **Mandatory**
  * Aql
  * BuildInfo


 **Optional**
  * FileSpec
  * Image
  * RemoteFile


###### Default Environment Variables
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration_name>_url`
URL of Artifactory  
`int_<integration_name>_user`
Username supplied in the integration  
`int_<integration_name>_apikey`
Access token supplied in the integration  
When you create an Aql, BuildInfo, or FileSpec resource with this integration, a set of environment variables is automatically made available in steps using that resource.
Environment variable
Description  
`res_<resource_name>_sourceArtifactory_url`
URL of Artifactory  
`res_<resource_name>_sourceArtifactory_user`
Username supplied in the integration  
`res_<resource_name>_sourceArtifactory_apikey`
Access token supplied in the integration  
When you create an Image resource with this integration, a set of environment variables is automatically made available in steps using that resource.
Environment variable
Description  
`res_<resource_name>_registry_url`
URL of Artifactory  
`res_<resource_name>_registry_user`
Username supplied in the integration  
`res_<resource_name>_registry_apikey`
Access token supplied in the integration  
When you create a RemoteFile resource with this integration, a set of environment variables is automatically made available in steps using that resource.
Environment variable
Description  
`res_<resource_name>_source_url`
URL of Artifactory  
`res_<resource_name>_source_user`
Username supplied in the integration  
`res_<resource_name>_source_apikey`
Access token supplied in the integration  
##### Examples
The following example creates a Docker image that is published to Artifactory. It assumes that an Artifactory integration named `myArtifactory` has been created.
  * This example uses a values.yml file to store the pipeline definitions.
  * In addition to the Artifactory integration, this example uses a GitHub Integration.
  * The YAML for this example is available in this repository in the JFrog GitHub account.
  * For a full tutorial, see Pipeline Example: Docker Build and Push.


[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: dbp_repo
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.gitIntegration }}                      
          path: {{ .Values.gitRepositoryPath }} 
          branches:
            include: master
    
      - name: dbp_image
        type: Image
        configuration:
          registry: {{ .Values.artifactoryIntegration }}
          sourceRepository:  {{ .Values.sourceRespository }}
          imageName:  {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}
          imageTag: latest
          autoPull: true
          
      - name: dbp_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
          buildName: dbp_build
          buildNumber: 1    
    
      - name: dbp_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
          buildName: dbp_build
          buildNumber: 1
          
    
    pipelines:
      - name: pipeline_dbp
        steps:
          - name: docker_build
            type: DockerBuild
            configuration:
              affinityGroup: dbp_group          # affinity group string that is the same as specified in the DockerPush step
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName:  {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}
              dockerImageTag: ${run_number}
              inputResources:
                - name: dbp_repo
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
    
          - name: docker_push
            type: DockerPush
            configuration:
              affinityGroup: dbp_group          # affinity group string that is the same as specified in the DockerBuild step
              targetRepository: docker-local
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
              inputSteps:
                - name: docker_build
              outputResources:
                - name: dbp_image
    
          - name: publish_dbp_build
            type: PublishBuildInfo
            configuration:
              inputSteps:
                - name: docker_push
              outputResources:
                - name: dbp_build_info
    
          - name: promote_dbp_build
            type: PromoteBuild
            configuration:
              targetRepository: demo-pipelines
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
              inputResources:
                - name: dbp_build_info
              outputResources:
                - name: dbp_promoted_build_info
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
27,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a aws-keys-integration?[/INST] The **AWS Keys** integration is used to provide access and secret keys to AWS services in order to connect to JFrog Pipelines.
The AWS Keys integration is used to connect to AWS services that include:
  * ECR
  * ECS
  * EC2
  * S3
  * Any other service that needs AWS Access and Secret keys to connect


##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Access Key Id** \-- access key Id obtained from AWS
  * **Secret Access Key** \-- secret access key obtained from AWS


##### Usage
The AWS Keys integration can be used in the following resources:
  * Image
  * RemoteFile


An AWS Keys integration can also be added directly to a step in the `integrations` section. This automatically configures the AWS CLI with the access key and secret key.
You can use this integration in any workflow where you need to connect to AWS for some reason, such as provisioning infrastructure, deploying to AWS EC2 or Amazon ECS, etc.
###### Default Environment Variables
When you create an Image resource with this integration, a set of environment variables is automatically made available in steps using that resource.
Environment variable
Description  
`res_<resource_name>_registry_accessKeyId`
Access key supplied in the integration  
`res_<resource_name>_registry_secretAccessKey`
Secret key supplied in the integration  
When you create a Remote File resource with this integration, a set of environment variables is automatically made available in steps using that resource.
Environment variable
Description  
`res_<resource_name>_source_accessKeyId`
Access key supplied in the integration  
`res_<resource_name>_source_secretAccessKey`
Secret key supplied in the integration  
When you add this integration to a step directly, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration_name>_accessKeyId`
Access key supplied in the integration  
`int_<integration_name>_secretAccessKey`
Secret key supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
28,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a azure-keys-integration?[/INST] The Microsoft Azure Integration is used to connect JFrog Pipelines to Microsoft Azure to manage cloud services and entities.
This integration uses a password-based authentication service principal for connecting to and commanding your Azure cloud service. You must create an Azure service principal for this purpose, and provide the generated keys to this integration.
For example, using the Azure CLI service principal creation function:
`az ad sp create-for-rbac --role=""Contributor"" --scopes=""/subscriptions/SUBSCRIPTION_ID""`
will produce the needed key values:  
Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **App Id** \-- the `appId` key used for service principal authentication.
  * **Password** \-- the `password` key used for service principal authentication.
  * **Tenant** \-- the `tenant` key used for service principal authentication.


##### Usage
An Azure Keys integration can be used in Bash steps for access to its properties through environment variables. This integraton cannot currently be used with any resources.
Make sure that your service principal has adequate permission to perform whatever actions you plan to take in your custom script. This could mean ACR reader role to pull images, AKS contributor to create deployments, etc.
### Required for Dynamic Nodes
In order for Pipelines to run dynamic node pools on Azure, you will need to accept Azure Marketplace image terms so that the JFrog Pipelines buildplane VM images can be used. The following Azure CLI commands must be run with the Azure service principal used by this integration:
[YAML]
    az vm image terms accept --plan x86_64-ubuntu_16_04 --offer pipelines-buildplane --publisher jfrog
    az vm image terms accept --plan x86_64-ubuntu_18_04 --offer pipelines-buildplane --publisher jfrog
    az vm image terms accept --plan x86_64-centos_7 --offer pipelines-buildplane --publisher jfrog
    az vm image terms accept --plan x86_64-windows_server_2019 --offer pipelines-buildplane --publisher jfrog
[/YAML]
###### Default Environment Variables
When you create a resource with this integration, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_appId`
AppId supplied in the integration  
`int_<integration-name>_password`
Password supplied in the integration  
`int_<integration-name>_tenant`
Tenant supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
29,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a digital-ocean-integration?[/INST] Digital Ocean Integration is used to connect JFrog Pipelines to Digital Ocean to interact with its cloud services in order to provision machines.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Token** \-- Token to connect to the Digital Ocean API


##### Usage
The Digital Ocean integration can be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`name_<integration-name>_apitoken`
API Token supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
30,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a bitbucket-integration?[/INST] A Bitbucket Integration is used to connect the JFrog Pipelines platform to bitbucket.org.
##### Creating an Integration
You can add this integration by following the steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **URL** \-- Bitbucket API endpoint. This is already hard-coded to https://api.bitbucket.org
  *  **User Name** \-- Username to connect to Bitbucket. This is the username found on the account settings page ( **Personal Settings** → **Bitbucket profile settings** ), and not your email Id or display name.
  *  **Token** \-- Bitbucket token with the permissions needed to run your commands


### Note
Bitbucket integration requires the following permissions:
  *  **Account** : Read
  *  **Pull requests** : Read
  *  **Workspace membership** : Read
  *  **Projects** : Read
  *  **Repositories** : Admin
  *  **Webhooks** : Read and Write


##### Usage
The Bitbucket integration can be used with the GitRepo resource.
The Bitbucket integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it in the `inputResources` section, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_gitProvider_url`
Bitbucket API location  
`res_<resource-name>_gitProvider_username`
Username used to connect to Bitbucket  
`res_<resource-name>_gitProvider_token`
The token used to connect to Bitbucket  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
Bitbucket API location  
`int_<integration-name>_username`
Username used to connect to Bitbucket  
`int_<integration-name>_token`
The Token used to connect to Bitbucket  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
31,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a bitbucket-server-integration?[/INST] A Bitbucket Server Integration is used to connect the JFrog Pipelines platform to an installation of Bitbucket Server.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **URL** \-- Bitbucket Server API endpoint. For example: `https://bitbucket.myserver.com/`
  *  **User Name** \-- username to connect to Bitbucket Server
  *  **Password** \-- password for the user account on Bitbucket Server


### Note
Bitbucket Server integration requires Admin permission for repositories.
##### Usage
The Bitbucket Server integration is most commonly used with the GitRepo resource or while adding a Pipeline source.
The Bitbucket Server integration can also be added directly to a step in the `integrations` section.
### URL Specification
When a Bitbucket Server repository is part of a project, the URL uses the project key rather than the project name. So if the ""Teams in Space"" project key is TIS, then the ""buildPipe"" repository in that project is addressed as TIS/buildpipe.
When a Bitbucket Server repository is a personal repository, the '~' (tilde) character must precede the user slug in the URL. For example: ~janedoe/test.
For additional information, see Atlassian Stash REST API Reference.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it in the `inputResources` section, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_gitProvider_url`
Bitbucket Server API location  
`res_<resource-name>_gitProvider_username`
Username used to connect to a Bitbucket Server user account  
`res_<resource-name>_gitProvider_password`
The password used to connect to the Bitbucket Server user account  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
Bitbucket Server API location  
`int_<integration-name>_username`
Username used to connect to the Bitbucket Server user account  
`int_<integration-name>_password`
The password used to connect to the Bitbucket Server user account  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
32,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a distribution-integration?[/INST] The Distribution Integration connects your JFrog Pipeline Automation platform to a Distribution instance.
##### Creating an Integration
Add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- Choose a friendly name for the integration
  *  **Distribution URL** \-- The Distribution URL is automatically added for the user adding the integration. If required, you can manually enter the HTTP Endpoint (URL) for your JFrog Distribution.
  *  **User** \-- The Distribution username is automatically added for the user adding the integration. If required, you can manually enter the username for your JFrog Distribution.
  *  **API Key** \-- Your Distribution API key.
Click **Get API Key** to automatically fetch API key for the user adding the integration. If the API key does not exist, a new one is generated. If required, you can manually enter the API key for your JFrog Distribution.
  *  **Signing Key Passphrase** \-- Your optional GPG signing key passphase.


##### Usage
A Distribution integration is used in the following resources:
  * DistributionRule
  * ReleaseBundle


###### Default Environment Variables
When you create a DistributionRule or ReleaseBundle resource with this integration, a set of environment variables is automatically made available that you can use in your scripts.
Environment variable
Description  
`res_<resource_name>_sourceDistribution_url`
Distribution URL  
`res_<resource_name>_sourceDistribution_user`
Username supplied in the integration  
`res_<resource_name>_sourceDistribution_apikey`
API Key supplied in the integration  
`res_<resource_name>_sourceDistribution_signingKeyPassphrase`
GPG signing key passphrase supplied in the integration  
When you add this integration directly to a step, a set of environment variables is automatically made available that you can use in your scripts.
Environment variable
Description  
int_<integration_name>_url
Distribution URL  
int_<integration_name>_user
Username supplied in the integration  
`int_<integration_name>_apikey`
API Key supplied in the integration  
`int_<integration_name>_signingKeyPassphrase`
GPG signing key passphrase supplied in the integration  
##### Examples
This example uses a BuildInfo resource to create, sign and distribute a release bundle. It assumes that a Distribution integration named `myDist` has been created.
  * In addition to the Distribution integration, this example requires an Artifactory integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


 **CreateReleaseBundle**
[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.myArtifactoryIntegration }}
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.demoArtifactoryIntegration }}
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
33,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a docker-registry-integration?[/INST] The Docker Registry Integration is used to connect the JFrog Pipelines platform to Docker Hub, Docker Trusted Registry, or a Docker registry in Artifactory so that you can pull and push Docker images.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **url** \-- an optional field that defaults to Docker Hub if left empty. To use with Docker Trusted Registry or Docker Private Registry, enter the location of your private registry. Format `https://foo.com`
  *  **User Name** \-- username of your Docker Registry Account
  *  **Password** \-- password of your Docker Registry Account


##### Usage
The Docker Registry integration can be used in the Image resource.
The Docker Registry integration can also be added directly to a step in the `integrations` section. This automatically configures the Docker CLI with access to the registry.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_registry_url`
URL supplied in the integration  
`res_<resource-name>_registry_username`
Username supplied in the integration  
`res_<resource-name>_registry_password`
Password supplied in the integration  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
URL supplied in the integration  
`int_<integration-name>_username`
Username supplied in the integration  
`int_<integration-name>_password`
Password supplied in the integration  
##### Example
  * This example uses a Docker Registry integration named `myDocker`, which is used in the `Image_1` resource.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:  
      - name: Image_1
        type: Image 
        configuration:         
          registry: myDocker              
          imageName: docker/jfreq_win             
          imageTag: latest  
          autoPull: true
    
    pipelines:
      - name: pipelines_Image_1
        steps:
          - name: step_Image_1
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: Image_1
            execution:
              onExecute:
                - write_output ""executing step...""
                - if($Image_1_isTrigger -ne ""true""){
                  Write-Error ""Stop"" -ErrorAction Stop
                  }
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
34,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a file-server-integration?[/INST] The **File Server** integration is used to connect JFrog Pipelines a remote File server to download and upload files.
##### Creating an Integration
You can create this from the integrations page by following instructions here: Managing Pipelines Integrations.
This is the information you would require to create this integration:
  *  ** **Name**** \-- choose a friendly name for the integration
  *  **Protocol** \-- select FTP, SFTP, or SMB
  *  **url** \-- the URL of your file server
  *  **User Name** \-- username for access to the file server
  *  **Password** \-- password for access to the file server


##### Usage
A File Server integration is required for the `source` tag of the RemoteFile resource.
###### Default Environment Variables
When you create a resource with this integration, a set of environment variables is automatically made available that you can use in your scripts.
Environment variable
Description  
`int_<integration_name>_url`
URL of the file server  
`int_<integration_name>_username`
Username supplied in the integration  
`<int_<integration_name>_password`
Password supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
35,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a generic-integration?[/INST] A Generic integration is used to store key/value information that can be accessed in steps as environment variables. Because all integration properties are stored in a secure, encrypted vault, this is suitable for storing secrets such as username/password credentials.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **Custom Environment Variables** \-- the _Key_ and _Value_ pair to store. Select **Secure** to secure the custom environment variable.


You can add as many key-value pairs as you need in the integration by clicking **Add** for each.
##### Usage
The Generic integration can be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you add this integration to a step, All key-value pairs are made available as environment variables.
Environment variable
Description  
`int_<integration-name>_<key>`
The value stored for the custom environment variable.  
####### Example
You can define a Generic integration with the following entries:
  *  **Name** : `myCredentials`
  *  **Custom Environment Variables** :
    * Key: `username`
    * Value: `janedoe`
    * Key: `password`
    * Value: `nAm30fMyp3t`


When `myCredentials` is specified in a step's `integrations` block, the key-value pairs stored there can then accessed in the step as environment variables:
[YAML]
    pipelines:
      - name: generic_integration_example
        steps:
        - name: step_1
          type: Bash
          configuration:
            integrations:
              - name: myCredentials
          execution:
            onExecute:
              - printenv $int_myCredentials_username
              - printenv $int_myCredentials_password
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
36,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a github-integration?[/INST] The GitHub Integration is used to connect the JFrog Pipelines platform to GitHub.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- Choose a friendly name for the integration
  *  **url** \-- Your GitHub API endpoint. This is already hard-coded to https://api.github.com
  *  **Token** \-- Your GitHub personal access token with the permissions needed to run your jobs
### Token permissions
To use this integration as a pipeline source, your token must be granted these permissions at minimum:
    *  **repo** (all)
    *  **admin:repo_hook** (read, write)
    *  **admin:public_key** (read, write)
Additionally, if your organization uses SAML single sign-on, you will need to authorize your token.
  *  **HTTP Header** \-- Optional. Use the key and value fields to provide the name and value for the custom HTTP header you would like to pass in the HTTP requests for Git clone events.


##### Usage
The GitHub integration can be used in the GitRepoGitRepo resource.GitRepo
The GitHub integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_gitProvider_url`
GitHub API location  
`res_<resource-name>_gitProvider_token`
The Token used to connect to GitHub  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
GitHub API location  
`int_<integration-name>_token`
The Token used to connect to GitHub  
##### Examples
This example shows how to configure an NpmBuild step using default locations and default commands. It assumes that a GitHub integration named m`yGithub` has been created.
  * This example uses a values.yml file to store the pipeline definitions.
  * The YAML for this example is available in this repository in the JFrog GitHub account.
  * For more information about running this example, see Pipeline Example: Npm Build.


 **NpmBuild**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: npm_repo_jfp_example
        type: GitRepo
        configuration:
          # SCM integration where the repository is located
          gitProvider: {{ .Values.myRepo.gitProvider }}
          # Repository path, including org name/repo name
          path: {{ .Values.myRepo.path }}
          branches:
            # Specifies which branches will trigger dependent steps
            include: master
    
      - name: npm_buildinfo_jfp_example
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
    
    pipelines:
      - name: npm_pipeline_jfp_example
        steps:
          - name: npm_build_step
            type: NpmBuild
            configuration:
              repositoryName: npm-virtual      # required, npm repository name on artifacctory
              sourceLocation: .      # required, location of package.json file
              integrations:
                - name:  demoArt  # required
              inputResources:
                - name: npm_repo_jfp_example         # required
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
37,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a github-enterprise-integration?[/INST] The GitHub Enterprise Integration is used to connect JFrog Pipelines to your instance of GitHub Enterprise Server.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **url** \-- location of your GHE server API. The format is `https://(GitHub Enterprise URL)/api/v3`
  * **Token** \-- your GitHub personal access tokenwith the permissions needed to run your job
### Token permissions
To use this integration as a pipeline source, your token must be granted these permissions at minimum:
    * **repo** (all)
    * **admin:repo_hook** (read, write)
    * **admin:public_key** (read, write)
Additionally, if your organization uses SAML single sign-on, you will need to authorize your token.


##### Usage
The GitHub Enterprise integration can be used in the GitRepo resource.
The GitHub Enterprise integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses the resource, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_gitProvider_url`
GitHub Enterprise API location  
`res_<resource-name>_gitProvider_token`
Token used to connect to GitHub Enterprise  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
GitHub Enterprise API location  
`int_<integration-name>_token`
Token used to connect to GitHub Enterprise  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
38,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a gitlab-integration?[/INST] The GitLab Integration is used to connect JFrog Pipelines to your instance of GitLab so that you can run builds for your repositories hosted there.
### Note
Pipelines does not support GitLab subgroups.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **URL** \-- location of your GitLab API. The URL should be in the format `https://(GitLab URL)/api/(api version)`. For example, if you're using gitlab.com, this will be `https://gitlab.com/api/v4`. Please note that if you're using Gitlab version 9.0 or later, you should use `v4` for api version. If you're using Gitlab version 8.17 or earlier, you should use `v3` for api version. API v3 is unsupported from Gitlab 9.5 according to this Gitlab notice
  *  **Token** \-- GitLab private token with the right levels of permission
### Note
 **Token Permissions**
To use this integration as a pipeline source, your token must be granted these scopes at minimum:
    *  **api**
    *  **write_repository**
Additionally, if your organization uses SAML single sign-on, and you want your tokens to work with the SAML, you will have to configure it.


##### Usage
The GitLab integration can be used in the GitRepo resource.
The GitLab integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_gitProvider_url`
GitLab API location  
`res_<resource-name>_gitProvider_token`
The token used to connect to GitLab  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`name_<integration-name>_url`
GitLab API location  
`name_<integration-name>`_token
The Token used to connect to GitLab  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
39,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a google-cloud-integration?[/INST] The Google Cloud Integration is used to connect JFrog Pipelines to Google Cloud and manage entities and services provided by Google Cloud.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
This is the information you would require to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **JSON Key** \-- JSON Security Key for Google Cloud


##### Usage
The Google Cloud integration can be used in the following resources:
  * Image
  * RemoteFile


The Google Cloud integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create an Image resource with this integration, and create a step that uses the resource, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_registry_jsonKey`
JSON key supplied in the integration  
When you create a Remote File resource with this integration, and create a step that uses the resource, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_source_jsonKey`
JSON key supplied in the integration  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_jsonKey`
JSON key supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
40,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a jenkins-integration?[/INST] The Jenkins Integration is used to connect the JFrog Pipelines platform to Jenkins. This will enable a Jenkins build job to trigger execution of a pipeline in Pipelines.
### Deprecated
As of Pipelines 1.6.0, this integration is deprecated and unavailable for new use. Pipelines using this integration will still function, but it has been retired in favor of the Jenkins Server Integration.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **url** \-- enter the HTTP endpoint (URL) of your Jenkins
  * **callback user** \-- JFrog Platform username to use for basic authentication in the callback URL to Pipelines
  * **callback password** \-- JFrog Platform user password


##### Usage
The Jenkins integration can be used in the BuildInfo resource.
A Jenkins integration can be added directly to a step in the `integrations` section to access its environment variables.
Once the integration has been created, it provides the format of the callback URL and curl command lines that your Jenkins job must invoke to trigger execution of a pipeline in Pipelines.  
###### Default Environment Variables
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
HTTP endpoint of Jenkins  
`int_<integration-name>_hookUsername`
Callback Username  
`int_<integration-name>_hookPassword`
Callback Password  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
41,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a jenkins-server-integration?[/INST] The Jenkins Integration is used to connect the JFrog Pipelines platform to Jenkins through the Jenkins Artifactory Plugin.Configuring Jenkins Artifactory Plug-in
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **url** \-- enter the HTTP endpoint (URL) of your Jenkins server
  * **Jenkins user** \-- the Jenkins account user name
  * **Jenkins API token** \-- the Jenkins user's API token for authentication
  * **Token to configure Jenkins Plugin** \-- generate the required **Token to configure Jenkins Plugin** to complete the integration.


### Note
For information about using the Jenkins server integration, see Using Jenkins With Pipelines.
##### Usage
The Jenkins Server integration can be used with the Jenkins native step. Operation requires that the Jenkins Artifactory Plugin be installed in your Jenkins server.Configuring Jenkins Artifactory Plug-in
For Jenkins to successfully interoperate with Pipelines, a bearer token generated by Pipelines must be provided to the required plugin for authentication of your JPD user account.
###### Configuration
To complete the Jenkins Server integration and connect it to the Jenkins Artifactory Plugin:
  1. In the Jenkins Server Integration UI, enter the required configuration information.
  2. Next to the field **Token to configure Jenkins Plugin** , click **Generate**.  
  3. Once the token is generated, click **Copy to Clipboard**.  
  4. In the Jenkins Artifactory Plugin configuration, in the **JFrog Pipelines server** section, **Add** the token to the _Credentials **.** _, pasting the value from your clipboard.
  5. Return to Pipelines, and open the Jenkins Server Integration again for editing. You should now see a **Callback URL** presented. Click **Copy webhook url to clipboard**.  
  6. In the Jenkins Artifactory Plugin configuration, past this URL into the _Integration URL_ field.  


###### Default Environment Variables
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
HTTP endpoint of Jenkins  
`int_<integration-name>_jenkinsUsername`
Jenkins username  
`int_<integration-name>_jenkinsToken`
Jenkins API token  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
42,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a jfrog-platform-access-token-integration?[/INST] The JFrog Platform Access Token integrationgives steps the ability to utilize an access token to interact with different pieces of the JFrog platform. These tokens can be used for many of JFrog's REST APIs, including the Pipelines API. This integration is also required when using the TriggerPipeline native step.
##### Creating an Integration
Add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- Choose a friendly name for the integration
  *  **Platform URL** \-- The Platform URL is automatically added for the user adding the integration. If required, you can manually enter the HTTP Endpoint (URL) for your JFrog Platform.
  *  **Access Token** \-- Your Artifactory API Key. You can automatically generate a token that represents the logged in user from the User Profile page, or you can follow the Access Tokens documentation to create a more nuanced token with particular scopes.Introduction to the User ProfileIntroduction to the User ProfileIntroduction to Access Tokens
When creating an Access Token, ensure that **Token scope** and **User name** are `Admin`.  


##### Usage
 **With native steps** : An Artifactory integration is mandatory in the `integrations` section of the TriggerPipeline native step:
 **Default Environment Variables**
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration_name>_url`
The API Endpoint of the JFrog Platform  
`int_<integration_name>_accessToken`
The token used to access the JFrog Platform  
##### Examples
[YAML]
    pipelines:
      steps:
        - name: scan_controller
          type: TriggerPipeline
          configuration:
            pipelineName: steptarget
            stepName: scanIt
            integrations:
              - name: myPlatformToken
            environmentVariables:
              scan_target:
                default: ""hello-world""
                allowCustom: true
                values:
                  - ""vault""
                  - ""redis""
                  - ""postgresql""
          execution:
            onStart:
              - set_trigger_payload pipelineVariables ""scan_target=${scan_target}""
              - set_trigger_payload stepVariables ""notify=email"" ""uploadReport=true""
            onComplete:
              - echo ""Final status is $nested_run_status""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
43,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a jira-integration?[/INST] A Jira integration is required to create issues on Jira server from JFrog Pipelines steps.
##### Adding an integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Url** \-- Jira URL (for example, `https://jira.mycompany.com`)
  * **Username** \-- account username/email used for login
  * **Token/Password** \-- either a Jira API Tokenfor the account with permissions on the Jira REST APIs(for Jira Cloud), or the account password (for Jira Server)


##### Usage
The Jira integration can be added directly to a step in the `integrations` section.
The  send_notificationutility function can be used with a Jira integration to create an issue in Jira. For example, if a deployment fails, a ticket can be created in Jira through this integration. For more information, see Creating Jira Issues From Pipelines.
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
Jira API endpoint URL  
`int_<integration-name>_username`
Username or email to use to login  
`int_<integration-name>_token`
Authorization to use for invoking the APIs  
##### Example
The following example pipeline demonstrates the use of the `send_notification` utility function to create a Jira issue (ticket).
  * The YAML for this example is available in this repository in the JFrog GitHub account.
  * In addition to the Jira integration, this example uses an Artifactory Integration.
  * This example sets environment variables for the project-id and type options.
  * This example performs a MvnBuild native step, which produces a log file.
  * On failure, this example uses the `send_notification` utility function to create the JIra ticket. The command line:
    * Relies on the environment variables for the project-id and type options
    * Attaches the log file produced by Maven to the Jira issue
    * Specifies the summary and description options using  standard environment variables


[YAML]
    resources:
      - name: my_repo
        type: GitRepo
        configuration:
          gitProvider: MyGithub
          path: myrepo/myproject
    
    pipelines:
      - name: jira_ticket_example
        configuration:
          environmentVariables:
            readOnly:
              NOTIFY_PROJECT_ID: ""DEMO""            # Jira project key for all tickets we create
              NOTIFY_TYPE: ""Bug""                   # Jira issue type for all tickets we create
        steps:
        ### with jira for log attachment
          - name: BuildSample
            type: MvnBuild
            configuration:
              sourceLocation: artifactory-maven-plugin-example
              configFileLocation: .
              configFileName: config
              mvnCommand: ""install -P release --log-file ${step_tmp_dir}/log.txt""
              inputResources:
                - name: my_repo
              integrations:
                - name: myArtifactory
                - name: myJira
            execution:
              onFailure:
                - send_notification myJira --attach-file ""$step_tmp_dir/log.txt"" --description ""Failure occured in $pipeline_name - $step_name"" --summary ""$step_name has failed"" 
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
44,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a incoming-webhook-integration?[/INST] An Incoming Webhook Integration is used to receive a webhook from an external system (for example, GitHub) to trigger a pipeline step.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Authorization Type** -
select **hmac** , **basic** , or **authorization**
  * **Secret** \-- secret to use with hmac authType
  * **User Name** \-- username to use with basic authType to trigger the webhook
  * **Password** \-- password for the user to trigger the webhook for basic authType
  * **Authorization** \-- authorization to use with authorization authType to trigger the webhook


###### Incoming Webhook URL
When an Incoming Webhook integration is created, it generates a unique webhook URL that can be used to post information into any resource that is associated with the integration.  
##### Usage
Incoming Webhook can be used in the Incoming Webhook resource.
The Incoming Webhook integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_webhookName_authType`
Select hmac, basic, or authorization  
`res_<resource-name>_webhookName_secret`
Secret to use with hmac  
`res_<resource-name>_webhookName_username`
Username to use to trigger the webhook  
`res_<resource-name>_webhookName_password`
Password for the user  
`res_<resource-name>_webhookName_authorization`
Authorization to trigger the webhook  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_authType`
Select hmac, basic, or authorization  
`int_<integration-name>_secret`
Secret to use with hmac  
`int_<integration-name>_username`
Username to use to trigger the webhook  
`int_<integration-name>_password`
Password for the user  
`int_<integration-name>_authorization`
Authorization to trigger the webhook  
##### Example
  * This example uses an Incoming Webhook Integration named `MyIncomingWebhookIntegration`. The unique Webhook URL this integration creates can be used to post information into any resource that is associated with the integration. In this example, this integration is used in the resource named `MyIncomingWebhookResource`. To trigger the `ExampleStep`step, the resource`MyIncomingWebhookResource` is used as an input resource in the step.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      - name:               MyIncomingWebhookResource
        type:               IncomingWebhook
        configuration:
          webhookName: MyIncomingWebhookIntegration
    
    pipelines:
      - name: ExamplePipeline
        steps:
        - name: ExampleStep
          type: Bash
          configuration:
            inputResources:
              - name: MyIncomingWebhookResource
          execution:
              onExecute:
                - echo ""$res_MyIncomingWebhookResource_payload"" | jq '.' > payload.json
                - read_json payload.json ""LESSER_SECRET_FORMULA""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
45,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a kubernetes-integration?[/INST] The Kubernetes Integration is used to connect JFrog Pipelines platform to self-hosted Kubernetes clusters so that you can deploy Docker based applications.
A Kubernetes integration can be used for these purposes:
  * To deploy builds to nodes in Kubernetes clusters from pipelines using a HelmChart resource with the HelmDeploy step.
  * To enable Pipelines to create and destroy build nodes in Kubernetes clusters on demand through a  dynamic node pool.


##### Creating an Integration
Kubernetes Integration can be added as:
  * A Pipeline integration, to use Kubernetes for deploying builds from a pipeline
  * An Admin integration, to use Kubernetes for a dynamic node pool


For more information, see Adding an Integration.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Kube Config** \-- configuration YAML to access Kubernetes cluster
### Note
The Kube Config file should include the following permissions:
    * Create/scale/destroy Deployments
    * Create/delete PVCs
    * Create/delete Secrets
    * Create/delete ConfigMaps
Note that ConfigMaps is required only when user is using the  insecure docker registries feature.


##### Usage
The Kubernetes integration can also be added directly to a step in the `integrations` section. This automatically configures the kubectl command line interface with access to the Kubernetes cluster.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_kubernetesConfig_kubeconfig`
Kubernetes cluster configuration  
When add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_kubeconfig`
Kubernetes cluster configuration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
46,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a newrelic-integration?[/INST] A NewRelic integration is used to connect JFrog Pipelines to NewRelic for recording deployments.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** : choose a friendly name for the integration
  * **URL** : NewRelic API endpoint. This is already hard-coded to `https://api.newrelic.com/v2`.
  * **Token** : NewRelic API Token


##### Usage
The NewRelic integration can be added directly to a step in the `integrations` section.
The send_notification utility function can be used with a NewRelic integration to send a notification message.
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
NewRelic API endpoint  
`int_<integration-name>_token`
Token used to communicate with NewRelic  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
47,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a outgoing-webhook-integration?[/INST] An Outgoing Webhook Integration is used by JFrog Pipelines to send webhook to an external system to trigger events on that system.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Webhook URL** \-- URL of endpoint to receive the payload
  * **Authorization Type** -
select hmac, basic, or authorization
  * **Secret** \-- secret to use with hmac authType
  * **User Name** \-- username to use with basic authType for invoking the webhook
  * **Password** \-- password for the user to use with basic authType
  * **Authorization** \-- authorization to use with authorization authType for invoking the webhook in the external system


##### Usage
Outgoing Webhook integration can be used in the Outgoing Webhook resource.
The Outgoing Webhook integration can also be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you create a resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_webhookName_webhookURL`
Webhook URL to receive the outgoing payload  
`res_<resource-name>_webhookName_authType`
AuthType: hmac, basic, or authorization  
`res_<resource-name>_webhookName_secret`
Secret to use with hmac  
`res_<resource-name>_webhookName_username`
Username to use for invoking the webhook  
`res_<resource-name>_webhookName_password`
Password for the user  
`res_<resource-name>_webhookName_authorization`
Authorization to use for invoking the webhook  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_webhookURL`
Webhook URL to receive the outgoing payload  
`int_<integration-name>_authType`
AuthType: hmac, basic, or authorization  
`int_<integration-name>_secret`
Secret to use with hmac  
`int_<integration-name>_username`
Username to use for invoking the webhook  
`int_<integration-name>_password`
Password for the user  
`int_<integration-name>_authorization`
Authorization to use for invoking the webhook  
##### Example
  * This example uses an Outgoing Webhook Integration named `MyOutgoingWebhookIntegration`, which is used in the resource named `My_OutHook_formurl_Resource`. The step `my_send_hook_formurl` uses this resource as an output resource to send a payload to the endpoint specified in the integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


**Outgoing Form URL Webhook**
[YAML] 
    resources:
      - name: My_OutHook_formurl_Resource
        type: OutgoingWebhook
        configuration:
          webhookName: MyOutgoingWebhookIntegration
          path: /url
          headers:
            content-type: application/x-www-form-urlencoded
    
    pipelines:
      - name: ExamplePipeline
        steps:
          - name: my_send_hook_formurl
            type: Bash
            configuration:
              outputResources:
                - name: My_OutHook_formurl_Resource
            execution:
              onExecute:
                - echo ""hello world""
                - formUrlEncodedPayload=""foo=bar&runNumber=$run_number&stepName=$step_name""
                - set_payload My_OutHook_formurl_Resource ""$formUrlEncodedPayload""
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
48,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pagerduty-events-integration?[/INST] The PagerDuty Events integration is used to connect JFrog Pipelines platform to PagerDuty for notifying events.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create an PagerDuty Events integration:
  * **Name** \-- choose a friendly name for the integration.
  * **Token** \-- PagerDuty integration key/token.


##### Usage
The PagerDuty integration can be used in the  send_notification utility function.
###### Default Environment Variables
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_token`
The integration key that identifies your PagerDuty service  
##### Example
  * This example uses a PagerDuty integration named `myPagerDuty`.
  * The send_notification utility function is used to send an `onExecute`notification.
  * The YAML for this example is available in this repository in the JFrog GitHub account.


**Sending notifications**
[YAML] 
    pipelines:
      - name: pipeline_pd
        steps:
          - name: bash_pagerduty_success
            type: Bash
            configuration:
              integrations:
                - name: myPagerDuty
            execution:
              onStart:
                - echo ""starting""
              onExecute:
                - send_notification myPagerDuty -text ""hello world from JFrog Pipelines run number $run_number""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
49,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pem-key-integration?[/INST] The PEM Key Integration is used to connect JFrog Pipelines to VMs that allow PEM-based authorization. This is typically used to SSH in and then run commands on the machine. Tools like Terraform and Ansible use this to execute scripts on a machine.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  *  **Name** \-- choose a friendly name for the integration
  *  **Key** \-- encrypted Key in PEM format


 **Note:** The PEM Key must be generated without passphrase.
##### Usage
The PEM Key integration can be added directly to a step in the `integrations` section.
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_key`
PEM key  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
50,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a slack-integration?[/INST] A Slack integration is required to send notifications to Slack channels or rooms for pipeline events (to learn more about the Slack integration, see Artifactory and Xray Integration with Slack).Artifactory and Xray Integration with Slack
##### Creating an Integration
  * Follow instructions in the Slack docs to create an incoming webhook. While you should specify a default or popular channel while creating the webhook, it can be overriden in your pipeline source when needed.
  * Copy the text in the **Webhook URL** section. It looks like this:
`https://hooks.slack.com/services/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`
  * To add your integration, follow steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
    *  **Name** \-- choose a friendly name for the integration
    *  **Webhook url** \-- Webhook URL to your Slack channel


##### Usage
The Slack integration can be added directly to a step in the `integrations` section.
The send_notification utility function can be used with a Slack integration to send a notification message.
For more information, see Sending Notifications from Pipelines.
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_url`
Webhook URL to your Slack channel  
##### Example
  * This example uses a Slack integration named `mySlack`.
  * The send_notification utility function is used to send the `onStart`and`onExecute`notifications.
  * The YAML for this example is available in this repository in the JFrog GitHub account.


 **Sending notifications**
[YAML]
    pipelines:
      - name: pipe1
        steps:
          - name: test_step_1
            type: Bash
            configuration:
              integrations:
                - name: mySlack       # This is the name of the Slack integration you created in your test instance
                    execution:
                      onStart:
                        - send_notification mySlack --text ""starting test_step_1""         # replace 'mySlack' with the name of your Slack integration
              onExecute:
                - send_notification mySlack --text ""onExecute, past test commands""  # replace 'mySlack' with the name of your Slack integration
                      onSuccess:
                        - send_notification mySlack      # without '--text' to send a generic notification that includes the pipeline name and step name
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
51,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a smtp-credentials-integration?[/INST] An SMTP Credentials integration is used to send notifications for pipeline events using SMTP email.
##### Creating an Integration
To add this integration, follow the steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Host** \-- endpoint to receive the payload
  * **Port** \-- port to use to send the email
  * **Host Name** \-- optional identifying label for the email
  * **Email Sender** \-- optional sender email address
  * **Email Auth User** \-- optional username for authorization to send the email
  * **Email Auth Password** \-- optional password for authorization to send the email
  * **Proxy** \-- optional proxy setting for the email server
  * **Secure** \-- optional secure option to connect to the email server
  * **ignoreTLS** \-- optional TLS option to connect to the email server without using TLS


##### Usage
The SMTP Credentials integration can be added directly to a step in the `integrations` section.
The send_notification utility function can be used with a SMTP Credentials integration to send a notification message.
Example:
`send_notification notifySMTP --body ` `""built docker image 112.32.101.173:8081/docker-local/demo:$PIPELINE_NAME.$RUN_NUMBER""`  
###### Default Environment Variables
When you add this integration to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_name`
Name of the integration  
`int_<integration-name>_host`
Endpoint to receive the payload  
`int_<integration-name>_port`
Port to use to send the email  
`int_<integration-name>_hostname`
Identifying label for the email  
`int_<integration-name>_senderEmail`
Sender email address  
`int_<integration-name>_username`
Username for authorization to send the email  
`int_<integration-name>_password`
Password for authorization to send the email  
`int_<integration-name>_proxy`
Proxy setting for the email server  
`int_<integration-name>_secure`
Secure option to connect to the email server  
`int_<integration-name>_ignoreTLS`
TLS option to connect to the email server without using TLS  
##### Example
  * This example uses an SMTP Credentials integration named notifySMTP.
  * The send_notification utility function is used to send the `onSuccess` and `onFailure` notifications.
  * The YAML for this example is available in this repository in the JFrog GitHub account.


**Sending notifications**
[YAML] 
    pipelines:
      - name: smtp_pipe
        steps:
          - name: test_step_1
            type: Bash
            configuration:
              integrations:
                - name: notifySMTP       # replace with friendly name of your SMTP Credentials integration
            execution:
              onSuccess:
                # replace 'notifySMTP' with the name of your SMTP Credentials integration
                - send_notification notifySMTP --recipients ""buildalerts@mycompany.com"" --subject ""Build Succeeded"" --body ""built docker image 112.32.101.173:8081/docker-local/demo:$PIPELINE_NAME.$RUN_NUMBER""
              onFailure:
                # replace 'notifySMTP' with the name of your SMTP Credentials integration
                - send_notification notifySMTP --recipients ""buildalerts@mycompany.com"" --subject ""Build Failed"" --body ""Failed docker image 112.32.101.173:8081/docker-local/demo:$PIPELINE_NAME.$RUN_NUMBER"" 
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
52,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a ssh-key-integration?[/INST] **SSH Key** Integration is used to connect JFrog Pipelines to VMs that allow SSH based auth. This is typically used to SSH in and then run commands on the machine. Tools like Terraform and Ansible use this to execute scripts on a machine.
##### Creating an Integration
You can add this integration by following steps on the Managing Pipelines Integrations page.
Here is the information you need to create this integration:
  * **Name** \-- choose a friendly name for the integration
  * **Public Key** \-- Public SSH Key
  * **Private Key** \-- Private SSH Key


**Note:** The SSH Key must be generated without passphrase.
##### Usage
The SSH Key integration can be used in the following resources:
  * VmCluster


The SSH Key integration can also be added directly to a step in the `integrations` section. This automatically adds the public and private SSH keys to the SSH client configuration file (~/.ssh) to enable a secure connection through the `ssh` command.
###### Default Environment Variables
When you create a VmCluster resource with this integration, and create a step that uses it, a set of environment variables is automatically made available.
Environment variable
Description  
`res_<resource-name>_sshKey_publicKey`
Public Key supplied in the integration  
`res_<resource-name>_sshKey_privateKey`
Private Key supplied in the integration  
When you add this integration directly to a step, a set of environment variables is automatically made available.
Environment variable
Description  
`int_<integration-name>_publicKey`
Public Key supplied in the integration  
`int_<integration-name>_privateKey`
Private Key supplied in the integration  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
58,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a aql?[/INST] An `Aql` resource specifies an Artifactory query using Artifactory Query Language.Artifactory Query Language
An Aql resource can be used in the `configuration` of a FileSpec resource to specify file properties to match against.
An Aql resource can be used as an `inputresource` for the CreateReleaseBundle step. It defines the query that is used to create a release bundle.
##### YAML Schema
The YAML schema for Aql:
[YAML]
    resources:
      - name:                       <string>
        type: Aql
        configuration:
          sourceArtifactory:   <artifactory integration name>
          query:               <string>
          addedProperties:
            key1:              val1
            key2:              val2
          mappings: 
            - name: mapping1
              input:    
              output: 
            - name: mapping2
              input:    
              output: 
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `Aql `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceArtifactory `
The name of an Artifactory Integration.
Required  
`query `
An Artifactory Query Language query string.Artifactory Query Language
Required  
`addedProperties`
List of added properties that will be added to the artifacts after distribution of the release bundle.
  * `key` \- Property key to be created or updated after distribution of the release bundle.
  * `value` \- List of values to be added to the property key after distribution of the release bundle.


Optional  
`mappings`
List of mappings that are applied to the artifact paths after distribution of the release bundle.
  * `name` \- The friendly name of the mapping entry.
  * `input` \- Regex match string for artifact paths.
  * `output` \- Replacement for artifact paths matched by the ""input"" match string. Capture groups can be used as ""$1"".


Optional  
##### Examples
  * These examples may require an Artifactory integration and/or a Distribution integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


###### Example 1: AQL with build name and number
[YAML]
    resources:
      - name: myAqlResource
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: items.find({""@build.name"":{""$eq"":""buildName""}})
      - name: myReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistributionIntegration
          name: name
          version: '1'
    
    pipelines:
      - name: pipelineToUpdateResource
        steps:
          - name: myBashStep
            type: Bash
            configuration:
              integrations:
                - name: myArtifactoryIntegration
              outputResources:
                - name: myAqlResource
            execution:
              onExecute:
                - write_output myAqlResource query='items.find({""@build.name"":{""$eq"":""myBuildName""}})'
    
      - name: CreateReleaseBundlePipeline
        configuration:
          inputResources:
            - name: myAqlResource
          outputResources:
            - name: myReleaseBundle
        steps:
          - name: myCreateReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: ${JFROG_CLI_BUILD_NAME}
              releaseBundleVersion: ${run_id}
              dryRun: true
[/YAML]
###### Example 2: AQL with file name
[YAML]
    resources:
      - name: myAqlResource
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: 'items.find({""$and"": [{""repo"": {""$eq"": ""example-repo-local""}}, {""name"": {""$match"": ""myFile.txt""}}]})'
    
      - name: myReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistributionIntegration
          name: name
          version: '1'
    
    pipelines:
      - name: myCreateReleaseBundlePipeline
        steps:
          - name: myCreateReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: ${JFROG_CLI_BUILD_NAME}
              releaseBundleVersion: ${run_id}
              dryRun: false
              inputResources:
                - name: myAqlResource
              outputResources:
                - name: myReleaseBundle
[/YAML]
###### Example 3: AQL to add an additional property to the artifacts in the release bundle
[YAML]
    resources:
      - name: myAqlResource
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: 'items.find({""$and"": [{""repo"": {""$eq"": ""example-repo-local""}}, {""name"": {""$match"": ""myFile.txt""}}]})'
          addedProperties:
            myRunNumber: ${run_id}
    
      - name: myReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistributionIntegration
          name: name
          version: '1'
    
    pipelines:
      - name: myCreateReleaseBundlePipeline
        steps:
          - name: myCreateReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: ${JFROG_CLI_BUILD_NAME}
              releaseBundleVersion: ${run_id}
              dryRun: false
              inputResources:
                - name: myAqlResource
              outputResources:
                - name: myReleaseBundle
[/YAML]
###### Example 4: AQL to map one path to another
[YAML]
    resources:
      - name: myAqlResource
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: 'items.find({""$and"": [{""repo"": {""$eq"": ""example-repo-local""}}, {""name"": {""$match"": ""myFile.txt""}}]})'
          mappings:
            - name: myMapping
              input: example-repo-local/myFile.txt
              output: example-repo-local/myFile_2.txt
    
      - name: myReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistributionIntegration
          name: name
          version: '1'
    
    pipelines:
      - name: myCreateReleaseBundlePipeline
        steps:
          - name: myCreateReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: ${JFROG_CLI_BUILD_NAME}
              releaseBundleVersion: ${run_id}
              dryRun: false
              inputResources:
                - name: myAqlResource
              outputResources:
                - name: myReleaseBundle
[/YAML]
###### Example 5: AQL mapping paths using capturing groups in the input
[YAML]
    resources:
      - name: myAqlResource
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: 'items.find({""$and"": [{""repo"": {""$eq"": ""example-repo-local""}}, {""name"": {""$match"": ""myFile.txt""}}]})'
          mappings:
            - name: myMapping
              input: example-repo-local/myFile.(.*)
              output: example-repo-local/myFile_2.$1
    
      - name: myReleaseBundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDistributionIntegration
          name: name
          version: '1'
    
    pipelines:
      - name: myCreateReleaseBundlePipeline
        steps:
          - name: myCreateReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: ${JFROG_CLI_BUILD_NAME}
              releaseBundleVersion: ${run_id}
              dryRun: false
              inputResources:
                - name: myAqlResource
              outputResources:
                - name: myReleaseBundle
[/YAML]
###### Example 6
 **pipelines.yml**
[YAML]
    resources:
      - name: Aql_ART_1
        type: Aql
        configuration:
          sourceArtifactory: myArtifactoryIntegration
          query: items.find({""@build.name"":{""$eq"":""art""}})
          addedProperties:
            key1: value1
            key2: value2
          mappings:
            - name: mapping1
              input: IN
              output: test1
            - name: mapping2
              input: IN
              output: test2
    
    pipelines:
      - name: pipelines_ART_1
        steps:
          - name: step_ART_1
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: Aql_ART_1
                  trigger: true
            execution:
              onExecute:
                - write_output ""executing step...""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
59,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a artifact?[/INST] An `Artifact` resource can be used to trigger a step or pipeline on any of the following Artifactory events:
  * Deploy
  * Delete
  * Copy
  * Move
  * Cache


When this resource is added as an input resource to any step, on resource update, the step or the pipeline is triggered.
### Important
If there are issues while using Artifact resource, see the Troubleshooting section below for possible solutions.
### Note
This resource cannot be added as an output to a step.
### See it Live!
Click here to see a live example that uses this resource.
##### YAML Schema
### Rename Resource Name
In your pipeline YAML, whenever you make a change to any tag in the Configuration section, the name of the resource must be renamed. If the resource name is not renamed, any changes to the Configuration section are not taken into consideration during pipeline sync.
The YAML schema for Artifact:
[YAML]
    resources:
      name:         <string>
      type: Artifact
      configuration:
        jfrogPlatformProvider:     <JFrogPlatformAccessToken_integration>    # required
        repositories:                                # required
          - repository_name
          - repository_name
        artifactPath:                                # optional
          include:
            - pattern_1
            - pattern_2
          exclude:
            - pattern_1
            - pattern_2
          branches:                                    # required
            include:     <includePattern>            
            exclude:     <excludePattern>   
          buildOn:                                     # required
            deployed:    <boolean>
            deleted:     <boolean>
            copied:      <boolean>
            moved:       <boolean>
            cached:      <boolean>
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `Artifact` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`jfrogPlatformProvider`
The name of the JFrog Platform Access Token integration.
Required  
`repositories`
Used to specify the Artifactory repositories on which the event is applied.
### Note
If this tag is not specified, all tags are matched by default.
Required  
`branches`
For use in  multibranch pipelines, though it will also work with single branch pipelines. An object where `include` and `exclude` regex patterns are used to control which branches are triggered by the resource.
  *  **Required** : When an `include` pattern is supplied, any branch not matching the pattern will not trigger.
  *  **Optional** : If an `exclude` pattern is supplied, any branch matching the pattern will not trigger.


### Supported regex
The `branches` tag supports the following regex:
`^ $ . * & ` **? @ = ; :** **+**
 **Examples** :
`^gitBranch$` (to trigger for that branch)
`example.*`
`example+.&`
`example+:&@=.*$`
`^._example+:&.@=+;?.*$`
`example+.:&@=*;?1661337986460$`
`branches.include` is required
`branches.exclude` is optional  
`buildOn`
Used to control whether the resource will be updated on specified events. These are set to `false` by default.
  *  **Deployed** : Trigger when an artifact is deployed to a repository.
  *  **Deleted** : Trigger when an artifact is deleted from a repository.
  *  **Moved** : Trigger when an artifact is moved from a repository. This will apply on the repositories from which the artifact is moved.
  *  **Copied** : Trigger when an artifact is copied from a repository. This will apply on the repositories from which the artifact is copied.
  *  **Cached** : Trigger when an artifact is cached.


Required  
`artifactPath`
  * `include` \-- (optional) Simple comma separated wildcard patterns for repository artifact paths (with no leading slash). Ant-style path expressions are supported (*, **, ?). For example: ""org/apache/**""
  * `exclude` \-- (optional) Simple comma separated wildcard patterns for repository artifact paths (with no leading slash). Ant-style path expressions are supported (*, **, ?). For example: ""org/apache/**""


Optional  
##### Environment Variables
Whenever `Artifact` is used in a step, a set of environment variables is automatically made available that can be used in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_event_type
[/YAML]  
The type of the event.
[YAML]
    res_<resource_name>_event_path
[/YAML]  
The artifact path that triggered the event.  
##### Examples
  * These examples require a GitRepo and JFrog Platform Access Token integration.
  * The Pipelines DSL for these examples are available in this repository in the JFrog GitHub account.
  * Click here to see a live example that uses this resource.


 **Example 1**
### Rename Resource Name
In your pipeline YAML, whenever you make a change to any tag in the Configuration section, the name of the resource must be renamed. If the resource name is not renamed, any changes to the Configuration section are not taken into consideration during pipeline sync.
[YAML]
    resources:
      - name: artifact_1
        type: Artifact
        configuration:
          jfrogPlatformProvider: jfrogPlafromTokenIntegration    # required
          repositories:                            # required
            - example-repo-local
          artifactPath:                            # optional
            include:
              - tools/*
            exclude:
              - tools/exclude/*
          buildOn:                                 # required
            deployed: true
            cached: true
            deleted: true
          branches:                                # required
            include: artifactResource              # required
      - name: artifact_2
        type: Artifact
        configuration:
          jfrogPlatformProvider: myIntegration    # JFrog Platform Access Token integration
          repositories:
            - example-repo-local
          artifactPath:
            include:
              - default/*
              - tools/*
          buildOn:
            deployed: true
          branches:
            include: artifactResource
    
    pipelines:
      - name: my_pipe_1
        configuration:
          jfrogCliVersion: 2
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: artifact_1
            execution:
              onExecute:
                - echo ""executing step_1""
      - name: my_pipe_2
        steps:
          - name: step_2
            type: Bash
            configuration:
              inputResources:
                - name: artifact_2
            execution:
              onExecute:
                - echo ""executing step_2""
                - echo ""sync started""
    
[/YAML]
##### Troubleshooting
When using the Artifact resource in Pipelines, if you see that the pipeline or step is not getting triggered on artifact deployment or event update as configured in the resource, check the following:
  * Ensure that the integration used in the Artifact resource has administrator privileges on the JPD (on that Artifactory). You can either:
    * Create a access token with **admin** permissions
or
    * Use the identity token of the user who has administrator privileges
  * Ensure that there are source branches that match the branch pattern mentioned in the Artifact resource’s `branches.include`.



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
60,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a buildinfo?[/INST] A `BuildInfo` resource is the metadata associated with a build in Artifactory.
JFrog Pipelines automatically creates BuildInfo when any of the Build steps like MvnBuild, NpmBuild, GradleBuild, or DockerBuild are used to generate packages.
BuildInfo is published to Artifactory:
  * Through the PublishBuildInfo step, by setting a flag in Build steps.
or
  * By providing that resource in steps like NpmPublish, GoPublishModule, GoPublishBinary and DockerPush.


##### YAML Schema
The YAML schema for BuildInfo:
[YAML]
    resources:
      - name:               <string>
        type: BuildInfo
        configuration:
          sourceArtifactory:        artifactoryIntegration
          buildName:                        <string>
          buildNumber:                      <number>
          pin:
            versionId:          <number>
            buildName:          <string>
            buildNumber:        <number>
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `BuildInfo` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceArtifactory`
The name of the Artifactory Integration.
Required  
`buildName `
The name of the build to associate with this BuildInfo. Recommend using the environment variable available from the step that generated the build.
Optional  
`buildNumber `
The number of the build to associate with this BuildInfo. Can be an integer, a semantic version, or a string.
Recommend using the environment variable available from the step that generated the build.
Optional  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the BuildInfo resource version to be pinned:
  * `versionId`: Resource version Id (can be obtained through the UI)


Or
  * `buildName`: The name of the build to be pinned
  * `buildNumber`: The number of the build to be pinned


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
##### Environment Variables
Whenever `Buildinfo` is used in a step, a set of environment variables is automatically made available that can be used in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_buildName
[/YAML]  
The name of the build associated with this BuildInfo.
[YAML]
    res_<resource_name>_buildNumber
[/YAML]  
The number of the build associated with this BuildInfo.
[YAML]
    res_<resource_name>_targetRepo
[/YAML]  
When set by PromoteBuild, the name of a repository in Artifactory for the BuildInfo.  
##### Examples
  * These examples require a GitHub Integration and an Artifactory Integration.
  * The Pipelines DSL for these examples are available in this repository in the JFrog GitHub account.


 **Example 1**
[YAML]
    resources:
      - name: myGitRepo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: jfrog/jfrog-pipelines-simple-example
          branches:
            include: master
      - name: myBuildInfo
        type: BuildInfo
        configuration:
          sourceArtifactory: art
          buildName: SampleApp
          buildNumber: 1          
    pipelines:  
      - name: basic_pipeline
        configuration:
          environmentVariables:
            readOnly:
              my_env_var: ""hello""  
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: myGitRepo
              outputResources:
                - name: myBuildInfo             
            execution:
              onExecute:
                - echo $my_env_var
                - if [ -z $app_version ]; then add_pipeline_variables app_version=1.0.0; else app_version=$(bump_semver $app_version minor); fi
                - write_output myBuildInfo ""buildNumber=$app_version""
                - add_run_variables run_var=""hello""
          - name: step_2
            type: Bash
            configuration:
              inputResources:
                - name: myBuildInfo            
              runtime:
                type: image
                image:
                  auto:
                    language: java
                    versions: ['11']
            execution:
              onExecute:
                - echo $run_var  
                - echo $res_myBuildInfo_buildNumber
          - name: step_3
            type: Bash
            configuration:
              environmentVariables:
                step_env_var1: ""hello""
              inputSteps:
                - name: step_2
            execution:
              onExecute:
                - echo $step_env_var1
          - name: step_4
            type: Bash
            configuration:
              inputSteps:
                - name: step_2
            execution:
              onExecute:
                - echo $app_version
          - name: step_5
            type: Bash
            configuration:
              inputSteps:
                - name: step_3
                - name: step_4  
              integrations:
                - name: my_github            
            execution:
              onExecute:
                - 'curl -H ""Authorization: token ${int_my_github_token}"" ${int_my_github_url}/rate_limit'
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
61,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a crontrigger?[/INST] A `CronTrigger` is used as an `inputResource` to a step to trigger execution of the step at a scheduled time or at a recurring interval. The minimum recurring interval for `CronTrigger` is one minute and it supports the cron syntaxand UTC time.
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
         type:              CronTrigger
         configuration:
           interval:    <string>  # should be a cron expression
           branches:
             include: ""^dev.+$"" <string>
             exclude: ""^dev-test.+$"" <string>
           environmentVariables:
             key: value
             key: value
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `CronTrigger` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`interval`
A cron expression string that specifies the time(s) or interval to trigger the step.
The `interval` tag uses standard Cron expression and takes the following format:
 **Format**
[YAML]
    interval: * * * * *                                 
              │ │ │ │ │_________ day of the week (1-7 or SUN-SAT)
              | | | |__________ month (1 - 12)
              | | |__________ day of the month (1 - 31)
              | |__________ hour (0 - 23)
              |__________ minute (0 - 59)
[/YAML]
 **Examples**
 **CRON Expression**
 **Meaning**  
`0 12 * * *`
12 PM (noon) every day.  
`25 10 * * *`
10:25 AM every day.  
`15 22 * * *`
10:15 PM every day.  
`* 16 * * *`
Every minute starting at 4 PM and ending at 4:59 PM, every day.  
`0/5 13 * * *`
Every 5 minutes starting at 1 PM and ending at 1:55 PM, every day.  
`0/5 15,18 * * *`
Every 5 minutes starting at 3 PM and ending at 3:55 PM, and every 5 minutes starting at 6 PM and ending at 6:55 PM, every day.  
`0-5 18 * * *`
Every minute starting at 6 PM and ending at 6:05 PM, every day.  
`10,44 15 * 5 MON`
3:10 PM and at 3:44 PM every Monday in the month of May.  
`45 10 * * MON-FRI `
10:45 AM every Monday, Tuesday, Wednesday, Thursday and Friday.  
`30 10 10 * *`
10:30 AM on the 10th day of every month.  
`0 12 1/5 * *`
12 PM (noon) on every 5th day-of-month from 1 through 31.  
`11 11 11 11 *`
Every November 11th at 11:11 AM.  
### Note
  * For more information, see Cron expression.
  * It is recommended that you first test your Cron expression using an online tool before using it.


Required  
`branches`
For use in  multibranch pipelines, though it will also work with single branch pipelines. An object where `include` and `exclude` regex patterns are used to control which branches are triggered by the resource.
  * If an `include` pattern is supplied, any branch not matching the pattern will not trigger.
  * If an `exclude` pattern is supplied, any branch matching the pattern will not trigger.


### Note
When `include` or `exclude` is not provided, the cronTrigger resource will trigger every branch in a multibranch pipeline. Most multibranch pipelines should use this configuration.
Optional  
`environmentVariables`
Add environment variables as key-value pairs.
 **Example**
[YAML]
        configuration:
          interval: '*/30 * * * *' # Every 10 minutes
          environmentVariables:
            first: env
            second: env
[/YAML]  
##### Environment Variables
Whenever `CronTrigger`is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_interval
[/YAML]  
The cron string that specifies the time(s) or interval  
##### Examples
The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
[YAML]
    resources:
      - name: cron_trigger
        type: CronTrigger
        configuration:
          interval: '*/5 * * * *'     # Every 5 minutes
          branches:
            include: ""^dev.+$""
            exclude: ""^dev-test.+$""
          environmentVariables:
            first: env
            second: env
    
    pipelines:
      - name: pipeline_scheduled_triggers
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: cron_trigger
            execution:
              onExecute:
                - printenv
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
62,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a distributionrule?[/INST] A `DistributionRule `resource is the set of Destination rules that can be applied to distributing a release bundle using JFrog Distribution.Distributing Release Bundles
##### YAML Schema
[YAML]
    resources:
      - name:                       <string>
        type:                       DistributionRule
        configuration:
          sourceDistribution:       <distribution integration name>
          serviceName:                      <string>                                          # required, only when used with Distribution 1.x
          siteName:                         <string>
          cityName:                         <string>
          countryCodes:
            - <string>
            - <string>
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `DistributionRule `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceDistribution`
The name of a Distribution Integration.
Required  
`siteName`
The Artifactory Edge Node site name specified with a wildcard pattern.
Required  
`cityName`
The Artifactory Edge Node city name with a wildcard pattern.
Required  
`countryCodes`
The Artifactory Edge Node country codes.
Required  
`serviceName `
The Artifactory Edge Node service name with a wildcard pattern.
Required only when used with Distribution v.1.  
##### Environment Variables
Whenever DistributionRule is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description  
res_<resource_name>_name
Name of the resource  
res_<resource_name>_servicename
The Artifactory Edge Node service name  
res_<resource_name>_sitename
The Artifactory Edge Node site name  
res_<resource_name>_cityname
The Artifactory Edge Node city name  
res_<resource_name>_countrycodes
The Artifactory Edge Node country codes  
##### Example
  * This examples requires an Artifactory Integration and Distribution Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: myArtifactory
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDist
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDist
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: myDist
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
63,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a filespec?[/INST] A `FileSpec` resource specifies a File Spec, which provides the details of files to upload or download from Artifactory.Using File Specs
Specifying a `FileSpec` in a step's `inputResources` downloads matching files and makes them available to that step.
Specifying a `FileSpec` in a step's `outputResources` signals a subsequent step that is dependent on that `FileSpec` to trigger. No file upload occurs.
### Remote Repositories
FileSpecs will not currently download artifacts from remote repositories in Artifactory. To do so, you must use the API in a step. The recommended way to do this is to use curl command in the JFrog CLI in a step. For example:
[YAML]
    jfrog rt curl ""my-github-remote-repo/mikefarah/yq/releases/download/v4.24.4/yq_linux_amd64.tar.gz"" -O
[/YAML]
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               FileSpec
        configuration:      
          sourceArtifactory:        artifactoryIntegration
          pattern:                          <string>   # required, either this or aql
          aql:                                      <string>   # required, either this or pattern
          target:                           <string>   # optional
          props:                            <string>   # optional
          recursive:                        <boolean>  # optional
          flat:                             <boolean>  # optional
          excludePatterns:          <string>   # optional
          archiveEntries:           <string>   # optional
          buildName:                        <string>   # optional
          buildNumber:                      <integer>  # optional
          sortBy:                           <string>   # optional
          sortOrder:                        <string>   # optional
          limit:                            <integer>  # optional
          offset:                           <integer>  # optional
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `FileSpec `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceArtifactory`
The name of the Artifactory Integration.
Required  
`pattern`
Specifies the source path in Artifactory, from which the artifacts should be downloaded, in the following format: `[repository name]/[repository path]`. You can use wildcards to specify multiple artifacts.
Required if `aql` is not specified  
`aql`
An AQL query that specified the artifacts to be downloaded.
### Note
  * AQL has to be in JSON format.
  * Ensure that there are no spaces in the AQL query.
  * The query does not support inputs such as limits and sorting. These are supported as inputs in filespec resource


Required if `pattern` is not specified  
`target`
Specifies the local file system path to which artifacts should be downloaded.
For flexibility in specifying the target path, you can include placeholders in the form of `{1`}, `{2`}, `{3`}....These are replaced by corresponding tokens in the pattern property that are enclosed in parenthesis. For more information, see Using Placeholders.Using File Specs
Optional  
`props`
List of ""key=value"" pairs separated by a semi-colon. (For example, ""`key1=value1;key2=value2;key3=` `value3`""). Only artifacts with all the specified properties and values will be downloaded.
Optional  
`flat`
[Default: false]
If true, artifacts are downloaded to the exact target path specified and their hierarchy in the source repository is ignored.
If false, artifacts are downloaded to the target path in the file system while maintaining their hierarchy in the source repository.
Optional  
`recursive`
[Default: true]
If true, artifacts are also downloaded from sub-paths under the specified path in the source repository.
If false, only artifacts in the specified source path directory are downloaded.
Optional  
`excludePatterns`
[Applicable only when '`pattern`' is specified]
An array (enclosed with square brackets) of patterns to be excluded from downloading. Unlike the ""pattern"" property, ""excludePatterns"" must NOT include the repository as part of the pattern's path. You can use wildcards to specify multiple artifacts.
For example: [""*.sha1"",""*.md5""]
Optional  
`archiveEntries`
If specified, only archive artifacts containing entries matching this pattern are matched. You can use wildcards to specify multiple artifacts.
Optional  
`buildName `
The name of the build to associate with this BuildInfo. Recommend using the environment variable available from the step that generated the build.
Optional  
`buildNumber `
The number of the build to associate with this BuildInfo. Recommend using the environment variable available from the step that generated the build.
Optional  
`sortBy`
A list of semicolon-separated fields to sort by. The fields must be part of the 'items' AQL domain. For more information read the AQL documentation.Artifactory Query Language
Optional  
`sortOrder`
[Default: asc]
The order by which fields in the _sortBy_ option should be sorted. Accepts _asc_ or _desc_.
Optional  
`limit`
The maximum number of items to fetch. Usually used with the _sortBy_ option.
Optional  
`offset`
The offset from which to fetch items (i.e. how many items should be skipped). Usually used with the _sortBy_ option.
Optional  
For the remaining values, see the Artifactory topic Using File Specs.Using File Specs
##### Environment Variables
Whenever `FileSpec` is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_resourcePath
[/YAML]  
Path to where the File Spec is downloaded to
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_sourceArtifactory
[/YAML]  
The name of the Artifactory integration  
##### Examples
  * These examples require an Artifactory Integration.
  * The Pipelines DSL for these examples are available in this repository in the JFrog GitHub account.


 **Example 1**
This example uses the following tags:
  * `sourceArtifactory`
  * `pattern`
  * `recursive`
  * `offset`


[YAML]
    resources:
      - name: FileSpec_0014
        type: FileSpec
        configuration:
          sourceArtifactory: s_artifactory
          pattern: ""test-local/setup/""
          recursive: false
          offset: 1@3
    
    pipelines:
      - name: pipelines_FLSPEC_0014
        steps:
          - name: step_FLSPEC_0014
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: FileSpec_0014
            execution:
              onExecute:
                - write_output ""executing step...""
[/YAML]
 **Example 2**
This example uses the following tags:
  * `sourceArtifactory`
  * `pattern`
  * `recursive`
  * `limit`
  * `sortBy`
  * `sortOrder`
  * `target`


[YAML]
    resources:
      - name: FileSpec_0013
        type: FileSpec
        configuration:
          sourceArtifactory: s_artifactory
          pattern: ""test-local/setup/""
          recursive: false
          limit: 1@3
          sortBy:
            - created
          sortOrder: invalid
          target: 'C:\Users\pipelines\Desktop\'
    
    pipelines:
    
      - name: pipelines_FLSPEC_0013
        steps:
          - name: step_FLSPEC_0013
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: FileSpec_0013
            execution:
              onExecute:
                - write_output ""executing step...""
[/YAML]
 **Example 3**
This example uses the following tags:
  * `sourceArtifactory`
  * `pattern`
  * `recursive`
  * `buildName`
  * `buildNumber`


[YAML]
    resources:
      - name: FileSpec_0010
        type: FileSpec
        configuration:
          sourceArtifactory: s_artifactory
          pattern: ""test-local/setup/""
          recursive: false
              buildName: pipelines_api
          buildNumber: 1@3
    
    pipelines:
      - name: pipelines_FLSPEC_0010
        steps:
          - name: step_FLSPEC_0010
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: FileSpec_0010
            execution:
              onExecute:
                - write_output ""executing step...""
[/YAML]
 **Example 4**
This example uses the following tags:
  * `sourceArtifactory`
  * `pattern`
  * `recursive`
  * `archiveEntries`


[YAML]
    resources:
      - name: FileSpec_0007
        type: FileSpec
        configuration:
          sourceArtifactory: s_artifactory
          pattern: ""test-local/setup/""
          recursive: false
          archiveEntries: ""#/artifactory""
    
    pipelines:
      - name: pipelines_FLSPEC_0007
        steps:
          - name: step_FLSPEC_0007
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: FileSpec_0007
            execution:
              onExecute:
                - write_output ""executing step...""
[/YAML]
 **Example 5**
This example uses the following tags:
  * `sourceArtifactory`
  * `aql`


[YAML]
    resources:
      - name: Get_Files
        type: FileSpec
        configuration:
          sourceArtifactory: s_artifactory
          aql: ""{'repo': 'docker-local'}""
        
    pipelines:
      - name: PropertyDemo
        steps:
          - name: ArtifactoryFiles
            type: Bash
            configuration:
              inputResources:
                - name: Get_Files
            execution:
              onExecute:
                - cat $HOME/.jfrog/logs/jfrog-cli*
                - ls
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
64,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a gitrepo?[/INST] A `GitRepo `is used to connect JFrog Pipelines to a source control repository. Adding it creates a webhook to the repo so that future commits will automatically create a new version with the webhook payload.
The first version of the resource, used if it is an input to a step that runs before any webhooks are received, will be for the latest commit in the default branch on the source provider at the time the resource is created. Later resource versions can be restricted by the `branches`, `tags`, and `buildOn` options.
### Note
To suppress triggering for an individual commit, include the text `skipRun` in the commit message.
By default the`GitRepo`resource uses `ssh` protocol with the created deploy key to fetch the source repository content. However, this only provides Pipelines with read-only access to the source repository. Optionally, users can configure a `GitRepo` resource to use `https` protocol. In this case Pipelines will have privileges to the source repository according to the credentials used to set up the Git provider integration.
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               GitRepo
        configuration:
          gitProvider:      <git integration name>
          path:             <string>
          files:
            include:        <regular expression>
            exclude:        <regular expression>
          branches:
            include:        <regular expression>
            exclude:        <regular expression>
          recurseSubmodules:    <Boolean>  
          pullRequestSourceBranches:
            include: <regular expression>
            exclude: <regular expression>
          pullRequestTargetBranches:
            include: <regular expression>
            exclude: <regular expression>
          tags:
            include:        <regular expression>
            exclude:        <regular expression>
          buildOn:
             branchCreate:      <Boolean>
             branchDelete:      <Boolean>
             commit:            <Boolean>
             pullRequestCreate: <Boolean>
             pullRequestClose:      <Boolean>
             releaseCreate:         <Boolean>
             tagCreate:                     <Boolean>
          cancelPendingRunsOn:
             newCommit:         <Boolean>
             pullRequestUpdate: <Boolean>
          shallowDepth:     <Positive Integer>        # used to set the depth at which the repo is (shallow)cloned/fetched
          cloneProtocol:    <ssh|https>               # Optional. Defaults to ssh
          pin:
            versionId:  <number>
            sha:        <string> 
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `GitRepo` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`gitProvider `
The name of the source control integration. Currently supported integration types are:
  * Bitbucket Integration
  * Bitbucket Server Integration
  * GitHub Integration
  * GitHub Enterprise Integration
  * GitLab Integration


Required  
`path`
The path of the repository from the integration root.
The path can be hard coded or you can use `{{.jfrog-pipelines.sourceRepository}}` variable to reference the current pipeline source path.
 **Example**
[YAML]
      - name: myFirstRepo
        type: GitRepo
        configuration:
          gitProvider: myGitHub
          path: {{.jfrog-pipelines.sourceRepository}}
[/YAML]
Required  
`files`
  * `include `\-- (optional) Regular expression to include files meeting the pattern from the repo
  * `exclude`\-- (optional) Regular expression to exclude files meeting the pattern from the repo


### Note
  * If this tag is not specified, all files are matched by default.
  * This filter only applies to commit events, and not pull requests, tags, or releases.


Optional  
`branches `
  * `include `\-- (optional) Regular expression to include branches from the repo
  * `exclude`\-- (optional) Regular expression to exclude branches from the repo


### Supported regex
The `branches` tag supports the following regex:
`^ $ . * & ` **? @ = ; :** **+**
 **Examples** :
`^gitBranch$` (to trigger for that branch)
`example.*`
`example+.&`
`example+:&@=.*$`
`^._example+:&.@=+;?.*$`
`example+.:&@=*;?1661337986460$`
### Note
If the tag is not specified, the default branch will be triggered.
Optional  
`recurseSubmodules`
Set as `true` to clone all submodules in the repository. Default value is `false`.
Optional  
`pullRequestSourceBranches`
  * `include `\-- (optional) Regular expression to include pull requests from branches matching the pattern
  * `exclude `\-- (optional) Regular expression to exclude pull requests from branches matching the pattern


### Note
If this tag is not specified, all branches are matched by default.
Optional  
`pullRequestTargetBranches`
  * `include `\-- (optional) Regular expression to include pull requests to branches matching the pattern
  * `exclude`\-- (optional) Regular expression to exclude pull requests to branches matching the pattern


### Note
If this tag is not specified, the `branches` tag setting will apply to the target branch. If neither is specified, all branches are matched by default.
Optional  
`tags `
Used to specify a collection of tags and releases upon which a new version is created
  * `include `\-- (optional) Regular expression to include tags meeting the pattern from the repo
  * `exclude`\-- (optional) Regular expression to exclude tags meeting the pattern from the repo


### Note
If this tag is not specified, all tags are matched by default.
Optional  
`buildOn `
Used to control whether the resource will be updated on specified events
  * `branchCreate` \-- (default is false) used to control whether the resource will be updated when a branch is created. Note that when this is set as `false`, the resource is not updated and the resource version is not created.
  * `branchDelete` \-- (default is false) used to control whether the resource will be updated when a branch is deleted. Note that when this is set as `false`, the resource is not updated and the resource version is not created.
  * `commit`\-- (default is true) used to control whether the resource will be updated for commit webhooks. Note that when this is set as `false`, the resource is not updated and the resource version is not created.
  * `pullRequestCreate` \-- (default is false) used to control whether the resource will be updated for pull request webhooks
  * `pullRequestClose` \-- (default is false) used to control whether the resource will be updated for closing a pull request webhook
  * `releaseCreate` \-- (default is false) used to control whether the resource will be updated for release webhooks
  * `tagCreate` \-- (default is false) used to control whether the resource will be updated for tag webhooks
### Note
In a multi-branch pipeline, when `tagCreate` is set as `true` and when you perform a commit or Git release with a specific tag, buildOn.commit will trigger only those branches that have the associated Git tag.
This is applicable for GitHub only.


### Note
If this tag is not specified, `commit:` `true` is the default setting.
For more information about using this tag, see Triggering On a Git Repository Change.
Optional  
`cancelPendingRunsOn`
Used to control whether previously triggered runs are canceled for new webhooks
  * `newCommit`\-- (default is false) Cancels previous processing or waiting runs for the same branch for commits, same tag name for tags, and same release name for releases.
  * `pullRequestUpdate` \-- (default is false) Cancels previous processing or waiting runs for the same pull request number when a webhook is received for a pull request.


### Note
All values for this tag are false by default.
For more information about using this tag, see Cancelling Previous Runs On a Git Repository Change.
Optional  
`shallowDepth`
Uses a positive integer to set the depth at which the repo is (shallow) cloned/fetched
### Note
If `shallowDepth` is not specified, no shallow clone options are passed to git. The full repo is cloned in its entirety.
Optional  
`cloneProtocol`
Used to select the protocol to be used when cloning the repo. Supported values are `ssh` or `https`. Defaults to `ssh`.
Optional  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the GitRepo resource version to be pinned:
  * `versionId` \-- Resource version Id (can be obtained through the UI)


Or
  * `sha` \-- The commit sha to be pinned


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
###### Using include and exclude Patterns
When both include and exclude patterns are used, the following rules apply on the list of all the files in the commit change list :
  * All files that do not match the include pattern are removed
  * All files that do match the exclude pattern are removed
  * If any files are left, the resource is updated


##### Environment Variables
Whenever `GitRepo` is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_path
[/YAML]  
Relative path of the resource
[YAML]
    res_<resource_name>_branchName
[/YAML]  
Name of branch on which the commit occurred. If it was created for a pull request, this is the base branch.
This environment variable is useful for getting the name of the branch.
[YAML]
    res_<resource_name>_commitMessage
[/YAML]  
Commit message of the version being used
[YAML]
    res_<resource_name>_commitSha
[/YAML]  
SHA of the commit of the version being used
[YAML]
    res_<resource_name>_commitUrl
[/YAML]  
URL to the commit
[YAML]
    res_<resource_name>_committerLogin
[/YAML]  
Name of the committer for the SHA being used
[YAML]
    res_<resource_name>_gitProvider_name
[/YAML]  
Git integration name
[YAML]
    res_<resource_name>_gitProvider_url
[/YAML]  
URL of the Git Provider integration
[YAML]
    res_<resource_name>_gitRepoFullName
[/YAML]  
Name of the Git repo
[YAML]
    res_<resource_name>_gitRepoRepositoryHttpsUrl
[/YAML]  
HTTPS URL for the Git repository
[YAML]
    res_<resource_name>_gitRepoRepositorySshUrl
[/YAML]  
SSH URL for the Git repository.
[YAML]
    res_<resource_name>_gitRepoRepositoryUrl
[/YAML]  
URL for the Git repository
[YAML]
    res_<resource_name>_gitRepoSourceDefaultBranch
[/YAML]  
Default branch of the Git repository
[YAML]
    res_<resource_name>_gitTagName
[/YAML]  
If a tag name was present in the current version, this will be the tag name
[YAML]
    res_<resource_name>_integrationName
[/YAML]  
The `name` string of the integration used by the GitRepo
[YAML]
    res_<resource_name>_isGitTag
[/YAML]  
`True` if the version is a git tag based build
[YAML]
    res_<resource_name>_isPrerelease
[/YAML]  
True if the version is a git release based build. Supported only if the integration is GitHub
[YAML]
    res_<resource_name>_isPullRequest
[/YAML]  
`True` if the version is a git open pull request based build
[YAML]
    res_<resource_name>_isPullRequestClose
[/YAML]  
`True`, if the version is a git closed pull request based step. Supported only if the integration is GitHub or Bitbucket or Gitlab.
[YAML]
    res_<resource_name>_isRelease
[/YAML]  
`True` if the version is a git release based build. Supported only if the integration is GitHub
[YAML]
    res_<resource_name>_pullRequestNumber
[/YAML]  
The ID number of the pull request
[YAML]
    res_<resource_name>_pullRequestBaseBranch
[/YAML]  
Name of the base branch into which the pull request changes will be merged
[YAML]
    res_<resource_name>_pullRequestSourceUrl
[/YAML]  
Source URL of the pull request
[YAML]
    res_<resource_name>_pullRequestSourceHttpsUrl
[/YAML]  
Source HTTPS URL of the pull request
[YAML]
    res_<resource_name>_pullRequestSourceSshUrl
[/YAML]  
Source SSH URL of the pull request
[YAML]
    res_<resource_name>_resourcePath
[/YAML]  
Full path of the resource folder on the node
[YAML]
    res_<resource_name>_operation
[/YAML]  
Input or output resource
[YAML]
    res_<resource_name>_shaData
[/YAML]  
JSON object containing the details of the SHA
[YAML]
    res_<resource_name>_beforeCommitSha
[/YAML]  
SHA before the commit was made
[YAML]
    res_<resource_name>_baseCommitRef
[/YAML]  
The commit SHA of base branch in a PR
[YAML]
    res_<resource_name>_headCommitRef
[/YAML]  
The source branch name in a PR
[YAML]
    res_<resource_name>_compareUrl
[/YAML]  
Compare URL for the commit
[YAML]
    res_<resource_name>_gitTagMessage
[/YAML]  
If a git tag is created, this holds the message if it is present
[YAML]
    res_<resource_name>_releaseName
[/YAML]  
If a release is created, this holds the name of the release
[YAML]
    res_<resource_name>_releaseBody
[/YAML]  
If a release is created, this holds the description of the release
[YAML]
    res_<resource_name>_releasedAt
[/YAML]  
If a release is created, this holds the date of the release
[YAML]
    res_<resource_name>_lastAuthorLogin
[/YAML]  
Details of when the last author logged in
[YAML]
    res_<resource_name>_lastAuthorEmail
[/YAML]  
Email of the author
[YAML]
    res_<resource_name>_isBranchCreate
[/YAML]  
`True` if a branch was created
[YAML]
    res_<resource_name>_isBranchDelete
[/YAML]  
`True` if a branch was deleted  
##### Examples
 **Example 1**
 **GitRepo resource listening to the master branch**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          branches:
            include: master
[/YAML]
 **Example 2**
 **GitRepo resource listening to all branches except master and release**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          branches:
            exclude: ^(master|release)$
    
[/YAML]
**Example 3**
The following example may be useful to reference a single directory in a source repository that contains source code for a single microservice within a larger project.
 **GitRepo resource listening only to certain files in a directory**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          branches:
            include: master
          files:
            include: ^folder1\/package\/docker\/micro1\/.+
    
[/YAML]
**Example 4**
By default, a GitRepo triggers on a commit, but this can be changed in the `buildOn` attributes. For more information about triggering, see Triggering Pipelines and Steps.
 **GitRepo triggers only for a pull request and not for commit**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          branches:
            include: master
          buildOn:
            commit: false
            pullRequestCreate: true
    
[/YAML]
**Example 5**
The following example pipeline triggers the first step on the GitRepo resource. The step uses the environment variables exposed by the GitRepo resource to navigate to the path where the Git repository is cloned on the build node, and to execute a script in that directory.
 **Usage in a Pipeline**
[YAML]
    pipelines:
      - name: java_pipeline
        steps:
          - name: step_1
            type: Bash
            configuration:
              inputResources:
                - name: my_app_repo
            execution:
              onExecute:
                - pushd $res_my_app_repo_resourcePath
                - ./execute.sh
                - popd
[/YAML]
 **Example 6**
 **GitRepo resource using HTTPS protocol during repo clone**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          branches:
            include: master
          cloneProtocol: https
[/YAML]
 **Example 7**
In this example, the GitRepo is only updated for pull requests to master.
 **GitRepo triggers only for a pull requests to master**
[YAML]
    resources:
      - name: my_app_repo
        type: GitRepo
        configuration:
          gitProvider: my_github
          path: myuser/repo-name
          pullRequestTargetBranches:
            include: ^master$
          buildOn:
            commit: false
            pullRequestCreate: true
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
65,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a helmchart?[/INST] The `HelmChart` resource maps to a specific chart in an  Artifactory Helm Repository.Kubernetes Helm Chart Repositories
The HelmChart resource can be used in two native steps. The HelmPublish step takes a HelmChart as a required `outputResource` . HelmPublish will create a new version of the chart, push it to Artifactory, and store the new version in the HelmChart resource. Then that HelmChart resource can be used as an `inputResource` to the HelmDeploy native step. This will automatically download the HelmChart, and use the Helm CLI to deploy it to the configured cluster.
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
         type:              HelmChart
         configuration:
           sourceArtifactory:    artifactoryIntegration
           repository:           <string>
           chart:                <string>
           version:              <version string>
           pin:
             versionId:          <number>
             version:            <version string> 
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `HelmChart `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceArtifactory `
The name of an Artifactory Integration
Required  
`repository `
The name of the Helm repository in Artifactory
Required  
`chart`
The name of the Helm chart
Required  
`version `
A string that identifies the version number of the Helm chart
Required  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the HelmChart resource version to be pinned:
  * `versionId` \-- Resource version Id (can be obtained through the UI)


Or
  * `version` \-- A string that identifies the version to be pinned


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
##### Environment Variables
Whenever `HelmChart `is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_sourceArtifactory
[/YAML]  
The name of the Helm repository in Artifactory
[YAML]
    res_<resource_name>_chart
[/YAML]  
The name of the Helm chart
[YAML]
    res_<resource_name>_version 
[/YAML]  
A string that identifies the version number of the Helm chart  
##### Example
  * This examples requires an Artifactory Integration and GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      - name: git_repo
        type: GitRepo
        configuration:
          gitProvider: git_r
          path: jfrog/charts
       
      - name: helm_repo
        type: HelmChart
        configuration: 
          sourceArtifactory: art_r
          repository: helm-local/
          chart: test
          version: 1.0.0
    
    pipelines:
      - name: test_helm
        configuration:
          nodePool: win_2019
    
        steps:
          - name: helm_pub
            type: HelmPublish
            configuration:
              chartPath: ./simplehelm
     
              inputResources:
                - name: git_repo                    
              outputResources:
                - name: helm_repo                  # required
     
            execution:
              onStart:
                - write_output ""executing step...""
                - Get-Variable | format-table
              onSuccess:
                - write_output ""executing step...""
                - Get-Variable | format-table
              onFailure:
                - write_output ""executing step...""
                - Get-Variable | format-table
    
              onComplete: #always
                - write_output ""executing step...""
                - Get-Variable | format-table
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
66,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a image?[/INST] An `Image `resource is used to add a reference to a Docker image to your pipeline.
An Image resource holds information of a docker image that is stored through a Docker Registry Integration that may in one of the following registry sources:
  * JFrog Artifactory / JFrog Container Registry
  * Amazon Container Registry
  * Google Container Registry
  * Azure Container Registry
  * Docker Hub


An Image resource can be used as both input and output. Step like DockerBuild will generate it as an output and steps like DockerPublish will use it as input to publish to a Docker registry.
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
         type:              Image
         configuration:
           registry:                <docker integration name>
           sourceRepository:        <name of the artifactory repo>   # required if registry is Artifactory
           region:              <string>                         # required if registry is on AWS
           imageName:               <imageRepo/imageName>
           imageTag:                        <string>
           autoPull:                        <boolean> #(default true)
           pin:
             versionId:         <number>
             imageTag:          <string> 
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `Image `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`registry`
The name of a Docker Registry Integration
Required  
`sourceRepository `
The name of the repository in Artifactory to which the images will be published. Required for Docker images in Artifactory when the pipeline is configured to use JFrog CLI v1.
This is not used when the pipeline is configured to use JFrog CLI v2.
Required if registry is in JFrog Artifactory with JFrog CLI v1  
`region`
An AWS region identifier
Required if registry is on AWS  
`imageName`
The file path of the Docker image, of the form imageRepo/imageName
Required  
`imageTag`
The version tag of the initial version of the Docker image
Required  
`autoPull `
Defaults to **true**. The image is automatically pulled to the machine running the step.
Optional  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the Image resource version to be pinned:
  * `versionId` \-- Resource version Id (can be obtained through the UI)


Or
  * `imageTag` \-- The version tag to be pinned


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
##### Environment Variables
Whenever `Image `is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_sourceRepository
[/YAML]  
The name of the repository in Artifactory  
res_<resource_name>_region
The AWS region identifier  
res_<resource_name>_imageName
The file path of the Docker image  
res_<resource_name>_imageTag
The version tag of the initial version of the Docker image  
res_<resource_name>_autoPull
Value of the autopull flag  
##### Examples
The Pipelines DSL for this example is available in t his repository in the JFrog GitHub account.
 **Example 1**
[YAML]
    resources:  
      - name: Image_1
        type: Image 
        configuration:         
          registry: PSSG_DockerRegistry           
          imageName: docker/jfreq_win             
          imageTag: latest
    
    pipelines:
      - name: pipelines_Image_1
        steps:
          - name: step_Image_1
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: Image_1
            execution:
              onExecute:
                - write_output ""executing step...""
                - if($Image_1_isTrigger -ne ""true""){
                  Write-Error ""Stop"" -ErrorAction Stop
                  }
[/YAML]
 **Example 2**
[YAML]
    resources:
      - name: Image_2
        type: Image
        configuration:
          registry: s_artifactory
          sourceRepository: docker-local
          imageName: ubuntu
          imageTag: latest
    
    pipelines:
      - name: pipelines_IMAGE_2
        steps:
          - name: step_IMAGE_2
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: Image_2
            execution:
              onExecute:
                - write_output ""executing step...""
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
67,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a incomingwebhook?[/INST] An `IncomingWebhook` resource can trigger one or more jobs in your pipeline whenever the associated URL is called using the HTTP POST method.
When an `IncomingWebhook` resource is created with an Incoming Webhook Integration, it can be specified as an input to any step in that step's `inputResources` section. By default, any incoming payload from the webhook will trigger execution of the step, but this can be changed by setting `trigger` to `false`.
Any JSON payload passed in by the received POST call is stored in the `payload` property of the resource.The step can access the JSON payload through the `res_<resource_name>_payload` environment variable. The value of the environment variable can be written to a file, and individual elements can be read from that file using the read_json utility function.
When using Webhook as an output resource, you can construct the outgoing payload through the write_output utility function to add key/value pairs.
You can use an `IncomingWebhook`resource to trigger execution of another pipeline. When specified as an output resource of a step in one pipeline and an input resource of a step in a different pipeline, setting a payload for the `IncomingWebhook` in the first step will trigger execution of the other step. In this way, completion of a step in one pipeline can trigger execution of a different pipeline.
### Incoming Webhooks and Multi Branch Pipelines
When working with  Multi Branch Pipelines, to trigger an `IncomingWebhook` resource for a specific Pipelines Source branch, you need to add the _branch_ query parameter to your  Incoming Webhook URL as follows:
`http://host:port/pipelines/api/v1/projectIntegrations/1/hook?branch=>`
 **Example**
`http://host:port/pipelines/api/v1/projectIntegrations/1/hook?branch=foo`
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               IncomingWebhook
        configuration:
          webhookName:      <string>
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `IncomingWebhook` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`webhookName`
The name of an Incoming Webhook Integration
Required  
##### Environment Variables
Whenever`IncomingWebhook`is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_payload
[/YAML]  
Webhook payload that was received in the incoming POST or created by the step for output  
##### Example
  * This example requires an Incoming Webhook Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      - name:               MyIncomingWebhookResource
        type:               IncomingWebhook
        configuration:
          webhookName: MyIncomingWebhookIntegration
    
    pipelines:
      - name: ExamplePipeline
            steps:
        - name: ExampleStep
          type: Bash
          configuration:
            inputResources:
              - name: MyIncomingWebhookResource
          execution:
              onExecute:
                - echo ""$res_MyIncomingWebhookResource_payload"" | jq '.' > payload.json
                - read_json payload.json ""LESSER_SECRET_FORMULA""
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
68,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a outgoingwebhook?[/INST] An `OutgoingWebhook` resource uses HTTP to send information from a step to an external API endpoint through an Outgoing Webhook Integration.
When an `OutgoingWebhook` resource is specified as an`outputResource`for a step and has an attached payload (including an empty payload), at the completion of the step's execution, the payload is sent to the endpoint specified in the Outgoing Webhook Integration. If required, this can be overridden by setting the resource's `skipWebhook` environment variable as `true`.
To attach the outgoing payload, use the set_payload utility function. If no payload is attached, no message is sent through the webhook.
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               OutgoingWebhook
        configuration:
          webhookName: <string>                           # name of an Outgoing Webhook Integration
          method: <GET|PUT|POST|DELETE|PATCH>             # default POST
          Parameters: <string>                            # query string to be appended to the outgoing URL
          path:       <string>
          headers:
            Content-type: <valid request content types> 
            accept: xyc
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `OutgoingWebhook` for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`webhookName`
The name of an Outgoing Webhook Integration
Required  
`method`
The HTTP method to use: GET | PUT | POST | DELETE | PATCH
The default is POST.
Optional  
`Parameters`
A query string to be appended to the outgoing URL
Optional  
`path`
Path for the outgoing URL
Optional  
`headers`
Any HTTP entity headers to include. For example, `content-type` to specify any the type of content returned.
Optional  
##### Environment Variables
Whenever `Webhook` is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
                                           res_<resource_name>_name
                                        
[/YAML]  
Name of the resource  
`res_<resource_name>_payload`
The JSON payload attached to the resource  
`res_<resource_name>_skipWebhook`
When set to **true** , the outgoing webhook will not be sent. This is useful to inhibit sending of a prepared webhook in a step's `onFailure`section.
Defaults to **false**.  
##### Examples
The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
 **Example 1**
 **Outgoing Form URL Webhook**
[YAML]
    resources:
      - name: My_OutHook_formurl_Resource
        type: OutgoingWebhook
        configuration:
          webhookName: My_OutgoingWebhookIntegration
          path: /url
          headers:
            content-type: application/x-www-form-urlencoded
    
    pipelines:
      - name: ExamplePipeline
        steps:
          - name: my_send_hook_formurl
            type: Bash
            configuration:
              outputResources:
                - name: My_OutHook_formurl_Resource
            execution:
              onExecute:
                - echo ""hello world""
                - formUrlEncodedPayload=""foo=bar&runNumber=$run_number&stepName=$step_name""
                - set_payload My_OutHook_formurl_Resource ""$formUrlEncodedPayload""
    
[/YAML]
**Example 2**
 **Outgoing JSON Webhook**
[YAML]
    resources:
      - name:               MyOutgoingJSONResource
        type:               OutgoingWebhook
        configuration:
          webhookName: MyOutgoingJSONResource
          path: /json
    
    pipelines:
      - name: ExamplePipeline
            steps:
        - name: my_send_hook_json
          type: Bash
          configuration:
            outputResources:
              - name: MyOutgoingJSONResource
          execution:
              onExecute:
                - jsonPayload=""{\""runNumber\"":\""$run_number\"", \""stepName\"":\""$step_name\""}""
                - set_payload MyOutgoingJSONResource ""$jsonPayload""
              onFailure:
                - export res_MyOutgoingJSONResource_skipWebHook=true
              onSuccess:
                - echo ""Payload sent by ""$res_MyOutgoingJSONResource_name"":""
                - echo $res_MyOutgoingJSONResource_payload
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
69,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a propertybag?[/INST] A `PropertyBag` resource is used to pass information from one pipeline to another and provide environment variables to a step in the format of a resource.
A PropertyBag resource can have any string as property, which is then available as environment variables when the key is an input to a step. When it is an output, steps can change the values of properties or add new ones.
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               PropertyBag
        configuration:
          <string>: <string>
          <string>: <string>
          pin:
            versionId: <number>
            <string>: <string>
            <string>: <string> 
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `PropertyBag `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`<string>`
A property for the PropertyBag resource. The tag should be a valid variable name (Bash or PowerShell) for the steps where it is to be an input or output and the value a string. At least one is required, multiple properties are allowed.
Required  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the PropertyBag resource version to be pinned:
  * `versionId` \-- Resource version Id (can be obtained through the UI)


Or
  * `<string>` \-- The collection of properties to be pinned. At least one is required, multiple properties are allowed.


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
##### Environment Variables
Whenever `PropertyBag `is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_<property>
[/YAML]  
Each property in the PropertyBag resource will be available as an environment variable in this format.
[YAML]
    <property>
[/YAML]  
If a PropertyBag resource is in inputResources, the properties will also be exported as environment variables with the tag as the variable name.  
##### Examples
The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
 **Example 1**
[YAML]
    resources:
      - name: propertyBag_1
        type: PropertyBag
        configuration:
          property1: ""jfrog""
          property2: ""pipelines""
    
    pipelines:
      - name: pipeline_PropertyBag_1
        steps:
          - name: step_propertyBag_1
            type: PowerShell
            configuration:
              nodePool: win_2019
              inputResources:
                - name: propertyBag_1
            execution:
              onExecute:
                - write_output ""onExecute""
                - get-variable -name property1
                - get-variable -name property2
              onSuccess:
                - write_output ""onSuccess""
[/YAML]
 **Example 2**
This example uses two steps, `step1` and `step2`. When the pipeline runs, the step `step2` is triggered, but the step `step1` is skipped. This is because, the `newVersionOnly` tag is set as `true` for the `myResource` input resource that is used in this step. During a run, if an input resource's version is not updated and if the `newVersionOnly` tag is set as true, it causes the step using that input resource to be skipped.
[YAML]
    resources:
      - name: myResource
        type: PropertyBag
        configuration:
          runNumber: 0
    
    pipelines:
      - name: myPipeline
        steps:
          - name: input1
            type: Bash
            configuration:
              outputResources:
                - name: myResource
            execution:
              onExecute:
                - write_output myResource runNumber=${run_number}
                - echo ""test""
          - name: step1
            type: Bash
            configuration:
              inputResources:
                - name: myResource
                  newVersionOnly: true
            execution:
              onExecute:
                - echo ""test""
          - name: step2
            type: Bash
            configuration:
              inputResources:
                - name: myResource
            execution:
              onExecute:
                - echo ""test""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
70,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a releasebundle?[/INST] A `ReleaseBundle` resource specifies a set of artifacts in Artifactory that are distributed to Artifactory Edge nodes as a JFrog Distribution Release Bundle.Distributing Release Bundles
This resource can be used with CreateReleaseBundle or DistributeReleaseBundle. This resource can be used to represent both signed and unsigned Release Bundle.Distributing Release Bundles
##### YAML Schema
[YAML]
    resources:
      - name:               <string>
        type:               ReleaseBundle
        configuration:
          sourceDistribution:   distributionIntegration
          name:                             <string>
          version:                          <string>
          isSigned:                         <boolean>
          pin:
            versionId:          <number>
            name:               <string>
            version:            <string>
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `ReleaseBundle `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sourceDistribution`
The name of a Distribution Integration
Required  
`name`
Name of the release bundle
Required  
`version`
Version number of the release bundle
Required  
`isSigned`
When **true** , the release bundle is signed.
Defaults to **false**.
Optional  
`pin`
This configuration can be used to pin the resource to a specific version. The pinned resource version will be used by the steps that reference this resource as an input and newer versions will be ignored.
Users have two configuration options when selecting the ReleaseBundle resource version to be pinned:
  * `versionId` \-- Resource version Id (can be obtained through the UI)


Or
  * `name` \-- The name of the release bundle to be pinned
  * `version` \-- The version of the release bundle to be pinned


Steps that use the resource as an output can still produce new versions. New versions will be visible for steps using the resource as an input as long as they are part of the same run of the step that created the version.
When creating a new run, manual custom trigger can still be used to override the pinned version to a different one.
Optional  
##### Environment Variables
Whenever `ReleaseBundle `is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the release bundle
[YAML]
    res_<resource_name>_sourceDistribution
[/YAML]  
Name of the Distribution integration
[YAML]
    res_<resource_name>_version 
[/YAML]  
Version number of the release bundle
[YAML]
    res_<resource_name>_isSigned 
[/YAML]  
Indicates whether the release bundle is signed  
##### Examples
  * This example requires an Artifactory Integration and a Distribution Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: myArtifactory
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDist
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: myDist
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: myDist
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
71,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a remotefile?[/INST] A `RemoteFile` resource enables using a file on a remote file server.
This resource holds information of a RemoteFile that is stored on the following registries:
  * JFrog Artifactory
  * AWS S3
  * Google Cloud Storage
  * File Server


##### YAML Schema
[YAML]
    resources:
      - name:                       <string>
        type:                       RemoteFile
        configuration:      
          source:                   <file server integration name>
          fileLocation:     <string>  # optional
          fileName:                 <string>  # optional
          fileDateTime:     <string>  # optional
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `RemoteFile `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`source`
The name of an integration of one the following types:
  * Artifactory Integration
  * AWS Keys Integration
  * Google Cloud Integration
  * File Server Integration


Required  
`fileLocation `
Directory path on the remote file server where the file is located
Optional  
`fileName`
The name of the file
Optional  
`fileDateTime`
The date and time of the file
Optional  
##### Environment Variables
Whenever `RemoteFile` is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description  
`res_<resource_name>_name`
Name of the resource  
`res_<resource_name>_source_name`
Name of the integration the file is associated with  
`res_<resource_name>_fileLocation`
The directory containing files for the resource  
`res_<resource_name>_fileName`
The name of the file  
`res_<resource_name>_fileDateTime`
The date and time of the file  
##### Examples
  * This examples requires an Artifactory Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    resources:
      - name: RemoteFile_1
        type: RemoteFile
        configuration:
          source: s_artifactory
          fileLocation: example-repo-local/setup/help # optional
          fileName: something.txt # optional
              fileDateTime: 19/12/19 # optional
    
    
    pipelines:
      - name: pipelines_1
        steps:
          - name: REFILE_1
            type: PowerShell
            configuration:
              nodePool: win_2019
              integrations:
                - name: s_artifactory
              outputResources:
                - name: RemoteFile_1
            execution:
              onExecute:
                - write_output ""executing step...""
                - jfrog rt dl example-repo-local/setup/test.sh
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
72,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a vmcluster?[/INST] A `VmCluster` is used to represent a set of virtual machines. It is mainly used to deploy services/apps to the specified clusters and in some cases, it can be used to run certain maintenance activities on the clusters as a whole.
##### YAML Schema
[YAML]
    - name:             <string>
      type:             VmCluster
      configuration:
        sshKey:         <sshKey integration name>
        targets:
          - <IP address>
          - <IP address>
          - <IP address>
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the resource.
###### type
Must be `VmCluster `for this resource type.
###### configuration
Specifies all configuration selections for the resource.
Tag
 **Description**
Required/Optional  
`sshKey`
The name of an SSH Key Integration
Required  
`targets`
Declares a collection of IP addresses of Virtual Machines in the clusters
Required  
##### Environment Variables
Whenever `VmCluster `is used in a step, a set of environment variables is automatically made available that you can use in your step.
Environment Variable
Description
[YAML]
    res_<resource_name>_name
[/YAML]  
Name of the resource
[YAML]
    res_<resource_name>_sshkey
[/YAML]  
Name of the SSH Key integration
[YAML]
    res_<resource_name>_targets
[/YAML]  
The collection of IP addresses  
##### Example
The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
[YAML]
    resources:
       - name: vmCluster_1
         type: VmCluster
         configuration:
           sshKey: SR_SSHKEYS
           targets:
             - 1.2.3.4
    
    pipelines:
      - name: pipelines_1
        steps:
          - name: VM_0001
            type: PowerShell
            configuration:
              inputResources:
                - name: vmCluster_1
            execution:
              onExecute:
                - write-output ""executing step...""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
73,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pipelines-steps?[/INST] A **Step** is a unit of execution in a pipeline. It is triggered by some event and uses resources to perform an action as part of the pipeline.
Steps take Inputs in the form of Integrations or Resources, execute tasks that perform the operations necessary and then produce a result i.e. Output(s). Now these Outputs can become Inputs to other steps and so on forming a dependency-based, event-driven pipeline.  
A trigger for a step is some external event, or combination of events, such as:
  * Completion of another step
  * A change to a resource, such as a commit to a source code repository or the generation of a new Docker image


 **Step Types**
  * Bash
  * CreateReleaseBundle
  * DistributeReleaseBundle
  * DockerBuild
  * DockerPush
  * GoBuild
  * GoPublishBinary
  * GoPublishModule
  * GradleBuild
  * HelmBlueGreenCleanup
  * HelmBlueGreenDeploy
  * HelmBlueGreenRoleSwitch
  * HelmDeploy
  * HelmPublish
  * Jenkins
  * LinuxVMDeploy
  * Matrix
  * MvnBuild
  * NpmBuild
  * NpmPublish
  * PostMatrix
  * PowerShell
  * PreMatrix
  * PromoteBuild
  * PublishBuildInfo
  * SignReleaseBundle
  * TriggerPipeline
  * UploadArtifact
  * XrayScan


### See it Live
Click  **here**  to see some of these steps in action.
#### Generic and Native Steps
Steps are categorized as one of the following:
  *  **Generic step** : A generic step is for general-purpose execution. The Bash and PowerShell steps, which execute any series of shell commands you specify, are the single generic steps for Linux and Windows runtimes, respectively.
  *  **Native Steps**: A native step performs a specific set of actions as an encapsulated unit. Native steps inherently know what shell commands to execute to perform their action. With native steps, you can create complex workflows that push, publish, and promote your builds in Artifactory using simple step definitions.
All native steps derive from the base generic step definition for their respective runtime environment, so all steps have the same base tag:entity structure in a pipeline config. Each native step defines additional tags that are specific to its function.


##### Generic Steps  
Bash
The Bash is a generic step type that enables executing any shell command. This general-purpose step can be used to execute any action that can be scripted, even with tools and services that haven't been integrated with JFrog Pipelines. This is the most versatile of the steps while taking full advantage of what the lifecycle offers.  
Matrix
The Matrix step commences multiple parallel build processes across multiple containers, with different settings for each.  
PostMatrix
The PostMatrix generic step may be used to perform post-execution tasks following a Matrix step.  
PowerShell
The PowerShell step type is a generic type that enables executing PowerShell commands. PowerShell steps can only run on Windows node pools and are similar to the Bash step on other node pools. As a general-purpose step that can execute any action that can be scripted, even with tools and services that haven't been integrated with JFrog Pipelines, it can be used to perform actions where complete control is required.  
PreMatrix
The PreMatrix generic step may be used to prepare a build environment for execution of a Matrix step.  
##### Native Steps  
CreateReleaseBundle
The CreateReleaseBundle is a native step that produces a Release Bundle for distribution to an Artifactory Edge Node. The step can be used to create a signed or unsigned release bundle.  
DistributeReleaseBundle
The DistributeReleaseBundle native step triggers the distribution of a Release Bundle to an Artifactory Edge Node. This step requires a signed release bundle and one or more distribution rules to successfully execute.  
DockerBuild
The DockerBuild native step performs a build to produce a Docker image from a Dockerfile in a Git source repository.  
DockerPush
The DockerPush native step pushes a Docker Image to a Docker registry.  
GoBuild
The GoBuild native step performs a build from Go (GoLang) source.  
GoPublishBinary
The GoPublishBinary native step publishes the GO (GoLang) binaries to Artifactory.  
GoPublishModule
The GoPublishModule native step publishes the GO (GoLang) modules to an Artifactory. This step should be used in conjunction with the GoBuild step.  
GradleBuild
The GradleBuild native step performs a Gradle build on files in a Git repository. Optionally, it can also publish build information to Artifactory.  
HelmBlueGreenCleanup
The HelmBlueGreenCleanup step uninstalls an Idle release previously deployed by a HelmBlueGreenDeploy step.  
HelmBlueGreenDeploy
The HelmBlueGreenDeploy step implements a Blue/Green strategy to deploy a Docker image to a Kubernetes cluster using a Helm chart.  
HelmBlueGreenRoleSwitch
The HelmBlueGreenRoleSwitch step flips the roles played by the Helm releases deployed through a HelmBlueGreenDeploy step.  
HelmDeploy
The HelmDeploy step deploys a Docker image to a Kubernetes cluster using a Helm chart.  
HelmPublish
The HelmPublish step publishes a Helm chart and associated build info from a location in a Git repo to a Helm repository in Artifactory.  
Jenkins
The Jenkins native step transfers execution to a Jenkins pipeline.  
MvnBuild
The MvnBuild native step performs a Maven project build on files in a Git repository. Optionally, it can also publish build information to Artifactory.  
NpmBuild
The NpmBuild native step builds an npm source. This step automatically performs npm-install on the source in a Git repository.  
NpmPublish
The NpmPublish step publishes an npm package to the registry in Artifactory following an NpmBuild step.  
PromoteBuild
The PromoteBuild native step promotes a BuildInfo and moves or copies the related artifacts from one Artifactory repository to another.  
PublishBuildInfo
The PublishBuildInfo step publishes BuildInfo to Artifactory. BuildInfo provides a manifest for the build and includes metadata about the modules, dependencies and other environment variables.  
SignReleaseBundle
The SignReleaseBundle native step signs a Release Bundle in preparation for distributing it to Edge nodes.  
XrayScan
The XrayScan native step triggers a scan by JFrog Xray for security vulnerabilities and license compliance. If there was a watch created that covers the selected build, Xray will scan the indexed build artifacts.  
#### Step Definition
Steps are defined in a pipeline config under the `steps` tag, as shown below.
[YAML]
    pipelines:
      - name: pipe1
        configuration:
          <optional configuration settings>
        steps:
          <collection of step types>
    
[/YAML]
All step are composed of these top-level tags:
Tag
 **Description**
Required/Optional  
`name`
An alphanumeric string (underscores are permitted) that identifies the step. This is the name that will be used when the step is assigned as an input to other steps. The name should be chosen to accurately describe what the step does, e.g. `prov_test_env` to represent a job that provisions a test environment. Names of steps must be unique within a pipeline.
Required  
`type`
A predefined step type.
### Note
After the pipeline performs a sync, a step's type should not be modified, as it can cause pipeline sync errors.
Though not recommended, if you do want to change a step's type, perform the following steps:
  1. Change the step's name and type
  2. Wait for pipeline sync
  3. After the sync completes, change the step's name back to the old name


Required  
`configuration`
Specifies all configuration settings for the step's execution environment. These settings may include:
  * Environment variable definitions
  * Runtime the step will execute in
  * Integrations
  * Input and output resources
  * Timeout duration
  * Queuing priority
  * Settings for native steps


While most configuration tags are optional, in practice you will at least need to define the integrations, steps, and/or resources that trigger the execution of the step. Many native steps will also require some settings in this tag section.
Optional for the generic steps
Required for most native steps  
`execution`
Specifies the actions to perform for each execution phase of the step. These phases may include:
  * the base set of commands
  * successful execution of the base set
  * failed execution of the base set
  * commands to execute on any completion (e.g., for cleanup)


Optional  
##### Step Configuration
When coding your steps, no matter what their function, these are some of the `configuration` section settings you will likely need to consider.
  * Environment variables
  * Runtime
  * Node Pool Assignment
  * Affinity Group
  * Queuing Priority
  * Timeout Duration
  * Integrations
  * Inputs/Triggers
  * Outputs


###### Environment variables
You may define or override a set of `environmentVariables` that will be available only for the duration of the execution of the step.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


###### Runtime
You may choose a runtime for the step to execute in.
###### Node Pool Assignment
You can designate a particular `nodePool` that you want your steps to draw execution nodes from. You may wish to do this to ensure your steps execute in a particular architecture/OS combination, and/or draw from a node pool that provides enough execution nodes to process multiple steps in your pipeline at once.
If you don't specify a `nodePool` then your steps will run in the node pool that has been set as the default.
###### Affinity Group
You can bind steps together in a named `affinityGroup` to ensure that they execute on the same node. This helps when several steps need access to files or artifacts that are created on a node. For example, DockerBuild and DockerPush steps must be assigned to the same `affinityGroup` so that the image built by DockerBuild is available for DockerPush.
###### Queuing Priority
You can assign a queuing `priority` to a step to ensure it gains appropriately ordered access to a build node when there are parallel steps in a pipeline or multiple pipelines are executing.
###### Timeout Duration
You can set a time limit for the step to complete execution, using the timeoutSeconds tag. If the step does not complete in the given number of seconds, the step will be forced to a completion state of `failed`.
###### Integrations
Your native step may require an integration to be declared, or you may wish to issue commands in your step that reference integrations, such as  sending notifications to Slack. If so, you must declare the `integrations` in your step.
###### Inputs/Triggers
When defining the first step in your pipeline, you will likely need to define triggering `inputResources` (such as GitRepo or a Webhook) that will automatically commence a new run of the pipeline. Resources may be from any pipeline source. If the resource is part of a multi-branch pipeline source and from a different branch than the step, the branch must be specified for the resource in `inputResources`.
You should define the remainder of your steps so they execute in an interdependent sequence. This means that each step is configured so that its execution will be triggered by the successful completion of a prior, prerequisite step (or steps). In this way, step 1's completion will trigger the execution of step 2, completion of step 2 triggers execution of step 3, and so on until all steps in the pipeline are executed.
To do this, each step will need to define the `inputSteps` that trigger it in order to build the desired execution sequence.
###### Outputs
Your step will likely produce changes to a resource as a result of performing its task. You must declare these as `outputResources`. If the updated resource is part of a multi-branch pipeline source and from a different branch than the step, the branch must be specified for the resource in `outputResources`.
##### Step Execution
Your automated pipeline issues shell commands to the execution node to perform the work. These are specified in each step's `execution` section during the specified execution stage of that step.
For example, in the generic Bash step:
[YAML]
            . . .
            execution:
              onStart:
                - echo ""prepare for main execution""
              onExecute:
                - echo ""executing task command 1""
                - echo ""executing task command 2""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: 
                - echo ""Cleaning up some stuff""
[/YAML]
### Note
You may not declare an `onExecute` section in a native step as they provide their own `onExecute` to perform their native function.
In addition to the conventional shell commands, you can also make use of built-in Utility Functionsto perform actions through integrations or preserve state information.
You can also leverage environment variables that are available during pipeline execution to flexibly automate any of your step execution actions.

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
74,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a bash?[/INST] The **Bash** is a generic step type that enables executing any shell command. This general-purpose step can be used to execute any action that can be scripted, even with tools and services that haven't been integrated with JFrog Pipelines. This is the most versatile of the steps while taking full advantage of what the lifecycle offers.
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
##### Usage
 **Bash**
[YAML]
    pipelines:
      - name:                       <string>
        steps:
          - name:           <string>
            type:           Bash
            configuration:
              affinityGroup:        bldGroup 
              priority:             <[0-10000]>
              timeoutSeconds:       <job timeout limit>
              nodePool:             <name of the nodePool>
              chronological:        <true/false>
              allowFailure:         <true/false>
              environmentVariables:
                env1:                       <string>
                env2:                       <string>
                env3:
                  default:      <string>
                  description:  <string>
                  values:       <array>
                  allowCustom:  <true/false>
              integrations:
                - name:             <integration name>
              inputSteps:
                - name:             <step name>
                  status:
                    - <terminal_status>
                    - <terminal_status>
                    - <terminal_status>
              inputResources:
                - name:             <resource name>
                  trigger:          <true/false>    # default true
                  newVersionOnly: <true/false>  # default false
                  branch:       <string>        # see description of defaults below
              outputResources:
                - name:             <resource name>
                  branch:       <string>        # see description of defaults below
              runtime:            
                type:                       <image/host>
                image:
                  auto:
                    language: <string>
                     version: <string>  # specifies a single version. Cannot be used if ""versions"" is defined.
                     versions:          # specifies multiple versions. Cannot be used if ""version"" is defined.
                                   - <string>
                  custom:
                    name:                  <string>
                    tag:                       <string>
                    options:           <string>
                    registry:          <integration>   # optional integration for private registry
                    sourceRepository:  <path>          # required if registry is Artifactory. e.g. docker-local
                    region:                            # required if registry is AWS. e.g. us-east-1
                    autoPull:       <true/false>             # default true; pulls image before run
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onExecute:
                - echo ""executing task command 1""
                - echo ""executing task command 2""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step. The name should be chosen to accurately describe what the step does, e.g. `prov_test_env` to represent a job that provisions a test environment. Names of steps must be unique within a pipeline.
###### type
Must be `Bash` for this step type.
###### configuration
Specifies all optional configuration selections for the step's execution environment.
Tag
 **Description of usage**
Required/Optional  
`affinityGroup`
Label that controls affinity to a Node. All the steps with the same affinityGroup will be executed on the same node. This will allow sharing state between the steps. An example is having the same affinityGroup for DockerBuild and DockerPush steps in a Pipeline so that Image being built in the DockerBuild step can be used to published in the DockerPush step.
Optional  
`priority`
Controls the priority of a step when there are parallel steps in a pipeline or multiple pipelines executing. It determines which step will run first across all steps that could run if there were no constraints on the number of steps running. Steps with a lower number will run before steps with higher numbers. For example, priority 10 will run before priority 100. The default priority is 9999.
Priority does not apply to steps that are still waiting for an input to complete or configured to run in a node pool with no available nodes. Also, if there are two steps ready to run and only one available node, the one with the lower priority number runs first, regardless of which pipeline each step belongs to.
Optional  
`timeoutSeconds`
Time limit, in the number of seconds, for the step to complete. If the step does not complete in the given time limit, the step will be forced to a completion state of failed.
Optional  
`nodePool`
Assigns the node pool the step executes on. If node pool isn't specified, a step will execute on the default node pool.  See here to learn more about node pool
Optional  
`chronological`
Specifies the step must execute in chronological order, to ensure receipt of all state updates from preceding steps.
A step with `chronological:true` will only execute when that step is complete in all previously triggered runs.
Optional  
`allowFailure`
If you do not want a step to contribute to the final status of the run, add allowFailure: true to the configuration section of that step. When this option is used, even when a step fails or is skipped, the final status of the run is not affected.
For more information, see Conditional Workflows.
Optional  
`condition`
Create a condition based on the values of add_run_variables environment variable, so that a step can be skipped based on dynamically set variables before it gets assigned to a node.
For more information, see Run Variable Conditional Workflow.
Optional  
`environmentVariables`
Assigns any environment variables and their values in key:value format. All environment variables assigned within a step definition are active only for the scope of the execution of that step.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


Optional  
`integrations`
A collection of integrations that will be used by this step. Integrations can be used directly in step without a resource.
Optional  
`inputSteps`
A collection of named steps whose completion will trigger execution of this step.
In addition, you can set status conditional workflow for input steps. When configured for a step, it executes only if an input step’s status, during its current run, is satisfied. You can configure any number of statuses for an input step.
### Note
It is important to note that the status of an input step in the current run only is considered for conditional workflows. If a step is not part of the current run, it is always assumed that the condition for that input step is met.
For more information, see Step Status Conditional Workflow.
Optional  
`inputResources`
A collection of named Pipelines Resources that will be used by this step as inputs.
  *  **trigger** : By default, changes to these named resources will trigger execution of this step. This can be changed by declaring `trigger` as `false`.
  * `newVersionOnly` : Setting `newVersionOnly` as `true` on one or more inputs will cause the step to be skipped if one or more of those resources is an output resource of another step in the same run and none of the `inputResources` have been updated in the current run.
  * `branch` : A `branch` is only required when using a resource from another branch of a multi-branch source. By default, the resource from a single-branch source or from the same branch of a multi-branch pipeline is used. To use a multi-branch resource in a single-branch pipeline or another branch in a multi-branch pipeline, `branch` should be used to specify the branch to which the resource belongs.


Optional  
`outputResources`
A collection of named Pipelines Resources that will be generated or changed by this step.
`branch`: A `branch` is only required for resources from another branch of a multi-branch source. By default, the resource from a single-branch source or from the same branch of a multi-branch pipeline is assumed. To update a multi-branch resource in a single-branch pipeline or another branch in a multi-branch pipeline, `branch` should be used to specify the branch to which the resource belongs.
Optional  
`runtime`
Specifies the runtime for the execution node.
Optional  
###### execution
Declare sets of shell command sequences to perform for different execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of `onExecute`
Optional  
`onExecute`
Main commands to execute for the step
Optional  
`onSuccess`
Commands to execute on successful completion of `onExecute`
Optional  
`onFailure`
Commands to execute on failed completion of `onExecute`
Optional  
`onComplete`
Commands to execute on any completion of `onExecute`
Optional  
### Note
`onExecute`, `onStart`, `onSuccess`, `onFailure`, and `onComplete` are reserved keywords. Using these keywords in any other context in your execution scripts can cause unexpected behavior.
##### Examples
The Pipelines DSL for these examples is available in this repository in the JFrog GitHub account.
###### Perform a build activity
This is an example of how to use the Bash step to perform a build activity.
 **Bash step to build**
[YAML]
    - name: build
      type: Bash
      configuration:
        nodePool: my_node_pool
        environmentVariables:
          env1: value1
          env2:
            default: value2
            description: Example Variable
            values:
              - value2
              - value3
            allowCustom: false
        runtime:
          type: image
          image:
            auto:
              language: node
              versions:
                - ""16""
        inputResources:
          - name: src
      execution:
        onExecute:
          - cd $res_src_resourcePath
          - npm install
          - mkdir -p testresults && mkdir -p codecoverage
          - $res_src_resourcePath/node_modules/.bin/mocha --recursive ""tests/**/*.spec.js"" -R mocha-junit-reporter --reporter-options mochaFile=testresults/testresults.xml
          - $res_src_resourcePath/node_modules/.bin/istanbul --include-all-sources cover -root ""routes"" node_modules/mocha/bin/_mocha -- -R spec-xunit-file --recursive ""tests/**/*.spec.js""
          - $res_src_resourcePath/node_modules/.bin/istanbul report cobertura --dir codecoverage
          - save_tests $res_src_resourcePath/testresults/testresults.xml
        onSuccess:
          - send_notification mySlack ""build completed""
[/YAML]
###### Python in bash step
This is an example of how to use Python in a bash step.
 **Python**
[YAML]
    resources:
      - name: script
        type: GitRepo
        configuration:
          path: jfrog/sample-script
          gitProvider: myGithub
    pipelines:
      - name: test_stepTestReports
        steps:
          - name: testReport
            type: Bash
            configuration:
              inputResources:
                - name: script
            execution:
              onExecute:
                - cd $res_script_resourcePath
                - ls
                - python -m py_compile calc.py
                - pip install --upgrade pip
                - hash -d pip
                - pip install pytest           
                - py.test --verbose --junit-xml test-reports/results.xml test_calc.py
              onComplete:
                - save_tests $res_script_resourcePath/test-reports/results.xml
[/YAML]
###### runtime, environmentVariables, and inputSteps tags
This example uses the `runtime`, `environmentVariables`, and `inputSteps` tags:
[YAML]
    pipelines:
      - name: api_steps
        steps:
          - name: api_steps
            type: Bash
            configuration:
              runtime:
                type: host
              environmentVariables:
                env1: value1
                env2: value2
            execution:
              onExecute:
                - touch cachefile.txt
                - add_cache_files cachefile.txt my_file
    
          - name: api_steps_2
            type: Bash
            configuration:
              runtime:
                type: host
              inputSteps:
                - name: api_steps
            execution:
              onExecute:
                - echo ""step 2..""
    
      - name: api_steps_ProjectAdmin
        steps:
          - name: api_steps_ProjectAdmin
            type: Bash
            configuration:
              runtime:
                type: host
              environmentVariables:
                env1: value1
                env2: value2
            execution:
              onExecute:
                - touch cachefile.txt
                - add_cache_files cachefile.txt my_file
    
          - name: api_steps_ProjectAdmin_2
            type: Bash
            configuration:
              runtime:
                type: host
              inputSteps:
                - name: api_steps_ProjectAdmin
            execution:
              onExecute:
                - echo ""step 2..""
    
    
[/YAML]
###### affinityGroup and priority tags
This example uses the `affinityGroup` and `priority` tags:
[YAML]
    pipelines:
      - name: S_WF_019
        steps:
          - name: S_WF_019_001
            type: Bash
            execution:
              onStart:
                - add_run_variables step_1_var=""step_1""
              onExecute:
                - echo ""step 1 is running""
    
          - name: S_WF_019_002
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_001
              affinityGroup: ag_foo
              priority: 4
            execution:
              onStart:
                - echo ""step_4_var - ${step_4_var}""
                - if [ ""$step_4_var"" != ""step_4"" ]; then exit 1; fi
                - add_run_variables step_2_var=""step_2""
              onExecute:
                - echo ""step 2 is running""
    
          - name: S_WF_019_003
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_001
              affinityGroup: ag_foo
              priority: 1
            execution:
              onStart:
                - echo ""step_1_var - ${step_1_var}""
                - if [ ""$step_1_var"" != ""step_1"" ]; then exit 1; fi
                - add_run_variables step_3_var=""step_3""
              onExecute:
                - echo ""step 3 is running""
    
          - name: S_WF_019_004
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_001
              affinityGroup: ag_foo
              priority: 3
            execution:
              onStart:
                - echo ""step_3_var - ${step_3_var}""
                - if [ ""$step_3_var"" != ""step_3"" ]; then exit 1; fi
                - add_run_variables step_4_var=""step_4""
              onExecute:
                - echo ""step 4 is running""
    
          - name: S_WF_019_005
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_002
                - name: S_WF_019_003
                - name: S_WF_019_004
              affinityGroup: ag_foo
              priority: 4
            execution:
              onStart:
                - echo ""step_6_var - ${step_6_var}""
                - if [ ""$step_6_var"" != ""step_6"" ]; then exit 1; fi
                - add_run_variables step_5_var=""step_5""
              onExecute:
                - echo ""step 5 is running""
    
          - name: S_WF_019_006
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_002
                - name: S_WF_019_003
                - name: S_WF_019_004
              affinityGroup: ag_foo
              priority: 2
            execution:
              onStart:
                - echo ""step_2_var - ${step_2_var}""
                - echo ""step_3_var - ${step_3_var}""
                - echo ""step_4_var - ${step_4_var}""
                - if [ ""$step_2_var"" != ""step_2"" ]; then exit 1; fi
                - if [ ""$step_3_var"" != ""step_3"" ]; then exit 1; fi
                - if [ ""$step_4_var"" != ""step_4"" ]; then exit 1; fi
                - add_run_variables step_6_var=""step_6""
              onExecute:
                - echo ""step 6 is running""
    
          - name: S_WF_019_007
            type: Bash
            configuration:
              inputSteps:
                - name: S_WF_019_005
                - name: S_WF_019_006
              affinityGroup: ag_foo
              priority: 2
            execution:
              onStart:
                - echo ""step_1_var - ${step_1_var}""
                - echo ""step_2_var - ${step_2_var}""
                - echo ""step_3_var - ${step_3_var}""
                - echo ""step_4_var - ${step_4_var}""
                - echo ""step_5_var - ${step_5_var}""
                - echo ""step_6_var - ${step_6_var}""
                - if [ ""$step_1_var"" != ""step_1"" ]; then exit 1; fi
                - if [ ""$step_2_var"" != ""step_2"" ]; then exit 1; fi
                - if [ ""$step_3_var"" != ""step_3"" ]; then exit 1; fi
                - if [ ""$step_4_var"" != ""step_4"" ]; then exit 1; fi
                - if [ ""$step_5_var"" != ""step_5"" ]; then exit 1; fi
                - if [ ""$step_6_var"" != ""step_6"" ]; then exit 1; fi
              onExecute:
                - echo ""step 7 is running""
    
    
[/YAML]
###### chronological tag
This examples uses the `chronological` tag:
[YAML]
    pipelines: 
      - name: bash_chronological
        steps: 
          - name: Start
            type: Bash
            execution: 
              onExecute: 
                - echo ""It's a start.""
          - name: Step1
            type: Bash
            configuration:
              chronological: true
              inputSteps: 
                - name: Start
            execution: 
              onExecute: 
                - add_run_variables step1=foo
          - name: Step2
            type: Bash
            configuration: 
              chronological: true
              inputSteps: 
                - name: Start
            execution: 
              onExecute: 
                - add_run_variables step2=bar
          - name: Step3
            type: Bash
            configuration: 
              chronological: true
              inputSteps: 
                - name: Start
            execution: 
              onExecute: 
                - add_run_variables step3=baz
          - name: Finish
            type: Bash
            configuration: 
              inputSteps: 
                - name: Step1
                - name: Step2
                - name: Step3
            execution: 
              onExecute: 
                - |
                  echo ""Step1: $step1""
                  echo ""Step2: $step2""
                  echo ""Step3: $step3""
    
    
[/YAML]
###### timeoutSeconds tag
This example uses the `timeoutSeconds` tag:
[YAML]
    pipelines:
      - name: pipelines_S_Bash_0023
        steps:
          - name: S_Bash_0023
            type: Bash
            configuration:
              timeoutSeconds: 10
            execution:
              onExecute:
                - sleep 3m
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
75,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a createreleasebundle?[/INST] The **CreateReleaseBundle** is a native step that produces a Release Bundlefor distribution to an Artifactory Edge Node.The step can be used to create a signed or unsigned release bundle. When the `sign` tag is set as `true` in your yaml, this step saves artifact information to support signed pipelines.Release Bundle RepositoriesJFrog Artifactory Edge
##### YAML Schema
The YAML schema for DockerBuild native step is as follows:
 **CreateReleaseBundle**
[YAML]
    pipelines:
      - name:    <string>
        steps:
          - name: <string>
            type: CreateReleaseBundle
            configuration:
              #inherits all the tags from bash
              releaseBundleName:        <string>
              releaseBundleVersion:     <string>
              dryRun:                   <boolean>   # default true
              sign:                     <boolean>   # default false
              description:              <string>    # optional
              failOnValidate:           <boolean>   # optional (Signed Pipelines must be enabled)
              releaseNotes:                         # optional
                content:                <string>    # ""markdown|asciidoc|plain_text""
                syntax:                 <string>    # required in releaseNotes
     
              inputResources:
                - name:                 <BuildInfo resource>  # one or more BuildInfo, or
                - name:                 <Aql resource>        # one Aql
              outputResources:
                - name:                 <ReleaseBundle resource>
     
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `CreateReleaseBundle `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify either a named BuildInfo resource(s) or an Aql resource. CreateReleaseBundle step does not accept other input resources.
If BuildInfo `inputResources` are provided, the query for the release bundle is constructed using the `buildName`, `buildNumber`, and `targetRepo` of each BuildInfo input.
Required  
`outputResources`
Must specify a ReleaseBundleresource.
The `name`, `version`, and `isSigned` settings in the output ReleaseBundle are updated to the step's `releaseBundleName`, `releaseBundleVersion`, and `sign` values respectively (or any environment variable values that replaced environment variable placeholders for those values).
Required  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`releaseBundleName`
An alphanumeric name for the release bundle.
Required  
`releaseBundleVersion`
Version string for the release bundle.
Required  
`dryRun`
When set to `true` parse and validate only to test whether a release bundle version can be created.
Default is true.
Optional  
`sign`
Specifies whether the release bundle version will be signed as part of this step.
Default is false.
### GPG Signing Key Passphrase
When configuring `sign=true`, and when your Distribution release bundle GPG signing keyis passphrase protected, remember to provide the GPG Signing Key Passphrase when creating/updating your  Distribution Integration.GPG Signing
Optional  
`description`
Description of the release bundle.
Optional  
`releaseNotes`
Describes the release notes for the release bundle version.
`syntax` specifies the format of release notes: `plain_text`, `markdown`, or `asciidoc`. Default is `plain_text`.
`content` is the release notes string in the specified syntax format. Use the `|` character to denote a string preserving newlines.
Optional  
`failOnValidate`
Fail the step if one of the signatures of the BuildInfo input resource artifacts cannot be verified.
Default is `false`.
Optional ( Signed Pipelines must be enabled)  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
### Note
`onExecute`, `onStart`, `onSuccess`, `onFailure`, and `onComplete` are reserved keywords. Using these keywords in any other context in your execution scripts can cause unexpected behavior.
##### Examples
The following examples show which settings to configure for a few different release bundles.
###### Unsigned Release Bundle Created using BuildInfo Resource
A simple, unsigned release bundle created using a BuildInfo resource. In this case, the release bundle version will be the run number and will have no description or release notes.
  * This example requires an Artifactory Integration and a Distribution Integration.
  * The Pipelines DSL for this example is available in this repositoryin the JFrog GitHub account.


 **CreateReleaseBundle**
[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.myArtifactoryIntegration }}
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.demoArtifactoryIntegration }}
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]
###### Create and Sign Release Bundle
Create and immediately sign a release bundle, with a description and release notes added to the release bundle.
 **CreateReleaseBundle**
[YAML]
    pipelines: 
      - name: createReleaseBundlePipeline
        steps:
          - name: createReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: myReleaseBundle
              releaseBundleVersion: ""${run_number}""
              dryRun: false
              sign: true
              description: ""My release bundle""
              releaseNotes:
                syntax: plain_text
                content: ""Release of ${run_number} by ${step_name}""
              inputResources:
                - name: myBuildInfo
              outputResources:
                - name: myReleaseBundle
    
[/YAML]
###### Trigger a Dry Run
Trigger a dry run of the release bundle creation. No release bundle will be created.
 **CreateReleaseBundle**
[YAML]
    pipelines: 
      - name: createReleaseBundlePipeline
        steps:
          - name: createReleaseBundleStep
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: myReleaseBundle
              releaseBundleVersion: ""${run_number}""
              dryRun: true
              inputResources:
                - name: myBuildInfo
              outputResources:
                - name: myReleaseBundle
    
[/YAML]
##### How it Works
When you use the **CreateReleaseBundle** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (configure cli with the integration in the input resource)
  * jfrog rt curl (get the Artifactory service_id)
  * validate_artifact (use the signed pipelines feature to verify the incoming BuildInfo)
  * Create the release bundle creation payload (the JSON object that will be in the request to Distribution)
  * curl (send the release bundle creation payload to Distribution)
  * write_output (update the output ReleaseBundle resource)
  * save_artifact_info (if the bundle was signed, save the data for use with signed pipelines)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
76,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a distributereleasebundle?[/INST] The **DistributeReleaseBundle** native step triggers the distribution of Release Bundles to an Artifactory Edge Node. This step requires a signed release bundle and one or more distribution rules to successfully execute.Distributing Release BundlesJFrog Artifactory Edge
##### YAML Schema
The YAML schema for DistributeReleaseBundle native step is as follows:
 **DistributeReleaseBundle**
[YAML]
    pipelines:
      - name:    <string>
        steps:
          - name: my_distribute
            type: DistributeReleaseBundle
            configuration:
                    #inherits all the tags from bash; 
              dryRun: <boolean>                # optional
              inputResources:
                - name: my_releaseBundle      # one ReleaseBundle is required
                  trigger: false   
                - name: my_distributionRule   # one DistributionRule is required
                  trigger: false    # default true
              outputResources:
                - name: my_releaseBundleOutput # one ReleaseBundle is optional
     
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `DistributeReleaseBundle `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify a ReleaseBundle resource and one DistributionRule resource.
Required  
`outputResources`
May specify a ReleaseBundle resource to be updated with the `name` and `version` of the input ReleaseBundle.
Optional  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`dryRun`
Controls whether this should be a dry run to test if the release bundle can distribute to the Edge nodes matching the distribution rule.
The default is true.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a DistributeReleaseBundle step to distribute or for a dry run.
###### Distribute Input Release Bundle Edge Node
Distributes the input release bundle to the edge nodes defined in the distribution rule.
  * This example requires an Artifactory Integration and a Distribution Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


 **DistributeReleaseBundle**
[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.myArtifactoryIntegration }}
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.demoArtifactoryIntegration }}
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]
###### Trigger a Dry Run
Triggers a dry run of the distribution.
 **DistributeReleaseBundle**
[YAML]
    pipelines: 
      - name: distributeReleaseBundlePipeline
        steps:
          - name: distributeReleaseBundleDryRun
            type: DistributeReleaseBundle
            configuration:
              dryRun: true
              inputResources:
                - name: myReleaseBundle
                - name: myInputDistributionRule
    
[/YAML]
###### Triggers Distribution and Updates Output Resource
Triggers a distribution and updates the output resource with the name and version of the input.
 **DistributeReleaseBundle**
[YAML]
    pipelines: 
      - name: distributeReleaseBundlePipeline
        steps:
          - name: distributeReleaseBundleDryRun
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: myReleaseBundle
                - name: myInputDistributionRule
              outputResources:
                - name: myOutputReleaseBundle
    
[/YAML]
###### Same Step for Dry Runs and to Distribute
In this example, the same step is used for both dry runs and to distribute the release bundle to Edge Nodes. The dry_run variable may be set in the  pipeline configuration section or  step configuration or added as a run variable by an earlier step in the pipeline using add_run_variable.
 **DistributeReleaseBundle**
[YAML]
    pipelines: 
      - name: distributeReleaseBundlePipeline
        steps:
          - name: distributeReleaseBundleStep
            type: DistributeReleaseBundle
            configuration:
              dryRun: ${dry_run}
              inputResources:
                - name: myReleaseBundle
                - name: myInputDistributionRule
    
[/YAML]
##### How it Works
When you use the **DistributeReleaseBundle** native step in a pipeline, it performs the following functions in the background:
  * Create the distribution payload (the JSON object that will be in the request to Distribution)
  * curl $distributionUrl/api/v1/distribution/$releaseBundleName/$releaseBundleVersion (send the distribution or dry run payload to Distribution)
  * curl $distributionUrl/api/v1/release_bundle/$releaseBundleName/$releaseBundleVersion/distribution/$trackerId (if not a dry run, using the tracker ID returned by Distribution, check if the distribution is complete)
  * write_output (update the output ReleaseBundle resource)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
77,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a dockerpush?[/INST] The `DockerPush `native step pushes a Docker Image to a Docker registry.
### Note
  * Currently, only Docker registries in Artifactory are supported.
  * DockerBuild and DockerPush steps must be assigned to the same `affinityGroup` to share state. If they are not, the output of DockerBuild will not be available to DockerPush. For more information on `affinityGroup` see Pipelines Steps.


### Docker Build and Push Quickstart
This Docker Build and Push quickstart demonstrates the definition of a pipeline that uses the DockerBuild and DockerPush native steps to build a single Docker Image, push it to Artifactory, and then publish the BuildInfo.
##### YAML Schema
The YAML schema for DockerPush native step is as follows:
 **DockerPush**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: DockerPush
            configuration:
              #inherits all the tags from bash
              affinityGroup:        <string>
              targetRepository:     <string>        #  may be required. Must be a local repository. Virtual repositories are not supported.
              forceXrayScan:        <boolean>       # default false
              failOnScan:           <boolean>       # default true
              autoPublishBuildInfo: <boolean>       # default false
    
              integrations:
                - name:             <artifactory integration>  # required
              inputSteps:
                - name:             <DockerBuild step>         # required
              outputResources:
                - name:                 <Image resource>           # optional
                - name:                 <BuildInfo resource>       # required if autoPublishBuildInfo is true
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: 
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `DockerPush` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`affinityGroup`
Must specify an affinity group string that is the same as specified in a prior DockerBuild step.
Optional  
`integrations`
Must specify an Artifactory Integration.
Required  
`inputSteps`
Typically the DockerBuild step that built the image. The DockerBuild step must always be in the same affinity group, but other steps, such as Bash or PowerShell, are also permitted in the same affinity group, between DockerBuild and DockerPush.
May be Required  
`outputResources`
May specify an Image resource. If one is specified, the `imageTag` property of that resource will be updated with the `dockerImageTag` of the preceding DockerBuild step.
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set to `true`. If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the input DockerBuild step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName`and `buildNumber` are `$pipeline_name` and `$run_number` respectively.
Optional
May be required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well as that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`targetRepository`
The name of the Docker repository in Artifactory. Required when using JFrog CLI v1 and not used when the pipeline is configured to use JFrog CLI v2.
### Note
Must be a local repository. Virtual repositories are not supported.
May be required  
`forceXrayScan`
When `true`, forces a scan of the pushed image by JFrog Xray.
Default is `false`.
Optional  
`failOnScan`
When set to `true`, and when the Xray Policy Rule  Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
`autoPublishBuildInfo`
When set to `true`, publishes build info with the Docker image.
Default is `false`.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a DockerPush step to push a Docker image.
###### Push Image to Artifactory
Pushes the image created by the DockerBuild input step to Artifactory. Does not trigger a scan.
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For a full tutorial, see Pipeline Example: Docker Build and Push.


 **DockerPush**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: app_repo1
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.gitIntegration }}                      
          path: {{ .Values.gitRepositoryPath }}    
          branches:
            include: master
    
      - name: app_buildinfo1
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
      - name: app_promoted_buildinfo1
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
    pipelines:
      - name: app_dev_pipeline
        steps:
          - name: app_build
            type: DockerBuild
            configuration:
              affinityGroup: docker_group
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}  
              dockerImageTag: ${run_number}
              inputResources:
                - name: app_repo
              integrations:
                - name: {{ .Values.artifactoryIntegration }}         
                
          - name: app_push
            type: DockerPush
            configuration:
              affinityGroup: docker_group
              targetRepository: {{ .Values.sourceRepository }}
              integrations:
                - name: {{ .Values.artifactoryIntegration }}                            
              inputSteps:
                - name: app_build
    
          - name: publish_app_build
            type: PublishBuildInfo
            configuration:
              affinityGroup: docker_group
              inputSteps:
                - name: app_push
              outputResources:
                - name: app_buildinfo
    
[/YAML]
###### Affinity Group
This extends one of the DockerBuild examples, pushing that image to Artifactory. Note that an affinity group has been specified in both steps.
 **DockerPush**
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              affinityGroup: dockerGroup
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc   # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              dockerOptions: --build-arg build_number_env_variable=${run_number}          
              inputResources:
                - name: gosvc_app
              integrations:
                - name: MyArtifactory
    
          - name: dockerPushStep
            type: DockerPush
            configuration:
              affinityGroup: dockerGroup
              targetRepository: dockerRepo
              inputSteps:
                - name: bld_image
              outputResources:
                - name: outputBuildInfo
              integrations:
                - name: MyArtifactory
    
[/YAML]
###### Publish Build Info, Trigger Xray Scan, Update Output Image Resource
In this, publishing build info, triggering an Xray scan, and updating an output Image resource has been added to the previous example.
 **DockerPush**
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              affinityGroup: dockerGroup
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc   # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              dockerOptions: --build-arg build_number_env_variable=${run_number}          
              inputResources:
                - name: gosvc_app
              integrations:
                - name: MyArtifactory
    
          - name: dockerPushStep
            type: DockerPush
            configuration:
              affinityGroup: dockerGroup
              targetRepository: dockerRepo
              autoPublishBuildInfo: true
              forceXrayScan: true
              inputSteps:
                - name: bld_image
              outputResources:
                - name: outputBuildInfo
                - name: outputImage
              integrations:
                - name: MyArtifactory
    
[/YAML]
##### How it Works
When you use the **DockerPush** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt use (to set the current default Artifactory configuration to the one set up for the integration in `integrations`)
  * restore_run_files (copy the build information saved from the DockerBuild step)
  * jfrog rt docker-push (push the image to Artifactory)
  * jfrog rt build-publish (if `autoPublishBuildInfo` is true, publish the build info)
  * write_output (if `autoPublishBuildInfo` is true, update the BuildInfo buildName and buildNumber)
  * write_output (if there is an output Image resource, update the Image imageTag)
  * jfrog rt build-scan (if forceXrayScan is true, trigger a scan)
  * add_run_files (save/update the build information in the run state for later publish steps)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
78,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a dockerbuild?[/INST] The **DockerBuild** native step performs a build to produce a Docker image from a Dockerfile in a GitRepo source repository resource.
In the step configuration, you must provide the name ( `dockerFileName` ) and directory ( `dockerFileLocation` ) of the Dockerfile that contains the command to be processed by a `docker build` command, as well as the name ( `dockerImageName` ) and tag ( `dockerImageTag` ) of the resulting image. The image is built on the build node, and information about that image is stored in the run state.
To build a Docker image that relies on a private base image:
  1. Define the base image as an Image resource, with `autoPull` set to `true`.
  2. Specify the Image resource as one of the `inputResources` of the DockerBuild step.


To include artifacts in the Docker image that are not part of the GitRepo source repository:
  1. Define a FileSpec resource that specifies the files to include from Artifactory.
  2. Specify the FileSpec resource as one of the `inputResources`of the DockerBuild step.


### Proper usage of DockerBuild step
DockerBuild and DockerPush steps must be assigned to the same `affinityGroup` to share state. If this is not done, the output of DockerBuild will not be available for DockerPush. For more information on using `affinityGroup`, see Running multiple steps on the same build node.
### Docker Build and Push Quickstart
This Docker Build and Push quickstart demonstrates the definition of a pipeline that uses the DockerBuild and DockerPush native steps to build a single Docker Image, push it to Artifactory, and then publish the BuildInfo.
##### YAML Schema
The YAML schema for DockerBuild native step is as follows:
 **DockerBuild**
[YAML]
    pipelines: 
      - name: <string>
        steps:
          - name: <string>
            type: DockerBuild
            configuration:
              #inherits all the tags from bash
              affinityGroup:       <string>
              dockerFileLocation:  <string>
              dockerFileName:      <string>
              dockerImageName:     <string>
              dockerImageTag:      <string>
              dockerOptions:       <string>
    
              integrations:
                - name:            <artifactory or docker registry integration>  # required
    
              inputResources:
                - name:             <GitRepo resource>        # required, git repository containing your Dockerfile
                - name:             <Image resource>          # optional base image
                - name:             <FileSpec resource>       # optional
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `DockerBuild` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`affinityGroup`
Must specify an affinity group string that is the same as specified in a subsequent DockerPush step.
Optional  
`inputResources`
Must specify:
  * a GitRepo resource (that contains the Dockerfile)


Optionally, you may also specify:
  * One or more Image resources to pull base images used in the build or to trigger this build.
  * One or more FileSpec resources that specify what files to include in the build context. These files are automatically copied to `dockerFileLocation`.


Required/Optional  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`dockerFileLocation`
Directory containing the Dockerfile, which is the file that has Docker build configuration. This file is also used as the context for the Docker build. The path provided should be relative to the root of the input GitRepo repository. If no location is provided, the default is the root of the GitRepo repository.
Required  
`dockerFileName`
Name of the Dockerfile.
Required  
`dockerImageName`
The name of the Docker image to create. This can be set using environment variables or triggering a run using parameters.
Required  
`dockerImageTag`
The tag for the Docker image to create. This can be set using environment variables or triggering a run using parameters.
Required  
`dockerOptions`
Additional options for the docker build command.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples use a GoLang Git repository represented by a GitRepo resource named `gosvc_app` to create a Docker image that is published to Artifactory. They assume that an Artifactory integration named `MyArtifactory` has been created, and that the Artifactory instance has a Docker repository mapped to `docker.artprod.company`.
  * These examples require an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for a similar example is available in this repository in the JFrog GitHub account.
  * For a full tutorial, see Pipeline Example: Docker Build and Push.


The following resources declarations support these examples. Not all of these resources are used in all examples.
###### Resources
[YAML]
    resources:
    # Application source repository
      - name: gosvc_app
        type: GitRepo
        configuration:
          gitProvider: myGithub
          path: myuser/myrepo                   # replace with your repository name
          branches:
            include: master
    
    # Docker image in an Artifactory repository
      - name: base_image
        type: Image
        configuration:
          registry: myArtifactory
          sourceRepository: docker-local        # replace with your repository name
          imageName: docker.artprod.mycompany.com/baseimage
          imageTag: latest
          autoPull: true
    
    # Files in an Artifactory repository
      - name: icon_files
        type: FileSpec
        configuration:
          sourceArtifactory: myArtifactory
          pattern: my-local-repo/all-my-images/
          target: icons/
[/YAML]
###### Build a Docker image from a source repository
This example builds a Docker image to a Docker registry in Artifactory. The tag for the image is set to the pipeline's run number.
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc    # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              inputResources:
                - name: gosvc_app
              integrations:
                - name: MyArtifactory
[/YAML]
###### Build a Docker image with dockerOptions
This example demonstrates use of the `dockerOptions` tag to set the `build-arg` option for the Docker command. An environment variable named `build_number_env_variable` is dynamically set to the pipeline's run number. The example assumes the environment variable is used in the Dockerfile commands.
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc   # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              dockerOptions: --build-arg build_number_env_variable=${run_number}           
              inputResources:
                - name: gosvc_app
              integrations:
                - name: MyArtifactory
[/YAML]
###### Build a Docker image with a private base image
This example builds a Docker image that relies on a private base image stored in an Artifactory Docker repository.
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc       # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              inputResources:
                - name: gosvc_app
                - name: base_image
              integrations:
                - name: MyArtifactory
[/YAML]
###### Build a Docker image with files outside the current path
This example demonstrates building a Docker image that includes files outside of the current path. It pulls icon files stored in an Artifactory repository for integration art named `my-local-repo`. It is assumed that the Dockerfile has a command that will include the files in `/icons` into the image.
[YAML]
    pipelines:
      - name: demo_pipeline
        steps:
          - name: bld_image
            type: DockerBuild
            configuration:
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: docker.artprod.mycompany.com/gosvc         # replace with your fully qualified Docker registry/image name
              dockerImageTag: ${run_number}
              inputResources:
                - name: gosvc_app
                - name: icon_files
              integrations:
                - name: MyArtifactory
[/YAML]
##### How it Works
When you use the **DockerBuild** native step in a pipeline, it performs the following functions in the background:
  * cp (if there is a FileSpec input, copy those files to the root of the cloned GitRepo input)
  * docker build
  * add_run_variables (add several variables that are later used when pushing the Docker image or publishing build info)
  * jfrog rt build-collect-env (collect environment information to be later published as part of build info)
  * add_run_files (save information collected for build info)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
79,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a gobuild?[/INST] The **GoBuild** native step performs a build from Go (GoLang) source.
##### YAML Schema
The YAML schema for GoBuild native step is as follows:
 **GoBuild**
[YAML]
    pipelines: 
      - name: <string>
        steps:
          - name: <string>
            type: GoBuild
            configuration:
              #inherits all the tags from bash
              sourceLocation:   <string>  # optional
              outputLocation:   <string>  # optional
              outputFile:       <string>  # optional
              resolverRepo:     <string>  # optional
              repository:       <string>  # optional
              goCommand:        <string>  # optional
    
              integrations:
                - name:         <artifactory integration>  # may be required
              inputResources:
                - name:         <GitRepo resource>         # required
                - name:         <FileSpec resource>        # optional
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete:                                  #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `GoBuild `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Specifies an Artifactory Integration where modules will be published. If a FileSpec resource is specified in `inputResources` then this is optional. Otherwise, it is required.
May be required  
`inputResources`
Must specify a GitRepo resource that has Go source files in `sourceLocation`.
Also may specify an optional FileSpec resource that specifies what files to copy to `sourceLocation`to build.
Required
Optional  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`sourceLocation`
Location where the Go source files are available, relative to the root of the GitRepo repository. If not specified, the default is the root of the GitRepo repository.
Optional  
`outputLocation`
Location where the built Go modules should be published.
Optional  
`outputFile`
File that has the output of the Go command. The default filename is the name of the step.
Optional  
`resolverRepo`
Name of the Artifactory repository to be used to resolve dependencies.
Optional  
`repository`
Alternative to `resolverRepo`. Only one of these options may be specified.
Optional  
`goCommand`
Specifies a command line string of options to use with the Go client.
 **Default** : `build -o $outputLocation/$outputFile`
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a GoBuild step.
###### Full Pipeline Example
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For a full tutorial, see Pipeline Example: Go Build.


[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file. For more information, see the 'Pipeline Example: Go Build' quickstart.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Sample Go app in a GitRepo
      - name: go_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repoPath }}
          branches:
            include: main
          gitProvider: {{ .Values.gitProvider }}
    
      # Build info for the published Go app
      - name: go_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactory }}
    
    
    pipelines:
      - name: go_build_pipeline_example
        steps:
          # Build the Go sample app from the GitRepo
          - name: build_go
            type: GoBuild
            configuration:
              sourceLocation: .
              resolverRepo: go-virtual
              noRegistry: true
              inputResources:
                - name: go_repo
              integrations:
                - name: {{ .Values.artifactory }}
    
                  # Publish the Go sample app binary to Artifactory
          - name: publish_go_binary
            type: GoPublishBinary
            configuration:
              inputSteps:
                - name: build_go
              targetRepository: go-local
              integrations:
                - name: {{ .Values.artifactory }}
    
          # Publish the Go sample app build info
          - name: publish_build
            type: PublishBuildInfo
            configuration:
              inputSteps:
                - name: publish_go_binary
              outputResources:
                - name: go_buildinfo
[/YAML]
###### Default Locations
A GoBuild step using default locations.
 **GoBuild**
[YAML]
    pipelines: 
      - name: goBuildPipeline
        steps:
          - name: goBuildStep
            type: GoBuild
            configuration: 
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
[/YAML]
###### FileSpec Input
A GoBuild step with a FileSpec input providing files for the build that are not in the GitRepo and resolverRepo specifying an Artifactory repository to use when resolving dependencies.
 **GoBuild**
[YAML]
    pipelines: 
      - name: goBuildPipeline
        steps:
          - name: goBuildStep
            type: GoBuild
            configuration:
              resolverRepo: repo
              inputResources:
                - name: gitRepoResource
                - name: fileSpec
    
[/YAML]
###### Alternative Source Location in Git Repo
A GoBuild step with an alternative source location in the GitRepo and an alternative Go command for the build.
 **GoBuild**
[YAML]
    pipelines: 
      - name: goBuildPipeline
        steps:
          - name: goBuildStep
            type: GoBuild
            configuration: 
              sourceLocation: ""app/go""
              goCommand: ""build -insecure -o output/outputFile""
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
##### How it Works
When you use the **GoBuild** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (to configure the JFrog CLI with the Artifactory credentials in the input FileSpec if there is no input integration)
  * jfrog rt use (to set the current default Artifactory configuration)
  * cp (if there is an input FileSpec, copy the files to the root of the cloned GitRepo)
  * jfrog rt go-config (configure the repository to resolve dependencies)
  * jfrog rt go (build)
  * add_run_variables (save information about this step for future steps)
  * add_run_files (save the output and the build information in the run state for later publish steps)
  * jfrog rt build-collect-env (collect environment variables)


##### Related Topics
Go Build Quickstart

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
80,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a gopublishbinary?[/INST] The **GoPublishBinary** native step publishes the GO (GoLang) binaries built in a GoBuild step to Artifactory.
### Tip
It is recommended, but not required, that the GoBuild and GoPublishBinary steps be in the same affinity group to optimize sharing files between the two steps.
##### YAML Schema
The YAML schema for GoPublishBinary native step is as follows:
 **GoPublishBinary**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: GoPublishBinary
            configuration:
              #inherits all the tags from bash
    
              forceXrayScan:                <boolean>  # optional
              failOnScan:           <boolean>  # default true
              autoPublishBuildInfo: <boolean>  # optional
              targetRepository:     <string>   # required
                      failOnValidate:       <boolean>  # optional (Signed Pipelines must be enabled)
    
              integrations:
                - name:         <artifactory integration>  # required if autoPublishBuildInfo is false
              outputResources:
                - name:         <BuildInfo resource>       # required if autoPublishBuildInfo is true
              inputSteps:
                - name:         <GoBuild, Bash, or PowerShell step> # required
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete:                                  #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `GoPublishBinary `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Specifies an Artifactory Integration. Required when `autoPublishBuildInfo`is set to false. Otherwise, the artifactory integration specified in the BuildInfo resource will be used.
May be required  
`outputResources`
Must specify a BuildInfo resource when`autoPublishBuildInfo`is set to true.
If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the input GoBuild step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number `respectively.
May be required  
`inputSteps`
Must specify a GoBuild, Bash, or PowerShell step.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`forceXrayScan`
When true, forces a scan of the pushed image by JFrog Xray.
Default is false.
Optional  
`failOnScan`
When set to t`rue`, and when the Xray Policy Rule Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
`autoPublishBuildInfo`
When set to `true`, publishes build info with the Docker image.
Default is `false`.
Optional  
`targetRepository`
Repository in Artifactory where the module will be published.
Required  
`failOnValidate`
Fail the step if signatures of build artifacts cannot be verified.
Default is `false`.
Optional (Signed Pipelines must be enabled)  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a GoPublishBinary step.
###### Upload to Artifactory
Uploads the binary built by a GoBuild step to an Artifactory repository named go-local.
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For more information about running this example, see Pipeline Example: Go Build.


[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file. For more information, see the 'Pipeline Example: Go Build' quickstart.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Sample Go app in a GitRepo
      - name: go_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repoPath }}
          branches:
            include: main
          gitProvider: {{ .Values.gitProvider }}
    
      # Build info for the published Go app
      - name: go_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactory }}
    
    
    pipelines:
      - name: go_build_pipeline_example
        steps:
          # Build the Go sample app from the GitRepo
          - name: build_go
            type: GoBuild
            configuration:
              sourceLocation: .
              resolverRepo: go-virtual
              noRegistry: true
              inputResources:
                - name: go_repo
              integrations:
                - name: {{ .Values.artifactory }}
    
          # Publish the Go sample app binary to Artifactory
          - name: publish_go_binary
            type: GoPublishBinary
            configuration:
              inputSteps:
                - name: build_go
              targetRepository: go-local
              integrations:
                - name: {{ .Values.artifactory }}
[/YAML]
###### Upload Build Info
Uploads the binary built by a GoBuild step to an Artifactory repository named go-repo and uploads build info. This extends example 1 in the GoBuild documentation.
 **GoPublishBinary**
[YAML]
    pipelines: 
      - name: goBuildPipeline
        steps:
          - name: goBuildStep
            type: GoBuild
            configuration:
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: goPublishBinaryStep
            type: GoPublishBinary
            configuration:
              targetRepository: go-repo
              autoPublishBuildInfo: true
              inputSteps:
                - name: goBuildStep
              outputResources:
                - name: outputBuildInfo
    
[/YAML]
###### Publish Build Info and Trigger Xray Scan
In this example, build info is published and an Xray scan triggered.
 **GoPublishBinary**
[YAML]
    pipelines: 
      - name: goBuildPipeline
        steps:
          - name: goBuildStep
            type: GoBuild
            configuration:
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: goPublishBinaryStep
            type: GoPublishBinary
            configuration:
              targetRepository: go-repo
              autoPublishBuildInfo: true
              forceXrayScan: true
              inputSteps:
                - name: goBuildStep
              outputResources:
                - name: outputBuildInfo
    
[/YAML]
##### How it Works
When you use the **GoPublishBinary** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (if there is a BuildInfo output, configure the JFrog CLI with those credentials)
  * jfrog rt use (specify the configured credentials to use)
  * restore_run_files (copy the output and build info from the GoBuild step)
  * jfrog rt upload (upload the binary)
  * jfrog rt build-publish (if autoPublishBuildInfo is true, publish the build info)
  * write_output (if autoPublishBuildInfo is true, update the output BuildInfo resource)
  * jfrog rt build-scan (if forceXrayScan is true, trigger a scan)
  * add_run_files (update the build info saved in run state)


##### Related Topics
Go Build Quickstart

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
81,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a gopublishmodule?[/INST] The **GoPublishModule** native step publishes GO (GoLang) modules to an Artifactory repository.
##### YAML Schema
The YAML schema for GoPublishModule native step is as follows:
 **GoPublishModule**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: GoPublishModule
            configuration:
              #inherits all the tags from bash
    
              forceXrayScan:        <boolean>         # optional
              failOnScan:           <boolean>         # default true
              autoPublishBuildInfo: <boolean>         # optional
              
              # for payloadType module:
              sourceLocation: <string>                # optional
              version: <string>                       # required
              targetRepository: <string>              # required
              self: <boolean>                         # optional
              deps: <string>                          # optional 
              resolverRepo: <string>                  # optional
    
              integrations:
                - name:         <artifactory integration>  # required if autoPublishBuildInfo is false
              inputResources:
                - name:         <gitrepo resource>         # required
                - name:         <filespec resource>        # optional
              outputResources:
                - name:         <buildinfo resource>       # required if autoPublishBuildInfo is true
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete:                                  #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `GoPublishModule `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Specifies an Artifactory Integration. Required when `autoPublishBuildInfo` is set to `false`. Otherwise, the Artifactory integration specified in the BuildInfo resource will be used.
May be required  
`inputResources`
Must specify a GitReporesource. The publish commands will run on the Git repository at `sourceLocation`.
Also may specify an optional FileSpec resource that specifies what files to copy to `sourceLocation`.
Required
Optional  
`outputResources`
Must specify a BuildInfo resource when`autoPublishBuildInfo`is set to `true`.
If `JFROG_CLI_BUILD_NAME`or `JFROG_CLI_BUILD_NUMBER`is set as an environment variable for the pipeline or the step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName`and `buildNumber`are `$pipeline_name` and `$run_number` respectively.
May be required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`sourceLocation`
Location of the Go source files relative to the root of the input GitRepo repository. If not specified, the default is the root of the GitRepo repository.
Optional  
`version`
Version of the module to build.
Required  
`targetRepository`
Repository in the Artifactory where the module will be published.
Required  
`resolverRepo`
Name of the Artifactory repository to be used to resolve dependencies.
Optional  
`self`
When `true`, uses the `--self` option to publish the Project to Artifactory. For more information, see the CLI for JFrog Artifactory documentation. Not supported with JFrog CLI v2.
Optional  
`deps`
When specified, uses the `--deps` option to specify a list of project dependencies to Artifactory. For more information, see the CLI for JFrog Artifactory documentation. Not supported with JFrog CLI v2.
Optional  
`forceXrayScan`
When set to true, force an Xray scan after publishing to Artifactory.
Default is false.
Optional  
`failOnScan`
When set to **true** , and when the Xray Policy Rule **Fail Build Creating Xray Policies and Rules** checkbox is checked, a failed Xray scan will result in a failure of the step.
Default is true.
Optional  
`autoPublishBuildInfo`
When set to true, automatically publish the implicitly created BuildInfo.
Default is false.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a GoPublishModule step.
###### Full Pipeline Example
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Sample Go app in a GitRepo
      - name: go_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repoPath }}
          branches:
            include: main
          gitProvider: {{ .Values.gitProvider }}
    
      # Build info for the published Go app
      - name: go_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactory }}    
    
    pipelines:
      - name: go_publish_binary_example
        steps:
          # Build the Go sample app from the GitRepo
          - name: build_go
            type: GoBuild
            configuration:
              sourceLocation: .
              resolverRepo: go-virtual
              noRegistry: true
              inputResources:
                - name: go_repo
              integrations:
                - name: {{ .Values.artifactory }}
                 
          # Publish the Go sample app binary to Artifactory
          - name: cicd_go_publish_binary
            type: GoPublishBinary
            configuration:
              # forceXrayScan: true
              # failOnScan: false
              autoPublishBuildInfo: true
              inputSteps:
                - name: build_go
              targetRepository: go-local
              integrations:
                - name: myArtifactory
              outputResources:
                - name: go_buildinfo
[/YAML]
###### Using Default Locations
A GoPublishModule step using default locations and publishing version v0.0.0 to an Artifactory repository named go-repo.
 **GoPublishModule**
[YAML]
    pipelines: 
      - name: goPublishModulePipeline
        steps:
          - name: goPublishModuleStep
            type: GoPublishModule
            configuration: 
              version: ""v0.0.0""
              targetRepository: go-repo
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
###### Different Source Location in the GitRepo
A GoPublishModule step specifying a different source location in the GitRepo and publishing the project and dependencies to the Artifactory repository named go-repo as well.
 **GoPublishModule**
[YAML]
    pipelines: 
      - name: goPublishModulePipeline
        steps:
          - name: goPublishModuleStep
            type: GoPublishModule
            configuration: 
              version: ""v0.0.${run_number}""
              targetRepository: go-repo
                      resolverRepo: go-virtual
              self: true
              deps: ALL
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
###### Publish Build Info and Trigger Xray Scan
A GoPublishModule step that publishes the build info and triggers an Xray scan.
 **GoPublishModule**
[YAML]
    pipelines: 
      - name: goPublishModulePipeline
        steps:
          - name: goPublishModuleStep
            type: GoPublishModule
            configuration: 
              version: ""v0.0.${run_number}""
              targetRepository: go-repo
              autoPublishBuildInfo: true
              forceXrayScan: true
              inputResources:
                - name: gitRepoResource
              outputResources:
                - name: outputBuildInfo
              integrations:
                - name: artifactory_integration
    
[/YAML]
##### How it Works
When you use the **GoPublishModule** native step in a pipeline, it performs the following functions in the background:
  * jfrog config add (if there is an output BuildInfo resource, configure the JFrog CLI with the Artifactory credentials in that resource)
  * jfrog config use (to set the current default Artifactory configuration)
  * cp (if there is an input FileSpec, copy the files to the root of the cloned GitRepo)
  * jfrog rt go-config (configure the repository to resolve dependencies)
  * jfrog rt go-publish (publish)
  * add_run_variables (save information about this step for future steps)
  * jfrog rt build-collect-env (collect environment variables)
  * jfrog rt build-publish (if autoPublishBuildInfo is true, publish the build info)
  * write_output (if autoPublishBuildInfo is true, update the output resource)
  * jfrog rt build-scan (if forceXrayScan is true, trigger a scan)
  * add_run_files (save the build information in the run state for later publish steps)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
82,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a gradlebuild?[/INST] The **GradleBuild** native step performs a Gradle build on files in a Git repository. Optionally, it can also publish build information to Artifactory.
This step utilizes the JFrog CLI to run a Gradle build for your project. The CLI config file can be created during execution, or you can generate it ahead of time with the settings you require and commit it to your repository. For information about gradle-config, see CLI for JFrog Artifactory documentation.
##### YAML Schema
The YAML schema for GradleBuild native step is as follows:
 **GradleBuild**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: GradleBuild
            configuration:
                      #inherits all the tags from bash
    
              gradleCommand:                <string>  #required
              sourceLocation:       <string>  #optional
              configFileLocation:   <string>  #optional
                      configFileName:               <string>  #optional
              resolverRepo:         <string>  #optional
                      deployerRepo:             <string>      #optional
                      usesPlugin:           <string>  #optional
                      useWrapper:               <string>      #optional
                      forceXrayScan:                <boolean> #optional
              failOnScan:           <boolean>   #optional; default true
                      autoPublishBuildInfo: <boolean> #optional
              inputResources:
                            - name: myGitRepo       
                            - name: artifactoryFileSpec     
                      outputResources:
                            - name: myBuildInfo
              integrations:
                            - name: myArtifactory 
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `GradleBuild` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify an  Artifactory Integration.
Required  
`inputResources`
Must specify a GitRepo resource. The Gradle command will perform the build on files in the Git repository at `sourceLocation`.
May also optionally specify a FileSpec resource that specifies what files to copy to `sourceLocation` to build.
Required  
`outputResources`
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set to `true`.
If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number.`
May be required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`gradleCommand`
Specifies a command line string of options to use with the Gradle build tool.
Required  
`sourceLocation`
Location of the source files to build relative to the root of the GitRepo repository. If no path is specified, the root of the repository will be used.
Optional  
`resolverRepo`
Artifactory repository to be used to resolve dependencies. May not be used with `configFileName` and `configFileLocation`.
Optional  
`deployerRepo`
Artifactory repository to which to deploy. May not be used with `configFileName` and `configFileLocation`.
Optional  
`usesPlugin`
Set to `true` if the Gradle Artifactory Plugin is already applied in the build script. Default is `false`.
Do not use with `configFileName` and`configFileLocation` (you can enable this option directly in your existing config file).
Optional  
`useWrapper`
If set to `true`, use the Gradle wrapper. Default is `false`.
Do not use with `configFileName` and `configFileLocation` (you can enable this option directly in your existing config file).
Optional  
`forceXrayScan`
When set to `true`, forces an Xray scan after publishing to Artifactory.
Default is `false`.
Optional  
`failOnScan`
When set to `true`, and when the Xray Policy Rule  Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
`autoPublishBuildInfo`
When set to `true`, publishes build info to Artifactory.
Default is `false`.
Optional  
`configFileName`
Gradle configuration file. If specified, the `configFileLocation` tag is also required. This file is generated using the JFrog CLI when you run `jfrog rt gradle-config` command.
The `serverID` in the configuration file should match the Artifactory integration name.
### Note
Use this option only if you have committed this configuration file to your source.
Optional  
`configFileLocation`
Gradle configuration file location. A `configFileName` tag is also required. Typically stored in the `.jfrog` directory in the repository.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show a few ways in which a GradleBuild step can be configured.
###### Using Default Values
The most basic form of GradleBuild. Uses all default values. This step will navigate to the root of the repo specified in the inputResources array, and run the specified Gradle command.
 **GradleBuild**
[YAML]
    pipelines: 
      - name: gradleBuildPipeline
        steps:
          - name: gradleBuildStep
            type: GradleBuild
            configuration: 
              gradleCommand: ""build""
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
###### Source Location is a Sub-directory of Git Repo
A more complicated GradleBuild step, in this one the source location is a subdirectory of the GitRepo (gradle-example-minimal) and the project uses the Gradle Artifactory Plugin. Resolver and deployer repositories have also been configured, resolving from jcenter and deploying to gradle-local.
 **GradleBuild**
[YAML]
    pipelines: 
      - name: gradleBuildPipeline
        steps:
          - name: gradleBuildStep
            type: GradleBuild
            configuration: 
              gradleCommand: ""artifactoryPublish clean -b build.gradle""
              sourceLocation: gradle-example-minimal
              deployerRepo: gradle-local
              resolverRepo: jcenter
              usesPlugin: true
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
###### Using Config File
This GradleBuild step is similar to the previous example, but has aconfig file committed to the repository as gradle-example-minimal/gradle-art-config. The config file contains the repository and plugin settings.
 **GradleBuild**
[YAML]
    pipelines: 
      - name: gradleBuildPipeline
        steps:
          - name: gradleBuildStep
            type: GradleBuild
            configuration: 
              gradleCommand: ""artifactoryPublish clean -b build.gradle""
              sourceLocation: gradle-example-minimal
              configFileLocation: .
              configFileName: gradle-art-config
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
    
[/YAML]
###### Publish Build Info and Trigger Xray Scan
In this example, build info is published as part of the GradleBuild step and an Xray scan is triggered.
 **GradleBuild**
[YAML]
    pipelines: 
      - name: gradleBuildPipeline
        steps:
          - name: gradleBuildStep
            type: GradleBuild
            configuration: 
              gradleCommand: ""build""
              autoPublishBuildInfo: true
              forceXrayScan: true
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
              outputResources:
                            - name: myBuildInfo
    
[/YAML]
##### How it Works
When you use the **GradleBuild** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (configure JFrog CLI with the integration listed in the yaml)
  * jfrog rt gradle-config (if configFileName and configFileLocation are not set in yaml)
  * jfrog rt gradle $gradleCommand (the main gradle build command)
  * add_run_variables (save information in run state for future steps to reference)
  * jfrog rt build-collect-env (collect the build environment, preparing for build publish)
  * jfrog rt build-publish (publish the build, only if autoPublishBuildInfo is true)
  * write_output (update the BuildInfo output resource with the published name/number)
  * jfrog rt build-scan (if forceXrayScan is true)
  * add_run_files (adds build info to run state)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
83,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a helmbluegreencleanup?[/INST] The **HelmBlueGreenCleanup** step uninstalls an Idle release previously deployed by a HelmBlueGreenDeploy step.
##### Blue/Green Deployment Strategy
Blue/Green deployment is a strategy of releasing new software that uses two production environments, as identical as possible, that take turns on serving users requests while the other one is used to perform the final stage of testing.
The **HelmBlueGreenDeploy** step refers to those two environments as **blue** and **green**. Those environments can be either playing **Idle** or **Live** roles, depending on which environment the users' requests are being forwarded to. The environment that is currently handling user requests is considered Live while the other one is considered Idle. To rollout a new release, we first deploy it to the environment playing the Idle role. There we can validate the new version to check if it is good to become available to users. If that is true, we can flip the environment roles so the previously Idle environment becomes Live and starts to handle user requests, while the previously Live environment goes Idle. One of the main advantages of this strategy is that rolling back new releases is as easy as flipping the environments roles again.
Both blue and green environments are represented in the context of Helm as two individual releases that co-exist in the same namespace. The **HelmBlueGreenDeploy** step assign Idle or Live roles to those releases by creating copies of their public Services dedicated to each role. By doing that we can easily change the releases role by updating those Services to be a copy of the Services from a specific environment. Those role Services created by Pipelines can be used to provision additional entrypoint components like DNS Records, Ingress Rules or Service Meshes.
To implement the complete Blue/Green deployment strategy workflow, Pipelines provides three native steps:
  *  **HelmBlueGreenDeploy** discovers from runtime which release is playing Idle role, deploys the chart to it and creates or updates Idle role Services.
  *  **HelmBlueGreenRoleSwitch** discovers from runtime which release is playing each role and flips them by creating or updating the role Services.
  *  **HelmBlueGreenCleanup** discovers from runtime which release is playing Idle role and uninstalls it.


##### YAML Schema
The YAML schema for HelmBlueGreenCleanup native step is as follows:
 **HelmBlueGreenCleanup**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: HelmBlueGreenCleanup
            configuration:
              #inherits all the tags from bash
              deployStep:       <string>
              flags:            <string>                               # optional
              
              integrations:
                - name:         <kubernetes integration>                         # required          
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `HelmBlueGreenCleanup` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a Kubernetes Integration.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`deployStep`
HelmBlueGreenDeploy step name that deployed the Helm Chart and where the blue/green strategy was configured. The referenced deploy step must be declared in the same Pipeline.
Required  
`flags`
String containing global flags to be included in the Helm command when uninstalling the release. For example: ""--debug""
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a HelmBlueGreenCleanup step.
###### Full Pipeline Example
This pipeline shows how to deploy a Helm chart using the blue/green strategy, promote it to the Live role and uninstall the previous deployed version (if present).
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmBlueGreenDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
         
         - name: helmRoleSwitch
           type: HelmBlueGreenRoleSwitch
           configuration: 
             deployStep: helmDeployStep
             integrations:
               - name: kubernetes_integration
             inputSteps:
               - name: helmDeployStep   
    
         - name: helmCleanup
           type: HelmBlueGreenCleanup
           configuration: 
             deployStep: helmDeployStep
             integrations:
               - name: kubernetes_integration
             inputSteps:
               - name: helmRoleSwitch
[/YAML]
##### How it Works
When you use the **HelmBlueGreenCleanup** native step in a pipeline, it performs the following functions in the background:
  * restore_pipeline_files (restores referenced HelmBlueGreenDeploy step configuration)
  * kubectl get service (fetch Live Services to read annotations and discover current Idle environment)
  * helm status (check if Idle release exists)
  * helm uninstall (if Idle release is present)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
84,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a helmbluegreendeploy?[/INST] The **HelmBlueGreenDeploy** step implements a Blue/Green strategy to deploy a Docker image to a Kubernetes cluster using a Helm chart.
The Helm chart used to deploy the image through this native step can be either:
  * A HelmChart resource that identifies a Helm chart stored in an Artifactory repository by HelmPublish.
  * A Helm chart in a source code repository identified by a GitRepo resource.


When properly configured, the HelmBlueGreenDeploy step also performs a replace_envs operation on all files specified under the `blueValueFilePaths` or `greenValueFilePaths` tag to replace all environment variables in those files with their values. This enables the step to derive information from a resource, such as the image name and tag from an Image, and use it to perform the deployment.
The HelmBlueGreenDeploy step uses the `helm upgrade` command to perform the deployment.
##### Blue/Green Deployment Strategy
Blue/Green deployment is a strategy for releasing new software that uses two production environments, as identical as possible, where they take turns in serving user requests and performing final stages of testing.
The **HelmBlueGreenDeploy** step refers to those two environments as **blue** and **green**. Those environments can play either **Idle** or **Live** roles, depending on which environment the users' requests are being forwarded to. The environment that is currently handling user requests is considered Live while the other one is considered Idle. To rollout a new release, it is first deployed to the environment playing the Idle role. There we can validate the new version to check if it is good to become available to users. If that is true, the environment roles can be flipped so that the previously Idle environment becomes Live and starts handling user requests, while the previously Live environment becomes Idle. One of the main advantages of this strategy is that rolling back new releases is as easy as flipping the environments roles again.
Both blue and green environments are represented in the context of Helm as two individual releases that co-exist in the same namespace. The **HelmBlueGreenDeploy** step assign Idle or Live roles to those releases by creating copies of their public Services dedicated to each role. By doing that we can easily change the release's role by updating those Services to be a copy of the Services from a specific environment. Those role Services created by Pipelines can be used to provision additional entrypoint components like DNS Records, Ingress Rules or Service Meshes.
To implement the complete Blue/Green deployment strategy workflow, Pipelines provides three native steps:
  *  **HelmBlueGreenDeploy** : From runtime, this step discovers which release is playing Idle role, deploys the chart to it and creates or updates Idle role Services.
  *  **elmBlueGreenRoleSwitch** : From runtime, this step discovers which release is playing each role and flips them by creating or updating the role Services.
  *  **HelmBlueGreenCleanup** : From runtime, this step discovers which release is playing Idle role and uninstalls it.


##### YAML Schema
The YAML schema for HelmBlueGreenDeploy native step is as follows:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: HelmBlueGreenDeploy
            configuration:
              #inherits all the tags from bash
              helmVersion:                  <2 | 3>         # optional, defaults to 3
              namespace:                    <string>
              blueReleaseName:              <string>
              blueFlags:                    <string>        # optional
              blueValueFilePaths:                           # optional
                - <path and filename>          
              greenReleaseName:             <string>
              greenFlags:                   <string>        # optional
              greenValueFilePaths:                      # optional
                - <path and filename>          
              roleServices:
                - liveName:             <string>
                  liveCustomProperties: <string>        # optional
                  idleName:             <string>
                  idleCustomProperties: <string>        # optional
                  blueReferenceName:    <string>
                              greenReferenceName:   <string>              
              chartPath:                    <path string>   # optional, may be required for certain input resources
                      chartName:                <path string>   # optional, may be required for certain input resources
                      chartVersion:             <path string>   # optional, may be required for certain input resources
              dryRun:                   <true | false>  # optional, only deploys if the dryrun is success
              lint:                     <true | false>  # lints chart before upgrade, default false
              lintFlags: “--strict”
              test:                     <true | false>  # runs helm test after upgrade, default false
              testFlags: “--parallel”
    
              integrations:
                - name:         <kubernetes integration>                         # required
              inputResources:
                - name:         <HelmChart, GitRepo, FileSpec or BuildInfo resource>        # required, only one input resource is allowed
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `HelmBlueGreenDeploy` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a Kubernetes Integration.
and
Must specify an  Artifactory Integration when `ReleaseBundle `is the `inputResource`for the step.
Required
May be required  
`inputResources`
Must specify HelmChart, GitRepo, FileSpec, BuildInfo, or ReleaseBundle resource that contains a Helm chart.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`helmVersion`
A number representing the major version of Helm to use. Can be either 2 or 3. Defaults to 3.
Optional  
`namespace`
Target kubernetes namespace for the deployment.
Required  
`blueReleaseName`
The release name for the blue environment. Equivalent to the --name (-n) option of the helm install command.
Required  
`blueFlags`
A flag string to be included in the Helm command when deploying the blue release. For example: `--set key=’value’ -f myTestValues.yaml`
Optional  
`blueValueFilePaths`
Specifies values YAML file(s) for use with a `--values (-f)` option of the helm install command when deploying the blue release.
All environment variables referenced in the specified file(s) are automatically replaced with the values of matching runtime environment variables.
Optional  
`greenReleaseName`
The release name for the green environment. Equivalent to the `--name (-n)` option of the helm install command.
Required  
`greenFlags`
A flag string to be included in the Helm command when deploying the green release. For example: `--set key=’value’ -f myTestValues.yaml`
Optional  
`greenValueFilePaths`
Specifies values YAML file(s) for use with a `--values (-f)` option of the helm install command when deploying the green release.
All environment variable referenced in the specified file(s) are automatically replaced with values of matching runtime environment variables.
Optional  
`roleServices`
List of services that should be created by Pipelines to assign Idle and Live roles to the releases.
Tag
Description of usage
Required/Optional  
`liveName`
Name of the Service that points to the environment playing Live role.
Required  
`liveCustomProperties`
`jq` expression to be executed on the Live Service after the content is copied from the reference Service and before applying it to the kubernetes cluster. Can be used to customize the Live Service content. For instance, to assign a custom label use the following expression: `"".metadata.labels.my-label = my-value"".`
Optional  
`idleName`
Name of the Service that points to the environment playing Idle role.
Required  
`idleCustomProperties`
`jq` expression to be executed on this Idle Service after the content is copied from the reference Service and before applying it to the kubernetes cluster. Can be used to customize the Idle Service content. For instance, to assign a custom label use the following expression: `"". metadata.labels.my-label = my-value"".`
Optional  
`blueReferenceName`
Name of the Service deployed by the blue release that will be used as reference when creating or updating this role Service. When blue is playing Live role, `liveName` will be a copy of this Service. If blue is playing Idle role, `idleName` will be a copy of this Service.
Required  
`greenReferenceName`
Name of the Service deployed by the green release that will be used as reference when creating or updating this role Service. When green is playing Live role, `liveName` will be a copy of this Service. If green is playing Idle role, `idleName` will be a copy of this Service.  
Required  
`chartPath`
The path to the Helm chart in the `GitRepo/FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart resource is specified.
May be required  
`chartName`
The name of the Helm chart in the `FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart/GitRepo resource is specified or when your input resource is not an Artifactory Helm repository.
May be required  
`chartVersion`
The name of the Helm chart in the `FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart/GitRepo resource is specified or when your input resource is not an Artifactory Helm repository.
May be required  
`dryRun`
When **true** , only deploys if the --dry-run install simulation is successful.
Defaults to `false`.
Optional  
`lint`
When set to **true** , performs a lint to examine a chart for possible issues.
Defaults to `false`.
Optional  
`lintFlags`
Flag string to pass to the `helm lint` command.
Optional  
`test`
When set to **true** , performs a test to run the tests for release.
Defaults to `false`.
Optional  
`testFlags`
Flag string to pass to the `helm test` command.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a HelmBlueGreenDeploy step.
###### Helm 3 and HelmChart Resource Input
A HelmBlueGreenDeploy step using Helm 3 and a HelmChart resource input.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
    
[/YAML]
###### Dry Run
A HelmBlueGreenDeploy step using Helm 3 and a HelmChart resource input with values files and a dry run before the deployment.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              greenReleaseName: app-green
              greenValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              dryRun: true          
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
    
[/YAML]
###### GitRepo Input
A HelmBlueGreenDeploy step with a GitRepo input instead of a HelmChart, lint and test commands.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueFlags: ""--set key=value""
              greenReleaseName: app-green
              greenFlags: ""--set key=value""
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: gitRepoResource
    
[/YAML]
###### FileSpec Input
A HelmBlueGreenDeploy step with a FileSpec input resource. When using the FileSpec input resource, use the `chartPath` property to specify where the Helm chart file is in the FileSpec resource. If the FileSpec represents a Generic repository in Artifactory that contains the Helm chart, use the following configuration as a reference:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""          
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: fileSpecResource
[/YAML]
If the FileSpec input resource represents a Helm repository in Artifactory, you will also need to provide the `chartName`and`chartVersion`configuration properties:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""
                      chartName: ""myChartName""
                      chartVersion: 0.0.1          
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: fileSpecResource
[/YAML]
###### BuildInfo Input
A HelmBlueGreenDeploy step with a BuildInfo input resource. When using the BuildInfo input resource, use the `chartPath` property to specify where the Helm chart file is in the BuildInfo resource. If the BuildInfo represents a Generic repository in Artifactory that contains the Helm chart, use the following configuration as a reference:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              greenReleaseName: app-green
              greenValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""          
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""          
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: buildInfoResource
[/YAML]
If the BuildInfo input resource represents a Helm repository in Artifactory, you will also need to provide the `chartName` and `chartVersion`configuration properties:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              greenReleaseName: app-green
              greenValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""
                      chartName: ""myChartName""
                      chartVersion: 0.0.1
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""          
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: buildInfoResource
[/YAML]
###### ReleaseBundle Input
A HelmBlueGreenDeploy step with a ReleaseBundle input resource. When using the ReleaseBundle input resource you will need to specify where the Helm chart file is in the ReleaseBundle resource using the `chartPath` property.
If the ReleaseBundle represents a Generic repository in Artifactory that contains the Helm chart, please use the following configuration as a reference:
### Artifactory integration is required
You must specify an Artifactory Integration when `ReleaseBundle `is the `inputResource` to `HelmBlueGreenDeploy` step. This integration will point to the Artifactory that will be the source to download the ReleaseBundle that will be used in this step.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines:
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration:
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              greenReleaseName: app-green
              greenValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""          
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              integrations:
                - name: kubernetes_integration
                - name: artifactory_integration
              inputResources:
                - name: releaseBundleResource
[/YAML]
If the ReleaseBundle input resource represents a Helm repository in Artifactory, you will also have to specify `chartName` and `chartVersion` configuration properties:
 **HelmBlueGreenDeploy**
[YAML]
    pipelines:
      - name: helmDeployPipeline   
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration:
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              blueValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              greenReleaseName: app-green
              greenValueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              chartPath: ""./myChart""
              chartName: ""myChartName""
              chartVersion: 0.0.1          
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              integrations:
                - name: kubernetes_integration
                            - name: artifactory_integration 
              inputResources:
                - name: releaseBundleResource
[/YAML]
###### Customizing Role Services
A HelmBlueGreenDeploy step with two custom labels assigned to a role Service.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  idleCustomProperties: "".metadata.labels.mylabel = \""value\"" | .metadata.labels.role = \""idle\""""
                  liveName: app-service
                  liveCustomProperties: "".metadata.labels.mylabel = \""value\"" | .metadata.labels.role = \""live\""""
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
[/YAML]
###### Multiple Public Services
A HelmBlueGreenDeploy step that deploys a Helm Chart that exposes two Services.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service              
                  liveName: app-service
                - blueReferenceName: app-blue-admin-service
                  greenReferenceName: app-green-admin-service 
                  idleName: app-idle-admin-service              
                  liveName: app-admin-service
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
[/YAML]
###### Helm 2
A HelmBlueGreenDeploy step using Helm 2.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 2
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service          
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
[/YAML]
##### How it Works
When you use the **HelmBlueGreenDeploy** native step in a pipeline, it performs the following functions in the background:
  * helm init --client-only (if the Helm version is 2)
  * jfrog rt config (if there is a HelmChart resource input, to configure the JFrog CLI with the Artifactory credentials)
  * jfrog rt use (if there is a HelmChart resource input, to set the current default Artifactory configuration)
  * kubectl get service (fetch Live Services to read annotations and discover current Idle environment)
  * helm fetch (if there is a HelmChart resource input and the Helm version is 2, to fetch the chart)
  * helm pull (if there is a HelmChart resource input and the Helm version is 3, to fetch the chart)
  * replace_envs (if there are valueFilePaths, to replace variable placeholders)
  * helm lint (if lint is true)
  * helm --dry-run (if dryRun is true)
  * helm (deploy)
  * kubectl get service (fetch reference Services deployed by helm to create or update role Services)
  * Cleanup and update role Services definition (uses `jq` on Bash nodes and `jq` and `ConvertFrom-Json` on PowerShell nodes)
  * kubectl apply (create or update role Services)
  * add_pipeline_files (saves HelmBlueGreenDeploy configuration to be used by HelmBlueGreenRoleSwitch and HelmBlueGreenCleanup steps later)
  * helm test (if test is true)


##### Pipelines Managed Services
The role Services managed by Pipelines can be identified by the following labels and annotations metadata:
Name
Type
Value  
app.kubernetes.io/managed-by
Label
""Pipelines""  
pipelines.jfrog.com/pipeline
Annotation
Pipeline name  
pipelines.jfrog.com/deploy-step
Annotation
Step name  
pipelines.jfrog.com/run
Annotation
Number of the last run to update the Service  
pipelines.jfrog.com/reference
Annotation
Reference Service name  
pipelines.jfrog.com/environment
Annotation
Environment currently playing that role  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
85,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a helmbluegreenroleswitch?[/INST] The **HelmBlueGreenRoleSwitch** step flips the roles played by the Helm releases deployed by a HelmBlueGreenDeploy step.
This role switch can be used to either make a new release available to users or to roll back a previously published release.
##### Blue/Green Deployment Strategy
Blue/Green deployment is a strategy of releasing new software that uses two production environments, as identical as possible, that take turns on serving users requests while the other one is used to perform the final stage of testing.
The **HelmBlueGreenDeploy** step refers to those two environments as **blue** and **green**. Those environments can be either playing **Idle** or **Live** roles, depending on which environment the users' requests are being forwarded to. The environment that is currently handling user requests is considered Live while the other one is considered Idle. To rollout a new release, we first deploy it to the environment playing the Idle role. There we can validate the new version to check if it is good to become available to users. If that is true, we can flip the environment roles so the previously Idle environment becomes Live and starts to handle user requests, while the previously Live environment goes Idle. One of the main advantages of this strategy is that rolling back new releases is as easy as flipping the environments roles again.
Both blue and green environments are represented in the context of Helm as two individual releases that co-exist in the same namespace. The **HelmBlueGreenDeploy** step assign Idle or Live roles to those releases by creating copies of their public Services dedicated to each role. By doing that we can easily change the releases role by updating those Services to be a copy of the Services from a specific environment. Those role Services created by Pipelines can be used to provision additional entrypoint components like DNS Records, Ingress Rules or Service Meshes.
To implement the complete Blue/Green deployment strategy workflow, Pipelines provides three native steps:
  *  **HelmBlueGreenDeploy** discovers from runtime which release is playing Idle role, deploys the chart to it and creates or updates Idle role Services.
  *  **HelmBlueGreenRoleSwitch** discovers from runtime which release is playing each role and flips them by creating or updating the role Services.
  *  **HelmBlueGreenCleanup** discovers from runtime which release is playing Idle role and uninstalls it.


##### YAML Schema
The YAML schema for HelmBlueGreenRoleSwitch native step is as follows:
 **HelmBlueGreenRoleSwitch**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: HelmBlueGreenRoleSwitch
            configuration:
              #inherits all the tags from bash
              deployStep:       <string>
              
              integrations:
                - name:         <kubernetes integration>                         # required          
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `HelmBlueGreenRoleSwitch` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a Kubernetes Integration.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`deployStep`
HelmBlueGreenDeploy step name that deployed the Helm Chart and where the blue/green strategy was configured. The referenced deploy step must be declared in the same Pipeline.
Required  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a HelmBlueGreenRoleSwitch step.
 **Example 1**
This pipeline shows how to deploy a Helm chart using the blue/green strategy and promote it to the Live role.
 **HelmBlueGreenDeploy**
[YAML]
    pipelines: 
      - name: helmBlueGreenDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmBlueGreenDeploy
            configuration: 
              helmVersion: 3
              namespace: app-namespace
              blueReleaseName: app-blue
              greenReleaseName: app-green
              roleServices:
                - blueReferenceName: app-blue-service
                  greenReferenceName: app-green-service 
                  idleName: app-idle-service
                  liveName: app-service
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
         
         - name: helmRoleSwitch
           type: HelmBlueGreenRoleSwitch
           configuration: 
             deployStep: helmDeployStep
             integrations:
               - name: kubernetes_integration
             inputSteps:
               - name: helmDeployStep   
[/YAML]
##### How it Works
When you use the **HelmBlueGreenRoleSwitch** native step in a pipeline, it performs the following functions in the background:
  * restore_pipeline_files (restores referenced HelmBlueGreenDeploy step configuration)
  * kubectl get service (fetch Live Services to read annotations and discover current role played by the environments)
  * kubectl get service (fetch reference Services deployed by helm to create or update role Services)
  * Cleanup and update role Services definition (uses `jq` on Bash nodes and `jq` and `ConvertFrom-Json` on PowerShell nodes)
  * kubectl apply (create or update role Services)


##### Pipelines Managed Services
The role Services managed by Pipelines can be identified by the following labels and annotations metadata:
Name
Type
Value  
app.kubernetes.io/managed-by
Label
""Pipelines""  
pipelines.jfrog.com/pipeline
Annotation
Pipeline name  
pipelines.jfrog.com/deploy-step
Annotation
Step name  
pipelines.jfrog.com/run
Annotation
Number of the last run to update the Service  
pipelines.jfrog.com/reference
Annotation
Reference Service name  
pipelines.jfrog.com/environment
Annotation
Environment currently playing that role  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
86,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a helmdeploy?[/INST] The **HelmDeploy** step deploys a Docker image to a Kubernetes cluster using a Helm chart.
The Helm chart used to deploy the image through this native step can be either:
  * A HelmChart resource that identifies a Helm chart stored in an Artifactory repository by HelmPublish.
  * A Helm chart in a source code repository identified by a GitRepo resource.


When properly configured, the HelmDeploy step also performs a replace_envs operation on all files specified under the `valueFilePaths` tag to replace all environment variables in those files with their values. This enables the step to derive information from a resource, such as the image name and tag from an Image, and use it to perform the deployment.
The HelmDeploy step uses the `helm upgrade` command to perform the deployment.
##### YAML Schema
The YAML schema for HelmDeploy native step is as follows:
 **HelmDeploy**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: HelmDeploy
            configuration:
              #inherits all the tags from bash
              helmVersion:         <2 | 3>          # optional, defaults to 2
              namespace:               <string>     # optional, namespace to which to deploy
              flags:               <string>         # optional
              valueFilePaths:                       # optional
                - <path and filename>
              releaseName:         <string> 
              chartPath:           <path string>    # optional, may be required for certain input resources
                      chartName:           <path string>    # optional, may be required for certain input resources
                      chartVersion:        <path string>    # optional, may be required for certain input resources
              dryRun:              <true | false>   # optional, only deploys if the dryrun is success
              lint:                <true | false>   # lints chart before upgrade, default false
              lintFlags: “--strict”
              test:                <true | false>   # runs helm test after upgrade, default false
              testFlags: “--parallel”
    
              integrations:
                - name:         <kubernetes integration>                         # required            
              inputResources:
                - name:         <HelmChart, GitRepo, FileSpec, BuildInfo or ReleaseBundle resource>    # required, one input resource from this list is required
                                                                                                                                                                                               # HelmChart and GitRepo input resources are allowed together       
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `HelmDeploy `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a Kubernetes Integration.
or
Must specify an  Artifactory Integration when `ReleaseBundle `is the `inputResource` to the step.
Required
May be required  
`inputResources`
Must specify HelmChart, GitRepo, FileSpec , BuildInfo or ReleaseBundle resource that contains a Helm chart.
Can also specify a second input resource of type GitRepo containing  Helm values files, if HelmChart was provided as the other input resource.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`helmVersion`
A number representing the major version of Helm to use. Can be 2 or 3. Defaults to 2.
Optional  
`namespace`
The namespace to which to deploy. This will be added to the deploy command as a `--namespace` parameter.
Optional  
`flags`
A flag string to be included in the Helm command. For example: ""--set key=’value’ -f myTestValues.yaml""
Optional  
`valueFilePaths`
Specifies values YAML file(s) for use with a--values (-f) option of the helm install command.
All environment variable referenced in the specified file(s) are automatically replaced with values of matching runtime environment variables.
Optional  
`releaseName`
The release name. Equivalent to the --name (-n) option of the helm install command.
Required  
`chartPath`
The path to the Helm chart in the `GitRepo/FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart resource is specified.
May be required  
`chartName`
The name of the Helm chart in the `FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart/GitRepo resource is specified or when your input resource is not an Artifactory Helm repository.
May be required  
`chartVersion`
The name of the Helm chart in the `FileSpec/BuildInfo/ReleaseBundle `resource specified in `inputResources`.
### Note
Do not use when a HelmChart/GitRepo resource is specified or when your input resource is not an Artifactory Helm repository.
May be required  
`dryRun`
When **true** , only deploys if the --dry-run install simulation is successful.
Defaults to `false`.
Optional  
`lint`
When set to **true** , performs a lint to examine a chart for possible issues.
Defaults to `false`.
Optional  
`lintFlags`
Flag string to pass to the `helm lint` command.
Optional  
`test`
When set to **true** , performs a test to run the tests for release.
Defaults to `false`.
Optional  
`testFlags`
Flag string to pass to the `helm test` command.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a HelmDeploy step.
###### Helm 3 and HelmChart Resource Input
A HelmDeploy step using Helm 3 and a HelmChart resource input.
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              releaseName: myHelmRelease
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
    
[/YAML]
###### Dry Run
A HelmDeploy step using Helm 3 and a HelmChart resource input with values files and a dry run before the deployment.
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              releaseName: myHelmRelease
              dryRun: true
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
    
[/YAML]
###### GitRepo Input
A HelmDeploy step with a GitRepo input instead of a HelmChart and lint and test commands.
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: gitRepoResource
    
[/YAML]
###### FileSpec Input
A HelmDeploy step with a FileSpec input resource. When using the FileSpec input resource, use the `chartPath` property to specify where the Helm chart file is in the FileSpec resource. If the FileSpec represents a Generic repository in Artifactory that contains the Helm chart, use the following configuration as a reference:
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: fileSpecResource
[/YAML]
If the FileSpec input resource represents a Helm repository in Artifactory, you will also need toprovide the `chartName`and`chartVersion` configuration properties:
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
                      chartName: ""myChartName""
                      chartVersion: 0.0.1
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: fileSpecResource
[/YAML]
###### BuildInfo Input
A HelmDeploy step with a BuildInfo input resource. When using the BuildInfo input resource, use the `chartPath` property to specify where the Helm chart file is in the BuildInfo resource. If the BuildInfo represents a Generic repository in Artifactory that contains the Helm chart, use the following configuration as a reference:
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: buildInfoResource
[/YAML]
If the BuildInfo input resource represents a Helm repository in Artifactory, you will also need toprovide the `chartName` and `chartVersion` configuration properties:
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
                      chartName: ""myChartName""
                      chartVersion: 0.0.1
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: buildInfoResource
[/YAML]
###### ReleaseBundle Input
A HelmDeploy step with a ReleaseBundle input resource. When using the ReleaseBundle input resource you will need to specify where the Helm chart file is in the ReleaseBundle resource using the _chartPath_ property.
If the ReleaseBundle represents a Generic repository in Artifactory that contains the Helm chart, please use the following configuration as a reference:
### Artifactory integration is required
You must specify an Artifactory Integration when `ReleaseBundle `is the `inputResource` to `HelmDeploy` step. This integration will point to the Artifacoty that will be the source to download the ReleaseBundle that will be used in this step.
 **HelmDeploy**
[YAML]
    pipelines:
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration:
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
                - name: artifactory_integration
              inputResources:
                - name: releaseBundleResource
[/YAML]
If the ReleaseBundle input resource represents a Helm repository in Artifactory, you will also have to specify _chartName_ and _chartVersion_ configuration properties:
 **HelmDeploy**
[YAML]
    pipelines:
      - name: helmDeployPipeline   
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration:
              helmVersion: 3
              namespace: my-namespace
              chartPath: ""./myChart""
              chartName: ""myChartName""
              chartVersion: 0.0.1
              releaseName: myHelmRelease
              lint: true
              lintFlags: ""--strict""
              test: true
              testFlags: ""--parallel""
              valueFilePaths:
                - values/values1.yaml
                - values/values2.yaml
              integrations:
                - name: kubernetes_integration
                            - name: artifactory_integration 
              inputResources:
                - name: releaseBundleResource
[/YAML]
###### Helm 2
A HelmDeploy step using Helm 2.
 **HelmDeploy**
[YAML]
    pipelines: 
      - name: helmDeployPipeline
        steps:
          - name: helmDeployStep
            type: HelmDeploy
            configuration: 
              helmVersion: 2
              namespace: my-namespace
              releaseName: myHelmRelease
              integrations:
                - name: kubernetes_integration
              inputResources:
                - name: helmChartResource
[/YAML]
##### How it Works
When you use the **HelmDeploy** native step in a pipeline, it performs the following functions in the background:
  * helm init --client-only (if the Helm version is 2)
  * jfrog rt config (if there is a HelmChart resource input, to configure the JFrog CLI with the Artifactory credentials)
  * jfrog rt use (if there is a HelmChart resource input, to set the current default Artifactory configuration)
  * helm fetch (if there is a HelmChart resource input and the Helm version is 2, to fetch the chart)
  * helm pull (if there is a HelmChart resource input and the Helm version is 3, to fetch the chart)
  * replace_envs (if there are valueFilePaths, to replace variable placeholders)
  * helm lint (if lint is true)
  * helm --dry-run (if dryRun is true)
  * helm (deploy)
  * helm test (if test is true)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
87,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a helmpublish?[/INST] The **HelmPublish** step publishes a Helm chart and associated build info from a location in a Git repo to a Helm repository in Artifactory.
This step uses the `helm package` command to publish the collection of Helm files in the GitRepo to Artifactory.
The step can optionally publish build info with the Helm chart to Artifactory using the `autoPublishBuildInfo` flag.
Once the Helm chart is published to Artifactory, it can be deployed to a Kubernetes cluster using HelmDeploy.
##### YAML Schema
The YAML schema for HelmPublish native step is as follows:
 **HelmPublish**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: HelmPublish
            configuration:
              #inherits all the tags from bash
              helmVersion:       <2 | 3>                               # optional, defaults to 2
              chartPath:         <path string>
              flags:             <option string>                       # optional
              lint:              <true | false>                        # optional, default false
              lintFlags:         <string>                              # optional
              valueFilePaths:                                          # optional
                - <path and filename>
              autoPublishBuildInfo: <true | false>                     # default false          
              inputResources:
                - name:         <GitRepo resource>                     # required
              outputResources:
                - name:         <HelmChart resource>                   # required
                - name:         <BuildInfo resource>                   # optional
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `HelmPublish `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify a GitRepo resource that contains the Helm chart.
The step will package the chart into a versioned chart archive file.
Required  
`outputResources`
Must specify a HelmChart resource to update. The `version` property of the HelmChart is automatically set to the new chart version.
Must specify a BuildInfo resource when `autoPublishBuildInfo` is set to true. If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number.`
Required
May be required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`helmVersion`
A number representing the major version of Helm to use. Can be 2 or 3. Defaults to 2.
Optional  
`chartPath`
The path to the Helm chart in the GitRepo
Required  
`flags`
Command line options to pass to the helm package command.
Optional  
`lint`
When true, performs a lint test to verify that the chart is well formed.
Defaults to false.
Optional  
`lintFlags`
Options string to pass to the `helm lint`command.
Optional  
`valueFilePaths`
Specifies values YAML file(s) that will be used in the lint command  helm lint command.
All environment variable references in the specified file(s) will be automatically replaced with values of matching runtime environment variables.
Optional  
`autoPublishBuildInfo`
When true, publishes build info with the Helm chart.
Default is false.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a HelmPublish step.
###### Publish to Artifactory using Helm 3
A HelmPublish step using Helm 3 to publish to Artifactory.
 **HelmPublish**
[YAML]
    pipelines: 
      - name: helmPublishPipeline
        steps:
          - name: helmPublishStep
            type: HelmPublish
            configuration: 
              helmVersion: 3
              chartPath: ""./myChart""
              inputResources:
                - name: gitRepoResource
              outputResources:
                - name: helmChartResource
    
[/YAML]
###### Using lint and values.yaml
A HelmPublish step with lint, using a values.yaml file, added before publishing the Helm chart.
 **HelmPublish**
[YAML]
    pipelines: 
      - name: helmPublishPipeline
        steps:
          - name: helmPublishStep
            type: HelmPublish
            configuration: 
              helmVersion: 3
              chartPath: ""./myChart""
              lint: true
              lintFlags: ""--strict""
              valueFilePaths:
                - values.yaml
              inputResources:
                - name: gitRepoResource
              outputResources:
                - name: helmChartResource
    
[/YAML]
###### Publish Build Info
A HelmPublish step that publishes build info.
 **HelmPublish**
[YAML]
    pipelines: 
      - name: helmPublishPipeline
        steps:
          - name: helmPublishStep
            type: HelmPublish
            configuration: 
              helmVersion: 3
              chartPath: ""./myChart""
              autoPublishBuildInfo: true
              inputResources:
                - name: gitRepoResource
              outputResources:
                - name: helmChartResource
                - name: buildInfoResource
    
[/YAML]
###### Publish to Artifactory using Helm 2
A HelmPublish step using Helm 2 to publish to Artifactory.
 **HelmPublish**
[YAML]
    pipelines: 
      - name: helmPublishPipeline
        steps:
          - name: helmPublishStep
            type: HelmPublish
            configuration: 
              helmVersion: 2
              chartPath: ""./myChart""
              inputResources:
                - name: gitRepoResource
              outputResources:
                - name: helmChartResource
    
[/YAML]
##### How it Works
When you use the **HelmPublish** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (to configure the JFrog CLI with the Artifactory credentials)
  * jfrog rt use (to set the current default Artifactory configuration)
  * helm init --client-only (if the Helm version is 2)
  * replace_envs (if lint is true and there are valueFilePaths, to replace variable placeholders)
  * helm lint (if lint is true)
  * helm package
  * jfrog rt upload (upload the Helm chart)
  * write_output (to update the version in the output HelmChart resource)
  * jfrog rt build-collect-env (if autoPublishBuildInfo is true, collect environment variables)
  * jfrog rt build-publish (if autoPublishBuildInfo is true, publish the build info)
  * write_output (if autoPublishBuildInfo is true, to update the version in the output BuildInfo resource)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
88,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a jenkins?[/INST] The **Jenkins** native step transfers execution to a Jenkins pipeline. For more information, see Using Jenkins With Pipelines.
##### YAML Schema
The YAML schema for Jenkins native step is as follows:
 **Jenkins**
[YAML]
    pipelines:
      - name: <string>
        steps:
          - name: <string>
            type: Jenkins
            configuration:
                      #inherits all the tags from bash
              jenkinsJobName: <string>            # Jenkins job/pipeline name
              jenkinsBranchName: <string>         # required for Jenkins Multibranch
              buildParameters:
                key1: <string>
                key2: <string>   
              integrations:
                - name: <jenkins integration>    # required Jenkins integration
              outputResources:                   # optional
                - name: <resource name>          
    
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `Jenkins` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash step configuration tags, with these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a named Jenkins Integration.
Required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`jenkinsJobName`
The name of the Jenkins job/pipeline
Required  
### Important
When adding the `jenkinsJobName`, do not include the `job` keyword. The only exception is when `job` is the name of a folder.
  *  **Correct** : `jenkinsJobName: infra/test/frontend-trigger-env`
  *  **Incorrect** : `jenkinsJobName: infra/job/test/job/frontend-trigger-env`

  
`jenkinsBranchName`
Required field when the Jenkins Job is configured for multibranch. This tells pipelines which branch to execute.
Optional  
`buildParameters`
Assigns any build parameters and their strings in key:value format. These parameters will be passed to the Jenkins job.
Optional  
###### execution
Execution takes place within the Jenkins job only. This section is not supported for this type of step.
##### Examples
The following examples show how to configure a Jenkins step for a few different Jenkins jobs.
###### Freestyle Jenkins Project or Pipeline
A Jenkins step for a freestyle Jenkins project or Pipeline. Note that this example will not work with Jenkins Multibranch Pipelines, see the next example for those.
This example uses `timeoutSeconds` Bash tag. For more information about other Bash tags that can be used, see Bash.
 **Jenkins**
[YAML]
    pipelines: 
      - name: jenkinsPipeline
        steps:
          - name: jenkinsStep
            type: Jenkins
            configuration: 
                      timeoutSeconds: 30 # uses Bash tag to set time limit, in seconds, for the step to complete
              jenkinsJobName: myJenkinsJob
              integrations:
                - name: jenkins_integration
    
[/YAML]
###### Jenkins Multibranch Pipeline
A Jenkins step for a Jenkins Multibranch Pipeline.
 **Jenkins**
[YAML]
    pipelines: 
      - name: jenkinsPipeline
        steps:
          - name: jenkinsStep
            type: Jenkins
            configuration: 
              jenkinsJobName: myJenkinsMultibranchJob
              jenkinsBranchName: myBranchName
              integrations:
                - name: jenkins_integration
    
[/YAML]
###### Trigger Jenkins Jobs using an Input Image Resource
A Jenkins step providing an image name and tag and run number as build parameters to trigger Jenkins jobs using an input Image resource.
 **Jenkins**
[YAML]
    pipelines: 
      - name: jenkinsPipeline
        steps:
          - name: jenkinsStep
            type: Jenkins
            configuration: 
              jenkinsJobName: myJenkinsMultibranchJob
              jenkinsBranchName: myBranchName
              buildParameters:
                imageName: ""${res_myImageResource_imageName}""
                imageTag: ""${res_myImageResource_imageTag}""
                jfrogPipelineRun: ""${run_number}""
              inputResources:
                - name: myImageResource
              integrations:
                - name: jenkins_integration
    
[/YAML]
###### Two Jenkins Jobs using Priority Tag
Two Jenkins jobs that use the `Priority` Bash tag. For more information about other Bash tags that can be used, see Bash.
[YAML]
    pipelines:
      - name: pipelines_Jen
        steps:
          - name: S_B_Jen_0022_1
            type: Jenkins
            configuration:
              jenkinsJobName: var_output_job    
              priority: 1 # bash tag
              buildParameters:
                key1: 1
                key2: 2
              integrations:
                - name: i_testJenkins
    
          - name: S_B_Jen_0022_2
            type: Jenkins
            configuration:
              jenkinsJobName: var_output_job   
              priority: 2 @ bash tag
              buildParameters:
                key1: 1
                key2: 2
              integrations:
                - name: i_testJenkins
              inputSteps:
                - name : S_B_Jen_0022_1
[/YAML]
##### Related Topics
Using Jenkins With Pipelines

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
89,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a linuxvmdeploy?[/INST] The **LinuxVMDeploy** native step uploads files to VMs in a VmCluster resource and runs commands on the VMs. If all deployments are successful, a copy of the deploy artifacts are stored in the `${HOME}/${step_name}/rollback` on each VM. The file(s) may be provided in a FileSpec, BuildInfo or ReleaseBundle.
##### YAML Schema
The YAML schema for LinuxVMDeploy native step is as follows:
 **LinuxVMDeploy**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: myLinuxVMDeployStep
            type: LinuxVMDeploy
            configuration:
              environmentVariables:
                dry_run:                 <string> # optional
                            deploy_targets_override: <string> # optional
              targetDirectory:           <string> # optional
              rolloutDelaySecs:          <integer> # optional
              fastFail:                  <boolean> # optional
              sshUser:                   <string> # required
              vmEnvironmentVariables:    <string[]> # optional
              strategy:                  <""rollingUpdate""|""blueGreen""> # optional; default ""rollingUpdate""
              inputResources: 
                - name:                  <VMCluster resource> # 1 required (exacly 1, exactly 2 for blueGreen strategy).
                - name:                  <BuildInfo resource|FileSpec resource|ReleaseBundle resource> # required (exactly 1).
              scripts:
                - name:                  <string> # required
                  context:               <""buildNode""|""targetCluster""|""currentCluster""> # required, ""currentCluster"" supported for blueGreen only.
                  commands:              <string[]> # optional
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `LinuxVMDeploy` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify a VmCluster resource.
Required  
`inputResources`
Must specify a FileSpec, BuildInfo or ReleaseBundle resource containing the file(s) to be uploaded. Exactly one must be specified.
Required  
In addition, these tags can be defined to support the step's native operation:
Tag
 **Description of usage**
Required/Optional  
`sshUser`
The ssh user used to connect to target VMs.
Required  
`targetDirectory`
Path of the directory where the deploy artifacts will be uploaded. The directory will be created if it does not exist.
Default directory is `<sshUser's $HOME>/$step_name/$run_id`.
Optional  
`vmEnvironmentVariables`
These variables will be exported on target VMs.
Optional  
`rolloutDelaySecs`
Time in seconds to wait between deploys.
Optional  
`fastFail`
If `true`, the step will not deploy to additional VMs after the first failure.
Default is `true`.
Optional  
strategy
The release strategy to be used. It can be either `rollingUpdate` or `blueGreen`.
  * `rollingUpdate`: Deploy to each VM in one cluster, one after the other.
  * `blueGreen`: Deploy to one of two VmCluster resources, swapping clusters each time the LinuxVMDeploy step runs.


Optional  
`scripts`
User-defined commands to run on a given context.
  * `name:` name for the command.
  * `context:` where the script should run. `buildNode` runs on the build node. `targetCluster` runs on the supplied vmCluster.


``
Required  
###### environment variables
The following environment variables are available for user-defined scripts written in the LinuxVMDeploy step.
Name
 **Description of usage**  
`target_cluster `
The name of the VM cluster being deployed to.  
`current_cluster`
The name of the VM cluster that was last deployed to.
### Note
Supported for `blueGreen` strategy only.  
`current_command`
The name of the command that was running when the step exited. This should be blank if no user-defined command was running.  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show a few ways in which a LinuxVMDeploy step can be configured.
###### Uploading an App in a FileSpec Resource to VMs and Running It
The most basic form of LinuxVMDeploy. Uses all default values. The step will upload files to the default `targetDirectory` and run a deploy command.
 **LinuxVMDeploy**
[YAML]
    resources:
      - name: myApplication
        type: FileSpec
        configuration:
          sourceArtifactory: myArtifactory
          pattern: ""example-repo-local/myApp""
      - name: myVM
        type: VmCluster
        configuration:
          sshKey: mySSHKey
          targets:
            - 0.0.0.0
            - 1.1.1.1
    pipelines: 
      - name: linuxVMDeployPipeline
        steps:
          - name: linuxVMDeploy
            type: LinuxVMDeploy
            configuration:
              inputResources:
                - name: myVM
                - name: myApplication
              scripts:
                - name: deployCommand
                  context: targetCluster 
                  commands:
                    - ""./myApp""
    
[/YAML]
###### Uploading an App in a BuildInfo Resource to VMs, Running postDeploy command, and Handling Rollback.
Upload a BuildInfo resource and try to run `deployCommand` and `postDeploy` commands. If `deployCommand` succeeds, then postDeploy will run. If there is a failure on a VM (in case a command or `postDeploy` did not succeed), then `rollbackCommand` will run on any VM that deployed successfully.
 **LinuxVMDeploy**
[YAML]
    resources:
      - name: myApplication
        type: BuildInfo
        configuration:
          sourceArtifactory: myArtifactory
          buildName: myBuild
          buildNumber: 1
      - name: myVM
        type: VmCluster
        configuration:
          sshKey: mySSHKey
          targets:
            - 0.0.0.0
            - 1.1.1.1
    pipelines: 
      - name: linuxVMDeployPipeline
        steps:
          - name: linuxVMDeploy
            type: LinuxVMDeploy
            configuration:
              inputResources:
                - name: myVM
                - name: myApplication
              sshUser: myUser
                      scripts:
                - name: deployCommand
                  context: targetCluster 
                  commands:
                    - ""./myApp""
              onFailure:
                - 'if [ ""$current_command"" == ""deployCommand"" ]; then ./doRollback.sh fi'
    
[/YAML]
###### Overriding VmCluster Targets
In this example, a VmCluster resource is supplied alongside `deploy_targets_override` environment variable. Artifacts will be deployed to the targets specified in `deploy_targets_override`. The ssh keys from the VmCluster resource will still be used.
 **LinuxVMDeploy**
[YAML]
    resources:
      - name: myApplication
        type: FileSpec
        configuration:
          sourceArtifactory: myArtifactory
          pattern: ""example-repo-local/myApp""
      - name: myVM
        type: VmCluster
        configuration:
          sshKey: s_VM_DEPLOY_SSHKEY
          targets:
            - 0.0.0.0
            - 1.1.1.1
    pipelines:
      - name: linuxVMDeployPipeline
        steps:
          - name: linuxVMDeploy
            type: LinuxVMDeploy
            configuration:
              environmentVariables:
                            deploy_targets_override: ""123.456.78.90,127.0.0.1""
              inputResources:
                - name: myVM
                - name: myApplication
              sshUser: myUser
              scripts:
                - name: deployCommand
                  context: targetCluster 
                  commands:
                    - ""./myApp""
    
[/YAML]
###### Passing Environment Variables to Commands Run on the VM
In this example, a VmCluster resource is supplied with `vmEnvironmentVariables`. These variables will be available to user-defined commands running on the `targetCluster`.
 **LinuxVMDeploy**
[YAML]
    resources:
      - name: myApplication
        type: FileSpec
        configuration:
          sourceArtifactory: myArtifactory
          pattern: ""example-repo-local/myApp""
      - name: myVM
        type: VmCluster
        configuration:
          sshKey: s_VM_DEPLOY_SSHKEY
          targets:
            - 0.0.0.0
            - 1.1.1.1
    pipelines: 
      - name: linuxVMDeployPipeline
        steps:
          - name: linuxVMDeploy
            type: LinuxVMDeploy
            configuration:
              vmEnvironmentVariables:
                - ""LOG_LEVEL=DEBUG""
              inputResources:
                - name: myVM
                - name: myApplication
              sshUser: myUser
              scripts:
                - name: deployCommand
                  context: targetCluster 
                  commands:
                    - ""./myApp $LOG_LEVEL""
    
[/YAML]
###### Setting up a blueGreen Deploy Strategy
In this example, two VmCluster resources are supplied and a strategy of `blueGreen` is specified. Each time the step runs, `targetCluster` and `currentCluster` swap between the two supplied VmCluster resources.
 **LinuxVMDeploy**
[YAML]
    resources:
      - name: myApplication
        type: FileSpec
        configuration:
          sourceArtifactory: myArtifactory
          pattern: ""example-repo-local/myApp""
      - name: myVM
        type: VmCluster
        configuration:
          sshKey: s_VM_DEPLOY_SSHKEY
          targets:
            - 0.0.0.0
            - 1.1.1.1
    pipelines: 
      - name: linuxVMDeployPipeline
        steps:
          - name: linuxVMDeploy
            type: LinuxVMDeploy
            configuration:
              vmEnvironmentVariables:
                - ""LOG_LEVEL=DEBUG""
              inputResources:
                - name: myBlueCluster
                - name: myGreenCluster
                - name: myApplication
              sshUser: myUser
              strategy: blueGreen
              scripts:
                - name: deployCommand
                  context: targetCluster 
                  commands:
                    - ""./myApp $LOG_LEVEL""
                            - name: cleanupCommand
                  context: currentCluster 
                  commands:
                    - ""./stopApp""
                - name: swapCluster
                  context: buildNode
                  commands:
                    - ""./swapLoadBalancer $target_cluster""
    
    
[/YAML]
##### How it Works
When you use the **LinuxVMDeploy** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt download (download files from FileSpec, BuildInfo or ReleaseBundle)
  * tar (compress all files to be uploaded to VMs)
  * scp (upload compressed artifacts to VMs)
  * ssh""mkdir -p"" (create the targetDirectory if it does not exist)
  * Script commands run on target VMs via ssh or on the build node itself if context is ""buildNode"".
    * Commands run in the order they were written in the yaml.
    * Commands running on the target cluster will have access to variables defined in vmEnvironmentVariables.
    * A rollback archive is created on the targetCluster (only happens when entire step succeeds)
      * Rollback directory location: /home/$ssh_user/LinuxVMDeploy/rollback
      * mkdir -p (create rollback directory if it does not exist).
      * rm -rf $rollbackDirectory/* (remove any old rollback artifacts)
      * cp -r $targetDirectory/* $rollbackDirectory (copy all uploaded artifacts to rollback archive)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
90,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a matrix?[/INST] The **Matrix** step commences multiple parallel build processes across multiple containers, with different settings for each.
Common use cases for Matrix steps are:
  * Splitting a large test suite into smaller units, and executing them in parallel to reduce total execution time.
  * Testing against multiple values of environment variables or multiple runtime images.
  * Testing against multiple base operating system versions.


The matrix configuration can specify:
  * Multiple sets of environment variable definitions
  * Multiple runtime images
  * Multiple node pools
  * Multiple operating systems


The Matrix step executes the specified shell scripts multiple times in parallel steplets, in each specified runtime for each set of environment variables on each specified platform. For example, if a Matrix step specifies 3 sets of environment variables and 2 runtime images, it will run a total of 6 steplets (3 times in runtime 1, 3 times in runtime 2). If the Matrix step also specifies 2 node pools, it will run 12 steplets.
A Matrix step can be optionally preceded by a PreMatrix step to prepare the build node environment for the steplets, and a PostMatrix step to aggregate information produced by the steplets. For more information, see Using the Matrix Step.
### Note
Matrix step is available for Linux nodes only.
##### Usage
 **Matrix**
[YAML]
    pipelines:
      - name: <string>
        steps:
          - name: <string>
            type: Matrix
            stepMode: Bash
            configuration: 
              #inherits from Bash 
              concurrency: <string>       
              multiNode: <boolean>        # optional, only needed if steplets 
                                          # need to execute on separate nodes
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onExecute:                 # required     
                - echo ""Executing steplet $step_name""
                - echo ""env1 = $env1""
                - echo ""env2 = $env2""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: # always
                - echo ""Cleaning up some stuff""
    
            stepletMultipliers:
               nodePools:                 # wants to execute step against       
                - windows                 # multiple operating systems
                - ubuntu_18
                - ubuntu_16
     
              environmentVariables:       # optional, only needed if user
                - env1: one               # wants to execute step against
                  env2: two               # multiple values of env
                - env1: abc                
                  env2: xyz
    
              runtimes:                   # optional, only needed if user
                - type: image             # wants to execute step against 
                    image:                # multiple images
                    auto:                 
                      language:
                      version:
                    custom:
                      <same config as Bash>
                  image:               
                    auto:
                      language:
                      version:
                    custom:
                      <same config as Bash>
    
              fastFail: <true/false>     # default false. If specified, matrix
                                         # step fails when any steplet fails, 
                                         # unless specified in allowFailures
    
              allowFailures:             # optional, array of combinations
                - nodePool:              # that are allowed to fail  
                  environmentVariables:
                    env1: abc                              
                    env2: xyz
                  runtime:               
    
              exclude:                   # optional, array of combinations
                - nodePool:              # that are not executed  
                  environmentVariables:
                    env1: one
                    env2: two
                  runtime:  
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `Matrix` for this step type.
###### stepmode
Specifies the runtime OS mode. May be either `Bash` or `PowerShell`. If not specified, defaults to `Bash`.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`inputStep`
May specify a PreMatrix preamble step.
Optional  
In addition, these tags can be defined to support the step's native operation:
Tag
 **Description of usage**
Required/Optional  
`multiNode`
When `true`, steplets will be required to run on separate nodes. Default is `false`.
For more information, see Multi-node Matrix.
### Note
When both `multiNode` and `concurrency` are used, `multiNode` takes precedence.
Optional  
`concurrency`
Define the number of steplets that can concurrently run on a node. This helps in optimizing resource usage and ensures that the node efficiently handles steplets without being overwhelmed.
### Note
When both `multiNode` and `concurrency` are used, `multiNode` takes precedence.
Optional  
###### stepletMultipliers
In addition, these tags can be defined to support the step's native operation:
Tag
 **Description of usage**
Required/Optional  
`nodePools`
A collection of node pool names. Recognized only when `multiNode` is set to true. When specified, the step will be executed in each of the defined `runtimes` for each set of `environmentVariables` on each node pool.
If not defined, the step will be executed in the default node pool, or the node pool specified in the `configuration` section.
 **Example**
[YAML]
    steps:
    - name: step_1
      type: Matrix
      stepMode: Bash
      configuration:
        multiNode: true
      stepletMultipliers:
        environmentVariables:
          - foo: foo
          - bar: bar
        nodePools:
          - gcp
          - aws 
[/YAML]
Optional  
`environmentVariables`
A collection of sets of environment variable definitions. Each set of definitions will be used in an execution of a steplet in each of the defined `runtimes`.
If not defined, then a single steplet will execute for each of the defined `runtimes` on each of the specified `nodePools`.
Optional  
`runtimes`
A collection of runtime definitions. The step will be executed in each defined runtime, on each of the specified `nodePools`, for each set of `environmentVariables`.
If not defined, then each steplet will be executed in the default runtime, or the runtime specified in the `configuration` section.
Optional  
`fastFail`
When set to `true` the step fails when any steplet fails, unless permitted to fail in `allowFailures.`
Default is false.
Optional  
`allowFailures`
A collection that specifies the combinations of `nodepools`, `environmentVariables`, and `runtimes` that are permitted to fail without terminating the entire Matrix step. This only has effect when `fastFail` is set to `true`.
Optional  
`exclude`
A collection that specifies the combinations of `nodepools`, `environmentVariables`, and `runtimes` that will not be executed.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of `onExecute`
Optional  
`onExecute`
Main commands to execute for the step
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
### Caution
`onExecute`, `onStart`, `onSuccess`, `onFailure`, and `onComplete` are reserved keywords. Using these keywords in any other context in your execution scripts can cause unexpected behavior.
##### Examples
###### Example 1
This is an example of how to use the Matrix step to perform a build activity.
 **Matrix Step Example**
[YAML]
          - name: matrix_example
            type: Matrix
            stepMode: Bash
            configuration:
              inputSteps: 
                - name: matrix_fan_out_example
    
            stepletMultipliers:
              environmentVariables:             # Sets of environment variables for steplets
                - animal:    dog                            # - Set 1
                  mineral:   copper
                  vegetable: carrot
                - animal:    goat                           # - Set 2
                  mineral:   iron
                  vegetable: broccoli
                - animal:    lizard             # - Set 3
                  mineral:   lead        
    
              nodePools:
                    - gcp
                    - aws 
              
              runtimes:                         # Runtimes for steplets
                - type: image
                  image:
                    auto:
                      language: java
                      versions:
                        - 13.0
                - type: image
                  image:
                    auto:
                      language: node
                      versions:
                        - 8.17.0
            execution:
              onExecute:
                - echo ""I am executing matrix steplet ${steplet_id}""
                - echo ""Envs animal= $animal , mineral = $mineral , vegetable = $vegetable""
                - echo ""Runtime image name: $step_image_name | Runtime image tag: $step_image_tag""
    
[/YAML]
When run, the above example will execute in a total of 6 steplets: in 2 runtimes for each of 3 sets of environment variables:
Runtime
Set 1
Set 2
Set 3  
java 13.0
dog, copper, carrrot
goat, iron, broccoli
lizard, lead, <null>  
node 8.17.0
dog, copper, carrrot
goat, iron, broccoli
lizard, lead, <null>  
###### Example 2 - allowFailures, Exclude, fastFail
[YAML]
    pipelines:
      - name: S_Matrix_0052
        steps:           
          - name: S_Matrix_0052
            type: Matrix
            stepMode: Bash
            stepletMultipliers:
              fastFail: true
              allowFailures:
                - environmentVariables:
                    delay: 1
                    fail: 'true'
                  nodePool: windows
                  runtime: host
                - environmentVariables:
                    delay: 1
                    fail: 'true'
                  nodePool: ubuntu
                  runtime: host
              exclude:
                - environmentVariables:
                    delay: 1
                    fail: 'true'
                  nodePool: windows
                  runtime: host
                - environmentVariables:
                    delay: 1
                    fail: 'true'
                  nodePool: ubuntu
                  runtime: host 
              environmentVariables:
                - delay: 1
                  fail: 'true'
                - delay: 2
                  fail: 'false'
                - delay: 3
                  fail: 'false'
                - delay: 5
                  fail: 'false' 
            execution:
              onStart:
                - sleep ""$delay""
              onExecute:
                - echo ""I am on matrix step""
                - |
                  [ $fail == 'false' ] 
                - ls -l .
[/YAML]
###### Example 3 - concurrency
[YAML]
    pipelines:
      - name: matrix_check
        steps:
          - name: step1
            type: Matrix
            stepMode: Bash
            configuration: 
              concurrency: 50
            stepletMultipliers:
              environmentVariables:            # Sets of environment variables for steplets
              - animal:    dog                 # - Set 1
                mineral:   copper
                vegetable: carrot
              - animal:    goat                # - Set 2
                mineral:   iron
                vegetable: broccoli
              - animal:    lizar1d             # - Set 3
                mineral:   lea1d
              - animal:    liza2rd             # - Set 3
                mineral:   le2ad  
              - animal:    liza3rd             # - Set 3
                mineral:   lea3d
              - animal:    liza4rd             # - Set 3
                mineral:   lea4d
              - animal:    liza5rd             # - Set 3
                mineral:   le5ad  
              - animal:    liz6ard             # - Set 3
                mineral:   lea6d  
              - animal:    lizar7d             # - Set 3
                mineral:   le7ad  
              - animal:    lizar8d             # - Set 3
                mineral:   lea8d 
              - animal:    liza9rd             # - Set 3
                mineral:   lea9d 
              - animal:    liza10rd            # - Set 3
                mineral:   lead10 
              - animal:    liza11rd            # - Set 3
                mineral:   lead11  
              - animal:    liza12rd            # - Set 3
                mineral:   lead12  
              - animal:    lizar13d            # - Set 3
                mineral:   lead13
              - animal:    d1og14              # - Set 1
                mineral:   copper14
                vegetable: carrot14
              - animal:    go1at15             # - Set 2
                mineral:   ir1on15
                vegetable: broccoli15
              - animal:    lizard16            # - Set 3
                mineral:   lead16
              - animal:    lizard17            # - Set 3
                mineral:   lead17
              - animal:    lizard18            # - Set 3
                mineral:   lead18
              - animal:    lizard19            # - Set 3
                mineral:   lead20
              - animal:    lizard20            # - Set 3
                mineral:   lead20
              - animal:    lizard21            # - Set 3
                mineral:   lead21
              - animal:    lizard22            # - Set 3
                mineral:   lead22
              - animal:    lizard23            # - Set 3
                mineral:   lead23
              - animal:    lizard24            # - Set 3
                mineral:   lead24
              - animal:    lizard25            # - Set 3
                mineral:   lead25 
              - animal:    lizard26            # - Set 3
                mineral:   lead26
              - animal:    lizard27            # - Set 3
                mineral:   lead27
              - animal:    lizard28            # - Set 3
                mineral:   lead28
              - animal:    dog29               # - Set 1
                mineral:   copper29
                vegetable: carrot29
              - animal:    goat30              # - Set 2
                mineral:   iron30
                vegetable: broccoli30
              - animal:    lizard31            # - Set 3
                mineral:   lead31
              - animal:    lizard32            # - Set 3
                mineral:   lead32  
              - animal:    lizard33            # - Set 3
                mineral:   lead33
              - animal:    lizard34            # - Set 3
                mineral:   lead34
              - animal:    lizard35            # - Set 3
                mineral:   lead35  
              - animal:    lizard36            # - Set 3
                mineral:   lead36  
              - animal:    lizard37            # - Set 3
                mineral:   lead37  
              - animal:    lizard38            # - Set 3
                mineral:   lead38
              - animal:    lizard39            # - Set 3
                mineral:   lead39
              - animal:    lizard40            # - Set 3
                mineral:   lead40  
              - animal:    lizard41            # - Set 3
                mineral:   lead41  
              - animal:    lizard42            # - Set 3
                mineral:   lead42
              - animal:    lizard43            # - Set 3
                mineral:   lead43     
              - animal:    dog44               # - Set 1
                mineral:   copper44
                vegetable: carrot44
              - animal:    goat45              # - Set 2
                mineral:   iron45
                vegetable: broccoli45
              - animal:    lizard46            # - Set 3
                mineral:   lead46
              - animal:    lizard47            # - Set 3
                mineral:   lead47  
              - animal:    lizard48            # - Set 3
                mineral:   lead48
              - animal:    lizard49            # - Set 3
                mineral:   lead49
              - animal:    lizard50            # - Set 3
                mineral:   lead50
     
            execution:
              onExecute:
                - echo ""I am executing matrix steplet ${steplet_id}""
                - echo ""Envs animal= $animal , mineral = $mineral , vegetable = $vegetable
    
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
91,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a mvnbuild?[/INST] The **MvnBuild** native step performs a Maven project build on files in a Git repository. Optionally, it can also publish build information to Artifactory.
### Note
If your maven project is already using the Maven Artifactory plugin, see  Setting Up Maven Builds for Pipelines for information about configuring your Maven settings for use in JFrog Pipelines.Ecosystem Integration
Using this step automatically selects Java as the language and bootstraps the runtime environment in the node appropriately.
##### YAML Schema
 **MvnBuild**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: MvnBuild
            configuration:
              #inherits all the tags from bash
              mvnCommand:           <string>      # optional
              sourceLocation:       <string>      # optional
              resolverSnapshotRepo: <string>      # optional
              resolverReleaseRepo:  <string>      # optional
              deployerSnapshotRepo: <string>      # optional
              deployerReleaseRepo:  <string>      # optional
              forceXrayScan:        <string>      # optional
              failOnScan:           <boolean>     # default true
              autoPublishBuildInfo: <string>      # optional
              configFileLocation:   <string>      # optional
              configFileName:       <string>      # optional
    
              integrations:
                - name:         <Artifactory integration>  # required
              inputResources:
                - name:         <GitRepo resource>         # required
                - name:         <FileSpec resource>        # optional
              outputResources:
                - name:         <BuildInfo resource>       # if autoPublishBuildInfo is true
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete:                                  #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `MvnBuild `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify an  Artifactory Integration
Required  
`inputResources`
Must specify a GitReporesource. The mvn command performs the build on files in the Git repository at `sourceLocation`.
May also optionally specify a FileSpec resource that specifies what files to copy to `sourceLocation` to build.
Required  
`outputResources`
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set to `true`. If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the step, that name and/or number will be used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number.`
May be required  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`mvnCommand`
Specifies a command line string of options to use with Maven.
Default is `clean install`.
Optional  
`sourceLocation`
Specifies the location of the source files. By default this is set to the root directory of the GitRepo resource. Defaults to the root of the GitRepo.
Required if the source files are not in the root directory.
May be required  
`resolverSnapshotRepo`
Artifactory repository to resolve snapshot dependencies. If set, `resolverReleaseRepo` is also required.
### Note
Do not use with `configFileName` and `configFileLocation`.
Optional  
`resolverReleaseRepo`
Artifactory repository to resolve release dependencies. If set, `resolverSnapshotRepo` is also required.
### Note
Do not use with `configFileName` and `configFileLocation`.
Optional  
`deployerSnapshotRepo`
Snapshot artifacts created by the MvnBuild are uploaded to this Artifactory repository. If set, `deployerReleaseRepo` is also required.
### Note
Do not use with `configFileName` and `configFileLocation`.
Optional  
`deployerReleaseRepo`
Release artifacts created by the MvnBuild are uploaded to this Artifactory repository. If set, deployerSnapshot`Repo` is also required.
### Note
Do not use with `configFileName` and `configFileLocation`.
Optional  
`forceXrayScan`
When set to `true`, forces an Xray scan after publishing to Artifactory. Default is `false`.
Optional  
`failOnScan`
When set to `true`, and when the Xray Policy Rule Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step. Default is `true`.Creating Xray Policies and Rules
Optional  
`autoPublishBuildInfo`
When set to `true`, publishes build info to Artifactory. Requires a BuildInfo resources in the outputResources list. Default is `false`.
Optional  
`configFileName`
Specifies the name of the JFrog CLI mvn-config file. The `serverID` in the configuration file should match the Artifactory integration name. If set, `configFileLocation`is also required.
### Note
This option should be used only if you have committed this configuration file to your source
Optional  
`configFileLocation`
Specifies the directory containing the JFrog CLI mvn-config file, relative to the `sourceLocation` . If set, `configFileName` is also required.
### Note
This option should be used only if you have committed this configuration file to your source
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show a few ways in which a MvnBuild step can be configured.
###### Full Pipeline Example
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For a full tutorial, see Pipeline Example: Maven Build.


[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: mvn_repo
        type: GitRepo
        configuration:
          path: {{ .Values.repoPath }}
          gitProvider: {{ .Values.gitProvider }}
    
      - name: mvn_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactory }}
    
    pipelines:
      - name: demo_maven
        steps:
          - name: mvn_build_step
            type: MvnBuild
            configuration:
              deployerSnapshotRepo: {{ .Values.deployerRepo }}
              deployerReleaseRepo: {{ .Values.deployerRepo }}
              inputResources:
                - name: mvn_repo
              integrations:
                - name: {{ .Values.artifactory }}
    
          - name: publish_build
            type: PublishBuildInfo
            configuration:
              forceXrayScan: false
              inputSteps:
                - name: mvn_build_step
              outputResources:
                - name: mvn_build_info
    
[/YAML]
###### Basic Example using Default Values
The most basic form of MvnBuild. Uses all default values. This step navigates to the root of the repo specified in the `inputResources` array, and performs an `mvn clean install`. It does not publish a build or request an Xray scan.
 **MvnBuild**
[YAML]
    pipelines: 
      - name: MyMavenPipeline
        steps:
          - name: MavenWithArtifactory
            type: MvnBuild
            configuration:
              integrations:
                - name: art
              inputResources:
                - name: mvn_repo
[/YAML]
###### Using Artifactory as Resolver and Deployer
This example uses Artifactory as a resolver and a deployer, so that your project's dependencies are pulled from the specified repo, and the resulting artifacts are pushed back up to Artifactory. It also publishes build info to Artifactory and requests an Xray scan.
 **MvnBuild**
[YAML]
    pipelines: 
      - name: MyMavenPipeline
        steps:
          - name: MavenWithAll
            type: MvnBuild
            configuration:
              integrations:
                - name: art
              inputResources:
                - name: mvn_repo
              outputResources:
                - name: mvn_build_info
              mvnCommand: clean install test
              sourceLocation: mvnproject
              resolverSnapshotRepo: snapshots-remote-maven
              resolverReleaseRepo: releases-remote-maven
              deployerSnapshotRepo: snapshots-local-maven
              deployerReleaseRepo: releases-local-maven
              forceXrayScan: true
              failOnScan: true
              autoPublishBuildInfo: true
    
[/YAML]
###### Using a Config File
This example uses a committed config file to accomplish the same thing that example 2 does.
 **MvnBuild**
[YAML]
    pipelines: 
      - name: MyMavenPipeline
        steps:
          - name: MvnWithConfig
            type: MvnBuild
            configuration:
              integrations:
                - name: art
              inputResources:
                - name: mvn_repo
              outputResources:
                - name: mvn_build_info
              mvnCommand: clean install test
              sourceLocation: mvnproject
              configFileLocation: "".""
              configFileName: mvn-art-config
              forceXrayScan: true
              failOnScan: true
              autoPublishBuildInfo: true
    
[/YAML]
##### How it Works
When you use the **MvnBuild** native step in a pipeline, it performs the following actions in the background:
 **MvnBuildCommands**
[YAML]
    jfrog rt config                        # configures JFrog CLI with the integration listed in the yaml
    jfrog rt mvn-config             # if configFileName and configFileLocation are not set in yaml
    jfrog rt mvn $mvnCommand        # the main maven build command
    add_run_variables                       # saves some information in run state for future steps to reference
    jfrog rt build-collect-env      # collects build environment, preparing for build publish
    jfrog rt build-publish          # only if autoPublishBuildInfo is true
    write_output                            # updates the buildinfo resource, if present
    jfrog rt build-scan             # if forceXrayScan is true
    add_run_files                           # adds BuildInfo to run state
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
92,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a npmbuild?[/INST] The **NpmBuild** native step builds an npm source. This step automatically performs `npm-install` on the source in a Git repository.
If you want to run tests, add a Bash step to trigger them.
### Important
The NpmBuild native step may fail on W19 nodes when JFrog CLI v1 is used. Workaround is to use JFrog CLI v2.
##### YAML Schema
The YAML schema for NpmBuild native step is as follows:
 **NpmBuild**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: NpmBuild
            configuration:
              #inherits all the tags from bash
              # for payloadType npm:
              npmArgs:          <string>      # optional
              repositoryName:   <string>      # optional, npm repository name on artifactory
              resolverRepo:     <string>      # optional, npm repository name on artifactory
              sourceLocation:   <string>      # optional, directory containing package.json file
    
              integrations:
                - name:         <artifactory integration>  # required, unless there is a FileSpec input
              inputResources:
                - name:         <gitrepo resource>         # required
                - name:         <filespec resource>        # optional
      
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete:                                  #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `NpmBuild `for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Specifies an  Artifactory Integrationwhere modules will be published. If a FileSpec resource is specified in `inputResources` then this is optional. Otherwise, it is required.
May be required  
`inputResources`
Must specify a GitReporesource. The `npm-install` runs on the Git repository at `sourceLocation`.
May also optionally specify a FileSpec resource that specifies what files to copy to `sourceLocation` prior to running `npm-install`.
Required
Optional  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`npmArgs`
Specifies a command line string of arguments to use with npm-install.
Optional  
`resolverRepo`
The name of the npm repository in Artifactory.
Optional  
`repositoryName`
Alternative to `resolverRepo`. Do not set both tags.
Optional  
`sourceLocation`
The directory containing the `package.json` file, relative to the GitRepo `path`.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure an NpmBuild step.
###### Using Default Settings
An NpmBuild step using default locations and default commands.
  * This example requires a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For more information about running this example, see Pipeline Example: Npm Build.


 **NpmBuild**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: npm_repo_jfp_example
        type: GitRepo
        configuration:
          # SCM integration where the repository is located
          gitProvider: {{ .Values.myRepo.gitProvider }}
          # Repository path, including org name/repo name
          path: {{ .Values.myRepo.path }}
          branches:
            # Specifies which branches will trigger dependent steps
            include: master
    
      - name: npm_buildinfo_jfp_example
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
    
    pipelines:
      - name: npm_pipeline_jfp_example
        steps:
          - name: npm_build_step
            type: NpmBuild
            configuration:
              repositoryName: npm-virtual      # required, npm repository name on artifacctory
              sourceLocation: .      # required, location of package.json file
              integrations:
                - name:  demoArt  # required
              inputResources:
                - name: npm_repo_jfp_example         # required
[/YAML]
###### Using FileSpec Input
An NpmBuild step with a FileSpec input providing files for the build that are not in the GitRepo and resolverRepo specifying an Artifactory repository to use when resolving dependencies.
 **NpmBuild**
[YAML]
    pipelines: 
      - name: npmBuildPipeline
        steps:
          - name: npmBuildStep
            type: NpmBuild
            configuration: 
              sourceLocation: ""source""
              resolverRepo: remote-npm
              inputResources:
                - name: gitRepoResource
                - name: fileSpecResource
              integrations:
                - name: artifactory_integration
[/YAML]
##### How it Works
When you use the **NpmBuild** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (if there is a FileSpec input, to configure the JFrog CLI with the Artifactory credentials in the input FileSpec)
  * jfrog rt use (to set the current default Artifactory configuration)
  * cp (if there is an input FileSpec, copy the files to the root of the cloned GitRepo)
  * jfrog rt npm-config (configure the repository to resolve dependencies)
  * jfrog rt npm-install
  * add_run_variables (save information about this step for future steps)
  * jfrog rt build-collect-env (collect environment variables)
  * add_run_files (save the output and the build information in the run state for later publish steps)


##### Related Topics
Npm Build Quickstart

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
93,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a npmpublish?[/INST] The NpmPublish step publishes an npm package to the registry in Artifactory following an NpmBuild step. It is recommended, but not required, that the NpmBuild and NpmPublish steps be in the same affinity group to optimize sharing files between the two steps.
### Important
The NpmBuild native step may fail on W19 nodes when JFrog CLI v1 is used. Workaround is to use JFrog CLI v2.
##### YAML Schema
The YAML schema for NpmPublish native step is as follows:
 **NpmPublish**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: NpmPublish
            configuration:
                      #inherits all the tags from bash
              # for payloadType npm:
              deployerRepo:         <string>   # optional, npm repository name on artifactory
              repositoryName:       <string>   # optional, npm repository name on artifactory
              forceXrayScan:                <boolean>  # optional
              failOnScan:           <boolean>  # default true
              autoPublishBuildInfo: <boolean>  # optional
    
              integrations:
                - name:         <artifactory integration>      # required
              inputSteps:
                - name:         <NpmBuild step>                # required
                      outputResources:
                            - name:                 <BuildInfo resource>           # optional
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `NpmPublish ` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify an  Artifactory Integration.
Required  
`inputSteps`
Must specify a named NpmBuild or Bash step.
Required  
`outputResources`
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set to `true`.
If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the input NpmBuild step, that name and/or number will be used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number.`
May be required  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`deployerRepo`
The name of the npm repository to publish in Artifactory.
Optional  
`repositoryName`
Alternative to `deployerRepo`. Do not set both tags.
Optional  
`forceXrayScan`
When set to `true`, forces an Xray scan after publishing to Artifactory.
Default is `false`.
Optional  
`failOnScan`
When set to `true`, and when the Xray Policy Rule  Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
`autoPublishBuildInfo`
When set to `true`, publishes build info to Artifactory.
Default is `false`.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure an NpmPublish step.
###### Full Pipeline Example
  * This example requires a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.
  * For more information about running this example, see Pipeline Example: Npm Build.


 **NpmBuild**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: npm_repo_jfp_example
        type: GitRepo
        configuration:
          # SCM integration where the repository is located
          gitProvider: {{ .Values.myRepo.gitProvider }}
          # Repository path, including org name/repo name
          path: {{ .Values.myRepo.path }}
          branches:
            # Specifies which branches will trigger dependent steps
            include: master
    
      - name: npm_buildinfo_jfp_example
        type: BuildInfo
        configuration:
          sourceArtifactory: demoArt
    
    pipelines:
      - name: npm_pipeline_jfp_example
        steps:
          - name: npm_build_step
            type: NpmBuild
            configuration:
              affinityGroup: npm_pipeline_jfp_example
              repositoryName: npm-virtual      # required, npm repository name on artifacctory
              sourceLocation: .      # required, location of package.json file
              integrations:
                - name:  demoArt  # required
              inputResources:
                - name: npm_repo_jfp_example         # required
    
          - name: npm_publish_step
            type: NpmPublish
            configuration:
              affinityGroup: npm_pipeline_jfp_example
              # for payloadType npm:
              repositoryName: npm-virtual        # required, npm repository name on artifactory
              autoPublishBuildInfo: true       # optional
              integrations:
                - name: demoArt      # required
              inputSteps:
                - name: npm_build_step                # required
              outputResources:
                - name: npm_buildinfo_jfp_example                 # optional
[/YAML]
###### Upload Artifactory Repo
Uploads to an Artifactory repository named npm-repo.
### Note
This example is not a complete pipeline. An NpmBuild step must be part of the same pipeline.
 **NpmPublish**
[YAML]
    pipelines: 
      - name: npmPublishPipeline
        steps:
          - name: npmPublishStep
            type: NpmPublish
            configuration:
              deployerRepo: npm-repo
              integrations:
                - name: artifactory_integration
              inputSteps:
                - name: npmBuildStep
[/YAML]
###### Upload the Result of NpmBuild Step to Artifactory Repo
Uploads the result of the NpmBuild step to an Artifactory repository named npm-repo and uploads build info. This extends example 1 in the NpmBuild documentation.
 **NpmPublish**
[YAML]
    pipelines: 
      - name: npmBuildPipeline
        steps:
          - name: npmBuildStep
            type: NpmBuild
            configuration:
              affinityGroup: npmBuildPipeline
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: npmPublishStep
            type: NpmPublish
            configuration:
              affinityGroup: npmBuildPipeline
              deployerRepo: npm-repo
              autoPublishBuildInfo: true
              inputSteps:
                - name: npmBuildStep
              outputResources:
                - name: outputBuildInfo
    
[/YAML]
###### Publish Build Info and Trigger Xray Scan
In this example, build info is published and an Xray scan triggered.
 **NpmPublish**
[YAML]
    pipelines: 
      - name: npmBuildPipeline
        steps:
          - name: npmBuildStep
            type: NpmBuild
            configuration:
              affinityGroup: npmBuildPipeline
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: npmPublishStep
            type: NpmPublish
            configuration:
              affinityGroup: npmBuildPipeline
              deployerRepo: npm-repo
              autoPublishBuildInfo: true
              forceXrayScan: true
              inputSteps:
                - name: npmBuildStep
              outputResources:
                - name: outputBuildInfo
    
[/YAML]
##### How it Works
When you use the **NpmPublish** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt use (specify the configured credentials to use)
  * restore_run_files (copy the output and build info from the NpmBuild step)
  * jfrog rt npm-config (configure the deploy repo)
  * jfrog rt npm-publish
  * jfrog rt build-publish (if autoPublishBuildInfo is true, publish the build info)
  * write_output (if autoPublishBuildInfo is true, update the output BuildInfo resource)
  * jfrog rt build-scan (if forceXrayScan is true, trigger a scan)
  * add_run_files (update the build info saved in run state)


##### Related Topics
Pipeline Example: Npm Build

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
94,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a prematrix?[/INST] The **PreMatrix** generic step may be used to prepare a build environment for execution of a Matrix step.
The PreMatrix step is optional for use with Matrix builds. For more information, see Using the Matrix Step.
##### Usage
 **FanOut**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: PreMatrix
            configuration: 
              # inherits Bash config, except affinityGroup and outputResources
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onExecute:                                                    # required     
                - echo ""this is a PreMatrix step""           
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `PreMatrix` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, except for thesetags, which are disallowed:
  * `affinityGroup`
  * `outputResources`


###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of `onExecute`
Optional  
`onExecute`
Main commands to execute for the step
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
95,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a postmatrix?[/INST] The **PostMatrix** generic step may be used to perform post-execution tasks following a Matrix step.
The PostMatrix step is optional for use with Matrix builds. For more information, see Using the Matrix Step.
##### Usage
 **FanIn**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: PostMatrix
            configuration: 
              # inherits Bash config, except affinityGroup and outputResources
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onExecute:                                                    # required     
                - echo ""this is a PostMatrix step""           
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `PostMatrix` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, except for thesetags, which are disallowed:
  * `affinityGroup`
  * `outputResources`


###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of `onExecute`
Optional  
`onExecute`
Main commands to execute for the step
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
96,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a powershell?[/INST] The **PowerShell** step type is a generic type that enables executing PowerShell commands. PowerShell steps can only run on Windows node pools and are similar to the Bash step on other node pools. As a general-purpose step that can execute any action that can be scripted, even with tools and services that haven't been integrated with JFrog Pipelines, it can be used to perform actions where complete control is required.
All native steps running in a Windows node pool derive from the PowerShell step. This means that all steps share the same base set of tags from PowerShell, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the PowerShell step definition when running steps on Windows, since it's the core of the definition of all other steps.
##### Usage
 **PowerShell**
[YAML]
    pipelines:
      - name:                       <string>
        steps:
          - name:           <string>
            type:           PowerShell
            configuration:
              affinityGroup:        bldGroup 
              priority:             <[0-10000]>
              timeoutSeconds:       <job timeout limit>
              nodePool:             <name of the nodePool>
              chronological:        <true/false>
              environmentVariables:
                env1:                       <string>
                env2:                       <string>
                env3:
                  default:      <string>
                  description:  <string>
                  values:       <array>
                  allowCustom:  <true/false>
              integrations:
                - name:             <integration name>
              inputSteps:
                - name:             <step name>
              inputResources:
                - name:             <resource name>
                  trigger:          <true/false>    # default true
                  branch:       <string>        # see description of defaults below
              outputResources:
                - name:             <resource name>
                  branch:       <string>        # see description of defaults below
              runtime:            
                type:                       <image/host>
                image:
                  auto:
                    language:
                    versions:
                  custom:
                    name:
                    tag:
                    options:
                    registry:          <integration>   # optional integration for a private registry
                    sourceRepository:  <path>          # required if registry is Artifactory. e.g. docker-local
                    region:                            # required if registry is AWS. e.g. us-east-1
                    autoPull:       <true/false>             # default true; pulls image before run
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onExecute:
                - echo ""executing task command 1""
                - echo ""executing task command 2""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step. The name should be chosen to accurately describe what the step does, e.g. `prov_test_env` to represent a job that provisions a test environment. Names of steps must be unique within a pipeline.
###### type
Must be `PowerShell` for this step type.
###### configuration
Specifies all optional configuration selections for the step's execution environment.
Tag
 **Description of usage**
Required/Optional  
`affinityGroup`
Label that controls affinity to a Node. All the steps with the same affinityGroup will be executed on the same node. This will allow sharing state between the steps. An example is having the same affinityGroup for DockerBuild and DockerPush steps in a Pipeline so that Image being built in the DockerBuild step can be used to published in the DockerPush step
Optional  
`priority`
Controls the priority of a step when there are parallel steps in a pipeline or multiple pipelines executing. It determines which step will run first across all steps that could run if there were no constraints on the number of steps running. Steps with a lower number will run before steps with higher numbers. For example, priority 10 will run before priority 100. The default priority is 9999.
Priority does not apply to steps that are still waiting for an input to complete or configured to run in a node pool with no available nodes. Also, if there are two steps ready to run and only one available node, the one with the lower priority number runs first, regardless of which pipeline each step belongs to.
Optional  
`timeoutSeconds`
Time limit, in the number of seconds, for the step to complete. If the step does not complete in the given time limit, the step will be forced to a completion state of failed.
Optional  
`nodePool`
Assigns the node pool on which the step executes. If node pool isn't specified, a step will execute on the default node pool.  See here to learn more about node pools.
Optional  
`chronological`
Specifies that the step must execute in chronological order, to ensure receipt of all state updates from preceding steps.
A step with `chronological: true` will only execute when that step is complete in all previously triggered runs.
Optional  
`environmentVariables`
Assigns any environment variables and their values in key:value format. All environment variables assigned within a step definition are active only for the scope of the execution of that step. Variables will be available in global scope and in the `env` drive.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


Optional  
`integrations`
A collection of integrations that will be used by this step. Integrations can be used directly in step without a resource.
Optional  
`inputSteps`
A collection of named steps whose completion will trigger execution of this step.
Optional  
`inputResources`
A collection of named resources that will be used by this step as inputs.
By default, changes to these named resources will trigger execution of this step. This can be changed by declaring `trigger` to be `false`.
A `branch` is only required when using a resource from another branch of a multi-branch source. By default, the resource from a single-branch source or from the same branch of a multi-branch pipeline is used. To use a multi-branch resource in a single-branch pipeline or another branch in a multi-branch pipeline, `branch` should be used to specify the branch to which the resource belongs.
Optional  
`outputResources`
A collection of named resources that will be generated or changed by this step.
A `branch` is only required for resources from another branch of a multi-branch source. By default, the resource from a single-branch source or from the same branch of a multi-branch pipeline is assumed. To update a multi-branch resource in a single-branch pipeline or another branch in a multi-branch pipeline, `branch` should be used to specify the branch to which the resource belongs.
Optional  
`runtime`
Specifies the runtime for the execution node.
Optional  
###### execution
Declare sets of shell command sequences to perform for different execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of `onExecute`
Optional  
`onExecute`
Main commands to execute for the step
Optional  
`onSuccess`
Commands to execute on successful completion of `onExecute`
Optional  
`onFailure`
Commands to execute on failed completion of `onExecute`
Optional  
`onComplete`
Commands to execute on any completion of `onExecute`
Optional  
##### Example
This is an example of how to use the PowerShell step to perform a build activity.
 **PowerShell step to build**
[YAML]
    - name: build
      type: PowerShell
      configuration:
        nodePool: my_windows_node_pool
        environmentVariables:
          env1: value1
          env2:
            default: value2
            description: Example Variable
            values:
              - value2
              - value3
            allowCustom: false
        runtime:
          type: image
          image:
            auto:
              language: node
              versions:
                - ""10.18""
        inputResources:
          - name: src
        integrations:
          - name: mySlack
      execution:
        onExecute:
          - Push-Location $res_src_resourcePath
          - npm install
          - New-Item testresults -Type Directory
          - New-Item codecoverage -Type Directory
          - .\node_modules\.bin\mocha --recursive ""tests/**/*.spec.js"" -R mocha-junit-reporter --reporter-options mochaFile=testresults/testresults.xml
          - .\node_modules\.bin\istanbul --include-all-sources cover -root ""routes"" node_modules/mocha/bin/_mocha -- -R spec-xunit-file --recursive ""tests/**/*.spec.js""
          - .\node_modules\.bin\istanbul report cobertura --dir codecoverage
          - save_tests testresults/testresults.xml
        onSuccess:
          - send_notification mySlack ""build completed""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
97,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a promotebuild?[/INST] The **PromoteBuild** native step promotes a BuildInfo and moves or copies the related artifacts from one Artifactory repository to another.
### Note
This step requires a BuildInfo resource as an input to successfully execute.
##### YAML Schema
The YAML schema for PromoteBuild native step is as follows:
 **PromoteBuild**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: PromoteBuild
            configuration:
              #inherits all the tags from bash
              targetRepository:      <string>
              includeDependencies:   <boolean>    # default false
              status:                                <string>  
              comment:                               <string>
              copy:                                  <boolean>
    
              inputResources:
                - name:         <BuildInfo resource>     # required
                  trigger: false
              outputResources:
                - name:         <BuildInfo resource>     # required
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `PromoteBuild` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify a named BuildInfo resource whose `buildName` and `buildNumber` properties identify the build to promote.
Required  
`outputResources`
Must specify a named BuildInfo resource to map to the promoted build. The BuildInfo will be updated with the `buildName` and `buildNumber` of the input BuildInfo resource, and its `targetRepo` with the value of `targetRepository`.
Required  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`targetRepository`
The name of the repository in Artifactory to promote the build to.
Required  
`includeDependencies`
When set to true, promotion will include the build dependencies.
Defaults to false.
Optional  
`status`
Name of the status to promote the build to.
Optional  
`comment`
Comment to include for the promotion.
Optional  
`copy`
When set to true, copies the artifacts to the targetRepository vs moving them to the targetRepository.
Defaults to false.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a PromoteBuild step.
###### Promote Build to Artifactory
Promotes a build specified by an input BuildInfo to an Artifactory repository named `docker-prod-local`. In this example, `trigger` is set as `false` on the input resource so that the step will not be automatically triggered.
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


 **PromoteBuild**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: app_repo
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.gitIntegration }}                      
          path: {{ .Values.gitRepositoryPath }}    
          branches:
            include: master
    
      - name: app_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
      - name: app_promoted_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
    pipelines:
      - name: app_dev_pipeline_su204
        steps:
          - name: app_build
            type: DockerBuild
            configuration:
              affinityGroup: docker_group
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}  
              dockerImageTag: ${run_number}
              inputResources:
                - name: app_repo
              integrations:
                - name: {{ .Values.artifactoryIntegration }}         
                
          - name: app_push
            type: DockerPush
            configuration:
              affinityGroup: docker_group
              targetRepository: {{ .Values.sourceRepository }}
              integrations:
                - name: {{ .Values.artifactoryIntegration }}                            
              inputSteps:
                - name: app_build
    
          - name: publish_app_build
            type: PublishBuildInfo
            configuration:
              affinityGroup: docker_group
              inputSteps:
                - name: app_push
              outputResources:
                - name: app_buildinfo
                
          - name: xrayScanStep
            type: XrayScan
            configuration:
              inputResources:
                - name: app_buildinfo   
    
      - name: app_prod_pipeline_su204
        steps:
          - name: app_promote_build
            type: PromoteBuild
            configuration:
              targetRepository: {{ .Values.prodRepository }}
              inputResources:
                - name: app_buildinfo
                  trigger: false
              outputResources:
                - name: app_promoted_buildinfo
    
          - name: app_deploy
            type: Bash
            configuration:
              integrations:
                - name: {{ .Values.artifactoryIntegration }}
              inputResources:
                - name: app_promoted_buildinfo
            execution:
              onStart:
                - jfrog rt docker-pull {{ .Values.artifactoryUrl }}/{{ .Values.prodRepository }}/{{ .Values.imageName }}:$res_app_promoted_buildinfo_buildNumber {{ .Values.prodRepository }}
                #- jfrog rt dl prod-go-local/app
                #- ls -latr
              onExecute:
                - docker run {{ .Values.artifactoryUrl }}/{{ .Values.prodRepository }}/{{ .Values.imageName }}:$res_app_promoted_buildinfo_buildNumber
[/YAML]
###### Promote Build with Status and Comment
Promotes the build with a status and comment.
 **PromoteBuild**
[YAML]
    pipelines: 
      - name: promoteBuildPipeline
        steps:
          - name: promoteBuildStep
            type: PromoteBuild
            configuration:
              targetRepository: my-promoted-builds
              status: TEST
              comment: ""Promoted by run number ${run_number}.""
              integrations:
                - name: artifactory_integration
              inputResources:
                - name: inputBuildInfo
                  trigger: false
              outputResources:
                - name: outputBuildInfo
    
[/YAML]
###### Promote Build and Copy
In this example, when the build is promoted, it is copied so that a copy remains in the previous repository and the dependencies are copied as well as the build.
 **PromoteBuild**
[YAML]
    pipelines: 
      - name: promoteBuildPipeline
        steps:
          - name: promoteBuildStep
            type: PromoteBuild
            configuration:
              targetRepository: my-promoted-builds
              includeDependencies: true
              copy: true
              integrations:
                - name: artifactory_integration
              inputResources:
                - name: inputBuildInfo
                  trigger: false
              outputResources:
                - name: outputBuildInfo
[/YAML]
##### How it Works
When you use the **PromoteBuild** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (configure the JFrog CLI with the credentials in the BuildInfo input)
  * jfrog rt build-promote
  * write_output (update the output BuildInfo resource)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
98,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a publishbuildinfo?[/INST] The **PublishBuildInfo** step publishes BuildInfo to Artifactory. BuildInfo provides a manifest for the build and includes metadata about the modules, dependencies and other environment variables.
BuildInfo can also be published by any of the language-specific publish steps, when its `autoPublishBuildInfo` tag is set to `true`.
### Note
A base exclude pattern always applies to published build info from Pipelines, to prevent environment variables containing security information (e.g., secrets) and other unnecessary data from being published. The base exclude pattern is:
`buildinfo.env.res_*;buildinfo.env.int_*;buildinfo.env.current_*;*password*;*secret*;*key*;*token*`
##### YAML Schema
The YAML schema for PublishBuildInfo native step is as follows:
 **PublishBuildInfo**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: PublishBuildInfo
            configuration:
              #inherits all the tags from bash
              envInclude:       <string>   
              envExclude:       <string>   
              forceXrayScan:    <boolean>    # default false
              failOnScan:       <boolean>    # default true
    
              inputSteps:
                - name:         <any of the build steps or Bash step>  # required
              outputResources:
                - name:         <BuildInfo resource>                             # required
    
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `PublishBuildInfo` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`inputSteps`
Must specify a named step of one of the following types:
  * Bash
  * PowerShell
  * DockerPush
  * GoPublishModule
  * GoPublishBinary
  * GradleBuild
  * HelmPublish
  * MvnBuild
  * NpmPublish


The step must not have set `autoPublishBuildInfo` to `true`.
Required  
`outputResources`
Must specify a BuildInfo resource to publish. The `buildName` and `buildNumber` properties are automatically set to `$buildName` and `$buildNumber` if defined, or the `JFROG_CLI_BUILD_NAME` and/or `JFROG_CLI_BUILD_NUMBER` from the environment of the named step for native steps. If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` was not set when the input step ran, the defaults are `$pipeline_name` and `$run_number`.
### Note
If `inputSteps` is a Bash step and `$buildName` or `$buildNumber` are not defined, then this step will fail.
Required  
In addition, these tags can be defined to support the step's native operation:
### Note
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`envInclude`
Pattern for which environment variables to include. Default is to include all the environment variables.
Optional  
`envExclude`
Pattern for which environment variables to exclude. This is applied in addition to the base exclude pattern applied to all build info.
Optional  
`forceXrayScan`
When set to true, forces an Xray scan after publishing to Artifactory.
Default is false.
Optional  
`failOnScan`
When set to **true** , and when the Xray Policy Rule **Fail Build Creating Xray Policies and Rules** checkbox is checked, a failed Xray scan will result in a failure of the step.
Default is true.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following examples show how to configure a PublishBuildInfo step.
###### Full Pipeline Example
Pushes the image created by the DockerBuild input step and published BuildInfo to Artifactory.
  * This example requires an Artifactory Integration and a GitHub Integration.GitHub Integration
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


 **DockerPush**
[YAML]
    # This config file is templatized so that it can be easily customized. Values can be provided with a values.yml file.
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: app_repo1
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.gitIntegration }}                      
          path: {{ .Values.gitRepositoryPath }}    
          branches:
            include: master
    
      - name: app_buildinfo1
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
      - name: app_promoted_buildinfo1
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
    pipelines:
      - name: app_dev_pipeline
        steps:
          - name: app_build
            type: DockerBuild
            configuration:
              affinityGroup: docker_group
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}  
              dockerImageTag: ${run_number}
              inputResources:
                - name: app_repo
              integrations:
                - name: {{ .Values.artifactoryIntegration }}         
                
          - name: app_push
            type: DockerPush
            configuration:
              affinityGroup: docker_group
              targetRepository: {{ .Values.sourceRepository }}
              integrations:
                - name: {{ .Values.artifactoryIntegration }}                            
              inputSteps:
                - name: app_build
    
          - name: publish_app_build
            type: PublishBuildInfo
            configuration:
              affinityGroup: docker_group
              inputSteps:
                - name: app_push
              outputResources:
                - name: app_buildinfo
[/YAML]
###### MvnBuild Example
This extends the first MvnBuild example to publish the build info using a PublishBuildInfo step.
 **PublishBuildInfo**
[YAML]
    pipelines:
      - name: MyMavenPipeline
        steps:
          - name: MavenWithArtifactory
            type: MvnBuild
            configuration:
              integrations:
                - name: art
              inputResources:
                - name: mvn_repo
          - name: publishBuildInfoStep
            type: PublishBuildInfo
            configuration:
              inputSteps:
                - name: MavenWithArtifactory
              outputResources:
                - name: buildInfoResource
    
[/YAML]
###### NpmBuild and NpmPublish Example
An NpmBuild and NpmPublish example publishing the build info using a PublishBuildInfo step and triggering an Xray scan in the PublishBuildInfo step.
 **PublishBuildInfo**
[YAML]
    pipelines:
      - name: npmBuildPipeline
        steps:
          - name: npmBuildStep
            type: NpmBuild
            configuration:
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: npmPublishStep
            type: NpmPublish
            configuration:
              deployerRepo: npm-repo
              inputSteps:
                - name: npmBuildStep
              outputResources:
                - name: outputBuildInfo
          - name: publishBuildInfoStep
            type: PublishBuildInfo
            configuration:
              forceXrayScan: true
              inputSteps:
                - name: npmPublishStep
              outputResources:
                - name: buildInfoResource
    
[/YAML]
###### Bash and GoPublishModule Step
A Bash step input to the PublishBuildInfo step following an earlier GoPublishModule step.
 **PublishBuildInfo**
[YAML]
    pipelines:
      - name: goPublishModulePipeline
        steps:
          - name: goPublishModuleStep
            type: GoPublishModule
            configuration:
              version: ""v0.0.${run_number}""
              targetRepository: go-repo
              self: true
              deps: ALL
              inputResources:
                - name: gitRepoResource
              integrations:
                - name: artifactory_integration
          - name: moduleTestStep
            type: Bash
            configuration:
              inputSteps:
                - name: goPublishModuleStep
            execution:
              onExecute:
                - ""echo 'Running some tests.'""
          - name: publishBuildInfoStep
            type: PublishBuildInfo
            configuration:
              environmentVariables:
                buildStepName: goPublishModuleStep
              inputSteps:
                - name: moduleTestStep
              outputResources:
                - name: buildInfoResource
[/YAML]
##### How it Works
When you use the **PublishBuildInfo** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (configure the JFrog CLI with the BuildInfo output credentials)
  * jfrog rt use (specify the configured credentials to use)
  * restore_run_files (copy the output and build info from the input step)
  * jfrog rt build-publish (publish the build info)
  * jfrog rt build-scan (if forceXrayScan is true, trigger a scan)
  * write_output (if autoPublishBuildInfo is true, update the output BuildInfo resource)
  * add_run_files (update the build info saved in run state)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
99,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a signreleasebundle?[/INST] The **SignReleaseBundle** native step signs a Release Bundle in preparation for distributing it to Edge nodes.
##### YAML Schema
The YAML schema for SignReleaseBundle native step is as follows:
 **SignReleaseBundle**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: <string>
            type: SignReleaseBundle
            configuration:
                      #inherits all the tags from bash
              inputResources:
                - name:            <release bundle to sign>  # required
                      trigger: false
              outputResources:
                - name:            <signed release bundle>  # required
    
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `SignReleaseBundle` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinenttags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify a ReleaseBundleresource whose `name` and `version` properties identify the release bundle to be signed.
Required  
`outputResources`
Must specify a ReleaseBundleresource. This ReleaseBundle is updated with the `name` and `version` properties of the input ReleaseBundle, and sets the `isSigned` property to true.
Required  
There is no additional configuration required.
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
### GPG Signing Key Passphrase
When your Distribution release bundle GPG signing keyis passphrase protected, remember to provide the GPG Signing Key Passphrase when creating/updating your Distribution Integration.GPG Signing
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
##### Examples
The following example shows how to configure a SignReleaseBundle step.
###### Sign a Release Bundle
Signs a ReleaseBundle provided as an input.
  * This example requires an Artifactory Integration and a Distribution Integration.
  * The Pipelines DSL for this example is available in this repository in the JFrog GitHub account.


 **SignReleaseBundle**
[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      # Build info of first build to bundle
      - name: gosvc_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.myArtifactoryIntegration }}
          buildName: svc_build
          buildNumber: 1
    
      # Build info of second build to bundle
      - name: appl_promoted_build_info
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.demoArtifactoryIntegration }}
          buildName: backend_build
          buildNumber: 1
    
      # Release bundle
      - name: release_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Signed version of the same release bundle
      - name: signed_bundle
        type: ReleaseBundle
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          name: demo_rb
          version: v1.0.0
    
      # Distribution rules
      - name: distribution_rules
        type: DistributionRule
        configuration:
          sourceDistribution: {{ .Values.distributionIntegration }}
          serviceName: ""*""
          siteName: ""*""
          cityName: ""*""
          countryCodes:
            - ""CN""
            - ""GB""
    
    pipelines:
      - name: demo_release_mgmt
        steps:
          - name: bundle
            type: CreateReleaseBundle
            configuration:
              releaseBundleName: demo_rb
              releaseBundleVersion: v1.0.${run_number}
              dryRun: false
              sign: false
              description: ""some random test description""
              inputResources:
                - name: gosvc_promoted_build_info
                  trigger: true
                - name: appl_promoted_build_info
                  trigger: true
              outputResources:
                - name: release_bundle
              releaseNotes:
                syntax: markdown
                content: |
                  ## Heading
                    * Bullet
                    * Points
                
          - name: sign
            type: SignReleaseBundle
            configuration:
              inputResources:
                - name: release_bundle
              outputResources:
                - name: signed_bundle
    
          - name: distribute
            type: DistributeReleaseBundle
            configuration:
              dryRun: false
              inputResources:
                - name: signed_bundle
                - name: distribution_rules
[/YAML]
##### How it Works
When you use the **SignReleaseBundle** native step in a pipeline, it performs the following functions in the background:
  * curl $distUrl/api/v1/release_bundle/$releaseBundleName/$releaseBundleVersion/sign (sign the release bundle)
  * write_output (update the output ReleaseBundle resource)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
100,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a triggerpipeline?[/INST] The **TriggerPipeline** native step will trigger another step and wait for the resulting run to complete before continuing. This allows you to embed another pipeline inside your own.
### Note
  * This step type will utilize a build node to poll for the status of the triggered run until it is complete. Ensure that the capacity of your node pool is sufficient to support both pipelines executing in parallel.
  * For more information about using the `TriggerPipeline` native step, see Embedded Pipelines.


You can use the `set_trigger_payload` utility function in the `onStart` section of this step to set customized parameters for the pipeline that you wish to trigger. In addition, you can add `pipelineVariables`, which will apply to all steps in the triggered run, as well as `stepVariables`, which will apply only to the specified step.
### Linux only
This step is supported for execution on Linux machines only. PowerShell support is not yet available.
##### YAML Schema
The YAML schema for the TriggerPipeline native step is as follows:
 **TriggerPipeline**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: TriggerPipeline
            configuration:
              #inherits all the tags from bash
    
              pipelineName:   <string>   # required
              stepName:       <string>   # required
              branchName:     <string>   # optional. recommended if target is multibranch.
              projectKey:     <string>   # optional. recommended if target belongs to a project.
              integrations:
                - name:       <JFrog Platform Access Token integration>  # required
    
            execution:
              onStart:
                - echo ""Preparing for work...""
                - set_trigger_payload stepVariables ""test=true""
                - set_trigger_payload pipelineVariables ""notify=true"" ""version=5.4.3""
                            - export pipelines_poll_interval_seconds=30 # defaults to 10
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""Something went wrong""
                            - echo ""Nested run status is: ${nested_run_status}""
              onComplete:
                - echo ""Cleaning up""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `TriggerPipeline` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Specifies an JFrog Platform Access Token integration. This integration should contain a token that has the appropriate level of access to trigger the specified pipeline.
Required  
`pipelineName`
The name of the pipeline that you want to trigger
Required  
`stepName`
The name of the step that you want to trigger
Required  
`branchName`
The name of the branch that you want to trigger. This should be used if your target pipeline comes from a  multibranch pipeline source.
Optional
May be required  
`projectKey`
The unique key of the Project that contains the pipeline that you want to trigger. This should only be used if your target pipeline belongs to a Project.
Optional
May be required  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation. This is the ideal place to utilize the ` set_trigger_payload` utility function to add custom step- and pipeline-level variables to your target.
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
###### environment variables
The TriggerPipeline step utilizes several new environment variables to provide as much information as possible.
These variables can be used to configure the behavior of the `TriggerPipeline` step
variable name
value  
pipelines_poll_interval_seconds
How frequently, in seconds, the step should poll for its nested run status.
Defaults to 10 seconds  
These variables are injected into the environment of the target pipeline
variable name
value  
parent_step_id
The unique ID of the TriggerPipeline step  
parent_step_name
The name of the TriggerPipeline step  
parent_step_url
The URL that links to the TriggerPipeline step  
parent_run_id
The unique ID of the run that the TriggerPipeline step belongs to  
parent_run_number
The run number of the run that the TriggerPipeline step belongs to  
These variables are exported into the `TriggerPipeline` step environment after the target pipeline's run completes.
variable name
value  
nested_run_status
The final status of the triggered run (can be success, failure, error, canceled, timeout, or skipped)  
nested_run_number
The run number of the triggered run.  
nested_run_duration_seconds
The total duration of the triggered run (in seconds)  
nested_run_url
The URL that links to the triggered step  
##### Examples
The following example shows how to configure a `TriggerPipeline` step.
This example requires a JFrog Platform Access Token integration.
[YAML]
    pipelines:
      - name: top_pipeline
        steps:
          - name: scan_controller
            type: TriggerPipeline
            configuration:
              pipelineName: scanner_pipeline
              stepName: scan_it
              integrations:
                - name: myPlatformToken
              environmentVariables:
                scan_target:
                  default: ""hello-world""
                  allowCustom: true
                  values:
                    - ""vault""
                    - ""redis""
                    - ""postgresql""
                    - ""hello-world""
            execution:
              onStart:
                - set_trigger_payload pipelineVariables ""scan_target=${scan_target}""
                - set_trigger_payload stepVariables ""notify=email"" ""uploadReport=true""
              onComplete:
                - echo ""Final status is $nested_run_status""
    
      - name: scanner_pipeline
        steps:
          - name: scan_it
            type: Bash
            execution:
              onExecute:
                - echo ""Image to scan is $scan_target.""
                - echo ""Triggered by parent step at $parent_step_url""
    
    
[/YAML]
##### How it Works
When you use the **TriggerPipeline** native step in a pipeline, it performs the following functions in the background:
  * GET /projects (if projectKey is provided, the projectId is found)
  * GET /pipelines (find the pipeline by its name)
  * GET /pipelineSteps (find the step by its name)
  * POST /pipelineSteps/:id/trigger (trigger a run starting from the specified step)
  * GET /runs/:id (poll the run to check its status)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
101,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a uploadartifact?[/INST] The **UploadArtifact** native step uploads artifacts to Artifactory. Optionally, it can also publish build information to Artifactory and trigger Xray scans.
This step utilizes the JFrog CLI to upload an artifact to Artifactory. The file(s) may be provided in a FileSpec, if already in Artifactory, or RemoteFile or GitRepo input.
##### YAML Schema
The YAML schema for UploadArtifact native step is as follows:
 **UploadArtifact**
[YAML]
    pipelines: 
      - name:   <string>
        steps:
          - name: <string>
            type: UploadArtifact
            configuration:
              targetPath:           <string>  #required
              sourcePath:           <string>  #optional
              properties:           <string>  #optional
                      regExp:                       <boolean> #optional
              flat:                 <boolean> #optional
                      module:                       <string>  #optional
                      deb:                  <string>  #optional
                      recursive:                <boolean>     #optional
                      dryRun:                       <boolean> #optional
              symlinks:                     <boolean> #optional
              explode:                      <boolean> #optional
              exclusions:               <string>      #optional
              includeDirs:              <boolean>     #optional
              syncDeletes:              <string>      #optional
              forceXrayScan:                <boolean> #optional
              failOnScan:           <boolean>   # default true
                      autoPublishBuildInfo: <boolean> #optional
              inputResources:
                            - name: myGitRepo       
                            - name: artifactoryFileSpec     
                - name: myRemoteFile        
                      outputResources:
                - name: myFileSpec
                            - name: myBuildInfo
              integrations:
                            - name: myArtifactory 
            execution:
              onStart:
                - echo ""Preparing for work...""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
    
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `UploadArtifact` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`integrations`
Must specify an  Artifactory Integration.
Required  
`inputResources`
May specify a GitRepo, FileSpec, or RemoteFile resource containing the file(s) to be uploaded. One of each type may be specified.
Optional  
`outputResources`
Must specify a BuildInfo resource if `autoPublishBuildInfo` is set as `true`.
If `JFROG_CLI_BUILD_NAME` or `JFROG_CLI_BUILD_NUMBER` is set as an environment variable for the pipeline or the step, that name and/or number is used for the output BuildInfo. Otherwise, the default `buildName` and `buildNumber` are `$pipeline_name` and `$run_number.`
May also specify a FileSpec resource to be updated with the pattern and properties of the uploaded Artifact.
May be required  
In addition, these tags can be defined to support the step's native operation:
Tag
 **Description of usage**
Required/Optional  
targetPath
Path to upload the files, including repository name.
Required  
`sourcePath`
Files to upload. If this is a relative path pattern, it is relative to the root of a GitRepo/FileSpec/RemoteFile input.
Default is `*` when `regExp` is `false` and `.*` when `regExp` is `true`.
Optional  
`properties`
Semi-colon separated properties for the uploaded artifact. For example: `myFirstProperty=one;mySecondProperty=two`.
Properties `pipelines_step_name`, `pipelines_run_number`, `pipelines_step_id`, `pipelines_pipeline_name`, `pipelines_step_url`, `pipelines_step_type`, and `pipelines_step_platform` will also be added.
Optional  
`regExp`
When set as `true`, regular expressions are used in other parameters, such as `sourcePath`, instead of wildcards. Expressions must be in parentheses.
Default is `false`.
Optional  
`flat`
When set as `true`, the uploaded files are flattened, removing the directory structure.
Default is `false`.
Optional  
`module`
A module name for the Build Info.
Optional  
`deb`
A `distribution/component/architecture` for Debian packages. If the distribution, component, or architecture includes a / it must be double-escaped, For example: `distribution/my\\\/component/architecture` for a `my/component` component.
Optional  
`recursive`
When set as `false`, do not upload any matches in subdirectories.
Default is true.
Optional  
`dryRun`
When set as `true`, nothing is uploaded.
Default is `false`.
Optional  
`symlinks`
When set as `true`, symlinks matching the other criteria are uploaded.
Default is `false`.
Optional  
`explode`
When set as `true` and the uploaded Artifact is an archive, the archive is expanded.
Default is `false`.
Optional  
`exclusions`
Semi-colon separated patterns to exclude.
Optional  
`includeDirs`
When set as `true`, empty directories matching the criteria are uploaded.
Default is `false`.
Optional  
`syncDeletes`
A path under which to delete any existing files in Artifactory.
Optional  
`forceXrayScan`
When set as `true`, forces an Xray scan after publishing to Artifactory.
Default is `false`.
Optional  
`failOnScan`
When set as `true`, and when the Xray Policy Rule  Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
`autoPublishBuildInfo`
When set as `true`, publishes build info to Artifactory.
Default is `false`.
Optional  
###### execution
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
### Note
`onExecute`, `onStart`, `onSuccess`, `onFailure`, and `onComplete` are reserved keywords. Using these keywords in any other context in your execution scripts can cause unexpected behavior.
##### Examples
The following examples show a few ways in which a UploadArtifact step can be configured.
###### Uploading an Artifact to Another Repository using a FileSpec Resource
The most basic form of UploadArtifact. Uses all default values. This step will download the file matching the FileSpec and upload it to the location in `targetPath`. The optional output FileSpec resource will be updated with the `targetPath` and the default properties added to the uploaded artifact.
 **UploadArtifact**
[YAML]
    pipelines: 
      - name: uploadArtifactPipeline
        steps:
          - name: uploadArtifactStep
            type: UploadArtifact
            configuration:
              targetPath: my-repository/myDirectory/myFile.txt
              integrations:
                - name: myArtifactoryIntegration
              inputResources:
                - name: myInputFileSpec
              outputResources:
                - name: myOutputFileSpec
    
[/YAML]
###### Uploading an Artifact from a RemoteFile Resource
In this example, the input is a RemoteFile resource. Otherwise, this is very similar to the previous example with an input that downloads a file that is then uploaded and an optional FileSpec output updated for the uploaded file.
 **UploadArtifact**
[YAML]
    pipelines: 
      - name: uploadArtifactPipeline
        steps:
          - name: uploadArtifactStep
            type: UploadArtifact
            configuration:
              targetPath: my-repository/myDirectory/myFile.txt
              integrations:
                - name: myArtifactoryIntegration
              inputResources:
                - name: myInputRemoteFile
              outputResources:
                - name: myOutputFileSpec
    
[/YAML]
###### Publish Build Info and Trigger Xray Scan
In this example, build info is published as part of the UploadArtifact step and an Xray scan is triggered.
 **UploadArtifact**
[YAML]
    pipelines: 
      - name: uploadArtifactPipeline
        steps:
          - name: uploadArtifactStep
            type: UploadArtifact
            configuration:
              targetPath: my-repository/myDirectory/myFile.txt
              autoPublishBuildInfo: true
              forceXrayScan: true
              integrations:
                - name: myArtifactoryIntegration
              inputResources:
                - name: myFileSpec
              outputResources:
                            - name: myBuildInfo
    
[/YAML]
##### How it Works
When you use the **UploadArtifact** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (configure JFrog CLI with the integration listed in the yaml)
  * jfrog rt use (configure JFrog CLI to use the config for the integration listed in the yaml)
  * mkdir (create a directory to use as the root of relative paths in the following actions)
  * cp (copy the FileSpec, RemoteFile, or GitRepo files to the new directory, limit one of each input type)
  * jfrog rt upload (upload the Artifact)
  * write_output (update the FileSpec output resource with the uploaded pattern and properties)
  * add_run_variables (save information in run state for future steps to reference)
  * jfrog rt build-collect-env (collect the build environment, preparing for build publish)
  * jfrog rt build-publish (publish the build, only if autoPublishBuildInfo is true)
  * write_output (update the BuildInfo output resource with the published name/number)
  * jfrog rt build-scan (if forceXrayScan is true)
  * add_run_files (adds build info to run state)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
102,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a xrayscan?[/INST] The **XrayScan** native step triggers a JFrog Xray scan for security vulnerabilities and license compliance. If there was a watch created that covers the selected build, Xray will scan the indexed build artifacts.
##### YAML Schema
The YAML schema for XrayScan native step is as follows:
 **XrayScan**
[YAML]
    pipelines:
      - name:   <string>
        steps:
          - name: scan
            type: XrayScan
            configuration:
              #inherits all the tags from bash
              failOnScan:       <boolean>                      # default true
              inputResources:
                - name:         <BuildInfo resource>           # required
                  trigger: false
                      outputResources:
                - name:         <BuildInfo resource>           # optional
    
            execution:
              onStart:
                - echo ""Preparing for work...""
                - echo ""Prepping build environment""
              onSuccess:
                - echo ""Job well done!""
              onFailure:
                - echo ""uh oh, something went wrong""
              onComplete: #always
                - echo ""Cleaning up some stuff""
[/YAML]
##### Tags
###### name
An alphanumeric string (underscores are permitted) that identifies the step.
###### type
Must be `XrayScan` for this step type.
###### configuration
Specifies all configuration selections for the step's execution environment. This step inherits the Bash/ PowerShell step configuration tags, including these pertinent tags:
Tag
 **Description of usage**
Required/Optional  
`inputResources`
Must specify a BuildInforesource . The `buildName` and `buildNumber` are used to trigger the scan.
Required  
`outputResources`
May specify a BuildInforesource. The BuildInfo will be updated with the `buildName`and `buildNumber`of the input BuildInfo resource.
Optional  
In addition, these tags can be defined to support the step's native operation:
### Tags derived from Bash
All native steps derive from the Bash step. This means that all steps share the same base set of tags from Bash, while native steps have their own additional tags as well that support the step's particular function. So it's important to be familiar with the Bash step definition, since it's the core of the definition of all other steps.
Tag
 **Description of usage**
Required/Optional  
`failOnScan`
When set to `true`, and when the Xray Policy Rule Fail Build checkbox is checked, a failed Xray scan will result in a failure of the step.Creating Xray Policies and Rules
Default is `true`.
Optional  
 **execution**
Declares collections of shell command sequences to perform for pre- and post-execution phases:
Tag
 **Description of usage**
Required/Optional  
`onStart`
Commands to execute in advance of the native operation
Optional  
`onSuccess`
Commands to execute on successful completion
Optional  
`onFailure`
Commands to execute on failed completion
Optional  
`onComplete`
Commands to execute on any completion
Optional  
The actions performed for the `onExecute` phase are inherent to this step type and may not be overridden.
### Note
`onExecute`, `onStart`, `onSuccess`, `onFailure`, and `onComplete` are reserved keywords. Using these keywords in any other context in your execution scripts can cause unexpected behavior.
##### Examples
The following examples show how to configure a XrayScan step.
###### Trigger an Xray Scan for Build Name and Number
Triggers an Xray scan for the build name and number provided by an input BuildInfo resource.
  * This example require a GitHub Integration and an Artifactory Integration.GitHub Integration
  * The Pipelines DSL for this example is available in t his repository in the JFrog GitHub account.


 **XrayScan**
[YAML]
    template: true   # required for local templates
    valuesFilePath: ./values.yml
    
    resources:
      - name: app_repo
        type: GitRepo
        configuration:
          gitProvider: {{ .Values.gitIntegration }}                      
          path: {{ .Values.gitRepositoryPath }}    
          branches:
            include: master
    
      - name: app_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
      - name: app_promoted_buildinfo
        type: BuildInfo
        configuration:
          sourceArtifactory: {{ .Values.artifactoryIntegration }}
    
    pipelines:
      - name: app_dev_pipeline
        steps:
          - name: app_build
            type: DockerBuild
            configuration:
              affinityGroup: docker_group
              dockerFileLocation: .
              dockerFileName: Dockerfile
              dockerImageName: {{ .Values.artifactoryUrl }}/{{ .Values.sourceRepository }}/{{ .Values.imageName }}  
              dockerImageTag: ${run_number}
              inputResources:
                - name: app_repo
              integrations:
                - name: {{ .Values.artifactoryIntegration }}         
                
          - name: app_push
            type: DockerPush
            configuration:
              affinityGroup: docker_group
              targetRepository: {{ .Values.sourceRepository }}
              integrations:
                - name: {{ .Values.artifactoryIntegration }}                            
              inputSteps:
                - name: app_build
    
          - name: publish_app_build
            type: PublishBuildInfo
            configuration:
              affinityGroup: docker_group
              inputSteps:
                - name: app_push
              outputResources:
                - name: app_buildinfo
                
          - name: xrayScanStep
            type: XrayScan
            configuration:
              inputResources:
                - name: app_buildinfo 
[/YAML]
###### Update Output BuildInfo Resource
Updates an output BuildInfo resource that can be used as an input to future steps. Using an output resource means that you can be sure that those versions have been scanned when using that resource as an input.
 **XrayScan**
[YAML]
    pipelines: 
      - name: xrayScanPipeline
        steps:
          - name: xrayScanStep
            type: XrayScan
            configuration:
              inputResources:
                - name: inputBuildInfo
              outputResources:
                - name: outputBuildInfo
[/YAML]
##### How it Works
When you use the **XrayScan** native step in a pipeline, it performs the following functions in the background:
  * jfrog rt config (if there is a BuildInfo output, configure the JFrog CLI with the credentials of the input BuildInfo resource)
  * jfrog rt use (specify the configured credentials to use)
  * jfrog rt build-scan (trigger a scan)
  * replicate_resource (if there is an output BuildInfo, copy the properties of the input BuildInfo resource to the output)
  * write_output (if there is an output BuildInfo, update the output BuildInfo resource buildName and buildNumber)



 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
103,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a pipelines-utility-functions?[/INST] The **Utility Functions** are built-in shell functions that can be used in steps to interact with the runtime environment.
Most utility functions are available in both Bash (Linux) and Powershell (Windows) OS runtimes.
### See it Live
Click  **here**  to see some of these utility functions in action.
#### bump_semver
 **Description**
Increments the provided semver version with the given action.
 **Usage**
[YAML]
    bump_semver <semver string> <action>
    
    
[/YAML]
  * `semver string` is the semver version to be incremented
  * `action` is the type of increment to be applied


The valid actions are:
  * major: Increment the major version. The minor and patch versions are reset to 0.
  * minor: Increment the minor version. The patch version is reset to 0.
  * patch: Increment the patch version.
  * alpha: Increment or add an `alpha` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-alpha` and `v1.1.1-alpha` becomes `v1.1.1-alpha.1`. Any other pre-release tags will be removed.
  * beta: Increment or add a `beta` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-beta` and `v1.1.1-beta` becomes `v1.1.1-beta.1`. Any other pre-release tags will be removed.
  * rc: Increment or add an `rc` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-rc` and `v1.1.1-rc`becomes `v1.1.1-rc.1`. Any other pre-release tags will be removed.
  * final: Remove any pre-release tags, leaving `major.minor.patch`.


Click for examples
`ubuntu:~$ bump_semver v1.0.0 major`
`v2.0.0`
`ubuntu:~$ bump_semver v1.0.0 minor`
`v1.1.0`
`ubuntu:~$ bump_semver v1.0.0 patch`
`v1.0.1`
`ubuntu:~$ bump_semver v1.0.0 rc`
`v1.0.0-rc`
`ubuntu:~$ bump_semver v1.0.0-rc rc`
`v1.0.0-rc.1`
`ubuntu:~$ bump_semver v1.0.0 alpha`
`v1.0.0-alpha`
`ubuntu:~$ bump_semver v1.0.0 beta`
`v1.0.0-beta`
`ubuntu:~$ bump_semver v1.0.0-rc.12 final`
`v1.0.0`
`ubuntu:~$ bump_semver v1. rc`
`error: Invalid semantic version given in the argument.`
`ubuntu:~$ bump_semver v1.0.0 badaction`
`error: Invalid action given in the argument.`
#### replace_envs
 **Description**
Replaces variables in a file with values based on your current shell `env`. This is useful to create config files from templates, for example.
If the file contains placeholders that are not defined in the environment, they will become empty strings (“”). The original file is overwritten with the modified file.
 **Usage**
[YAML]
    replace_envs <filename1> <filename2> <filenameN>
    
                         
[/YAML]
where your files have placeholders in the format `$ENVIRONMENT_VARIABLE_NAME` or `${ENVIRONMENT_VARIABLE_NAME}`.
Click for examples
`replace_envs properties.json deploy.json run.json`
#### retry_command
 **Description**
Execute any command up to three times if it returns a non-zero error code. This is useful when you need to execute a command that can be flaky as a result of network hiccups, for example.
 **Usage**
Shell
Function  
Bash
`retry_command <shell command>`  
PowerShell
`retry_command <shell command>`  
  * `shell command `is the command to be retried


Click for Bash example
`retry_command docker push mydirectory/myImage`
Click for Powershell example
`retry_command docker push mydirectory/myImage`
#### get_uuid
 **Description**
Puts a uuid to stdout. Uses `/proc/sys/kernel/random/uuid` if available and falls back to `uuidgen` if not. The function calls `exit 1` if neither of these are available.
 **Usage**
Shell
Function  
Bash
`get_uuid`  
Click for Bash example
`my_uuid=$(get_uuid) `
#### save_artifact_info
 **Description**
Saves metadata about an artifact. When saved, this metadata is used to enable signed pipelines for the artifacts.
 **Usage**
Shell
Function  
Bash
`save_artifact_info <artifact type> <file path> [--build-name <build name> --build-number <build number> --project-key <project key>]`  
PowerShell
`save_artifact_info <artifact type> <file path> [-build-name <build name> -build-number <build number> -release-bundle-name <name> -release-bundle-version <version> -project-key <project key>]`  
  * `artifact type` : This is the type of artifact. Either `file`, `buildInfo`, or `releaseBundle`.
  * `file path` : This is the path to the metadata file to be saved.


Bash
PowerShell
Description  
 **\--build-name**
 **-build-name**
This is name of the build. Required when artifact type is `buildInfo`.  
 **\--build-number**
 **-build-number**
This is number of the build. Required when artifact type is `buildInfo`.  
 **\--release-bundle-name**
 **-release-bundle-name**
This is name of the Release Bundle. Required when artifact type is `releaseBundle`.  
 **\--release-bundle-version**
 **-release-bundle-version**
This is version of the Release Bundle. Required when artifact type is `releaseBundle`.  
 **\--project-key**
 **-project-key**
(optional) Defaults to the environment's `project_key`. Can be specified to save info for a different project.  
Click for Bash example
`save_artifact_info 'buildInfo' './myBuildDetailedSummary.json' --build-name myBuild --build-number 42 `
`save_artifact_info 'file' './myFileDetailedSummary.json' `
Click for PowerShell example
`save_artifact_info buildInfo myBuildDetailedSummary.json -build-name myBuild -build-number 42 `
`save_artifact_info file myFileDetailedSummary.json `
`save_artifact_info releaseBundle myFileDetailedSummary.json -release-bundle-name myBundle -release-bundle-version 1 `
#### validate_artifact
 **Description**
Validates the signature of an artifact. Requires signed pipelines to be enabled.
 **Usage**
Shell
Function  
Bash
`validate_artifact <artifact type> <file path> [--build-name <build name> --build-number <build number> --project-key <project key>]`  
PowerShell
`validate_artifact <artifact type> <file path> [-build-name <build name> -build-number <build number> -project-key <project key>]`  
  * `artifact type`: This is the type of artifact. Supports `buildinfo`.
  * `file path`: This is the path to the metadata file to be validated.


Bash
PowerShell
Description  
 **\--build-name**
 **-build-name**
This is name of the build. Required when artifact type is `buildInfo`.  
 **\--build-number**
 **-build-number**
This is number of the build. Required when artifact type is `buildInfo.`  
 **\--project-key**
 **-project-key**
(optional) Defaults to the environment's `project_key`. Can be specified to validate info for a different project.  
Click for Bash example
`validate_artifact 'buildInfo' './myBuildDetailedSummary.json' --build-name myBuild --build-number 42 `
Click for PowerShell example
`validate_artifact buildInfo myBuildDetailedSummary.json -build-name myBuild -build-number 42 `
#### configure_jfrog_cli
 **Description**
Configures the JFrog CLI (version 1 or 2 specified as `jfrogCliVersion` in the  pipeline configuration) with the provided credentials, handling the different formats for different minor versions. Artifactory integrations listed in the integrations section of the step will be automatically configured, but this may be useful for resources or if the credentials are provided to the step in another way. When using v2, a non-default Xray URL may also be specified using the environment variable JFROG_XRAY_URL.
 **Usage**
Bash
`configure_jfrog_cli --artifactory-url <url> [--xray-url <url> --user USER --apikey <key> --access-token <token> --server-name <name>]`  
PowerShell
`configure_jfrog_cli -artifactory-url <url> [-xray-url <url> -user USER -apikey <key> -access-token <token> -server-name <name>]`  
  * `artifactory-url` : Required. The Artifactory URL.
  * `xray-url` : Optional. The Xray URL. Only used with CLI v2.
  *  **user** : The user. Required when an API key is provided.
  * `apikey` : An API key. Requires --user and may not be used with --access-token.
  * `access-token` : An Access token. May not be used with --access-token or --user.
  * `server-name` : Defaults to `default`. Can be specified to configure the CLI with that name.


Click for Bash example
`configure_jfrog_cli --artifactory-url  https://my-artifactory.local/artifactory --user me --apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R --server-name myAPIKeyArtifactory`
`configure_jfrog_cli --artifactory-url  https://my-artifactory.local/artifactory --access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg --server-name myAccessTokenArtifactory`
Click for PowerShell example
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -user me -apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R -server-name myAPIKeyArtifactory`
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg -server-name myAccessTokenArtifactory`
#### check_xray_available
 **Description**
With JFrog CLI v2 (version 2 specified as `jfrogCliVersion` in the  pipeline configuration), checks that Xray is available for the specified CLI configuration, or the default configuration if none is specified.
 **Usage**
Bash
`check_xray_available [--server-name <name>]`  
PowerShell
`check_xray_available [-server-name <name>]`  
  * `server-name` : Optional. Specifies a JFrog CLI configuration to check.


Click for Bash example
`check_xray_available --server-name myArtifactoryIntegration`
Click for PowerShell example
`check_xray_available --server-name myArtifactoryIntegration`
#### cleanup_jfrog_cli
 **Description**
Removes configuration for the JFrog CLI (v1), handling the different formats for different minor versions. Artifactory integrations listed in the integrations section of the step will be automatically removed at the end of the step, but this may be useful to remove the credentials earlier or when using `configure_jfrog_cli`.
 **Usage**
Bash
`cleanup_jfrog_cli [--server-name <name>]`  
PowerShell
`cleanup_jfrog_cli [-server-name <name>]`  
  * `server-name` : Defaults to `default`. Can be specified to remove that configuration.


Click for Bash example
`cleanup_jfrog_cli --server-name myArtifactory`
Click for PowerShell example
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -user me -apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R -server-name myAPIKeyArtifactory`
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg -server-name myAccessTokenArtifactory`
#### set_trigger_payload
 **Description**
Adds one or more key=value pairs as custom step or pipeline variables to a JSON payload that can be used to call the pipelines trigger API.
Multiple key=value pairs can be given in a single command or can be split across multiple commands.
If a key is added twice, the original value will be replaced.
 **Usage**
Bash
`set_trigger_payload [stepVariables|pipelineVariables] <key value pairs>`  
PowerShell
Not currently supported in PowerShell  
  * `stepVariables`: These key value pairs will be present in the environment of the triggered step
  *  **pipelineVariables** : These key value pairs will be present in the environment of every step in the run that is triggered.


Click for Bash example
`set_trigger_payload stepVariables ""shouldScan=true"" ""notify=email""`
`set_trigger_payload pipelinesVariables ""releaseVersion=1.2.3""`
`set_trigger_payload pipelinesVariables ""collectData=true""`
#### get_trigger_payload
 **Description**
Prints on stdout a payload that can be used with the pipelines trigger API.
Use `set_trigger_payload` to add additional values to the payload.
 **Usage**
Bash
`get_trigger_payload `  
PowerShell
Not currently supported in PowerShell  
Click for Bash example
curl -XPOST -H ""authorization: Bearer <token>"" -H ""content-type: application/json"" <endpoint>/pipelines/api/v1/pipelineSteps/:id/trigger -d ""$(get_trigger_payload)""
#### end_step
 **Description**
Stops execution of the onStart or onExecute section and immediately continues to the onSuccess, onFailure, or onComplete section setting the specified status. If ""success"" is provided, the onSuccess and onComplete sections will be executed. For ""failure,"" the onFailure and onComplete sections will be executed. And for ""skipped,"" only the onComplete section. The end_step utility function may only be called in the onStart and onExecute sections and the only statuses supported are success, failure, and skipped.
 **Usage**
Bash
`end_step [success|failure|skipped]`  
PowerShell
`end_step [success|failure|skipped]`  
Click for Bash example
`end_step success`
Click for PowerShell example
`end_step success`
#### update_run_description
 **Description**
Provide dynamic description for a run, which will be shown in the UI. This is useful for providing more context about a run. Every step in a pipeline can include a run description.
 **Usage**
Bash
`update_run_description ""description""`  
PowerShell
`update_run_description ""description""`  
Click for Bash example
`update_run_description ""$msg""`
Click for PowerShell example
`update_run_description ""$msg""`
Full YAML Example
[YAML]
    name: UpdateRunDescription
    steps:
            - name: update_l
              type: Bash
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
            - name: update_2
              type: Bash
              configuration:
                    inputSteps:
                            - name: update_l
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
[/YAML]
#### set_run_name
 **Description**
Provide a unique, dynamic name for a run, which will be shown in the UI. This is useful for providing more context about a run. Unlike run description, run name is unique for a run.
In the UI, run name appears in the Run View:  
Run name appears also appears in Pipeline of Pipelines view and Active Board view:  
 **Usage**
Bash
`set_run_name ""1.0.1""`  
PowerShell
`set_run_name ""1.0.1""`  
Click for Bash example
`set_run_name ""1.0.1""`
Click for PowerShell example
`set_run_name ""1.0.1""`
Full YAML Example
[YAML]
    name: UpdateRunDescription
    steps:
            - name: update_l
              type: Bash
              execution:
                    onStart:
                            - set_run_name ""1.0.1""
            - name: update_2
              type: Bash
              configuration:
                    inputSteps:
                            - name: update_l
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
[/YAML]
#### Source Control
#### Test Reports
##### save_tests
 **Description**
Copies test reports given as input to later be parsed and uploaded (if file storage is available).
 **Usage**
`save_tests <file or directory>`
  * `file` or `directory`specifies either a filename for the test report file, or a directory name for a directory of test report files


Click for examples
`save_tests testreport.xml`
#### Encryption
##### encrypt_string
 **Description**
Uses the provided public key to encrypt the specified string.
 **Usage**
Shell
Function  
Bash
`encrypt_string --key <path> <source string> `  
PowerShell
`encrypt_string -key <path> <source string>`  
  * `key` is the fully qualified path of the public key file
  * `source string` is the string to be encrypted


Click for Bash examples
`ubuntu:~$ encrypt_string --key pub.pem""admin:passw0rd"" LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+ 06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6t dVK0X/5pCTWBKk5nIvw=`
Click for Powershell examples
`PS C:\Users\windowsuser> encrypt_string ""admin:passw0rd"" -key pub.pem LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+ 06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6t dVK0X/5pCTWBKk5nIvw=`
##### decrypt_string
 **Description**
Uses the provided private key to decrypt the specified string.
This is typically used to decrypt information that was encrypted using `encrypt_string` with the corresponding public key. It helps you avoid building your own encrypt-decrypt system.
 **Usage**
Shell
Function  
Bash
`decrypt_string --key <path> <encrypted string>`  
PowerShell
`decrypt_string -key <path> <encrypted string> `  
  * `key` is the fully qualified path of the private key file
  * `encrypted string` is the string to be decrypted


Click for Bash examples
ubuntu:~$ decrypt_string --key key.pem LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=
`admin:passw0rd`
`ubuntu:~$ decrypt_string LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=`
`decrypt_string: ERROR - Key file not found`
Click for Powershell examples
`PS C:\Users\windowsuser> decrypt_string -key key.pem LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=`
`admin:passw0rd`
`PS C:\Users\windowsuser> decrypt_string LbuD69yxEC6wi1M2B+/06ZY0vS+VcahcWeHovWi8GnUC04zBFrFXBmkAbG4TWRRvwbROgSsj2fo+06SsaSPnb8fZDKuFP6Z89yJnLsh8UCq3gUbvFcGtgTQHUzIHQ0PVfrKrk9IyIvJex6+0ZPkVqa6tdVK0X/5pCTWBKk5nIvw=`
`key is mandatory, please provide a value.`
##### encrypt_file
 **Description**
Uses the provided public key to encrypt the specified file to a new file.
 **Usage**
Shell
Function  
Bash
`encrypt_file --key <path> [--output <filename>] <source filename> `  
PowerShell
`encrypt_file -key <path> [-output <filename>] <source filename>`  
  * `key` is the fully qualified path of the public key file
  * `output` is the name of the resulting encrypted file. Defaults to “encrypted”
  * `source filename` is the file to be decrypted


Click for Bash examples
`ubuntu:~$ cat secrets.json {""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""} ubuntu:~$ encrypt_file secrets.json --key pub.pem encrypt_file: Encrypting secrets.json using key pub.pem encrypt_file: Encrypted secrets.json to encrypted ubuntu:~$ cat encrypted dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+ 5xHBCvlenuFanIFLDbo=`
Click for Powershell examples
`PS C:\Users\windowsuser> type secrets.json {""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""} PS C:\Users\windowsuser> encrypt_file secrets.json -key pub.pem encrypt_file: Encrypting secrets.json using key pub.pem encrypt_file: Encrypted secrets.json to encrypted PS C:\Users\windowsuser> type encrypted dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+ 5xHBCvlenuFanIFLDbo=`
##### decrypt_file
 **Description**
Uses the provided private key to decrypt the specified file to a new file.
This is typically used to decrypt information that was encrypted using `encrypt_file` with the corresponding public key. It helps you avoid building your own encrypt-decrypt system.
 **Usage**
Shell
Function  
Bash
`decrypt_file --key <path> [--output <filename>] <source filename>`  
PowerShell
`decrypt_file -key <path> [-output <filename>] <source filename> `  
  * `key` is the fully qualified path of the private key file
  * `output` is the name of the resulting decrypted file. Defaults to “decrypted”
  * `source filename` is the file to be decrypted


Click for Bash examples
`ubuntu:~$ cat encrypted`
`dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ`
`YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+`
`5xHBCvlenuFanIFLDbo=`
`ubuntu:~$ decrypt_file encrypted --key key.pem`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to decrypted`
`ubuntu:~$ cat decrypted`
`{""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""}`
`ubuntu:~$ decrypt_file encrypted --key key.pem --output secrets.json`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to secrets.json`
Click for Powershell examples
`PS C:\Users\windowsuser> type encrypted`
`dGpLiI2IORfkRpSEhhuN9A8U/dQLlyHD6EKermRM5bnoIBcM7TWpLU3Y53f4zsAKTSmmQKHucPxJ`
`YFOVc6F1AWItYIgTSp2dEY4ugIZ7uTn/IIa0qU7EGUREtPyrdu9N5phS2UybTn0u80CSP7Bf/HF+`
`5xHBCvlenuFanIFLDbo=`
`PS C:\Users\windowsuser> decrypt_file encrypted -key key.pem`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to decrypted`
`PS C:\Users\windowsuser> type decrypted`
`{""TOP_SECRET_CONFIDENTIAL_FORMULA"": ""uuddlrlrbas""}`
`PS C:\Users\windowsuser> decrypt_file encrypted -key key.pem -output secrets.json`
`decrypt_file: Decrypting encrypted using key key.pem`
`decrypt_file: Decrypted encrypted to secrets.json`
#### Notifications
##### send_notification
 **Description**
Utilizes notification integration to send custom messages at any time during the build to any recipient.
For more information, see Sending Notifications from Pipelines.
 **Usage**
[YAML]
    send_notification <integration> [options]
    
    
[/YAML]
The options can be specified as part of the command, or defined as environment variables before the command is issued.
The command line arguments take priority over the environment variables.
Click for Bash examples
`- send_notification pipelines_mqtest_slack --text ""Pipeline - $pipeline_name | Step - $step_name \n Committer - <$committer> ""set_payloadmyExternalHookResource""{\""test\"":\""payload\""}""`
###### AirBrake
Creates an AirBrake deployment through an Airbrake Integration. Not supported in PowerShell.
Bash
Option Description  
 **\--project-id**
the project ID to send the notification for  
 **\--environment**
the environment value to use when posting the deployment  
 **\--username**
used when posting an AirBrake deployment  
 **\--email**
the email to be used when posting the AirBrake deployment  
 **\--repository**
the repository to use when posting the AirBrake deployment  
 **\--revision**
the deployment revision  
 **\--version**
the version to use when posting the AirBrake deployment  
 **\--type**
currently only type “deploy” is supported  
 **\--description**
description of the deployment  
 **\--payload**
path to a valid JSON file that contains a payload to use to POST the AirBrake deployment  
###### Jira
Creates a Jira issue (also known as a ticket).
Bash
PowerShell
Option Description  
 **\--project-id**
 **-project-id**
the Project Key of the project to associate the new issue with. The project key is the short string that begins all issue numbers for the project (e.g., ""EXAMPLE-1234"")  
 **\--type**
 **-type**
the issue type for the new issue (e.g., ""Bug"", ""Task"", etc.). This string must be one of the recognized Jira issue types  
 **\--summary**
 **-summary**
a string for the new issue's Summary field (it's title)  
 **\--description**
 **\--description**
(optional) a string for the new issue's Description field  
 **\--attach-file**
 **\--attach-file**
(optional) a path to a file that you’d like to attach to the issue  
###### NewRelic
Creates a NewRelic deployment through a NewRelic Integration. Not supported in PowerShell.
Bash
Option Description  
 **\--type**
the type of object to be posted. At the moment, only “deployment” is supported  
 **\--description**
description of the deployment  
 **\--username**
the user recording the deployment. Defaults to “JFrog Pipelines”  
 **\--changelog**
the changelog value to use in the deployment  
 **\--revision**
the deployment revision (required)  
 **\--appId**
the ID of the app being deployed. If not provided, --appName must be present  
 **\--appName**
the name of the app being deployed. If not provided, --appId must be present  
 **\--payload**
path to a valid JSON file that contains a payload to use to POST the NewRelic deployment  
###### PagerDuty Events
Sends an event through a PagerDuty Events Integration.
Bash
PowerShell
Option Description  
 **\--text**
 **-text**
The main text to display in the event on PagerDuty.  
Slack
Sends a message on Slack through a Slack Integration.
Bash
PowerShell
Option Description  
 **\--payload**
 **-payload**
(optional) A path to a valid json file to act as the payload of the message. If a payload is provided, all other parameters are ignored. This payload is directly sent to Slack, so please view the Slack API documentation for information on how the payload should be formatted.  
 **\--username**
 **-username**
(optional) shows in the heading of the Slack message  
 **\--pretext**
 **-pretext**
(optional) a string that becomes the first part of the Slack message. Defaults to current date/time  
 **\--text**
 **-text**
(optional) the main text to display in the message.  
 **\--color**
 **-color**
(optional) hex string that changes the color of the status bar to the left of the Slack message.  
 **\--recipient**
 **-recipient**
(optional) the target of the message. Should start with “@” or “#” for user or channel, respectively.  
 **\--icon-url**
 **-icon-url**
(optional) the url of the icon to show next to the message  
###### smtpCreds (email)
Sends an email through an SMTP Credentials Integration.
Bash
PowerShell
Option Description  
 **\--recipients**
 **-recipients**
one or more email addresses  
 **\--subject**
 **-subject**
(optional) add a message to the subject. Does not replace the default subject  
 **\--body**
 **-body**
(optional) specify some text to add to the body of the email. Does not replace the existing body information  
 **\--status**
 **-status**
(optional) can be set to a valid status string. By default it will be set based on the section of scrip the command is executed in.  
 **\--attachments**
 **-attachments**
(optional) a list of files to attach to the email. Combined total of all files cannot exceed 5MB  
 **\--attach-logs**
 **-attach-logs**
(optional) ‘true’ or ‘false’. Defaults to false. All available logs for the step will be attached to the email. Note that it can only attach logs that have already been created, so using this option in the onStart section, for example, would not have very detailed logs.  
 **\--show-failing-commands**
 **-show-failing-commands**
(optional) 'true’ or ‘false’. Defaults to false. The existing logs for the step will be parsed. Any failed command that is detected will be added to the body of the email, along with up to 100 preceding lines (if printed from the same command)  
Environment Options
All of the above options can also be included as environment variables instead of arguments. The command line argument will have priority over the environment. Here is the full list of ENVs:
  * `NOTIFY_USERNAME` (--username/-username)
  * `NOTIFY_PASSWORD` (--password/-password)
  * `NOTIFY_RECIPIENT` (--recipient/-recipient)
  * `NOTIFY_PRETEXT` (--pretext/-pretext)
  * `NOTIFY_TEXT` (--text/-text)
  * `NOTIFY_COLOR` (--color/-color)
  * `NOTIFY_ICON_URL` (--icon-url/-icon-url)
  * `NOTIFY_PAYLOAD` (--payload/-payload)
  * `NOTIFY_TYPE` (--type/-type)
  * `NOTIFY_PROJECT_ID` (--project-id/-project-id)
  * `NOTIFY_ENVIRONMENT` (--environment/-environment)
  * `NOTIFY_REVISION` (–revision/-revision)
  * `NOTIFY_SUMMARY` (--summary/-summary)
  * `NOTIFY_ATTACH_FILE` (--attach-file/-attach-file)
  * `NOTIFY_REPOSITORY` (--repository/-repository)
  * `NOTIFY_EMAIL` (–email--email/-email)
  * `NOTIFY_STATUS` (--status/-status)
  * `NOTIFY_VERSION` (--version/-version)
  * `NOTIFY_CHANGELOG` (--changelog/-changelog)
  * `NOTIFY_DESCRIPTION` (–description--description/-description)
  * `NOTIFY_ATTACHMENTS` (–attachments--attachments/-attachments)
  * `NOTIFY_ATTACH_LOGS` (--attach-logs/-attach-logs)
  * `NOTIFY_SHOW_FAILING_COMMANDS` (--show-failing-commands/-show-failing-commands)
  * `NOTIFY_SUBJECT` (--subject/-subject)
  * `NOTIFY_BODY` (–body)


#### JSON
##### set_payload
 **Description**
Sets an optional JSON payload (string or file) for an OutgoingWebhook resource. When the OutgoingWebhook is specified in a step's `outputresources` the payload is sent when the step is complete.
 **Usage**
Shell
Function  
Bash
`set_payload <resource> <payload> [--file]`  
PowerShell
`set_payload <resource> <payload> [-file]`  
  * `resource` is the name of an OutgoingWebhook resource.
  * `payload` is a JSON string or file to attach to the resource that will be sent as part of the outgoing webhook. A file can be specified as a path relative to the current directory, absolute path, or path relative to the step workspace directory.
  * `file` option specifies that the `payload` parameter is a file. If not specified, `payload` will be processed as a string.


Click for Bash examples
`set_payloadmyExternalHookResource""{\""test\"":\""payload\""}""`
`echo""{\""test\"":\""payload\""}""> testpayload.json set_payload myExternalHookResource testpayload.json --file`
Click for Powershell examples
`set_payloadmyExternalHookResource""{`""test`"":`""payload`""}""`
`Set-Content -Path testpayload.json -Value ""{`""test`"":`""payload`""}"" set_payload myExternalHookResource testpayload.json -file`
##### read_json
 **Description**
Extracts the json property value from the specified file.
This simplifies handling of a JSON file to read specific property values that are required for your workflow.
### Not supported in PowerShell
In PowerShell, `ConvertTo-Json` is suggested as an alternative.
 **Usage**
[YAML]
    read_json <path to file> <field name>
    
    
[/YAML]
  * `path to file` is the fully qualified path of the JSON file
  * field name is the field for which you want to read the value. Use dot notation and [n] for arrays.


Click for examples
ubuntu:~$ cat secrets.json
{
""TOP_SECRET_CONFIDENTIAL_FORMULA"":""uuddlrlrbas"",
""LESSER_SECRET_FORMULA"":""dyddy"",
""TOP_5_PASSWORDS_LIST"": [
""admin"",
""passw0rd"",
""testing123"",
""correcthorsebatterystaple""
],
“nesting”: {
“is”: {
“fun”: “yay”
}
}
}
ubuntu:~$ read_json secrets.json""LESSER_SECRET_FORMULA""
dyddy
ubuntu:~$ read_json secrets.json""TOP_5_PASSWORDS_LIST[3]""
correcthorsebatterystaple
ubuntu:~$ read_json secrets.json "" nesting.is.fun""
yay
#### Resources
##### replicate_resource
 **Description**
This command takes an input resource and creates an exact copy. This helps you to transfer metadata from one step to the next.
 **Usage**
Shell
Function  
Bash
`replicate_resource <from_resource> <to_resource> [--options]`  
PowerShell
`replicate_resource <from_resource> <to_resource> [-options] `
[/YAML]
[YAML]  * `from_resource` is the name of the `inputResources` resource that you're copying from.  
  * `to_resource` is the name of the `outputResources` resource that will receive the replicated data from the `from_resource`.Any pre-existing files or key-value pairs in the `to_resource` will be replaced.
  * `match-settings` option should be set when you want the replication to adhere to any branch/tag settings in the `to_resource`. For example, If your `from_resource` gitRepo can trigger on both commits and pull requests, but you only want to update your `to_resource` on commits, you can replicate with --match-settings, and the `to_resource` will only be updated when the `from_resource` had a commit.


Click for Bash examples
`replicate_resource myRepo1 myRepo2 --match-settings replicate_resource myTestImage myStageImage`
Click for Powershell examples
`replicate_resource myRepo1 myRepo2 -match-settings replicate_resource myTestImage myStageImage`
##### write_output
 **Description**
Adds data to an output resource in the form of key/value pairs that will become properties of the resource.
 **Usage**
Bash
`write_output <resource> <key value pair>... [--overwrite]`  
PowerShell
`write_output <resource> <key value pair>... [-overwrite]`  
  * `resource` ``is the resource to update
  * `key value pair` is a single string with a key and a value, separated by an “=”. Multiple of these strings can be supplied as input. A value with spaces should be surrounded by quotes.


Bash
Powershell
Option Description  
 **\--overwrite**
 **-overwrite**
If supplied, all key value pairs will be replaced.  
Click for examples
`write_output myImageimageTag=master sha=$commitSha description=\""hello world\""`
The newly attached properties can be accessed as environment variables of the form `res_{Resource Name}_{Key Name}`.
For example, the above created properties can be accessed as these environment variables:
[YAML]
    $ printenv res_myImage_master
    master
    $ printenv res_myImage_sha
    d6cd1e2bd19e03a81132a23b2025920577f84e37
    $ printenv res_myImage_description
    ""hello world""
[/YAML]
#### Caching
Caching helps you speed up execution of your steps by preserving and restoring packages and dependencies between runs of a step. In this way, you can reduce build times by avoiding repeating the installation or loading of large dependencies.
##### add_cache_files
 **Description**
Copies files given as input to later be uploaded if file storage is available.
For more information about using this utility function, see Caching Step Runtimes.
 **Usage**
`add_cache_files <file or directory> <name>`
  * `file or directory` is a file or directory to store in the cache
  * `name` is a name to give the stored file or directory (without spaces)


Click for examples
`add_cache_files cachefile.txt my_file`
`add_cache_files directory/subdirectory my_directory`
##### restore_cache_files
 **Description**
Copies stored cache (if file storage is available) to the specified location. No error will occur if nothing is available for <name> in the cache.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`restore_cache_files <name> <path>`
  * `name` is the name the file or driectory to be restored was given when cached.
  * `path` is a path at which to place the file or directory.


Click for examples
`restore_cache_files my_file cachefile.txt`
`restore_cache_files my_directory directory/subdirectory`
#### Run State Management
##### add_run_variables
 **Description**
Allows you to add environment variables that will be available in the following steps of the run.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


 **Usage**
`add_run_variables <key value pair>...`
  * `key value pair` ``is a single string with a key and a value, separated by an “=”. Multiple of these strings can be supplied as input. Each value will be exported as an environment variable at the time this command is used and automatically in any later steps within the run.


Click for examples
add_run_variables imageTag=""master""
##### export_run_variables
 **Description**
Sources the file containing the run variables. This will be done automatically, but may also be used to “reset” the environment variables in the current step.
 **Usage**
`export_run_variables`
Click for examples
`export_run_variables`
##### add_run_files
 **Description**
Copies files given as input into the run state for use in later steps in the run, if file storage is available.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`add_run_files <file or directory> <name>`
`file or directory` is a file or directory to store in the run state
`name` is a name to give the stored file or directory (without spaces). This cannot be run.env.
Click for examples
`add_run_files cachefile.txt my_file`
`add_run_files directory/subdirectory my_directory`
`add_run_files directory/*/subdirectory my_directory`
##### restore_run_files
 **Description**
Copies files stored in the run state (if file storage is available) to the specified location. No error will occur if nothing is available for <name> in the run state.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`restore_run_files <name> <path>`
  * `path` is the name the files to be restored were given when added to the run state.
  * `file or directory` ``is a path at which to place the file or files.


Click for examples
`restore_run_files my_file cachefile.txt`
`restore_run_files my_directory directory/subdirectory`
#### Affinity Group State Management
##### add_affinity_group_files
 **Description**
Copies files or directories given as input into the affinity group workspace for use in later steps in the affinity group. Files or directories may be specified using wildcards (*) or as multiple input parameters preceding the name under which the files will be stored.
 **Usage**
`add_affinity_group_files <file or directory> <name>`
`file or directory` is a file or directory to store in the run state.
`name` ``is a name to give the stored file or directory (without spaces).
Click for examples
`add_affinity_group_files cachefile.txt my_file`
`add_affinity_group_files myDirectory myOtherDirectory/subdirectory my_directories`
`add_affinity_group_files myDirectory/*.txt my_text_files`
`add_affinity_group_files ../adjacentDirectory my_adjacent_directory`
##### restore_affinity_group_files
 **Description**
Copies files stored in the affinity group workspace by `add_affinity_group_files` to the specified location. The restore behavior differs from the other state restore functions to better handle wildcard patterns with a variable number of matching files or directories. With `restore_affinity_group_files`, the relative path specified in the `add_affinity_group_files` function will be preserved (excluding any traversal to parent directories) and the path specified should be the base for that relative path. The following examples are to symmetrically restore the files saved in the examples above for `add_affinity_group_files.` No error will occur if no files were saved for <name>.
 **Usage**
`restore_affinity_group_files <name> <path>`
  * `name` is the name the files to be restored were given when storing the files with `add_affinity_group_files`.
  * `path` ``is a path at which to place the file or files.


Click for examples
`restore_affinity_group_files my_file .`
`restore_affinity_group_files my_directories .`
`restore_affinity_group_files my_text_files .`
`restore_affinity_group_files my_adjacent_directory ..`
#### Pipeline State Management
##### add_pipeline_variables
 **Description**
Allows you to add environment variables that will be available in the following steps of the run and in future runs. These variables may be overridden by another variable with the same key added to the current run.
If the following variables are set, they will be used:
  * `JFROG_CLI_BUILD_NAME`: If set, the pipeline uses this value instead of the default pipeline name for the build info collected.
  * `JFROG_CLI_BUILD_NUMBER`: If set, the pipeline uses this value instead of the default run number for the build info collected.
  * `USE_LOCAL_JFROG_CLI`: If set as `true`, the local JFrog CLI on the host or in the image (depending on `runtime` configuration) is used instead of the version packaged with JFrog Pipelines. This is not recommended and native steps may not be able to run with the local JFrog CLI version.


 **Usage**
`add_pipeline_variables <key value pair>...`
  * `key value pair` ``is a single string with a key and a value, separated by an “=”. Multiple of these strings can be supplied as input. Each value will be exported as an environment variable at the time this command is used and automatically in any steps that start after this run is complete.


Click for examples
`add_pipeline_variables imageTag=""master""`
`add_pipeline_variables imageName=""myimage"" imageTag=""master""`
##### export_pipeline_variables
 **Description**
Sources the file containing the pipeline variables. This will be done automatically, but may also be used to “reset” the environment variables in the current step.
 **Usage**
`export_pipeline_variables`
Click for examples
`export_pipeline_variables`
##### add_pipeline_files
 **Description**
Copies files given as input into the pipeline state for use in later steps in the run and future runs, if file storage is available.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`add_pipeline_files <file or directory> <name>`
  * `file or directory` is a file or directory to store in the pipeline state.
  * `name` is a name to give the stored file or directory (without spaces). This cannot be pipeline.env.


Click for examples
`add_pipeline_files cachefile.txt my_file`
`add_pipeline_files directory/subdirectory my_directory`
##### restore_pipeline_files
 **Description**
Copies files stored in the pipeline state (if file storage is available) to the specified location. No error will occur if nothing is available for <name> in the run state.
Use this utility function when you want to make use of files generated from the first step in the second step, which runs only after first step finishes.
 **Usage**
`restore_pipeline_files <name> <path>`
  * `name` is the name the file to be restored was given when added to the pipeline state.
  * `path` is a path at which to place the file or files.


Click for examples
`restore_pipeline_files my_file cachefile.txt`
`restore_pipeline_files my_directory directory/subdirectory`
#### Step Properties
##### find_resource_variable
 **Description**
Retrieves the value of the named property of a resource.
 **Usage**
`find_resource_variable <resourceName> <propertyName>`
  * `resourceName` is the name of the resource.
  * `propertyName` is the name of the resource property whose value to retrieve.


Click for examples
`find_resource_variable myGitHub commitSha`
`find_resource_variable myImage imageTag`
##### get_integration_name
 **Description**
Retrieves the name of the first integration found of the type specified. Available to extension steps to get the name of the first input integration of a particular type.
 **Usage**
`get_integration_name --type <integration type>`
  * `integration type` is the name of an Pipelines Integration type


Click for examples
`get_integration_name --type Slack`
`get_integration_name --type ""Docker Registry""`
##### get_resource_name
 **Description**
Retrieves the name of the first resource found of the type specified in `inputResources` or `outputResources`. Available to extension steps to get the name of the first input or output resource of a particular type.
 **Usage**
get_resource_name --type <resource type> \--operation <IN | OUT> \--syntax-version <semver>
  * `resource type` is the name of a Pipelines Resource type
  * `IN | OUT` selects whether the resource is named in `inputResources` or `outputResources`
  * `semver` is the semantic version number of the resource's syntax version


Click for examples
`get_resource_name --type GitRepo --operation IN`
get_resource_name --type BuildInfo --operation OUT
`get_resource_name --type GitRepo --operation IN --syntax-version 1.5.0`
##### get_resource_names
 **Description**
Retrieves an array of names of the type specified in `inputResources` or `outputResources`. Available to extension steps to get the names of input or output resource of a particular type.
 **Usage**
get_resource_names --type <resource type> \--operation <IN | OUT> \--syntax-version <semver>
  * `resource type` is the name of a Pipelines Resource type
  * `IN | OUT` selects whether the resource is named in `inputResources` or `outputResources`
  * `semver` is the semantic version number of the resource's syntax version
  * In PowerShell, a native PowerShell array is returned. In Bash, a JSON array is returned that can be handled with jq.


Click for examples
`get_resource_names --type GitRepo --operation IN`
get_resource_names --type BuildInfo --operation OUT
`get_resource_names --type GitRepo --operation IN --syntax-version 1.5.0`
##### get_affinity_group_step_names
 **Description**
Retrieves a JSON array of names of steps of the type specified in the current affinity group.
 **Usage**
`get_affinity_group_step_names [--type <step type>]  [--syntax-version <semver>] [--namespace <namespace>]find_step_configuration_value <propertyName>`
  * `type` is used to specify the type of the steps to be found. If not specified, steps of all types will be returned.
  * `syntax-version` is used to specify the type of the steps to be found. If not specified, steps of all types will be returned.
  * `namespacetype` is used with extension steps to specify the namespace of the steps to be found.


Click for examples
`get_affinity_group_step_namesfind_step_configuration_value forceXrayScan`
`get_affinity_group_step_names --type GitRepo`
`get_affinity_group_step_names --type MyExtension --syntax-version 1.5.0`
`get_affinity_group_step_names --type MyExtension --namespace myNamespace --syntax-version 1.5.0`
##### find_step_configuration_value
 **Description**
Retrieves the value of the `configuration` property for the currently executing step. If the property is a collection, the first value will be returned. Available to extension steps to get the value of a configuration.
 **Usage**
`find_step_configuration_value <propertyName>`
  * `propertyName` is the name of the step's configuration property whose value to retrieve


Click for examples
`find_step_configuration_value forceXrayScan`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
104,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a bump_semver?[/INST] **Description**
Increments the provided semver version with the given action.
 **Usage**
[YAML]
    bump_semver <semver string> <action>
    
    
[/YAML]
  * `semver string` is the semver version to be incremented
  * `action` is the type of increment to be applied


The valid actions are:
  * major: Increment the major version. The minor and patch versions are reset to 0.
  * minor: Increment the minor version. The patch version is reset to 0.
  * patch: Increment the patch version.
  * alpha: Increment or add an `alpha` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-alpha` and `v1.1.1-alpha` becomes `v1.1.1-alpha.1`. Any other pre-release tags will be removed.
  * beta: Increment or add a `beta` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-beta` and `v1.1.1-beta` becomes `v1.1.1-beta.1`. Any other pre-release tags will be removed.
  * rc: Increment or add an `rc` pre-release tag. For example, `v1.1.1` becomes `v1.1.1-rc` and `v1.1.1-rc`becomes `v1.1.1-rc.1`. Any other pre-release tags will be removed.
  * final: Remove any pre-release tags, leaving `major.minor.patch`.


Click for examples
`ubuntu:~$ bump_semver v1.0.0 major`
`v2.0.0`
`ubuntu:~$ bump_semver v1.0.0 minor`
`v1.1.0`
`ubuntu:~$ bump_semver v1.0.0 patch`
`v1.0.1`
`ubuntu:~$ bump_semver v1.0.0 rc`
`v1.0.0-rc`
`ubuntu:~$ bump_semver v1.0.0-rc rc`
`v1.0.0-rc.1`
`ubuntu:~$ bump_semver v1.0.0 alpha`
`v1.0.0-alpha`
`ubuntu:~$ bump_semver v1.0.0 beta`
`v1.0.0-beta`
`ubuntu:~$ bump_semver v1.0.0-rc.12 final`
`v1.0.0`
`ubuntu:~$ bump_semver v1. rc`
`error: Invalid semantic version given in the argument.`
`ubuntu:~$ bump_semver v1.0.0 badaction`
`error: Invalid action given in the argument.`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
105,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a replace_envs?[/INST] **Description**
Replaces variables in a file with values based on your current shell `env`. This is useful to create config files from templates, for example.
If the file contains placeholders that are not defined in the environment, they will become empty strings (“”). The original file is overwritten with the modified file.
 **Usage**
[YAML]
    replace_envs <filename1> <filename2> <filenameN>
    
                         
[/YAML]
where your files have placeholders in the format `$ENVIRONMENT_VARIABLE_NAME` or `${ENVIRONMENT_VARIABLE_NAME}`.
Click for examples
`replace_envs properties.json deploy.json run.json`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
106,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a retry_command?[/INST] **Description**
Execute any command up to three times if it returns a non-zero error code. This is useful when you need to execute a command that can be flaky as a result of network hiccups, for example.
 **Usage**
Shell
Function  
Bash
`retry_command <shell command>`  
PowerShell
`retry_command <shell command>`  
  * `shell command `is the command to be retried


Click for Bash example
`retry_command docker push mydirectory/myImage`
Click for Powershell example
`retry_command docker push mydirectory/myImage`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
107,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a get_uuid?[/INST] **Description**
Puts a uuid to stdout. Uses `/proc/sys/kernel/random/uuid` if available and falls back to `uuidgen` if not. The function calls `exit 1` if neither of these are available.
 **Usage**
Shell
Function  
Bash
`get_uuid`  
Click for Bash example
`my_uuid=$(get_uuid) `

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
108,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a save_artifact_info?[/INST] **Description**
Saves metadata about an artifact. When saved, this metadata is used to enable signed pipelines for the artifacts.
 **Usage**
Shell
Function  
Bash
`save_artifact_info <artifact type> <file path> [--build-name <build name> --build-number <build number> --project-key <project key>]`  
PowerShell
`save_artifact_info <artifact type> <file path> [-build-name <build name> -build-number <build number> -release-bundle-name <name> -release-bundle-version <version> -project-key <project key>]`  
  * `artifact type` : This is the type of artifact. Either `file`, `buildInfo`, or `releaseBundle`.
  * `file path` : This is the path to the metadata file to be saved.


Bash
PowerShell
Description  
 **\--build-name**
 **-build-name**
This is name of the build. Required when artifact type is `buildInfo`.  
 **\--build-number**
 **-build-number**
This is number of the build. Required when artifact type is `buildInfo`.  
 **\--release-bundle-name**
 **-release-bundle-name**
This is name of the Release Bundle. Required when artifact type is `releaseBundle`.  
 **\--release-bundle-version**
 **-release-bundle-version**
This is version of the Release Bundle. Required when artifact type is `releaseBundle`.  
 **\--project-key**
 **-project-key**
(optional) Defaults to the environment's `project_key`. Can be specified to save info for a different project.  
Click for Bash example
`save_artifact_info 'buildInfo' './myBuildDetailedSummary.json' --build-name myBuild --build-number 42 `
`save_artifact_info 'file' './myFileDetailedSummary.json' `
Click for PowerShell example
`save_artifact_info buildInfo myBuildDetailedSummary.json -build-name myBuild -build-number 42 `
`save_artifact_info file myFileDetailedSummary.json `
`save_artifact_info releaseBundle myFileDetailedSummary.json -release-bundle-name myBundle -release-bundle-version 1 `

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
109,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a validate_artifact?[/INST] **Description**
Validates the signature of an artifact. Requires signed pipelines to be enabled.
 **Usage**
Shell
Function  
Bash
`validate_artifact <artifact type> <file path> [--build-name <build name> --build-number <build number> --project-key <project key>]`  
PowerShell
`validate_artifact <artifact type> <file path> [-build-name <build name> -build-number <build number> -project-key <project key>]`  
  * `artifact type`: This is the type of artifact. Supports `buildinfo`.
  * `file path`: This is the path to the metadata file to be validated.


Bash
PowerShell
Description  
 **\--build-name**
 **-build-name**
This is name of the build. Required when artifact type is `buildInfo`.  
 **\--build-number**
 **-build-number**
This is number of the build. Required when artifact type is `buildInfo.`  
 **\--project-key**
 **-project-key**
(optional) Defaults to the environment's `project_key`. Can be specified to validate info for a different project.  
Click for Bash example
`validate_artifact 'buildInfo' './myBuildDetailedSummary.json' --build-name myBuild --build-number 42 `
Click for PowerShell example
`validate_artifact buildInfo myBuildDetailedSummary.json -build-name myBuild -build-number 42 `

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
110,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a configure_jfrog_cli?[/INST] **Description**
Configures the JFrog CLI (version 1 or 2 specified as `jfrogCliVersion` in the  pipeline configuration) with the provided credentials, handling the different formats for different minor versions. Artifactory integrations listed in the integrations section of the step will be automatically configured, but this may be useful for resources or if the credentials are provided to the step in another way. When using v2, a non-default Xray URL may also be specified using the environment variable JFROG_XRAY_URL.
 **Usage**
Bash
`configure_jfrog_cli --artifactory-url <url> [--xray-url <url> --user USER --apikey <key> --access-token <token> --server-name <name>]`  
PowerShell
`configure_jfrog_cli -artifactory-url <url> [-xray-url <url> -user USER -apikey <key> -access-token <token> -server-name <name>]`  
  * `artifactory-url` : Required. The Artifactory URL.
  * `xray-url` : Optional. The Xray URL. Only used with CLI v2.
  *  **user** : The user. Required when an API key is provided.
  * `apikey` : An API key. Requires --user and may not be used with --access-token.
  * `access-token` : An Access token. May not be used with --access-token or --user.
  * `server-name` : Defaults to `default`. Can be specified to configure the CLI with that name.


Click for Bash example
`configure_jfrog_cli --artifactory-url  https://my-artifactory.local/artifactory --user me --apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R --server-name myAPIKeyArtifactory`
`configure_jfrog_cli --artifactory-url  https://my-artifactory.local/artifactory --access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg --server-name myAccessTokenArtifactory`
Click for PowerShell example
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -user me -apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R -server-name myAPIKeyArtifactory`
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg -server-name myAccessTokenArtifactory`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
111,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a use_jfrog_cli?[/INST] **Description**
Sets a configured Artifactory server as the default.
 **Usage**
Bash
`use_jfrog_cli <server_name>`  
PowerShell
`use_jfrog_cli <server_name>`  
  * `server-name`: Name of the configured Artifactory server to be set as default.


Click for Bash example
`use_jfrog_cli myArtifactoryServer`
[YAML]
    configure_jfrog_cli --server-name Artifactory_Server --artifactory-url  https://my-artifactory.io/artifactory --access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVL
    use_jfrog_cli Artifactory_Server
[/YAML]
Click for PowerShell example
`use_jfrog_cli myArtifactoryServer`
[YAML]
    configure_jfrog_cli --server-name Artifactory_Server --artifactory-url  https://my-artifactory.io/artifactory --access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVL
    use_jfrog_cli Artifactory_Server
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
112,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a check_xray_available?[/INST] **Description**
With JFrog CLI v2 (version 2 specified as `jfrogCliVersion` in the  pipeline configuration), checks that Xray is available for the specified CLI configuration, or the default configuration if none is specified.
 **Usage**
Bash
`check_xray_available [--server-name <name>]`  
PowerShell
`check_xray_available [-server-name <name>]`  
  * `server-name` : Optional. Specifies a JFrog CLI configuration to check.


Click for Bash example
`check_xray_available --server-name myArtifactoryIntegration`
Click for PowerShell example
`check_xray_available --server-name myArtifactoryIntegration`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
113,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a cleanup_jfrog_cli?[/INST] **Description**
Removes configuration for the JFrog CLI (v1), handling the different formats for different minor versions. Artifactory integrations listed in the integrations section of the step will be automatically removed at the end of the step, but this may be useful to remove the credentials earlier or when using `configure_jfrog_cli`.
 **Usage**
Bash
`cleanup_jfrog_cli [--server-name <name>]`  
PowerShell
`cleanup_jfrog_cli [-server-name <name>]`  
  * `server-name` : Defaults to `default`. Can be specified to remove that configuration.


Click for Bash example
`cleanup_jfrog_cli --server-name myArtifactory`
Click for PowerShell example
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -user me -apikey AKCp8jQTdMTFdbkX14MJ8whtyV5Su5eawFhi2granysttMuyRPRA9FgxpfKzi1vFwDJXqYw9R -server-name myAPIKeyArtifactory`
`configure_jfrog_cli -artifactory-url  https://my-artifactory.local/artifactory -access-token eyJ2ZXIiOiIyIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYiLCJraWQiOiJTMmVLcmRLdElWR2JnSTJWbVM2T0NQRDFHYW1ZbVpwSXYtWS1meFN6TFJJIn0.eyJzdWIiOiJqZmZlQDAwMFwvdXNlcnNcL2FkbWluIiwic2NwIjoiYXBwbGllZC1wZXJtaXNzaW9uc1wvYWRtaW4gYXBpOioiLCJhdWQiOiJqZnJ0QCoiLCJpc3MiOiJqZmZlQDAwMCIsImV4cCI6MTYzMDY0NTM1OCwiaWF0IjoxNjMwNjQxNzU4LCJqdGkiOiI5YzcyMjMxOC01ZjBmLTQ1MTEtYTY2Yi1lMzVhMDkyYmFlMWMifQ.YOP6a2GmooC9nQcYROxGuN_1ZS7wt5YaTyqUs8ZIrUvkXxeMzLO0GI5q6KxFNIagVwetb5RP2tmLKm29HM7qp4HoK_EW5QnGQPZOY-kOPm8Z1IzXKHaAKBpPttFkCKu1unj8bByRJL8mn63nG1gMORS2Eh2fvpzej3yyuRDnD65273AC5Qs4uNxp_4DqaqhgB_Xu0b2xzHAjqLKBqWelICKBLcRbZQc4oSvphw2lj10wQHFqedOFej0akNaz8WGUo28l4GlJR7uPzQjAO0VWarKC_U3JySGSM_SAxH1z72AVHAErHCzM7tr5gDbRsfRCuAHok66PeLiDAQxfXmsYZg -server-name myAccessTokenArtifactory`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
114,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a set_trigger_payload?[/INST] **Description**
Adds one or more key=value pairs as custom step or pipeline variables to a JSON payload that can be used to call the pipelines trigger API.
Multiple key=value pairs can be given in a single command or can be split across multiple commands.
If a key is added twice, the original value will be replaced.
 **Usage**
Bash
`set_trigger_payload [stepVariables|pipelineVariables] <key value pairs>`  
PowerShell
Not currently supported in PowerShell  
  * `stepVariables`: These key value pairs will be present in the environment of the triggered step
  *  **pipelineVariables** : These key value pairs will be present in the environment of every step in the run that is triggered.


Click for Bash example
`set_trigger_payload stepVariables ""shouldScan=true"" ""notify=email""`
`set_trigger_payload pipelinesVariables ""releaseVersion=1.2.3""`
`set_trigger_payload pipelinesVariables ""collectData=true""`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
115,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a get_trigger_payload?[/INST] **Description**
Prints on stdout a payload that can be used with the pipelines trigger API.
Use `set_trigger_payload` to add additional values to the payload.
 **Usage**
Bash
`get_trigger_payload `  
PowerShell
Not currently supported in PowerShell  
Click for Bash example
curl -XPOST -H ""authorization: Bearer <token>"" -H ""content-type: application/json"" <endpoint>/pipelines/api/v1/pipelineSteps/:id/trigger -d ""$(get_trigger_payload)""

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
116,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a end_step?[/INST] **Description**
Stops execution of the onStart or onExecute section and immediately continues to the onSuccess, onFailure, or onComplete section setting the specified status. If ""success"" is provided, the onSuccess and onComplete sections will be executed. For ""failure,"" the onFailure and onComplete sections will be executed. And for ""skipped,"" only the onComplete section. The end_step utility function may only be called in the onStart and onExecute sections and the only statuses supported are success, failure, and skipped.
 **Usage**
Bash
`end_step [success|failure|skipped]`  
PowerShell
`end_step [success|failure|skipped]`  
Click for Bash example
`end_step success`
Click for PowerShell example
`end_step success`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
117,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a update_run_description?[/INST] **Description**
Provide dynamic description for a run, which will be shown in the UI. This is useful for providing more context about a run. Every step in a pipeline can include a run description.
 **Usage**
Bash
`update_run_description ""description""`  
PowerShell
`update_run_description ""description""`  
Click for Bash example
`update_run_description ""$msg""`
Click for PowerShell example
`update_run_description ""$msg""`
Full YAML Example
[YAML]
    name: UpdateRunDescription
    steps:
            - name: update_l
              type: Bash
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
            - name: update_2
              type: Bash
              configuration:
                    inputSteps:
                            - name: update_l
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
118,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a set_run_name?[/INST] **Description**
Provide a unique, dynamic name for a run, which will be shown in the UI. This is useful for providing more context about a run. Unlike run description, run name is unique for a run.
In the UI, run name appears in the Run View:  
Run name appears also appears in Pipeline of Pipelines view and Active Board view:  
 **Usage**
Bash
`set_run_name ""1.0.1""`  
PowerShell
`set_run_name ""1.0.1""`  
Click for Bash example
`set_run_name ""1.0.1""`
Click for PowerShell example
`set_run_name ""1.0.1""`
Full YAML Example
[YAML]
    name: UpdateRunDescription
    steps:
            - name: update_l
              type: Bash
              execution:
                    onStart:
                            - set_run_name ""1.0.1""
            - name: update_2
              type: Bash
              configuration:
                    inputSteps:
                            - name: update_l
              execution:
                    onStart:
                            - msg=""Run description updated from step $step_id""
                            - update_run_description ""$msg""
[/YAML]

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
119,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a source-control?[/INST] 

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
120,"<s>[INST]<<SYS>> You are a helpful, respectful and honest assistant. Helps user to write jFrog pipline and answers about jFrog pipline process <</SYS>> Write a jFrog pipeline to do a compare_git?[/INST] Lists the files/directories containing changes within a commit range. This function is useful when building a monorepo (monolithic repository) to determine which services have changes.
Shell
Function  
Bash
`compare_git [--path | --resource] [options]`  
PowerShell
`compare_git [-path | -resource] [options] `  
  * `path` is the file system path to a git repository.
  * `resource` is the name of the gitRepo resource.
  * `commit-range` option specifies the range of commits to look for changes (Example: `HEAD~1..HEAD`).
  * `directories-only` option lists only the directories containing changes.
  * `depth` option returns file/folder at certain depth. Root directory has depth value 1.


Click for Bash examples
`ubuntu:~$ compare_git --path ./microservices --commit-range HEAD~2..HEAD api/common/utilities/leftpad.js api/main.js notifier/main.js ubuntu:~$ compare_git --path ./microservices --commit-range HEAD~2..HEAD --directories-only api/common/utilities/ api/ notifier/ ubuntu:~$ compare_git --path ./microservices --commit-range HEAD~2..HEAD --directories-only --depth 1 api notifier`
`ubuntu:~$ compare_git --path ./microserv --commit-range HEAD~2..HEAD --directories-only --depth 1`
`git repository not found at path: ./microserv`
Click for Powershell examples
`PS C:\Users\windowsuser> compare_git -path microservices -commit-range HEAD~2..HEAD api/common/utilities/leftpad.js api/main.js notifier/main.js PS C:\Users\windowsuser> compare_git -path microservices -commit-range HEAD~2..HEAD -directories-only api/common/utilities/ api/ notifier/ PS C:\Users\windowsuser> compare_git -path microservices -commit-range HEAD~2..HEAD -directories-only -depth 1 api notifier`
`PS C:\Users\windowsuser> compare_git -path microserv -commit-range HEAD~2..HEAD -directories-only -depth 1`
`git repository not found at path: ./microserv`

 Document url for reference - https://jfrog.com/help/r/jfrog-pipelines-documentation/pipelines-steps</s>"
